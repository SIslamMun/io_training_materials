# runtime-deployment

> Jarvis-cd is a unified platform for deploying various applications

## Repository Info

- **Stars:** 11
- **Forks:** 3
- **Language:** Python
- **License:** Other
- **Topics:** ai4hpc
- **Source:** `https://github.com/iowarp/runtime-deployment`
- **Branch:** `main`
- **Commit:** `d4189d381a1f`
- **Last Commit:** 2026-01-05 13:31:30 -0600
- **Commits:** 1
- **Extracted:** 2026-01-08T12:00:56.153621


## Directory Structure

```
runtime-deployment/
├── .claude/
│   └── agents/
│       ├── code-documenter.md
│       ├── docker-python-test-expert.md
│       ├── git-expert.md
│       ├── jarvis-pipeline-builder.md
│       └── python-code-updater.md
├── .github/
│   └── workflows/
│       ├── build-containers.yaml
│       └── main.yml
├── .vscode/
│   └── launch.json
├── ai-prompts/
│   ├── docker/
│   │   └── phase1.md
│   ├── new-pipeline.md
│   ├── phase-14-update.md
│   ├── phase1-argparse.md
│   ├── phase10-pipeline-index.md
│   ├── phase11-template.md
│   ├── phase12-pipeline.md
│   ├── phase13-jarvis-mod
│   ├── phase14-jarvis-ppl-pkg.md
│   ├── phase15-containers.md
│   ├── phase16-installer.md
│   ├── phase2-hostfile.md
│   ├── phase2-logging.md
│   ├── phase3-launch.md
│   ├── phase4-resource-graph.md
│   ├── phase5-jarvis-repos.md
│   ├── phase6-jarvis-env.md
│   ├── phase8-paths.md
│   └── phase9-pipeline-scripts.md
├── bin/
│   ├── example.slurm
│   ├── jarvis
│   ├── jarvis-imports
│   └── jarvis_resource_graph
├── builtin/
│   ├── builtin/
│   │   ├── adios2_gray_scott/
│   │   │   ├── config/
│   │   │   ├── INSTALL.md
│   │   │   ├── pkg.py
│   │   │   ├── README.md
│   │   │   └── USE.md
│   │   ├── arldm/
│   │   │   ├── example_config/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── asan/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── builtin_pkg/
│   │   │   └── package.py
│   │   ├── cm1/
│   │   │   ├── config/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── cosmic_tagger/
│   │   │   ├── config/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── darshan/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── data_stagein/
│   │   │   └── pkg.py
│   │   ├── ddmd/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── dlio_benchmark/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── echo/
│   │   │   └── pkg.py
│   │   ├── example_app/
│   │   │   └── pkg.py
│   │   ├── example_interceptor/
│   │   │   └── pkg.py
│   │   ├── filebench/
│   │   │   ├── config/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── fio/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── gadget2/
│   │   │   ├── config/
│   │   │   ├── paramfiles/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── gadget2_df/
│   │   │   ├── paramfiles/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── gray_scott/
│   │   │   ├── config/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── InCompact3D/
│   │   │   ├── config/
│   │   │   ├── spack/
│   │   │   ├── incompact3D.yaml
│   │   │   ├── INSTALL.md
│   │   │   ├── pkg.py
│   │   │   ├── README.md
│   │   │   └── USE.md
│   │   ├── InCompact3D_post/
│   │   │   ├── config/
│   │   │   ├── incompact3d_post.yaml
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── ior/
│   │   │   ├── __init__.py
│   │   │   ├── container.py
│   │   │   ├── default.py
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── lammps/
│   │   │   ├── config/
│   │   │   ├── INSTALL.md
│   │   │   ├── pkg.py
│   │   │   ├── README.md
│   │   │   └── USE.md
│   │   ├── mkfs/
│   │   │   └── pkg.py
│   │   ├── my_shell/
│   │   │   ├── pkg.py
│   │   │   └── test_scirpt.sh
│   │   ├── nyx_lya/
│   │   │   ├── config/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── orangefs/
│   │   │   ├── __init__.py
│   │   │   ├── ares.py
│   │   │   ├── custom_kern.py
│   │   │   ├── Dockerfile
│   │   │   ├── fuse.py
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── paraview/
│   │   │   ├── INSTALL.MD
│   │   │   ├── pkg.py
│   │   │   ├── README.md
│   │   │   ├── server_setup.png
│   │   │   └── USE.MD
│   │   ├── post_wrf/
│   │   │   ├── config/
│   │   │   └── pkg.py
│   │   ├── pyflextrkr/
│   │   │   ├── example_config/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── pymonitor/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── redis/
│   │   │   ├── config/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── redis-benchmark/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── spark_cluster/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   ├── test_pkg/
│   │   │   └── package.py
│   │   ├── wrf/
│   │   │   ├── config/
│   │   │   ├── INSTALL.md
│   │   │   ├── pkg.py
│   │   │   ├── README.md
│   │   │   ├── USE.md
│   │   │   └── wrf_workflow.png
│   │   ├── ycsbc/
│   │   │   ├── pkg.py
│   │   │   └── README.md
│   │   └── __init__.py
│   ├── config/
│   │   ├── ares.yaml
│   │   ├── deception.yaml
│   │   └── polaris.yaml
│   ├── pipelines/
│   │   ├── examples/
│   │   │   ├── ior_container_test.yaml
│   │   │   └── ior_podman_test.yaml
│   │   ├── unit_tests/
│   │   │   └── test_interceptor.yaml
│   │   ├── simple_test.yaml
│   │   ├── test_interceptor.yaml
│   │   └── test_simple.yaml
│   ├── resource_graph/
│   │   ├── ares.yaml
│   │   ├── deception.yaml
│   │   ├── delta.yaml
... (truncated)
```

## File Statistics

- **Files Processed:** 247
- **Files Skipped:** 0


## README

# Jarvis-CD

Jarvis-CD is a unified platform for deploying various applications, including storage systems and benchmarks. Many applications have complex configuration spaces and are difficult to deploy across different machines.

## Installation

```bash
cd /path/to/jarvis-cd
python3 -m pip install -r requirements.txt
python3 -m pip install -e .
```

## Configuration (Build your Jarvis setup)

```bash
jarvis init [CONFIG_DIR] [PRIVATE_DIR] [SHARED_DIR]
```
- CONFIG_DIR: Stores Jarvis metadata for pkgs/pipelines (any path you can access)
- PRIVATE_DIR: Per-machine local data (e.g., OrangeFS state)
- SHARED_DIR: Shared across machines with the same view of data

On a personal machine, these can point to the same directory.

## Hostfile (set target nodes)

The hostfile lists nodes for multi-node pipelines (MPI-style format):

Example:
```text
host-01
host-[02-05]
```

Set the active hostfile:
```bash
jarvis hostfile set /path/to/hostfile
```

After changing the hostfile, update the active pipeline:
```bash
jarvis ppl update
```

## Resource Graph (discover storage)

```bash
jarvis rg build
```

## License

BSD-3-Clause License - see [LICENSE](LICENSE) file for details.

**Copyright (c) 2024, Gnosis Research Center, Illinois Institute of Technology**


## Source Files

### `builtin/builtin/InCompact3D/README.md`

```markdown

# The Xcompact3D(Incompact3D) 

## what is the Incompact3D application?
Xcompact3d is a Fortran-based framework of high-order finite-difference flow solvers dedicated to the study of turbulent flows. Dedicated to Direct and Large Eddy Simulations (DNS/LES) for which the largest turbulent scales are simulated, it can combine the versatility of industrial codes with the accuracy of spectral codes. Its user-friendliness, simplicity, versatility, accuracy, scalability, portability and efficiency makes it an attractive tool for the Computational Fluid Dynamics community.

XCompact3d is currently able to solve the incompressible and low-Mach number variable density Navier-Stokes equations using sixth-order compact finite-difference schemes with a spectral-like accuracy on a monobloc Cartesian mesh.  It was initially designed in France in the mid-90's for serial processors and later converted to HPC systems. It can now be used efficiently on hundreds of thousands CPU cores to investigate turbulence and heat transfer problems thanks to the open-source library 2DECOMP&FFT (a Fortran-based 2D pencil decomposition framework to support building large-scale parallel applications on distributed memory systems using MPI; the library has a Fast Fourier Transform module).
When dealing with incompressible flows, the fractional step method used to advance the simulation in time requires to solve a Poisson equation. This equation is fully solved in spectral space via the use of relevant 3D Fast Fourier transforms (FFTs), allowing the use of any kind of boundary conditions for the velocity field. Using the concept of the modified wavenumber (to allow for operations in the spectral space to have the same accuracy as if they were performed in the physical space), the divergence free condition is ensured up to machine accuracy. The pressure field is staggered from the velocity field by half a mesh to avoid spurious oscillations created by the implicit finite-difference schemes. The modelling of a fixed or moving solid body inside the computational domain is performed with a customised Immersed Boundary Method. It is based on a direct forcing term in the Navier-Stokes equations to ensure a no-slip boundary condition at the wall of the solid body while imposing non-zero velocities inside the solid body to avoid discontinuities on the velocity field. This customised IBM, fully compatible with the 2D domain decomposition and with a possible mesh refinement at the wall, is based on a 1D expansion of the velocity field from fluid regions into solid regions using Lagrange polynomials or spline reconstructions. In order to reach high velocities in a context of LES, it is possible to customise the coefficients of the second derivative schemes (used for the viscous term) to add extra numerical dissipation in the simulation as a substitute of the missing dissipation from the small turbulent scales that are not resolved. 

Xcompact3d is currently being used by many research groups worldwide to study gravity currents, wall-bounded turbulence, wake and jet flows, wind farms and active flow control solutions to mitigate turbulence.  ​

## what this model generate:

### Numerical flow solutions
Xcompact3D produces high-fidelity numerical solutions to the Navier–Stokes equations, including: Velocity fields (u, v, w) in 3D. Pressure fields (p). Scalar fields (e.g., temperature, concentration) if configured. Derived quantities such as vorticity, dissipation rates, or turbulent stresses.

### 3D snapshots and flow visualizations
The solver can output 3D snapshots of flow variables at user-defined intervals.
These snapshots can be used for: Flow visualization (e.g., isosurfaces, slices, contours). Statistical analysis (mean fields, fluctuations). Detailed inspection of turbulent structures.

## Benchmark data and case studies
As demonstrated in the paper, Xcompact3D generates data for well-known CFD test cases, including:

1. Taylor–Green vortex: Transition from laminar to turbulent states.

2. Turbulent channel flow: Wall-bounded turbulence with comparisons to reference data.

3. Flow past a cylinder: Including wake dynamics and vortex shedding.

4. Lock-exchange flow: Variable-density gravity currents.

5. Fractal-generated turbulence: Turbulence control and mixing studies.

6. Wind farm simulations: Detailed turbine wake interactions.

##  Key Input Parameters (Xcompact3d)

| **Parameter**     | **Description**                                            | **Example / Options**                       |
|--------------------|------------------------------------------------------------|--------------------------------------------|
| **p_row, p_col**  | Domain decomposition for parallel computation             | Auto-tune (0), or set to match core layout |
| **nx, ny, nz**   | Number of mesh points per direction                         | E.g., 1024, 1025 (non-periodic)           |
| **xlx, yly, zlz** | Physical domain size (normalized or dimensional)          | E.g., 20D (cylinder case)                 |
| **itype**        | Flow configuration                                        | 0–11 (custom, jet, channel, etc.)         |
| **istret**      | Mesh refinement in Y direction                               | 0: none, 1–3: various center/bottom       |
| **beta**         | Refinement strength parameter                               | Positive values (trial & error tuning)     |
| **iin**           | Initial condition perturbations                            | 0: none, 1: random, 2: fixed seed        |
| **inflow_noise**| Noise amplitude at inflow                                   | 0–0.1 (as % of ref. velocity)           |
| **re**            | Reynolds number                                           | E.g., Re = 1/ν                           |
| **dt**            | Time step size                                            | User-defined, depends on resolution        |
| **ifirst, ilast**| Start and end iteration numbers                            | E.g., 0, 50000                            |
| **numscalar**   | Number of scalar fields                                     | Integer ≥ 0                               |
| **iscalar**     | Enable scalar fields                                        | Auto-set if numscalar > 0                |
| **iibm**        | Immersed Boundary Method                                    | 0: off, 1–3: various methods             |
| **ilmn**       | Low Mach number solver                                      | 0: off, 1: on                            |
| **ilesmod**   | LES model selection                                          | 0: off, 1–4: various models             |
| **nclx1...nclzn** | Boundary conditions per direction                         | 0: periodic, 1: free-slip, 2: Dirichlet |
| **ivisu**       | Enable 3D snapshots output                                 | 1: on                                    |
| **ipost**      | Enable online postprocessing                                 | 1: on                                    |
| **gravx, gravy, gravz** | Gravity vector components                          | E.g., (0, -1, 0)                        |
| **ifilter, C_filter** | Solution filtering controls                         | E.g., 1, 0.5                             |
| **itimescheme** | Time integration scheme                                    | E.g., 3: Adams-Bashforth 3, 5: RK3      |
| **iimplicit** | Y-diffusive term scheme                                     | 0: explicit, 1–2: implicit options      |
| **nu0nu, cnu** | Hyperviscosity/viscosity ratios                            | Default: 4, 0.44                        |
| **ipinter**     | Interpolation scheme                                      | 1–3 (Lele or optimized variants)       |
| **irestart**    | Restart from file                                          | 1: enabled                              |
| **icheckpoint** | Checkpoint file frequency                                 | E.g., every 5000 steps                 |
| **ioutput**    | Output snapshot frequency                                 | E.g., every 500 steps                  |
| **nvisu**      | Snapshot size control                                      | Default: 1                             |
| **initstat**  | Start step for statistics collection                        | E.g., 10000                            |
| **nstat**      | Statistics collection spacing                              | Default: 1                             |
| **sc, ri, uset, cp** | Scalar-related parameters                             | Schmidt, Richardson, settling, init conc.|
| **nclxS1...nclzSn** | Scalar BCs                                            | 0: periodic, 1: no-flux, 2: Dirichlet |
| **scalar_lbound, scalar_ubound** | Scalar bounds                           | E.g., 0, 1                             |
| **sc_even, sc_skew** | Scalar symmetry flags                               | True/False                            |
| **alpha_sc, beta_sc, g_sc** | Scalar wall BC params                         | For implicit solvers                   |
| **Tref**       | Reference temperature for scalar                          | Problem-specific                      |
| **iibmS**    | IBM treatment for scalars                                   | 0: off, 1–3: various modes          |

---
```

### `builtin/builtin/InCompact3D_post/README.md`

```markdown

This is the post-processing to read file with adios2 bp5 from incompact3D examples.

### how to install
Installing the coeus-adapter will also generate an executable file named inCompact3D_analysis

### Jarvis(ADIOS2)

step 1: Build environment
```
spack load incompact3D@coeus
spack load openmpi
export PATH=~/coeus-adapter/build/bin/:$PATH
```
step 3: add jarvis repo
```
jarvis repo add coeus_adapter/test/jarvis/jarvis_coeus
```
step 4: Set up the jarvis packages
```
jarvis ppl create incompact3D_post
jarvis ppl append InCompact3D_post file_location=/path/to/data.bp5 nprocs=16 ppn=16 engine=bp5
jarvis ppl env build
```

step 5: Run with jarvis
```
jarvis ppl run
```

### Jarvis (Hermes)
This is the procedure for running the application with Hermes as the I/O engine.<br>
step 1: Place the run scripts in the example folder and copy an existing script as input.i3d.
The following example demonstrates this setup for the Pipe-Flow benchmark.
```
cd Incompact3d/examples/Pipe-Flow
cp input_DNS_Re1000_LR.i3d input.i3d
```

step 2: Build environment
```
spack load hermes@master
spack load incompact3D@coeus
spack load openmpi
export PATH=/incompact3D/bin:$PATH
export PATH=~/coeus-adapter/build/bin:$PATH
export LD_LIBRARY_PATH=~/coeus-adapter/build/bin:LD_LIBRARY_PATH
```
step 3: add jarvis repo
```
jarvis repo add coeus_adapter/test/jarvis/jarvis_coeus
```
step 4: Set up the jarvis packages
```
jarvis ppl create incompact3d
jarvis ppl append hermes_run provider=sockets
jarvis ppl append Incompact3d example_location=/path/to/incompact3D-coeus engine=hermes nprocs=16 ppn=16 benchmarks=Pipe-Flow
jarvis ppl append InCompact3D_post file_location=/path/to/data.bp5 nprocs=16 ppn=16 engine=hermes
jarvis ppl env build
```

step 5: Run with jarvis
```
jarvis ppl run
```
```

### `builtin/builtin/adios2_gray_scott/README.md`

```markdown
# Gray-Scott Model
## what is the Gray-Scott application?
The Gray-Scott system is a **reaction–diffusion system**, meaning it models a process involving both chemical reactions and diffusion across space. In the case of the Gray-Scott model that reaction is a chemical reaction between two substances 
 u and v, both of which diffuse over time. During the reaction gets used up, while  is produced. The densities of the substances 
 and are represented in the simulation.

## what this model generate:
The Gray-Scott system models the chemical reaction

U + 2V ->  3V

This reaction consumes U and produces V. Therefore, the amount of both substances needs to be controlled to maintain the reaction. This is done by adding U at the "feed rate" F and removing V at the "kill rate" k. The removal of V can also be described by another chemical reaction:

V -> P

For this reaction P is an inert product, meaning it doesn't react and therefore does not contribute to our observations. In this case the Parameter k controls the rate of the second reaction.


## what is the input of gray-scott

The Gray-Scott system models two chemical species:

U: the feed chemical, continuously added to the system.

V: the activator chemical, which is produced during the reaction and also removed.




##  Key Input Parameters

| Parameter      | Description                                    | Typical Range or Example Values         |
|----------------|------------------------------------------------|----------------------------------------|
| **F**          | Feed rate of U (controls how quickly U is replenished in the system) | 0.01 – 0.08         |
| **k**          | Kill rate of V (controls how quickly V is removed from the system) | 0.03 – 0.07         |
| **Du**         | Diffusion coefficient for U                   | Typically ~2 × Dv        |
| **Dv**         | Diffusion coefficient for V                   | Lower than Du (e.g., half) |
| **Grid size(L)**  | Spatial resolution of the simulation grid     | 256×256, 512×512       |
| **Time step(Steps)**  | Time integration step size                   | 0.01 – 1.0            |
| **Initial condition** | Initial distribution of U and V         | U = 1, V = 0 with small localized perturbations (e.g., center patch with V = 1) |
| **Simulation speed** | Controls visual update or iteration speed | 1×, 2×, etc.          |
| **Color scheme** | Display mode for concentration visualization | Black & white or RGB sliders |
| **Noise(noise)** | add noise for the simulation | 0.01~0.1 |
| **I/O frequency(plotgap)** | the frequecey of I/O between simulation steps  | 1~10 |
```

### `builtin/builtin/arldm/README.md`

```markdown
The AR-LDM (Auto-Regressive Latent Diffusion Models) is a latent diffusion model auto-regressively conditioned on history captions and generated images.
See the [official repo](https://github.com/xichenpan/ARLDM) for more detail.

# Table of Content
0. [Dependencies](#0-dependencies)
1. [Installation](#1-installation)
2. [Running ARLDM](#2-running-arldm)
3. [ARLDM with Slurm](#3-arldm-with-slurm)
4. [ARLDM + Hermes](#4-arldm--hermes)
5. [ARLDM on Node Local Storage](#5-arldm-on-node-local-storage)
6. [ARLDM + Hermes on Node Local Storage](#6-arldm--hermes-on-node-local-storage)
7. ARLDM + Hermes with Multinodes Slurm (Not supported)



# 0. Dependencies

## 0.1. conda
- Prepare Conda
Get the miniconda3 installation script and run it
```
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh.sh
```

## 0.2. jarvis-cd & scspkg
Follow steps here: https://github.com/grc-iit/jarvis-cd


## 0.3. spack
Spack is used to install HDF5, MPICH, and Hermes.
Install spack steps are here: https://spack.readthedocs.io/en/latest/getting_started.html#installation

## 0.4. HDF5 (1.14.0+)
HDF5 is require, (1.14.0+) is required by Hermes and h5py==3.8.0.

Use spack to install hdf5
```bash
spack install hdf5@1.14.0+hl~mpi
```

## 0.5. MPI
Either OpenMPI or MPICH works, this is required by Hermes and mpi4py


Use spack to install mpich
```bash
spack install mpich@3.4.3
```

## 0.6. Installation Tools
You need `wget` and `gdown` to download the datasets online:
- wget
You can install wget either with `apt-get` or `spack`
```bash
sudo apt-get install wget
# or
spack install wget
spack load wget
# check if wget is usable
which wget
```
- gdown
```shell
python3 -m pip install gdown==4.5.1 # or 4.6.0
pip show gdown
```


# 1. Installation

## 1.1 create arldm scs package
```bash
scspkg create arldm
cd `scspkg pkg src arldm`
git clone https://github.com/candiceT233/ARLDM
cd ARLDM
git switch ares # Use the ares branch
export ARLDM_PATH=`scspkg pkg src arldm`/ARLDM
scspkg env set arldm ARLDM_PATH=$ARLDM_PATH HDF5_USE_FILE_LOCKING=FALSE
```


## 1.2 Prepare conda environment and python packages:
```bash
cd `scspkg pkg src arldm`/ARLDM

YOUR_HDF5_DIR="`which h5cc |sed 's/.\{9\}$//'`"
conda env create -f arldm_conda.yaml -n arldm
conda activate arldm
pip uninstall h5py;
HDF5_MPI="OFF" HDF5_DIR=${YOUR_HDF5_DIR} pip install --no-cache-dir --no-binary=h5py h5py==3.8.0
conda deactivate
```



# 2. Running ARLDM

## 2.1.0 Internet Access
Internet access is required when running this program for the first time, you will encounter below error:
```log
  File "/home/$USER/miniconda3/envs/arldm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1761, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'runwayml/stable-diffusion-v1-5'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'runwayml/stable-diffusion-v1-5' is the correct path to a directory containing all relevant files for a CLIPTokenizer tokenizer.
```

## 2.1. Setup Environment
Currently setup input path in a shared storage, below is a example on Ares cluster.
Setup experiment input and ouput paths:
```bash
EXPERIMENT_PATH=~/experiments/arldm_run
export EXPERIMENT_INPUT_PATH=$EXPERIMENT_PATH/input_data

scspkg env set arldm EXPERIMENT_INPUT_PATH=$EXPERIMENT_INPUT_PATH

mkdir -p $EXPERIMENT_INPUT_PATH $EXPERIMENT_INPUT_PATH/zippack
```

## 2.2. Download Pretrain Model
The pretrain model is ~ 3.63 GB (~ 10 mins on Ares)
```bash
cd $EXPERIMENT_PATH
conda activate arldm
wget https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth
export PRETRAIN_MODEL_PATH=`realpath model_large.pth`
scspkg env set arldm PRETRAIN_MODEL_PATH=$PRETRAIN_MODEL_PATH
conda deactivate
```

## 2.3. Download Input Data
You should prepare at least one dataset to run the script. There are 4 available datasets for download `vistsis`, `vistdii`, `pororo`, and `flintstones`.

### 2.3.1 VISTSIS and VISTDII

1. Download VISTSIS, original VIST-SIS (~23MB) url links [here](https://visionandlanguage.net/VIST/json_files/story-in-sequence/SIS-with-labels.tar.gz)
```shell
cd $EXPERIMENT_INPUT_PATH
wget https://visionandlanguage.net/VIST/json_files/story-in-sequence/SIS-with-labels.tar.gz
tar -vxf SIS-with-labels.tar.gz
mv sis vistsis # ~ 172M

# save downloaded package to different directory
mv SIS-with-labels.tar.gz $EXPERIMENT_INPUT_PATH/zippack
```

2. Download VISTSIS, original VIST-DII (~18MB) url links [here](https://visionandlanguage.net/VIST/json_files/description-in-isolation/DII-with-labels.tar.gz)
```shell
cd $EXPERIMENT_INPUT_PATH
wget https://visionandlanguage.net/VIST/json_files/description-in-isolation/DII-with-labels.tar.gz
tar -vxf DII-with-labels.tar.gz
mv dii vistdii # ~ 125M

# save downloaded package to different directory
mv DII-with-labels.tar.gz $EXPERIMENT_INPUT_PATH/zippack
```

3. Download the VIST images by running below command (this will take over 2 hours on Ares)
```shell
cd $ARLDM_PATH
conda activate arldm
python data_script/vist_img_download.py --json_dir $EXPERIMENT_INPUT_PATH/vistdii --img_dir $EXPERIMENT_INPUT_PATH/visit_img --num_process 12
```

### 2.3.3 flintstones 
* Original FlintstonesSV dataset [here](https://drive.google.com/file/d/1kG4esNwabJQPWqadSDaugrlF4dRaV33_/view?usp=sharing).
```shell
cd $EXPERIMENT_INPUT_PATH
gdown "1kG4esNwabJQPWqadSDaugrlF4dRaV33_&confirm=t" # ~10 mins on Ares
unzip flintstones_data.zip # 4.9G, ~2 mins on Ares
mv flintstones_data flintstones # 6.6G
mv flintstones_data.zip $EXPERIMENT_INPUT_PATH/zippack
```
<!-- gdown https://drive.google.com/u/0/uc?id=1kG4esNwabJQPWqadSDaugrlF4dRaV33_&export=download -->


### 2.3.2 pororo 
* Original PororoSV dataset [here](https://drive.google.com/file/d/11Io1_BufAayJ1BpdxxV2uJUvCcirbrNc/view?usp=sharing).
```shell
cd $EXPERIMENT_INPUT_PATH
gdown "11Io1_BufAayJ1BpdxxV2uJUvCcirbrNc&confirm=t" # ~30 mins on Ares
unzip pororo.zip # 15GB
mv pororo_png pororo # 17GB
mv pororo.zip $EXPERIMENT_INPUT_PATH/zippack
```
<!-- gdown https://drive.google.com/u/0/uc?id=11Io1_BufAayJ1BpdxxV2uJUvCcirbrNc&export=download -->


## 2.3. Create a Resource Graph

If you haven't already, create a resource graph. This only needs to be done
once throughout the lifetime of Jarvis. No need to repeat if you have already
done this for a different pipeline.

For details building resource graph, please refer to https://github.com/grc-iit/jarvis-cd/wiki/2.-Resource-Graph.

If you are running distributed tests, set path to the hostfile you are  using.
```bash
jarvis hostfile set /path/to/hostfile
```

Next, collect the resources from each of those pkgs. Walkthrough will give
a command line tutorial on how to build the hostfile.
```bash
jarvis resource-graph build +walkthrough
```

## 2.4 Create a Pipeline

The Jarvis pipeline will store all configuration data needed by ARLDM.

```bash
jarvis pipeline create arldm_test
```

## 2.5. Save Environment
Create the environment variables needed by ARLDM.
```bash
spack load hdf5@1.14.0+hl~mpi
module load arldm
```
<!-- conda activate arldm -->

Store the current environment in the pipeline.
```bash
jarvis env build arldm \
+EXPERIMENT_PATH +EXPERIMENT_INPUT_PATH +EXPERIMENT_OUTPUT_PATH \
+ARLDM_PATH +PRETRAIN_MODEL_PATH
jarvis pipeline env copy arldm
```

## 2.6. Add pkgs to the Pipeline
Create a Jarvis pipeline with ARLDM.
```bash
jarvis pipeline append arldm runscript=vistsis
```

## 2.7. Run Experiment

Run the experiment, output are generated in `$EXPERIMENT_INPUT_PATH/output_data`.
```bash
jarvis pipeline run
```

## 2.8. Clean Data

Clean data produced by ARLDM
```bash
jarvis pipeline clean
```



# 3. ARLDM With Slurm

## 3.1 Local Cluster
`ppn` must equal or greater than `num_workers`,which is default to 1.
```bash
jarvis pipeline sbatch job_name=arldm_test nnodes=1 ppn=2 output_file=./arldm_test.out error_file=./arldm_test.err
```

## 3.2 Multi Nodes Cluster (TODO)
ARLDM with jarvis-cd is currently only set to run with single node and using CPU.
    - Multiple CPU worker not tested
    - GPU not tested



# 4. ARLDM + Hermes

## 4.0. Dependencies
### 4.0.1 HDF5
Hermes must compile with HDF5, makesure [download HDF5-1.14.0 with spack](#04-hdf5-1140).

### 4.0.2 Install Hermes dependencies with spack
```bash
spack load hdf5@1.14.0+hl~mpi mpich@3.4.3
spack install hermes_shm ^hdf5@1.14.0+hl~mpi ^mpich@3.4.3
```

### 4.0.3 Install Hermes with scspkg
1. Option 1: build with POSIX adaptor
```bash
spack load hermes_shm
scspkg create hermes
cd `scspkg pkg src hermes`
git clone https://github.com/HDFGroup/hermes
cd hermes
mkdir build
cd build
cmake ../ -DCMAKE_BUILD_TYPE="Release" \
    -DCMAKE_INSTALL_PREFIX=`scspkg pkg root hermes` \
    -DHERMES_MPICH="ON" \
    -DHERMES_ENABLE_POSIX_ADAPTER="ON" \
```

2. Option 2: build with VFD adaptor (This is not working yet)
```bash
spack load hermes_shm
scspkg create hermes
cd `scspkg pkg src hermes`
git clone https://github.com/HDFGroup/hermes
cd hermes
mkdir build
cd build
cmake ../ -DCMAKE_BUILD_TYPE="Release" \
    -DCMAKE_INSTALL_PREFIX=`scspkg pkg root hermes` \
    -DHERMES_ENABLE_MPIIO_ADAPTER="ON" \
    -DHERMES_MPICH="ON" \
    -DHERMES_ENABLE_POSIX_ADAPTER="ON" \
    -DHERMES_ENABLE_STDIO_ADAPTER="ON" \
    -DHERMES_ENABLE_VFD="ON" \
```

## 4.1. Setup Environment

Create the environment variables needed by Hermes + ARLDM
```bash
RUN_SCRIPT=vistsis # can change to other datasets
spack load hermes_shm
module load hermes arldm
```

## 4.2. Create a Resource Graph

Same as [above](#2-create-a-resource-graph).

## 4.3. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by Hermes
and ARLDM.

```bash
jarvis pipeline create hermes_arldm_test
```

## 4.4. Save Environment

Store the current environment in the pipeline.
```bash
jarvis pipeline env build +PRETRAIN_MODEL_PATH +EXPERIMENT_INPUT_PATH +ARLDM_PATH
```

## 4.5. Add pkgs to the Pipeline

Create a Jarvis pipeline with Hermes, using the Hermes POSIX interceptor.
```bash
jarvis pipeline append hermes_run --sleep=10 include=$EXPERIMENT_INPUT_PATH/${RUN_SCRIPT}_out.h5
jarvis pipeline append hermes_api +posix
jarvis pipeline append arldm runscript=vistsis with_hermes=true
```

## 4.6. Run the Experiment

Run the experiment, output are generated in `$EXPERIMENT_INPUT_PATH/output_data`.
```bash
jarvis pipeline run
```

## 4.7. Clean Data

To clean data produced by Hermes + ARLDM:
```bash
jarvis pipeline clean
```



# 5. ARLDM on Node Local Storage
For cluster that has node local storage, you can stagein data from shared storage, then run arldm.

## 5.1 Setup Environment
Currently setup DEFAULT input path in a shared storage, below is a example on Ares cluster using node local nvme.
```bash
RUN_SCRIPT=vistsis # can change to other datasets
EXPERIMENT_PATH=~/experiments/arldm_run # NFS
SHARED_INPUT_PATH=$EXPERIMENT_PATH/input_data # NFS
cd $EXPERIMENT_PATH; export PRETRAIN_MODEL_PATH=`realpath model_large.pth`

LOCAL_EXPERIMENT_PATH=/mnt/nvme/$USER/arldm_run
LOCAL_INPUT_PATH=$LOCAL_EXPERIMENT_PATH/input_data
```

## 5.2. Download Pretrain Model and Input Data
Same as above [download pretrain](#22-download-pretrain-model) and [download input](#23-download-input-data).

## 5.3. Create a Resource Graph
Same as [above](#23-create-a-resource-graph)

## 5.4. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by ARLDM.

```bash
jarvis pipeline create arldm_local
```

## 5.5. Save Environment
Create the environment variables needed by ARLDM.
```bash
spack load hdf5@1.14.0+hl~mpi mpich@3.4.3
module load arldm
```


Store the current environment in the pipeline.
```bash
jarvis pipeline env build +PRETRAIN_MODEL_PATH +EXPERIMENT_INPUT_PATH +ARLDM_PATH
```


## 5.6. Add pkgs to the Pipeline
Add data_stagein to pipeline before arldm.
- For `RUN_SCRIPT=vistsis` you need to stage in three different input directories:
```bash 
jarvis pipeline append data_stagein dest_data_path=$LOCAL_INPUT_PATH \
user_data_paths=$SHARED_INPUT_PATH/vistdii,$SHARED_INPUT_PATH/vistsis,$SHARED_INPUT_PATH/visit_img,$PRETRAIN_MODEL_PATH \
mkdir_datapaths=$LOCAL_INPUT_PATH
```

- For other `RUN_SCRIPT`, you only need to stagein one directory:
```bash 
RUN_SCRIPT=pororo
jarvis pipeline append data_stagein dest_data_path=$LOCAL_INPUT_PATH \
user_data_paths=$SHARED_INPUT_PATH/$RUN_SCRIPT \
mkdir_datapaths=$LOCAL_INPUT_PATH
```


Create a Jarvis pipeline with ARLDM.
```bash
jarvis pipeline append arldm runscript=$RUN_SCRIPT local_exp_dir=$LOCAL_INPUT_PATH
```

## 5.7. Run the Experiment

Run the experiment, output are generated in `$LOCAL_INPUT_PATH/output_data`.
```bash
jarvis pipeline run
```

## 5.8. Clean Data

To clean data produced by Hermes + ARLDM:
```bash
jarvis pipeline clean
```



# 6. ARLDM + Hermes on Node Local Storage
Every step the same as [ARLDM + Hermes](#4-arldm-with-hermes), except for when creating a Jarvis pipeline with Hermes, using the Hermes VFD interceptor:
- Example using `RUN_SCRIPT=vistsis` you need to stage in three different input directories.
```bash
# Setup env
RUN_SCRIPT=vistsis # can change to other datasets
EXPERIMENT_PATH=~/experiments/arldm_run # NFS
SHARED_INPUT_PATH=$EXPERIMENT_PATH/input_data # NFS
cd $EXPERIMENT_PATH; export PRETRAIN_MODEL_PATH=`realpath model_large.pth`

LOCAL_EXPERIMENT_PATH=/mnt/nvme/$USER/arldm_run
LOCAL_INPUT_PATH=$LOCAL_EXPERIMENT_PATH/input_data

# add pkg to pipeline
jarvis pipeline append data_stagein dest_data_path=$LOCAL_INPUT_PATH \
user_data_paths=$SHARED_INPUT_PATH/vistdii,$SHARED_INPUT_PATH/vistsis,$SHARED_INPUT_PATH/visit_img,$PRETRAIN_MODEL_PATH \
mkdir_datapaths=$LOCAL_INPUT_PATH

jarvis pipeline append hermes_run --sleep=10 include=$LOCAL_INPUT_PATH/${RUN_SCRIPT}_out.h5

jarvis pipeline append hermes_api +posix

jarvis pipeline append arldm runscript=vistsis arldm_path="`scspkg pkg src arldm`/ARLDM" with_hermes=true local_exp_dir=$LOCAL_INPUT_PATH
```


# 7. ARLDM + Hermes with Multinodes Slurm (TODO)
Multinodes ARLDM is not supported yet.
```

### `builtin/builtin/asan/README.md`

```markdown
Hermes is a multi-tiered I/O buffering platform. This is module encompasses the
interceptors (MPI-IO, STDIO, POSIX, and VFD) provided in Hermes.

# Installation

```bash
spack install hermes@master
```

```bash
jarvis pipeline create hermes
jarvis pipeline append hermes --sleep=5
jarvis pipeline append hermes_api +posix -mpich
jarvis pipeline run
```
```

### `builtin/builtin/cm1/README.md`

```markdown
CM1 is a simulation code.

# Dependencies

```bash
spack install intel-oneapi-compilers
spack load intel-oneapi-compilers
spack compilers add
spack install h5z-zfp%intel
```

# Compiling / Installing

```bash
git clone git@github.com:lukemartinlogan/cm1r19.8-LOFS.git
cd cm1r19.8-LOFS
# COREX * COREY is the number of cores you intend to use on the system
# They do not need to be 2 and 2 here, but this is how our configurations are compiled for now
COREX=2 COREY=2 bash buildCM1-spack.sh
export PATH=${PWD}/run:${PATH}
export CM1_PATH=${PWD}
```

# Usage

```bash
jarvis pipeline create cm1
jarvis pipeline append cm1 corex=2 corey=2
```
```

### `builtin/builtin/cosmic_tagger/README.md`

```markdown
# Conda
Get the miniconda3 installation script and run it
```
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
```

# Cosmic Tagger
[Cosmic Tagger](https://github.com/coreyjadams/CosmicTagger) trains a CNN to separate cosmic pixels.

Conda:
```
conda create -n cosmic_tagger python==3.7
conda activate cosmic_tagger
conda install cmake hdf5 scikit-build numpy
```

Install Larc3
```
git clone https://github.com/DeepLearnPhysics/larcv3.git
cd larcv3
git submodule update --init
pip install -e .
```

Download cosmic tagger
```
git clone https://github.com/coreyjadams/CosmicTagger.git
cd CosmicTagger
pip install -r requirements.txt
```
```

### `builtin/builtin/darshan/README.md`

```markdown
Darshan is an I/O profiling tool. This repo intercepts application I/O and
dumps them into a log that can be parsed to collect metrics such as I/O time.

# Dependencies

# Compiling / Installing

```bash
scspkg create darshan
cd $(scspkg pkg src darshan)
git clone https://github.com/darshan-hpc/darshan.git
cd darshan
git fetch --all --tags --prune
git checkout tags/darshan-3.4.4
./prepare.sh

cd darshan-runtime
./configure --with-log-path=/darshan-logs \
--with-jobid-env=PBS_JOBID \
--with-log-path-by-env=DARSHAN_LOG_DIR \
--prefix=$(scspkg pkg root darshan) \
--enable-hdf5-mod \
CC=mpicc
# --enable-pnetcdf-mod \
make -j32
make install

cd ../darshan-util
./configure \
--prefix=$(scspkg pkg root darshan) \
--enable-pydarshan
make -j32
make install
```

# Usage

Create darshan environment:
```bash
module load darshan
jarvis env build darshan
```

Create a pipeline:
```bash
jarvis pipeline append darshan log_dir=${HOME}/darshan_logs
jarvis pipeline append ior
```

Run the pipeline:
```bash
jarvis pipeline run
```

# Analysis

There are several ways to analyze the output of Darshan:
```
darshan-job-summary.pl ${HOME}/darshan_logs
```
```

### `builtin/builtin/ddmd/README.md`

```markdown
# DeepDriveMD-F (DDMD)
DeepDriveMD: Deep-Learning Driven Adaptive Molecular Simulations (file-based continual learning loop).
See the [official repo](https://github.com/DeepDriveMD/DeepDriveMD-pipeline) for more detail.

# Table of Content
0. [Dependencies](#0-dependencies)
1. [Installation](#1-installation)
2. [Running DDMD](#2-running-ddmd)
3. [DDMD with Slurm](#3-ddmd-with-slurm)
4. [DDMD + Hermes](#4-ddmd--hermes)
5. [DDMD on Node Local Storage (FIXME)](#5-ddmd-on-node-local-storage)
6. [DDMD + Hermes on Node Local Storage (FIXME)](#6-ddmd--hermes-on-node-local-storage)
7. DDMD + Hermes with Multinodes Slurm (TODO)



# 0. Dependencies

## 0.1. conda
- Prepare Conda
Get the miniconda3 installation script and run it
```
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh.sh
```

## 0.2. jarvis-cd & scspkg
Follow steps here: https://github.com/grc-iit/jarvis-cd


## 0.3. spack
Spack is used to install HDF5, MPICH, and Hermes.
Install spack steps are here: https://spack.readthedocs.io/en/latest/getting_started.html#installation

## 0.4. HDF5 (1.14.0+)
HDF5 is require, (1.14.0+) is required by Hermes and h5py==3.8.0.

Use spack to install hdf5
```bash
spack install hdf5@1.14.0+hl~mpi
```


## 0.5. MPI
Either OpenMPI or MPICH works, this is required by Hermes(VFD depend on MPIIO adaptor)


Use spack to install mpich
```bash
spack install mpich@3.4.3
``` 



# 1. Installation

## 1.1 create ddmd scs package
```bash
scspkg create ddmd
cd `scspkg pkg src ddmd`
git clone https://github.com/candiceT233/deepdrivemd_pnnl.git deepdrivemd
cd deepdrivemd
export DDMD_PATH="`pwd`"
scspkg env set ddmd DDMD_PATH=$DDMD_PATH HDF5_USE_FILE_LOCKING=FALSE
```


## 1.2 Prepare conda environment and python packages
### 1.2.1 Set conda environment variable
```bash
export CONDA_OPENMM=hermes_openmm7_ddmd
export CONDA_PYTORCH=hm_ddmd_pytorch
```


### 1.2.2 Create the respective conda environment with YML files
```bash
cd "`scspkg pkg src ddmd`/deepdrivemd"
conda env create -f ddmd_openmm7.yaml --name=${CONDA_OPENMM}
conda env create -f ddmd_pytorch.yaml --name=${CONDA_PYTORCH}
```



### 1.2.2 Update the conda environment python packages
- for `CONDA_OPENMM`
```bash
cd `scspkg pkg src ddmd`
conda activate $CONDA_OPENMM
export DDMD_PATH="`pwd`"

cd $DDMD_PATH/submodules/MD-tools
pip install -e .

cd $DDMD_PATH/submodules/molecules
pip install -e .

pip uninstall h5py;
HDF5_MPI="OFF" HDF5_DIR=${YOUR_HDF5_DIR} pip install --no-cache-dir --no-binary=h5py h5py==3.8.0

conda deactivate
```


- for `CONDA_PYTORCH`
```bash
cd `scspkg pkg src ddmd`
conda activate $CONDA_PYTORCH
export DDMD_PATH="`pwd`"

cd $DDMD_PATH/submodules/MD-tools
pip install .

cd $DDMD_PATH/submodules/molecules
pip install .

cd $DDMD_PATH
pip install .

pip uninstall h5py;
HDF5_MPI="OFF" HDF5_DIR=${YOUR_HDF5_DIR} pip install --no-cache-dir --no-binary=h5py h5py==3.8.0

conda deactivate
```


# 2. Running DDMD

## 2.1. Setup Environment
Currently setup input path in a shared storage.
Setup experiment input and ouput paths:
```bash
EXPERIMENT_PATH=~/experiments/ddmd_runs #NFS
mkdir -p $EXPERIMENT_PATH
```


## 2.2. Create a Resource Graph

If you haven't already, create a resource graph. This only needs to be done
once throughout the lifetime of Jarvis. No need to repeat if you have already
done this for a different pipeline.

For details building resource graph, please refer to https://github.com/grc-iit/jarvis-cd/wiki/2.-Resource-Graph.

If you are running distributed tests, set path to the hostfile you are  using.
```bash
jarvis hostfile set /path/to/hostfile
```

Next, collect the resources from each of those pkgs. Walkthrough will give
a command line tutorial on how to build the hostfile.
```bash
jarvis resource-graph build +walkthrough
```

## 2.3 Create a Pipeline

The Jarvis pipeline will store all configuration data needed by DDMD.

```bash
jarvis pipeline create ddmd_test
```

## 2.4. Save Environment
Create the environment variables needed by DDMD.
```bash
spack load hdf5@1.14.0+hl~mpi mpich
module load ddmd
```

Store the current environment in the pipeline.
```bash
jarvis pipeline env build +CONDA_OPENMM +CONDA_PYTORCH +DDMD_PATH
```

## 2.5. Add pkgs to the Pipeline
Create a Jarvis pipeline with DDMD.
```bash
jarvis pipeline append ddmd
```

## 2.6. Run Experiment

Run the experiment
```bash
jarvis pipeline run
```

## 2.8. Clean Data

Clean data produced by DDMD
```bash
jarvis pipeline clean
```



# 3. DDMD With Slurm

## 3.1 Local Cluster
`ppn` must equal or greater than `num_workers`,which is default to 1.
```bash
jarvis pipeline sbatch job_name=ddmd_test nnodes=1 ppn=2 output_file=./ddmd_test.out error_file=./ddmd_test.err
```

## 3.2 Multi Nodes Cluster (TODO)
DDMD with jarvis-cd is currently only set to run with single node and using CPU.
    - Multiple CPU worker not tested
    - GPU not tested



# 4. DDMD + Hermes

## 4.0. Dependencies
### 4.0.1 HDF5
Hermes must compile with HDF5, makesure [download HDF5-1.14.0 with spack](#04-hdf5-1140).

### 4.0.2 Install Hermes dependencies with spack
```bash
spack load hdf5@1.14.0+hl~mpi mpich@3.4.3
spack install hermes_shm ^hdf5@1.14.0+hl~mpi ^mpich@3.4.3
```

### 4.0.3 Install Hermes with scspkg
```bash
spack load hermes_shm
scspkg create hermes
cd `scspkg pkg src hermes`
git clone https://github.com/HDFGroup/hermes
cd hermes
mkdir build
cd build
cmake ../ -DCMAKE_BUILD_TYPE="Release" \
    -DCMAKE_INSTALL_PREFIX=`scspkg pkg root hermes` \
    -DHERMES_ENABLE_MPIIO_ADAPTER="ON" \
    -DHERMES_MPICH="ON" \
    -DHERMES_ENABLE_POSIX_ADAPTER="ON" \
    -DHERMES_ENABLE_STDIO_ADAPTER="ON" \
    -DHERMES_ENABLE_VFD="ON" \

```

## 4.1. Setup Environment

Create the environment variables needed by Hermes + DDMD
```bash
spack load hermes_shm
module load hermes ddmd
```

## 4.2. Create a Resource Graph

Same as [above](#2-create-a-resource-graph).

## 4.3. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by Hermes
and DDMD.

```bash
jarvis pipeline create hermes_ddmd_test
```

## 4.4. Save Environment

Store the current environment in the pipeline.
```bash
jarvis pipeline env build +CONDA_OPENMM +CONDA_PYTORCH +DDMD_PATH
```

## 4.5. Add pkgs to the Pipeline

Create a Jarvis pipeline with Hermes, using the Hermes VFD interceptor.
```bash
jarvis pipeline append hermes_run --sleep=10 include=$EXPERIMENT_PATH
jarvis pipeline append hermes_api +vfd
jarvis pipeline append ddmd update_envar=true
```

## 4.6. Run the Experiment (TODO)

Run the experiment
```bash
jarvis pipeline run
```

## 4.7. Clean Data

To clean data produced by DDMD:
```bash
jarvis pipeline clean
```



# 5. DDMD on Node Local Storage
For cluster that has node local storage, you can stagein data from shared storage, then run ddmd.

## 5.1 Setup Environment
Currently setup DEFAULT input path in a shared storage, below is a example on Ares cluster using node local nvme.
```bash
RUN_SCRIPT=vistsis # can change to other datasets
EXPERIMENT_PATH=~/experiments/ddmd_run # NFS
INPUT_PATH=$EXPERIMENT_PATH/input_data # NFS
cd $EXPERIMENT_PATH; export PRETRAIN_MODEL_PATH=`realpath model_large.pth`

LOCAL_EXPERIMENT_PATH=/mnt/nvme/$USER/ddmd_run
LOCAL_INPUT_PATH=$LOCAL_EXPERIMENT_PATH/input_data
LOCAL_OUTPUT_PATH=$LOCAL_EXPERIMENT_PATH/output_data
```

## 5.2. Download Pretrain Model and Input Data
Same as above [download pretrain](#22-download-pretrain-model) and [download input](#23-download-input-data).

## 5.3. Create a Resource Graph
Same as [above](#23-create-a-resource-graph)

## 5.4. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by DDMD.

```bash
jarvis pipeline create ddmd_local
```

## 5.5. Save Environment
Create the environment variables needed by DDMD.
```bash
spack load hdf5@1.14.0+hl~mpi mpich@3.4.3
module load ddmd
```


Store the current environment in the pipeline.
```bash
jarvis pipeline env build +CONDA_OPENMM +CONDA_PYTORCH +DDMD_PATH
```


## 5.6. Add pkgs to the Pipeline
Add data_stagein to pipeline before ddmd.
- For `RUN_SCRIPT=vistsis` you need to stage in three different input directories:
```bash 
jarvis pipeline append data_stagein dest_data_path=$LOCAL_INPUT_PATH \
user_data_paths=$INPUT_PATH/vistdii,$INPUT_PATH/vistsis,$INPUT_PATH/visit_img,$PRETRAIN_MODEL_PATH \
mkdir_datapaths=$LOCAL_INPUT_PATH,$LOCAL_OUTPUT_PATH
```

- For other `RUN_SCRIPT`, you only need to stagein one directory:
```bash 
RUN_SCRIPT=pororo
jarvis pipeline append data_stagein dest_data_path=$LOCAL_INPUT_PATH \
user_data_paths=$INPUT_PATH/$RUN_SCRIPT \
mkdir_datapaths=$LOCAL_INPUT_PATH,$LOCAL_OUTPUT_PATH
```


Create a Jarvis pipeline with DDMD.
```bash
jarvis pipeline append ddmd runscript=$RUN_SCRIPT ddmd_path="`scspkg pkg src ddmd`/DDMD" local_exp_dir=$LOCAL_EXPERIMENT_PATH
```

## 5.7. Run the Experiment

Run the experiment
```bash
jarvis pipeline run
```

## 5.8. Clean Data

To clean data produced by Hermes + DDMD:
```bash
jarvis pipeline clean
```



# 6. DDMD + Hermes on Node Local Storage
Every step the same as [DDMD + Hermes](#4-ddmd-with-hermes), except for when creating a Jarvis pipeline with Hermes, using the Hermes VFD interceptor:
- Example using `RUN_SCRIPT=vistsis` you need to stage in three different input directories.
```bash
# Setup env
RUN_SCRIPT=vistsis # can change to other datasets
EXPERIMENT_PATH=~/experiments/ddmd_run # NFS
INPUT_PATH=$EXPERIMENT_PATH/input_data # NFS
cd $EXPERIMENT_PATH; export PRETRAIN_MODEL_PATH=`realpath model_large.pth`

LOCAL_EXPERIMENT_PATH=/mnt/nvme/$USER/ddmd_run
LOCAL_INPUT_PATH=$LOCAL_EXPERIMENT_PATH/input_data
LOCAL_OUTPUT_PATH=$LOCAL_EXPERIMENT_PATH/output_data

# add pkg to pipeline
jarvis pipeline append data_stagein dest_data_path=$LOCAL_INPUT_PATH \
user_data_paths=$INPUT_PATH/vistdii,$INPUT_PATH/vistsis,$INPUT_PATH/visit_img,$PRETRAIN_MODEL_PATH \
mkdir_datapaths=$LOCAL_INPUT_PATH,$LOCAL_OUTPUT_PATH

jarvis pipeline append hermes_run --sleep=10 include=$LOCAL_EXPERIMENT_PATH

jarvis pipeline append hermes_api +vfd

jarvis pipeline append ddmd runscript=vistsis ddmd_path="`scspkg pkg src ddmd`/DDMD" update_envar=true local_exp_dir=$LOCAL_EXPERIMENT_PATH
```


# 7. DDMD + Hermes with Multinodes Slurm (TODO)
Multinodes DDMD is not supported yet.
```

### `builtin/builtin/dlio_benchmark/README.md`

```markdown
DLIO is an I/O benchmark for Deep Learning, aiming at emulating the I/O behavior of various deep learning applications.

# Installation

```bash
git clone https://github.com/argonne-lcf/dlio_benchmark
cd dlio_benchmark/
pip install .
```

# DLIO

## 1. Create a Resource Graph

If you haven't already, create a resource graph. This only needs to be done
once throughout the lifetime of Jarvis. No need to repeat if you have already
done this for a different pipeline.

If you are running distributed tests, set path to the hostfile you are  using.
```bash
jarvis hostfile set /path/to/hostfile
```

Next, collect the resources from each of those pkgs. Walkthrough will give
a command line tutorial on how to build the hostfile.
```bash
jarvis resource-graph build +walkthrough
```

## 2. Create a Pipeline

The Jarvis pipeline will store all configuration data.
```bash
jarvis pipeline create dlio_test
```

## 4. Add pkgs to the Pipeline

Create a Jarvis pipeline
```bash
jarvis pipeline append dlio_benchmark workload=unet3d_a100 generate_data=True data_path=/path/to/generated_data checkpoint_path=/path/to/checkpoints
```
Note: you can modify the dlio_benchmark configuration file by changing the modifying the dlio_benchmark.yaml file directly, and then execute `jarvis ppl update` to update the configuration. 

## 5. Run Experiment

Run the experiment
```bash
jarvis pipeline run
```

## 6. Clean Data

Clean produced data
```bash
jarvis pipeline clean
```
```

### `builtin/builtin/filebench/README.md`

```markdown
Filebench is a cloud workload generator.

# Installation

```bash
spack install filebench
```
```

### `builtin/builtin/fio/README.md`

```markdown
FIO

# Installation

```bash
spack install fio
```

# FIO
```

### `builtin/builtin/gadget2/README.md`

```markdown
GADGET is a freely available code for cosmological N-body/SPH simulations on massively parallel computers with distributed memory. GADGET uses an explicit communication model that is implemented with the standardized MPI communication interface. The code can be run on essentially all supercomputer systems presently in use, including clusters of workstations or individual PCs.

GADGET computes gravitational forces with a hierarchical tree algorithm (optionally in combination with a particle-mesh scheme for long-range gravitational forces) and represents fluids by means of smoothed particle hydrodynamics (SPH). The code can be used for studies of isolated systems, or for simulations that include the cosmological expansion of space, both with or without periodic boundary conditions. In all these types of simulations, GADGET follows the evolution of a self-gravitating collisionless N-body system, and allows gas dynamics to be optionally included. Both the force computation and the time stepping of GADGET are fully adaptive, with a dynamic range which is, in principle, unlimited.

https://wwwmpa.mpa-garching.mpg.de/gadget/

# Installation

```bash
spack install hdf5@1.14.1 gsl@2.1 fftw@2
scspkg create gadget2
cd $(scspkg pkg src gadget2)
git clone https://github.com/lukemartinlogan/gadget2.git
export GADGET2_PATH=$(scspkg pkg src gadget2)/gadget2
export FFTW_PATH=$(spack find --format "{PREFIX}" fftw@2)
```

# Create environment

```bash
spack load hdf5@1.14.1 gsl@2.1 fftw@2
jarvis env build gadget2 +GADGET2_PATH +FFTW_PATH
```

# Gassphere Pipeline

```bash
jarvis pipeline create gassphere
jarvis pipeline env copy gadget2
jarvis pipeline append gadget2
jarvis pkg configure gadget2 \
test_case=gadget2 \
out=${HOME}/gadget2
jarvis pipeline run
```

# NGenIC Pipeline

```bash
jarvis pipeline create gassphere
jarvis pipeline env copy gadget2
jarvis pipeline append gadget2
jarvis pkg configure gadget2 \
test_case=gassphere-ngen \
out=${HOME}/gadget2 \
ic=hello
jarvis pipeline run
```
```

### `builtin/builtin/gadget2_df/README.md`

```markdown
GADGET is a freely available code for cosmological N-body/SPH simulations on massively parallel computers with distributed memory. GADGET uses an explicit communication model that is implemented with the standardized MPI communication interface. The code can be run on essentially all supercomputer systems presently in use, including clusters of workstations or individual PCs.

GADGET computes gravitational forces with a hierarchical tree algorithm (optionally in combination with a particle-mesh scheme for long-range gravitational forces) and represents fluids by means of smoothed particle hydrodynamics (SPH). The code can be used for studies of isolated systems, or for simulations that include the cosmological expansion of space, both with or without periodic boundary conditions. In all these types of simulations, GADGET follows the evolution of a self-gravitating collisionless N-body system, and allows gas dynamics to be optionally included. Both the force computation and the time stepping of GADGET are fully adaptive, with a dynamic range which is, in principle, unlimited.

https://wwwmpa.mpa-garching.mpg.de/gadget/

# Installation

Check the README for gadget2.

# Create Pipeline

```bash
jarvis pipeline create ngenic
jarvis pipeline env copy gadget2
jarvis pipeline append gadget2_df
jarvis pkg configure gadget2_df \
nparticles=100000 \
nprocs=4
jarvis pipeline run
```
```

### `builtin/builtin/gray_scott/README.md`

```markdown
Gray-Scott is a 3D 7-Point stencil code

# Installation

```bash
scspkg create gray-scott
cd `scspkg pkg src gray-scott`
git clone https://github.com/pnorbert/adiosvm
cd adiosvm/Tutorial/gs-mpiio
mkdir build
pushd build
cmake ../ -DCMAKE_BUILD_TYPE=Release
make -j8
export GRAY_SCOTT_PATH=`pwd`
scspkg env set gray_scott GRAY_SCOTT_PATH="${GRAY_SCOTT_PATH}"
scspkg env prepend gray_scott PATH "${GRAY_SCOTT_PATH}"
module load gray_scott
spack load mpi adios2
```

# Gray Scott

## 1. Setup Environment

Create the environment variables needed by Gray Scott
```bash
module load gray_scott
spack load mpi
```````````

## 1. Create a Resource Graph

If you haven't already, create a resource graph. This only needs to be done
once throughout the lifetime of Jarvis. No need to repeat if you have already
done this for a different pipeline.

If you are running distributed tests, set path to the hostfile you are  using.
```bash
jarvis hostfile set /path/to/hostfile
```

Next, collect the resources from each of those pkgs. Walkthrough will give
a command line tutorial on how to build the hostfile.
```bash
jarvis resource-graph build +walkthrough
```

## 2. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by Gray Scott.

```bash
jarvis pipeline create gray-scott-test
```

## 3. Save Environment

Store the current environment in the pipeline.
```bash
jarvis pipeline env build
```

## 4. Add pkgs to the Pipeline

Create a Jarvis pipeline with Gray Scott
```bash
jarvis pipeline append gray_scott
```

## 5. Run Experiment

Run the experiment
```bash
jarvis pipeline run
```

## 6. Clean Data

Clean data produced by Gray Scott
```bash
jarvis pipeline clean
```

# Gray Scott With Hermes

## 1. Setup Environment

Create the environment variables needed by Hermes + Gray Scott
```bash
# On personal
spack install hermes@master adios2
spack load hermes adios2
# On Ares
module load hermes/master-feow7up adios2/2.9.0-mmkelnu
# export GRAY_SCOTT_PATH=${HOME}/adiosvm/Tutorial/gs-mpiio/build
export PATH="${GRAY_SCOTT_PATH}:$PATH"
```

## 2. Create a Resource Graph

If you haven't already, create a resource graph. This only needs to be done
once throughout the lifetime of Jarvis. No need to repeat if you have already
done this for a different pipeline.

If you are running distributed tests, set path to the hostfile you are  using.
```bash
jarvis hostfile set /path/to/hostfile.txt
```

Next, collect the resources from each of those pkgs. Walkthrough will give
a command line tutorial on how to build the hostfile.
```bash
jarvis resource-graph build +walkthrough
```

## 3. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by Hermes
and Gray Scott.

```bash
jarvis pipeline create gs-hermes
```

## 3. Save Environment

Store the current environment in the pipeline.
```bash
jarvis pipeline env build
```

## 4. Add pkgs to the Pipeline

Create a Jarvis pipeline with Hermes, the Hermes MPI-IO interceptor,
and gray-scott
```bash
jarvis pipeline append hermes --sleep=10 --output_dir=${HOME}/gray-scott
jarvis pipeline append hermes_api +mpi
jarvis pipeline append gray_scott
```

## 5. Run the Experiment

Run the experiment
```bash
jarvis pipeline run
```

## 6. Clean Data

To clean data produced by Hermes + Gray-Scott:
```bash
jarvis pipeline clean
```
```

### `builtin/builtin/ior/README.md`

```markdown
LabStor is a distributed semi-microkernel for building data processing services.

# Installation

```bash
spack install ior mpi
```

# IOR

## 1. Create a Resource Graph

If you haven't already, create a resource graph. This only needs to be done
once throughout the lifetime of Jarvis. No need to repeat if you have already
done this for a different pipeline.

If you are running distributed tests, set path to the hostfile you are  using.
```bash
jarvis hostfile set /path/to/hostfile
```

Next, collect the resources from each of those pkgs. Walkthrough will give
a command line tutorial on how to build the hostfile.
```bash
jarvis resource-graph build +walkthrough
```

## 2. Create a Pipeline

The Jarvis pipeline will store all configuration data.
```bash
jarvis pipeline create ior
```

## 3. Load Environment

Create the environment variables
```bash
spack load ior mpi
```````````

## 4. Add pkgs to the Pipeline

Create a Jarvis pipeline
```bash
jarvis pipeline append ior
```

## 5. Run Experiment

Run the experiment
```bash
jarvis pipeline run
```

## 6. Clean Data

Clean produced data
```bash
jarvis pipeline clean
```
```

### `builtin/builtin/lammps/README.md`

```markdown
# LAMMPS

## what is lammps

LAMMPS is a classical molecular dynamics simulation code designed to
run efficiently on parallel computers.  It was developed at Sandia
National Laboratories, a US Department of Energy facility, with
funding from the DOE.  It is an open-source code, distributed freely
under the terms of the GNU Public License (GPL) version 2.


## what is the output of lammps
log file of thermodynamic info, text dump files of atom coordinates, velocities, other per-atom quantities, dump output on fixed and variable intervals, based timestep or simulated time, binary restart files, parallel I/O of dump and restart files, per-atom quantities (energy, stress, centro-symmetry parameter, CNA, etc.). user-defined system-wide (log file) or per-atom (dump file) calculations
custom partitioning (chunks) for binning, and static or dynamic grouping of atoms for analysis spatial, time, and per-chunk averaging of per-atom quantities time averaging and histogramming of system-wide quantities atom snapshots in native, XYZ, XTC, DCD, CFG, NetCDF, HDF5, ADIOS2, YAML formats on-the-fly compression of output and decompression of read in files.

## lammps tutorial
Please refer to this [website](https://docs.lammps.org/) for more details.
```

### `builtin/builtin/nyx_lya/README.md`

```markdown
Nyx is an adaptive mesh, massively-parallel, cosmological simulation code. 

# Installation

To compile the code we require C++11 compliant compilers that support MPI-2 or higher implementation. If threads or accelerators are used, we require OpenMP 4.5 or higher, Cuda 9 or higher, or HIP-Clang.

## 1. Install Dependencies


## 1. Install AMReX

```bash
git clone https://github.com/AMReX-Codes/amrex.git
pushd amrex
mkdir build
pushd build
cmake .. -DAMReX_HDF5=ON -DAMReX_PARTICLES=ON -DAMReX_PIC=ON -DBUILD_SHARED_LIBS=ON -DCMAKE_INSTALL_PREFIX=/path/to/amrex/install
make -j8
make install
popd
popd
```

## 2. Install Nyx

```bash
git clone https://github.com/AMReX-astro/Nyx.git
pushd Nyx
mkdir build
pushd build
cmake .. -DCMAKE_PREFIX_PATH=/path/to/amrex/install -DAMReX_DIR=/path/to/amrex/install/Tools/CMake/ -DNyx_SINGLE_PRECISION_PARTICLES=OFF -DNyx_OMP=OFF
make -j8
export NYX_PATH=`pwd`/Exec
popd
popd
```

# Nyx LyA

## 1. Create a Resource Graph

If you haven't already, create a resource graph. This only needs to be done
once throughout the lifetime of Jarvis. No need to repeat if you have already
done this for a different pipeline.

If you are running distributed tests, set path to the hostfile you are  using.
```bash
jarvis hostfile set /path/to/hostfile
```

Next, collect the resources from each of those pkgs. Walkthrough will give
a command line tutorial on how to build the hostfile.
```bash
jarvis resource-graph build +walkthrough
```

## 2. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by Nyx LyA.

```bash
jarvis pipeline create nyx-lya-test
```

## 4. Add pkgs to the Pipeline

Create a Jarvis pipeline with Nyx LyA
```bash
jarvis pipeline append nyx_lya --nyx_install_path=$NYX_PATH --initial_z=190.0 --final_z=180.0 --plot_z_values="188.0 186.0" --output=/path/to/output_files
```
**nyx_install_path**: this argument is required, otherwise it will report an error.
You can use the default arguments for other arguments. But it may take a while.

## 5. Run Experiment

Run the experiment
```bash
jarvis pipeline run
```

## 6. Clean Data

Clean data produced by Nyx LyA
```bash
jarvis pipeline clean
```
```

### `builtin/builtin/orangefs/README.md`

```markdown
In this section we go over how to install and deploy OrangeFS.
NOTE: if running in Ares, OrangeFS is already installed, so skip
to section 5.3.

# Install Various Dependencies

```bash
sudo apt update
sudo apt install -y fuse
sudo apt install gcc flex bison libssl-dev libdb-dev linux-headers-$(uname -r) perl make libldap2-dev libattr1-dev
```

For fuse
```bash
sudo apt -y install fuse
spack install libfuse@2.9
```

NOTE: This package expects a working, passwordless SSH setup if you are using multiple nodes. On systems like Chameleon Cloud, you
must distribute the keys and set this up yourself before using jarvis. On single-node systems, SSH is not required.

# Install OrangeFS (Linux)

OrangeFS is located [on this website](http://www.orangefs.org/?gclid=CjwKCAjwgqejBhBAEiwAuWHioDo2uu8wel6WhiFqoBDgXMiVXc7nrykeE3sf3mIfDFVEt0_7SwRN8RoCdRYQAvD_BwE)
The official OrangeFS github is [here](https://github.com/waltligon/orangefs/releases/tag/v.2.9.8).

```bash
scspkg create orangefs
cd `scspkg pkg src orangefs`
wget https://github.com/waltligon/orangefs/releases/download/v.2.10.0/orangefs-2.10.0.tar.gz
tar -xvzf orangefs-2.10.0.tar.gz
cd orangefs
./prepare
./configure --prefix=`scspkg pkg root orangefs` --enable-shared --enable-fuse
make -j8
make install
scspkg env prepend orangefs ORANGEFS_PATH `scspkg pkg root orangefs`
```

# Using MPICH with OrangeFS

MPICH requires a special build when using OrangeFS. Apparantly it's for
performance, but it's a pain to have to go through the extra step.

```bash
scspkg create orangefs-mpich
cd `scspkg pkg src orangefs-mpich`
wget http://www.mpich.org/static/downloads/3.2/mpich-3.2.tar.gz --no-check-certificate
tar -xzf mpich-3.2.tar.gz
cd mpich-3.2
./configure --prefix=`scspkg pkg root orangefs-mpich` --enable-fast=O3 --enable-romio --enable-shared --with-pvfs2=`scspkg pkg root orangefs` --with-file-system=pvfs2
make -j8
make install
```

# Creating a pipeline

## Main Parmaeters
There are a few main parameters:
* ``ofs_data_dir``: The place where orangefs should store data or metadata. 
This needs to be a directory private to each node. For example, like /tmp or a burst buffer.
* ``mount``: Where the client should be mounted. This is where users will typically place data.
* ``ofs_mode``: The deployment method to use. Either fuse, kern, or ares.
* ``name``: The semantic name of the OrangeFS deployment. Typically just leave as default unless 
you have multiple deployments

## Performance Parameters
* ``stripe_size``: Size in bytes for stripes. Default 65536 (i.e., 64KB). 
* ``protocol``: Either tcp or ib. Only tcp has been tested.

## The Hostfile
OrangeFS can be picky about the hostfile. We recommend using only IP addresses
in your jarvis hostfile at this time when using OrangeFS.

An example hostfile for a single-node deployment is below:
```bash
echo '127.0.0.1' > ~/hostfile.txt
jarvis hostfile set ~/hostfile.txt
```

## libfuse
```bash
module load orangefs
jarvis pipeline create orangefs
jarvis pipeline env build +ORANGEFS_PATH
jarvis pipeline append orangefs \
mount=${HOME}/orangefs_client \
ofs_data_dir=${HOME}/ofs_data \
ofs_mode=fuse
```

## For kernel module
```bash
module load orangefs
jarvis pipeline create orangefs
jarvis pipeline env build +ORANGEFS_PATH
jarvis pipeline append orangefs \
mount=${HOME}/orangefs_client \
ofs_data_dir=/mnt/nvme/$USER/ofs_data \
ofs_mode=kern
```

## Ares Machine at IIT
```bash
module load orangefs
jarvis pipeline create orangefs
jarvis pipeline env build +ORANGEFS_PATH
jarvis pipeline append orangefs \
mount=${HOME}/orangefs_client \
ofs_data_dir=/mnt/nvme/$USER/ofs_data \
ofs_mode=ares
```
```

### `builtin/builtin/paraview/README.md`

```markdown
# paraview 
ParaView is an open-source data analysis and visualization application designed to handle large-scale scientific datasets. It supports interactive and batch processing for visualizing complex simulations and performing quantitative analysis.
```

### `builtin/builtin/pyflextrkr/README.md`

```markdown
The Python FLEXible object TRacKeR (PyFLEXTRKR) is a flexible atmospheric feature tracking software package.
See the [official repo](https://github.com/FlexTRKR/PyFLEXTRKR) for more detail.

# Table of Content
0. [Dependencies](#0-dependencies)
1. [Installation](#1-installation)
2. [Running Pyflextrkr](#2-running-pyflextrkr)
3. [Pyflextrkr with Slurm](#3-pyflextrkr-with-slurm)
4. [Pyflextrkr + Hermes](#4-pyflextrkr--hermes)
5. [Pyflextrkr on Node Local Storage](#5-pyflextrkr-on-node-local-storage)
6. [Pyflextrkr + Hermes on Node Local Storage](#6-pyflextrkr--hermes-on-node-local-storage)
7. [Pyflextrkr + Hermes with Multinodes Slurm (TODO)](#7-pyflextrkr--hermes-on-multinodes-slurm-todo)



# 0. Dependencies

## 0.1. conda
- Prepare Conda
Get the miniconda3 installation script and run it
```
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh.sh
```

## 0.2. jarvis-cd & scspkg
Follow steps here: https://github.com/grc-iit/jarvis-cd


## 0.3. spack
Spack is used to install HDF5, MPICH, and Hermes.
Install spack steps are here: https://spack.readthedocs.io/en/latest/getting_started.html#installation

## 0.4. HDF5 (1.14.0+)
HDF5 is require, (1.14.0+) is required by Hermes and h5py==3.8.0.

Use spack to install hdf5
```bash
spack install hdf5@1.14.0+hl~mpi
```

## 0.5. MPI
Either OpenMPI or MPICH works, this is required by Hermes and mpi4py


Use spack to install mpich
```bash
spack install mpich@3.4.3
```

## 0.6. Installation Tools
You need `wget` to download the datasets online:
- wget
You can install wget either with `apt-get` or `spack`
```bash
sudo apt-get install wget
# or
spack install wget
spack load wget
# check if wget is usable
which wget
```



# 1. Installation

## 1.1 create pyflextrkr scs package
```bash
scspkg create pyflextrkr
cd `scspkg pkg src pyflextrkr`
git clone https://github.com/candiceT233/PyFLEXTRKR
cd PyFLEXTRKR
git switch ares # User the ares branch
scspkg env set pyflextrkr PYFLEXTRKR_PATH="`pwd`"
```


## 1.2 Prepare conda environment and python packages:
```bash
cd `scspkg pkg src pyflextrkr`/PyFLEXTRKR

YOUR_HDF5_DIR="`which h5cc |sed 's/.\{9\}$//'`"

conda env create -f ares_flextrkr.yml -n flextrkr
conda activate flextrkr
pip install -e .
HDF5_MPI="OFF" HDF5_DIR=${YOUR_HDF5_DIR} pip install --no-cache-dir --no-binary=h5py h5py==3.8.0
pip install xarray[io] mpi4py
conda deactivate
```



# 2. Running Pyflextrkr
Example is using `TEST_NAME=run_mcs_tbpfradar3d_wrf` (only supported with dataset download).
## 2.1. Setup Environment
Currently setup input path in a shared storage, below is a example on Ares cluster.
```bash
TEST_NAME=run_mcs_tbpfradar3d_wrf
EXPERIMENT_PATH=~/experiments/pyflex_run # NFS

export EXPERIMENT_INPUT_PATH=$EXPERIMENT_PATH/input_data # NFS
scspkg env set pyflextrkr EXPERIMENT_INPUT_PATH=$EXPERIMENT_INPUT_PATH
mkdir -p $EXPERIMENT_INPUT_PATH
```

## 2.2. Download Input Data
Setup example input data `wrf_tbradar.tar.gz` and path.to `run_mcs_tbpfradar3d_wrf.tar.gz`
```bash
cd $EXPERIMENT_INPUT_PATH
wget https://portal.nersc.gov/project/m1867/PyFLEXTRKR/sample_data/tb_radar/wrf_tbradar.tar.gz -O $TEST_NAME.tar.gz
mkdir $TEST_NAME
tar -xvzf $TEST_NAME.tar.gz -C $TEST_NAME

# Remove downloaded tar file
rm -rf $EXPERIMENT_INPUT_PATH/$TEST_NAME.tar.gz
```


## 2.3. Create a Resource Graph

If you haven't already, create a resource graph. This only needs to be done
once throughout the lifetime of Jarvis. No need to repeat if you have already
done this for a different pipeline.

For details building resource graph, please refer to https://github.com/grc-iit/jarvis-cd/wiki/2.-Resource-Graph.

If you are running distributed tests, set path to the hostfile you are  using.
```bash
jarvis hostfile set /path/to/hostfile
```

Next, collect the resources from each of those pkgs. Walkthrough will give
a command line tutorial on how to build the hostfile.
```bash
jarvis resource-graph build +walkthrough
```

## 2.4 Create a Pipeline

The Jarvis pipeline will store all configuration data needed by Pyflextrkr.

```bash
jarvis pipeline create pyflextrkr_test
```

## 2.5. Save Environment
Create the environment variables needed by Pyflextrkr.
```bash
spack load hdf5@1.14.0+hl~mpi mpich@3.4.3
module load pyflextrkr
```
<!-- conda activate flextrkr -->

Store the current environment in the pipeline.
```bash
jarvis env build pyflextr +PYFLEXTRKR_PATH +EXPERIMENT_INPUT_PATH
jarvis pipeline env copy pyflextr
```

## 2.6. Add pkgs to the Pipeline
** Currently only support running `runscript=run_mcs_tbpfradar3d_wrf` **

Create a Jarvis pipeline with Pyflextrkr.
```bash
jarvis pipeline append pyflextrkr runscript=run_mcs_tbpfradar3d_wrf
```


## 2.7. Run Experiment

Run the experiment, output are generated in `$EXPERIMENT_INPUT_PATH/output_data`.
```bash
jarvis pipeline run
```

## 2.8. Clean Data

Clean data produced by Pyflextrkr
```bash
jarvis pipeline clean
```



# 3. Pyflextrkr With Slurm

## 3.1 Local Cluster
Do the above and `ppn` must match `nprocesses`
```bash
jarvis pipeline sbatch job_name=pyflex_test nnodes=1 ppn=8 output_file=./pyflex_test.out error_file=./pyflex_test.err
```

## 3.2 Multi Nodes Cluster
Do the above and `ppn` must greater than match `nprocesses`/`nnodes` 
    (e.g. `nnodes=2 ppn=8` allocates 16 processes in total, and `nprocesses` must not greater than 16)

Configure Pyflextrkr to parallel run mode with MPI-Dask (0: serial, 1: Dask one-node cluster, 2: Dask multinode cluster)
```bash
jarvis pkg configure pyflextrkr run_parallel=2 nprocesses=8
```

```bash
jarvis pipeline sbatch job_name=pyflex_2ntest nnodes=2 ppn=4 output_file=./pyflex_2ntest.out error_file=./pyflex_2ntest.err
```



# 4. Pyflextrkr + Hermes

## 4.0. Dependencies
### 4.0.1 HDF5
Hermes must compile with HDF5, makesure [download HDF5-1.14.0 with spack](#4-hdf5-1140).

### 4.0.2 Install Hermes dependencies with spack
```bash
spack load hdf5@1.14.0+hl~mpi mpich@3.4.3
spack install hermes_shm ^hdf5@1.14.0+hl~mpi ^mpich@3.4.3
```

### 4.0.3 Install Hermes with scspkg
```bash
spack load hermes_shm
scspkg create hermes
cd `scspkg pkg src hermes`
git clone https://github.com/HDFGroup/hermes
cd hermes
mkdir build
cd build
cmake ../ -DCMAKE_BUILD_TYPE="Release" \
    -DCMAKE_INSTALL_PREFIX=`scspkg pkg root hermes` \
    -DHERMES_ENABLE_MPIIO_ADAPTER="ON" \
    -DHERMES_MPICH="ON" \
    -DHERMES_ENABLE_POSIX_ADAPTER="ON" \
    -DHERMES_ENABLE_STDIO_ADAPTER="ON" \
    -DHERMES_ENABLE_VFD="ON" \

```

## 4.1. Setup Environment

Create the environment variables needed by Hermes + Pyflextrkr
```bash
spack load hermes_shm
module load hermes pyflextrkr
```

## 4.2. Create a Resource Graph

Same as [above](#2-create-a-resource-graph).

## 4.3. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by Hermes
and Pyflextrkr.

```bash
jarvis pipeline create hermes_pyflextrkr_test
```

## 4.4. Save Environment

Store the current environment in the pipeline.
```bash
jarvis pipeline env build +PYFLEXTRKR_PATH
```

## 4.5. Add pkgs to the Pipeline

Create a Jarvis pipeline with Hermes, the Hermes VFD interceptor,
and pyflextrkr (must use `flush_mode=sync` to prevent [this error](#oserror-log))
```bash
jarvis pipeline append hermes_run --sleep=10 include=$EXPERIMENT_PATH flush_mode=sync
jarvis pipeline append hermes_api +vfd
jarvis pipeline append pyflextrkr runscript=$TEST_NAME update_envar=true
```

## 4.6. Run the Experiment

Run the experiment, output are generated in `$EXPERIMENT_INPUT_PATH/output_data`.
```bash
jarvis pipeline run
```

## 4.7. Clean Data

To clean data produced by Hermes + Pyflextrkr:
```bash
jarvis pipeline clean
```



# 5. Pyflextrkr on Node Local Storage
For cluster that has node local storage, you can stagein data from shared storage, then run pyflextrkr.

## 5.1 Setup Environment
Currently setup DEFAULT input path in a shared storage, below is a example on Ares cluster using node local nvme.


The shared storage path is same as before:
```bash
export TEST_NAME=run_mcs_tbpfradar3d_wrf
EXPERIMENT_PATH=~/experiments/pyflex_run # NFS
EXPERIMENT_INPUT_PATH=$EXPERIMENT_PATH/input_data # NFS
mkdir -p $EXPERIMENT_INPUT_PATH
```
Setup the node local experiment paths:
```bash
LOCAL_EXPERIMENT_PATH=/mnt/nvme/$USER/pyflex_run
LOCAL_INPUT_PATH=$LOCAL_EXPERIMENT_PATH/input_data
```

## 5.2. Download Input Data
Same as [above](#22-download-input-data).

## 5.3. Create a Resource Graph
Same as [above](#23-create-a-resource-graph)

## 5.4. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by Pyflextrkr.

```bash
jarvis pipeline create pyflextrkr_local
```

## 5.5. Save Environment
Create the environment variables needed by Pyflextrkr.
```bash
spack load hdf5@1.14.0+hl~mpi mpich@3.4.3
module load pyflextrkr
```


Store the current environment in the pipeline.
```bash
jarvis pipeline env build +PYFLEXTRKR_PATH
```

## 5.6. Add pkgs to the Pipeline
Add data_stagein to pipeline before pyflextrkr.
```bash 
jarvis pipeline append data_stagein dest_data_path=$LOCAL_INPUT_PATH \
user_data_paths=$EXPERIMENT_INPUT_PATH/$TEST_NAME \
mkdir_datapaths=$LOCAL_INPUT_PATH
```

Create a Jarvis pipeline with Pyflextrkr.
```bash
jarvis pipeline append pyflextrkr runscript=$TEST_NAME local_exp_dir=$LOCAL_INPUT_PATH
```

## 5.7. Run the Experiment

Run the experiment, output are generated in `$LOCAL_INPUT_PATH/output_data`.
```bash
jarvis pipeline run
```

## 5.8. Clean Data

To clean data produced by Hermes + Pyflextrkr:
```bash
jarvis pipeline clean
```



# 6. Pyflextrkr + Hermes on Node Local Storage
Every step the same as [Pyflextrkr + Hermes](#4-pyflextrkr--hermes), except for when creating a Jarvis pipeline with Hermes, using the Hermes VFD interceptor:
```bash
# Setup env
TEST_NAME=run_mcs_tbpfradar3d_wrf
EXPERIMENT_PATH=~/experiments/pyflex_run # NFS
INPUT_PATH=$EXPERIMENT_PATH/input_data # NFS
mkdir -p $EXPERIMENT_INPUT_PATH

LOCAL_EXPERIMENT_PATH=/mnt/nvme/$USER/pyflex_run
LOCAL_INPUT_PATH=$LOCAL_EXPERIMENT_PATH/input_data

# add pkg to pipeline
jarvis pipeline append data_stagein dest_data_path=$LOCAL_INPUT_PATH \
user_data_paths=$EXPERIMENT_INPUT_PATH/$TEST_NAME \
mkdir_datapaths=$LOCAL_INPUT_PATH

jarvis pipeline append hermes_run --sleep=10 include=$LOCAL_EXPERIMENT_PATH

jarvis pipeline append hermes_api +vfd

jarvis pipeline append pyflextrkr runscript=$TEST_NAME update_envar=true local_exp_dir=$LOCAL_INPUT_PATH
```



# 7. Pyflextrkr + Hermes on Multinodes Slurm (TODO)
Steps are the same as [Pyflextrkr with Slurm](#3-pyflextrkr-With-slurm), but not working yet due to OSError.

## OSError Log
```log
2024-01-03 18:49:03,523 - pyflextrkr.idfeature_driver - INFO - Identifying features from raw data
2024-01-03 18:49:04,969 - pyflextrkr.idfeature_driver - INFO - Total number of files to process: 17
free(): invalid size
2024-01-03 18:49:07,944 - distributed.worker - WARNING - Compute Failed
Key:       idclouds_tbpf-0bb7839d-ac6e-4e51-b309-ec2bc6667641
Function:  execute_task
args:      ((<function idclouds_tbpf at 0x7fce75d6da80>, '/home/mtang11/experiments/pyflex_runs/input_data/run_mcs_tbpfradar3d_wrf/wrfout_rainrate_tb_zh_mh_2015-05-06_04:00:00.nc', (<class 'dict'>, [['ReflThresh_lowlevel_gap', 20.0], ['abs_ConvThres_aml', 45.0], ['absolutetb_threshs', [160, 330]], ['area_thresh', 36], ['background_Box', 12.0], ['clouddata_path', '/home/mtang11/experiments/pyflex_runs/input_data/run_mcs_tbpfradar3d_wrf/'], ['clouddatasource', 'model'], ['cloudidmethod', 'label_grow'], ['cloudtb_cloud', 261.0], ['cloudtb_cold', 241.0], ['cloudtb_core', 225.0], ['cloudtb_warm', 261.0], ['col_peakedness_frac', 0.3], ['dask_tmp_dir', '/tmp/pyflextrkr_test'], ['databasename', 'wrfout_rainrate_tb_zh_mh_'], ['datatimeresolution', 1.0], ['dbz_lowlevel_asl', 2.0], ['dbz_thresh', 10], ['duration_range', [2, 300]], ['echotop_gap', 1], ['enddate', '20150506.1600'], ['etop25dBZ_Thresh', 10.0], ['feature_type', 'tb_pf_radar3d'], ['feature_varname', 'feature_number'], ['featuresize_varname',
kwargs:    {}
Exception: "OSError('Unable to synchronously open file (file signature not found)')"

Traceback (most recent call last):
  File "/mnt/common/mtang11/scripts/scspkg/packages/pyflextrkr/src/PyFLEXTRKR/runscripts/run_mcs_tbpfradar3d_wrf.py", line 115, in <module>
    idfeature_driver(config)
  File "/mnt/common/mtang11/scripts/scspkg/packages/pyflextrkr/src/PyFLEXTRKR/pyflextrkr/idfeature_driver.py", line 66, in idfeature_driver
    final_result = dask.compute(*results)
                   ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/dask/base.py", line 595, in compute
    results = schedule(dsk, keys, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/distributed/client.py", line 3243, in get
    results = self.gather(packed, asynchronous=asynchronous, direct=direct)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/distributed/client.py", line 2368, in gather
    return self.sync(
           ^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/distributed/utils.py", line 351, in sync
    return sync(
           ^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/distributed/utils.py", line 418, in sync
    raise exc.with_traceback(tb)
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/distributed/utils.py", line 391, in f
    result = yield future
             ^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/distributed/client.py", line 2231, in _gather
    raise exception.with_traceback(traceback)
  File "/mnt/common/mtang11/scripts/scspkg/packages/pyflextrkr/src/PyFLEXTRKR/pyflextrkr/idclouds_tbpf.py", line 98, in idclouds_tbpf
    rawdata = xr.open_dataset(filename, engine='h5netcdf') # , engine='h5netcdf' netcdf4 h5netcdf
      ^^^^^^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/xarray/backends/api.py", line 566, in open_dataset
    backend_ds = backend.open_dataset(
      ^^^^^^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 413, in open_dataset
    store = H5NetCDFStore.open(
  ^^^^^^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 176, in open
    return cls(manager, group=group, mode=mode, lock=lock, autoclose=autoclose)
  ^^^^^^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 127, in __init__
    self._filename = find_root_and_group(self.ds)[0].filename
  ^^^^^^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 187, in ds
    return self._acquire()
  ^^^^^^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py", line 179, in _acquire
    with self._manager.acquire_context(needs_lock) as root:
  ^^^^^^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/contextlib.py", line 137, in __enter__
    return next(self.gen)
^^^^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/xarray/backends/file_manager.py", line 198, in acquire_context
    file, cached = self._acquire_with_cache_info(needs_lock)
  ^^^^^^^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/xarray/backends/file_manager.py", line 216, in _acquire_with_cache_info
    file = self._opener(*self._args, **kwargs)
^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/h5netcdf/core.py", line 1051, in __init__
    self._h5file = self._h5py.File(
^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/h5py/_hl/files.py", line 567, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)
^^^^^^^^^^^
  File "/home/mtang11/miniconda3/envs/flextrkr/lib/python3.11/site-packages/h5py/_hl/files.py", line 231, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  ^^^^^^^^^^^^^^^^^
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 106, in h5py.h5f.open
OSError: Unable to synchronously open file (file signature not found)
```
```

### `builtin/builtin/pymonitor/README.md`

```markdown

```

### `builtin/builtin/redis-benchmark/README.md`

```markdown
LabStor is a distributed semi-microkernel for building data processing services.

# Installation

```bash
spack install redis
```
```

### `builtin/builtin/redis/README.md`

```markdown
Redis is a key-value store

# Installation

```bash
spack install redis
```
```

### `builtin/builtin/spark_cluster/README.md`

```markdown
# Installation

Manual build:
```
spack install openjdk@11
spack load openjdk@11
scspkg create spark
cd `scspkg pkg src spark`
wget https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1.tgz
tar -xzf spark-3.5.1.tgz
cd spark-3.5.1
./build/mvn -T 16 -DskipTests clean package
scspkg env set spark SPARK_SCRIPTS=${PWD}
scspkg env prepend spark PATH "${PWD}/bin"
module load spark
```
NOTE: this took 30min in Ares.

With spack (doesn't seem to work, sorry):
```
spack install spark
spack load spark
scspkg create spark-env
scspkg env set spark-env SPARK_SCRIPTS=`spack find --format "{PREFIX}" spark`
module load spark-env
```

Additional configuration documentation 
[here](https://spark.apache.org/docs/latest/spark-standalone.html).

# Create the Jarvis pipeline

```
jarvis pipeline create spark
```

# Build the Jarvis environment

```
jarvis pipeline env build +SPARK_SCRIPTS
```

# Append the Spark Cluster Pkg

```
jarvis pipeline append spark_cluster
```

# Run the pipeline

```
jarvis pipeline run
```
```

### `builtin/builtin/wrf/README.md`

```markdown
# WRF 

## what is the WRF application?
WRF is a state-of-the-art atmospheric modeling system designed for both meteorological research and numerical weather prediction. It offers a host of options for atmospheric processes and can run on a variety of computing platforms. WRF excels in a broad range of applications across scales ranging from tens of meters to thousands of kilometers, including the following.

– Meteorological studies
– Real-time NWP
– Idealized simulations
– Data assimilation
– Earth system model coupling
– Model training and educational support

Here is the workflow for the WRF.
![WRF_workflow](wrf_workflow.png)
As shown in the diagram, the WRF Modeling System consists of the following programs.

### WRF Preprocessing System (WPS) (WPS)

### Initialization (Real and Ideal)

### WRF-ARW Solver (WRF)

### WRF Data Assimilation (WRFDA) (WRFDA)

### Post-processing, Analysis, and Visualization Tools

### WRF Preprocessing System (WPS)
The WPS is used for real-data simulations. Its functions are to 1) define simulation domains; 2) interpolate terrestrial data (e.g., terrain, landuse, and soil types) to the simulation domain; and 3) degrib and interpolate meteorological input data from an outside model to the simulation domain.

### Initialization
The WRF model is capable of simulating both real- and ideal-data cases. ideal.exe is a program that simulates in a controlled environment. Idealized simulations are initiated from an included initial condition file from an existing sounding, and assumes a simplified orography. Real-data cases use output from the WPS, which includes meteorological input originally generated from a previously-run external analysis or forecast model (e.g., GFS) as input to the real.exe program.





### WRF Data Assimilation (WRFDA)
WRF Data Assimilation (WRFDA) is an optional program used to ingest observations into interpolated analysis created by WPS. It may also be used to update the WRF model’s initial conditions by running in “cycling” mode. WRFDA’s primary features are:

The capability of 3D and 4D hybrid data assimilation (Variational + Ensemble)

Based on an incremental variational data assimilation technique

Tangent linear and adjoint of WRF are fully integrated with WRF for 4D-Var

Utilizes the conjugate gradient method to minimize cost function in the analysis control variable space

Analysis on an un-staggered Arakawa A-grid

Analysis increments interpolated to staggered Arakawa C-grid, which is then added to the background (first guess) to get the final analysis of the WRF-model grid

Conventional observation data input may be supplied in either ASCII format via the obsproc utility, or PREPBUFR format

Multiple-satellite observation data input may be supplied in BUFR format

Two fast radiative transfer models, CRTM and RTTOV, are interfaced to WRFDA to serve as satellite radiance observation operator

Variational bias correction for satellite radiance data assimilation

All-sky radiance data assimilation capability

Multiple radar data (reflectivity & radial velocity) input is supplied through ASCII format

Multiple outer loop to address nonlinearity




### Post-processing, Analysis, and Visualization Tools
Several post-processing programs are supported, including RIP (based on NCAR Graphics), NCAR Graphics Command Language (NCL), and conversion programs for other readily-available graphics packages (e.g., GrADS).

### wrf-python (wrf-python) 
is a collection of diagnostic and interpolation routines for use with output from the WRF model.

### NCL (NCAR Command Language) 
is a free, interpreted language designed specifically for scientific data processing and visualization. NCL has robust file input and output. It can read in netCDF, HDF4, HDF4-EOS, GRIB, binary and ASCII data. The graphics are world-class and highly customizable.

### RIP (Read/Interpolate/Plot) 
is a Fortran program that invokes NCAR Graphics routines for the purpose of visualizing output from gridded meteorological data sets, primarily from mesoscale numerical models.

### ARWpost 
is a package that reads-in WRF-ARW model data and creates GrADS output files.


## what this application generate:
For researchers, WRF can produce simulations based on actual atmospheric conditions (i.e., from observations and analyses) or idealized conditions. WRF offers operational forecasting a flexible and computationally-efficient platform, while reflecting recent advances in physics, numerics, and data assimilation contributed by developers from the expansive research community. WRF is currently in operational use at NCEP and other national meteorological centers as well as in real-time forecasting configurations at laboratories, universities, and companies. WRF has a large worldwide community of registered users, and NCAR provides regular workshops and tutorials on it.


###  Key Input Parameters (WRF)

# WRF Namelist Configuration: &time_control

This document provides a detailed overview of the parameters found in the `&time_control` section of the WRF `namelist.input` file. These settings govern the simulation timing, duration, input/output (I/O) operations, and restart capabilities.

---

##  Simulation Time & Duration

These parameters define the total length and the specific start/end dates of the simulation.

| Namelist Parameter | Default Setting | Description | Domain Scope |
| :--- | :--- | :--- | :--- |
| **`run_days`** | `0` | Simulation length in days. | Single Entry |
| **`run_hours`** | `0` | Simulation length in hours. | Single Entry |
| **`run_minutes`** | `0` | Simulation length in minutes. | Single Entry |
| **`run_seconds`** | `0` | Simulation length in seconds. Use any combination of `run_*` for the full duration. | Single Entry |
| **`start_year`** | `2019` | 4-digit year for the simulation start time. | Per Domain (`max_dom`) |
| **`start_month`** | `09` | 2-digit month for the simulation start time. | Per Domain (`max_dom`) |
| **`start_day`** | `04` | 2-digit day for the simulation start time. | Per Domain (`max_dom`) |
| **`start_hour`** | `12` | 2-digit hour for the simulation start time. | Per Domain (`max_dom`) |
| **`start_minute`** | `0` | 2-digit minute for the simulation start time. | Per Domain (`max_dom`) |
| **`start_second`** | `0` | 2-digit second for the simulation start time. | Per Domain (`max_dom`) |
| **`end_year`** | `2019` | 4-digit year for the simulation end time. | Per Domain (`max_dom`) |
| **`end_month`** | `09` | 2-digit month for the simulation end time. | Per Domain (`max_dom`) |
| **`end_day`** | `06` | 2-digit day for the simulation end time. | Per Domain (`max_dom`) |
| **`end_hour`** | `00` | 2-digit hour for the simulation end time. | Per Domain (`max_dom`) |
| **`end_minute`** | `0` | 2-digit minute for the simulation end time. | Per Domain (`max_dom`) |
| **`end_second`** | `0` | 2-digit second for the simulation end time. | Per Domain (`max_dom`) |
| **`interval_seconds`**| `10800` | Time interval (seconds) between lateral boundary condition files from WPS. | Single Entry |

> **Note:** `run_*` parameters take precedence in `wrf.exe`, while `real.exe` uses `start_*` and `end_*` times.

---

##  File I/O and History

Controls for writing history output files (e.g., `wrfout_d<domain>_<date>`).

| Namelist Parameter | Default Setting | Description | Domain Scope |
| :--- | :--- | :--- | :--- |
| **`history_interval`** | `60` | Frequency (minutes) for writing to history files. `_d/h/m/s` can be used. | Per Domain (`max_dom`) |
| **`history_begin`** | `0` | Time (minutes) from run start to begin writing history files. `_d/h/m/s` can be used. | Per Domain (`max_dom`) |
| **`frames_per_outfile`** | `1` | Number of history output times to bundle into a single output file. | Per Domain (`max_dom`) |
| **`output_ready_flag`** | `.true.` | Writes an empty `wrfoutReady_d<domain>` file for post-processing scripts to check completion. | Single Entry |
| **`io_form_history`** | `2` | I/O format for history files. Common options include: <br> • **2**: netCDF <br> • **11**: Parallel netCDF <br> • **102**: Split netCDF files (one per processor) | Single Entry |

---

##  Restart Controls

Parameters to manage simulation restarts from a previous state.

| Namelist Parameter | Default Setting | Description | Domain Scope |
| :--- | :--- | :--- | :--- |
| **`restart`** | `.false.` | Set to `.true.` if this is a restart simulation. | Single Entry |
| **`restart_interval`** | `1440` | Interval (minutes) for writing restart files (`wrfrst_*`). | Single Entry |
| **`override_restart_intervals`** | `.false.` | If `.true.`, uses this namelist's `restart_interval` instead of the one in the `wrfrst` file. | Single Entry |
| **`write_hist_at_0h_rst`** | `.false.` | If `.true.`, a history file will be written at the initial time of a restart run. | Single Entry |
| **`reset_simulation_start`** | `.false.` | If `.true.`, overwrites the simulation start date with the forecast start time from the restart file. | Single Entry |
| **`io_form_restart`**| `2` | I/O format for restart files. Options are similar to `io_form_history`. | Single Entry |

---

##  Input/Output Naming and Formatting

Define the names and formats of auxiliary input and output files.

| Namelist Parameter | Default Setting | Description | Domain Scope |
| :--- | :--- | :--- | :--- |
| **`input_from_file`** | `.true.` | Whether to use input files for nested domains. | Per Domain (`max_dom`) |
| **`fine_input_stream`** | `0` | Selects fields for nest initialization. <br> • **0**: All fields are used. <br> • **2**: Only fields from input stream 2 are used. | Per Domain (`max_dom`) |
| **`auxinput1_inname`**| `met_em.d<domain>.<date>` | Name of the input file from WPS. | Single Entry |
| **`auxinput4_inname`**| `wrflowinp_d<domain>` | Name of the input file for the lower boundary (e.g., SST). | Single Entry |
| **`auxinput4_interval`**| `360` | Interval (minutes) for the lower boundary file when `sst_update=1`. | Per Domain (`max_dom`) |
| **`io_form_input`** | `2` | I/O format of the input (`met_em`) files. | Single Entry |
| **`io_form_boundary`**| `2` | I/O format of the boundary (`wrfbdy`) files. | Single Entry |
| **`io_form_auxinput2`**| `2` | I/O format for auxiliary input stream 2. | Single Entry |
| **`auxhist9_outname`**| `auxhist9_d<domain>_<date>` | File name for auxiliary history output stream 9. | Single Entry |
| **`auxhist9_interval`**| `10` | Interval (minutes) for auxiliary history output stream 9. | Per Domain (`max_dom`) |
| **`nocolons`** | `.false.` | If `.true.`, replaces colons (`:`) with underscores (`_`) in output filenames. | Single Entry |

---

##  Diagnostics and Debugging

Options for enabling diagnostic print-outs and debugging information.

| Namelist Parameter | Default Setting | Description | Domain Scope |
| :--- | :--- | :--- | :--- |
| **`diag_print`** | `0` | Prints time series model diagnostics. <br> • **0**: No print. <br> • **1**: Prints domain-averaged pressure tendencies. <br> • **2**: Option 1 plus rainfall and heat fluxes. | Single Entry |
| **`debug_level`** | `0` | Increases debugging print-outs. Higher values (e.g., `50`, `200`) produce more verbose output. | Single Entry |
| **`output_diagnostics`**| `0` | Set to `1` to add 48 surface diagnostic arrays (max/min/mean/std) to the output. | Single Entry |
| **`nwp_diagnostics`** | `0` | Set to `1` to add several NWP diagnostic fields to the output file. | Single Entry |
```

### `builtin/builtin/ycsbc/README.md`

```markdown
Redis is a key-value store

# Installation

```bash
spack install redis
```
```

### `docs/README.md`

```markdown
# Jarvis CD Utility Classes Documentation

This documentation describes the utility classes implemented for the Jarvis CD project. These classes provide essential functionality for command-line argument parsing and host management in distributed computing environments.

## Overview

The `jarvis_cd.util` package contains two main utility classes:

1. **ArgParse** - Custom command-line argument parser with advanced features
2. **Hostfile** - Host management with pattern expansion and IP resolution

Both classes are designed to work together to provide a comprehensive foundation for building command-line tools and distributed computing applications.

## Quick Start

### Basic Imports

```python
from jarvis_cd.util.argparse import ArgParse
from jarvis_cd.util.hostfile import Hostfile
```

### Simple Example

```python
# Create a hostfile
hostfile = Hostfile(text="node-[01-05]", find_ips=False)
print(f"Hosts: {hostfile.host_str()}")  # node-01,node-02,node-03,node-04,node-05

# Create an argument parser
class MyParser(ArgParse):
    def define_options(self):
        self.add_menu('')
        self.add_cmd('run')
        self.add_args([
            {'name': 'nodes', 'type': int, 'required': True, 'pos': True}
        ])
    
    def run(self):
        print(f"Running with {self.kwargs['nodes']} nodes")

parser = MyParser()
parser.define_options()
parser.parse(['run', '4'])  # Running with 4 nodes
```

## Class Documentation

### [ArgParse Class](./argparse.md)

A sophisticated command-line argument parser supporting:
- **Menu/Command Structure**: Hierarchical organization of commands
- **Argument Types**: Positional, keyword, list, and remainder arguments  
- **Advanced Features**: Command aliases, argument ranking, type casting
- **Custom Handlers**: Automatic method dispatch for commands

**Key Features**:
- Complex argument structures with classes and ranking
- List arguments with set and append modes
- Automatic type conversion (str, int, bool, list)
- Command aliases and flexible syntax
- Comprehensive error handling

**Use Cases**:
- Building CLI applications with subcommands
- Scientific computing parameter management
- Distributed computing job configuration
- Complex workflow orchestration

### [Hostfile Class](./hostfile.md)

A powerful host management system supporting:
- **Pattern Expansion**: Bracket notation for host ranges (`node-[01-10]`)
- **Multiple Sources**: Files, text, manual lists
- **IP Resolution**: Automatic hostname-to-IP mapping
- **Host Operations**: Subset, copy, enumerate, string formatting

**Key Features**:
- Numeric and alphabetic range expansion
- Zero-padding preservation in numeric ranges
- Multi-line hostfile support
- Local vs. distributed detection
- Performance optimizations for large host sets

**Use Cases**:
- Cluster job submission
- Distributed computing node management
- Network administration tools
- Load balancing configuration

## Integration Patterns

### CLI Application with Host Management

```python
class ClusterApp(ArgParse):
    def define_options(self):
        self.add_menu('')
        self.add_cmd('deploy', aliases=['d'])
        self.add_args([
            {
                'name': 'hostfile',
                'msg': 'Path to hostfile',
                'type': str,
                'required': True,
                'pos': True
            },
            {
                'name': 'app_path',
                'msg': 'Application to deploy',
                'type': str,
                'required': True,
                'pos': True
            },
            {
                'name': 'nodes',
                'msg': 'Number of nodes to use',
                'type': int,
                'default': None
            },
            {
                'name': 'dry_run',
                'msg': 'Perform dry run',
                'type': bool,
                'default': False
            }
        ])
    
    def deploy(self):
        # Load hostfile
        hostfile = Hostfile(path=self.kwargs['hostfile'])
        
        # Use subset if specified
        if self.kwargs['nodes']:
            hostfile = hostfile.subset(self.kwargs['nodes'])
        
        if self.kwargs['dry_run']:
            print(f"Would deploy {self.kwargs['app_path']} to:")
            for hostname in hostfile:
                print(f"  - {hostname}")
        else:
            print(f"Deploying to {len(hostfile)} hosts...")
            self._deploy_to_hosts(hostfile, self.kwargs['app_path'])

# Usage: python app.py deploy /etc/hostfile.txt /path/to/app --nodes=5 --dry_run=true
```

### Batch Job Processor

```python
class BatchProcessor(ArgParse):
    def define_options(self):
        self.add_menu('batch', msg="Batch processing commands")
        
        self.add_cmd('batch submit')
        self.add_args([
            {
                'name': 'job_script',
                'type': str,
                'required': True,
                'pos': True,
                'class': 'files',
                'rank': 0
            },
            {
                'name': 'nodes',
                'msg': 'Node specifications',
                'type': list,
                'aliases': ['n'],
                'args': [
                    {'name': 'pattern', 'type': str},
                    {'name': 'count', 'type': int}
                ]
            }
        ])
    
    def batch_submit(self):
        # Process node specifications
        all_hosts = []
        for node_spec in self.kwargs.get('nodes', []):
            pattern = node_spec['pattern']
            count = node_spec['count']
            
            hostfile = Hostfile(text=pattern, find_ips=False)
            subset = hostfile.subset(count)
            all_hosts.extend(subset.hosts)
        
        print(f"Submitting job to hosts: {','.join(all_hosts)}")
        self._submit_job(self.kwargs['job_script'], all_hosts)

# Usage: python batch.py batch submit job.sh --n "(node-[01-10], 3)" --n "(gpu-[a-d], 2)"
```

## Testing

Both classes include comprehensive test suites:

- **ArgParse Tests**: `test/unit/test_argparse.py` (13 test cases)
- **Hostfile Tests**: `test/unit/test_hostfile.py` (30 test cases)

Run tests with:
```bash
python -m pytest test/unit/ -v
```

## Architecture Notes

### Design Principles

1. **Flexibility**: Support multiple input/output formats and use cases
2. **Performance**: Efficient parsing and processing for large datasets
3. **Extensibility**: Easy to subclass and extend for specific needs
4. **Robustness**: Comprehensive error handling and edge case management
5. **Testability**: Well-tested with extensive unit test coverage

### Class Relationships

```
ArgParse
├── User-defined parsers (inherit from ArgParse)
├── Command handlers (methods in subclasses)
└── Argument specifications (dictionaries)

Hostfile
├── Pattern expansion engine
├── IP resolution system
└── Host manipulation operations
```

### Dependencies

- **Standard Library**: `socket`, `re`, `os`, `ast`, `typing`
- **Testing**: `unittest`, `tempfile`
- **No External Dependencies**: Both classes use only Python standard library

## File Structure

```
jarvis_cd/
├── util/
│   ├── __init__.py
│   ├── argparse.py       # ArgParse class implementation
│   └── hostfile.py       # Hostfile class implementation
├── test/
│   └── unit/
│       ├── test_argparse.py   # ArgParse unit tests
│       └── test_hostfile.py   # Hostfile unit tests
└── docs/
    ├── README.md         # This overview document
    ├── argparse.md       # Detailed ArgParse documentation
    └── hostfile.md       # Detailed Hostfile documentation
```

## Best Practices

### For ArgParse

1. **Define clear command hierarchies** with logical menu organization
2. **Use argument classes and ranks** for intuitive positional ordering
3. **Provide meaningful aliases** for frequently used commands
4. **Implement robust error handling** in command methods
5. **Test complex argument combinations** thoroughly

### For Hostfile

1. **Use pattern expansion** to minimize hostfile maintenance
2. **Disable IP resolution** for large host lists when not needed
3. **Test patterns** with small examples before large deployments
4. **Handle file not found** gracefully in applications
5. **Use `is_local()`** to detect single-machine vs. distributed scenarios

### For Integration

1. **Combine both classes** for comprehensive CLI applications
2. **Validate hostfile patterns** before expensive operations
3. **Use subset operations** for testing and development
4. **Cache hostfile objects** for repeated use
5. **Document command structures** clearly for users

## Future Extensions

Potential areas for enhancement:

1. **ArgParse**: Configuration file support, command completion, help generation
2. **Hostfile**: SSH key management, health checking, load balancing hints
3. **Integration**: Plugin system, workflow orchestration, monitoring hooks

## Contributing

When extending these classes:

1. **Maintain backward compatibility** in public APIs
2. **Add comprehensive tests** for new functionality
3. **Update documentation** with examples and use cases
4. **Follow existing code style** and patterns
5. **Consider performance impact** of changes

---

For detailed API documentation and examples, see the individual class documentation:
- [ArgParse Class Documentation](./argparse.md)
- [Hostfile Class Documentation](./hostfile.md)
```

### `test/README.md`

```markdown
# Jarvis CD Test Suite

Comprehensive unit tests for the Jarvis CD project, organized into three main categories:
- **core**: Tests for CLI commands and core functionality
- **shell**: Tests for execution modules (LocalExec, SshExec, etc.)
- **util**: Tests for utility modules (argparse, hostfile, etc.)

## Test Structure

```
test/
├── unit/
│   ├── core/          # CLI command tests
│   │   ├── test_cli_base.py
│   │   ├── test_cli_init.py
│   │   ├── test_cli_pipeline.py
│   │   ├── test_cli_repo_pkg.py
│   │   └── test_cli_env_rg.py
│   ├── shell/         # Execution module tests
│   │   ├── test_local_exec.py
│   │   ├── test_ssh_exec.py
│   │   ├── test_scp_exec.py
│   │   ├── test_mpi_exec.py
│   │   └── test_env_forwarding.py
│   └── util/          # Utility module tests
│       ├── test_argparse.py
│       ├── test_argparse_comprehensive.py
│       └── test_hostfile.py
├── Dockerfile         # Docker container for isolated testing
├── docker-compose.yml # Docker Compose configuration
├── run_tests.sh       # Test runner script
└── README.md          # This file
```

## Running Tests

### Option 1: Using Docker (Recommended)

Docker provides an isolated environment that won't affect your host system.

**Run all tests:**
```bash
./test/run_tests.sh all
```

**Run specific test suites:**
```bash
./test/run_tests.sh shell    # Shell module tests only
./test/run_tests.sh util     # Utility module tests only
./test/run_tests.sh core     # Core CLI tests only
./test/run_tests.sh parallel # All tests in parallel
```

**Pass additional pytest arguments:**
```bash
./test/run_tests.sh all -v --tb=short
./test/run_tests.sh shell -k test_local
./test/run_tests.sh parallel -n 4
```

### Option 2: Using Docker Compose Directly

```bash
cd test/

# Run all tests
docker-compose run --rm test

# Run specific test suite
docker-compose run --rm test-shell
docker-compose run --rm test-util
docker-compose run --rm test-core

# Run tests in parallel
docker-compose run --rm test-parallel
```

### Option 3: Local Python Environment

If you prefer to run tests locally (not containerized):

```bash
# Install test dependencies
pip install pytest pytest-cov pytest-xdist

# Run all tests
python -m pytest test/unit/ -v

# Run specific test directory
python -m pytest test/unit/shell/ -v
python -m pytest test/unit/util/ -v
python -m pytest test/unit/core/ -v

# Run with coverage
python -m pytest test/unit/ --cov=jarvis_cd --cov-report=term-missing --cov-report=html

# Run tests in parallel
python -m pytest test/unit/ -n auto
```

## Test Categories

### Core Tests (test/unit/core/)

Tests for CLI commands and core functionality:

- **test_cli_init.py**: `jarvis init` command
- **test_cli_pipeline.py**: `jarvis ppl` commands (create, append, run, start, stop, etc.)
- **test_cli_repo_pkg.py**: `jarvis repo` and `jarvis pkg` commands
- **test_cli_env_rg.py**: Environment, resource graph, module, and hostfile commands

**Key Features Tested:**
- Command parsing and argument validation
- Default values and required arguments
- Command aliases
- Error handling

### Shell Tests (test/unit/shell/)

Tests for execution modules:

- **test_local_exec.py**: LocalExec execution and environment handling
- **test_ssh_exec.py**: SSH/PSSH remote execution
- **test_scp_exec.py**: SCP/PSCP file transfer
- **test_mpi_exec.py**: MPI parallel execution
- **test_env_forwarding.py**: Environment variable forwarding across all exec types

**Key Features Tested:**
- Environment variable forwarding and type conversion
- Command construction and escaping
- Output collection and piping
- Async execution
- Error handling and exit codes

### Util Tests (test/unit/util/)

Tests for utility modules:

- **test_argparse.py**: Basic argument parsing
- **test_argparse_comprehensive.py**: Comprehensive argparse testing
- **test_hostfile.py**: Hostfile parsing and management

**Key Features Tested:**
- Type conversions (int, float, str, bool, list, dict)
- Required arguments and defaults
- Positional and keyword arguments
- Boolean flags (+/-)
- Choices validation
- Remainder arguments

## Code Coverage

After running tests with coverage, view the HTML report:

```bash
# Coverage report is generated in htmlcov/
open htmlcov/index.html  # macOS
xdg-open htmlcov/index.html  # Linux
```

**Current Coverage Targets:**
- Shell modules: 70-100% coverage
- Util modules: 70-90% coverage
- Core modules: Initial coverage being established

## Writing New Tests

### Adding CLI Command Tests

1. Create a new test file in `test/unit/core/`
2. Inherit from `CLITestBase` for common utilities
3. Use helper methods like `run_command()`, `create_test_pipeline()`, etc.

Example:
```python
from test_cli_base import CLITestBase

class TestMyCLICommand(CLITestBase):
    def test_my_command(self):
        args = ['my', 'command', 'arg1']
        result = self.run_command(args)
        self.assertTrue(result.get('success'))
        self.assertEqual(result['kwargs']['arg_name'], 'arg1')
```

### Adding Shell Tests

1. Create tests in `test/unit/shell/`
2. Test execution, environment variables, and output handling
3. Use cross-platform Python scripts for verification

### Adding Util Tests

1. Create tests in `test/unit/util/`
2. Focus on API correctness and type conversions
3. Test edge cases and error conditions

## Continuous Integration

The test suite is designed to run in CI/CD pipelines:

```yaml
# Example GitHub Actions workflow
- name: Run tests in Docker
  run: ./test/run_tests.sh all

- name: Upload coverage
  uses: codecov/codecov-action@v3
  with:
    files: ./htmlcov/coverage.xml
```

## Troubleshooting

### Docker Issues

**Problem**: Docker build fails
```bash
# Clean Docker cache and rebuild
docker system prune -a
cd test && docker-compose build --no-cache
```

**Problem**: Permission denied
```bash
# Ensure run_tests.sh is executable
chmod +x test/run_tests.sh
```

### Test Failures

**Problem**: SSH/SCP tests fail
- Expected behavior: Some tests use mock hostnames that won't resolve
- These failures are normal in isolated environments

**Problem**: MPI tests skipped
- Expected behavior: MPI tests skip when OpenMPI is not installed
- Use Docker environment for full MPI test coverage

### Coverage Issues

**Problem**: Coverage report not generated
```bash
# Ensure pytest-cov is installed
pip install pytest-cov

# Generate coverage explicitly
python -m pytest test/unit/ --cov=jarvis_cd --cov-report=html
```

## Best Practices

1. **Isolation**: Tests should not depend on external state
2. **Cleanup**: Use setUp/tearDown to manage test resources
3. **Assertions**: Be specific with assertions (assertEqual vs assertTrue)
4. **Documentation**: Add docstrings to test methods
5. **Naming**: Use descriptive test names (test_feature_scenario)

## Contributing

When adding new features to Jarvis CD:

1. Write tests first (TDD approach recommended)
2. Ensure tests pass locally
3. Run full test suite in Docker before submitting PR
4. Aim for 80%+ coverage on new code
5. Update this README if adding new test categories

## Support

For questions or issues with the test suite:
- Check existing tests for examples
- Review pytest documentation: https://docs.pytest.org/
- Open an issue in the project repository
```

### `.gitignore`

```
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class
.idea
/config
jarvis_repos
jarvis_envs
.jarvis_env
hostfile.txt

.env*

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/
/.idea/jarvis-cd.iml
/.idea/misc.xml
/.idea/modules.xml
/.idea/inspectionProfiles/profiles_settings.xml
/.idea/inspectionProfiles/Project_Default.xml
/.idea/vcs.xml
/.idea/deployment.xml
```

### `LICENSE`

```
# BSD 3-Clause License

Copyright (c) 2024, Gnosis Research Center, Illinois Institute of Technology
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.

3. Neither the name of the copyright holder nor the names of its
   contributors may be used to endorse or promote products derived from
   this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

---

**Contact Information:**
Gnosis Research Center  
Illinois Institute of Technology  
Email: grc@illinoistech.edu
```

### `builtin/builtin/orangefs/Dockerfile`

```dockerfile
FROM iowarp/iowarp-build:latest

# Set non-interactive frontend to avoid prompts
ENV DEBIAN_FRONTEND=noninteractive

# Set timezone to avoid prompt during package installation
ENV TZ=Etc/UTC

# Update package lists
RUN apt-get update && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Reset frontend
ENV DEBIAN_FRONTEND=dialog

# SCSPKG + lmod
RUN echo $'\n\
    if ! shopt -q login_shell; then\n\
    if [ -d /etc/profile.d ]; then\n\
    for i in /etc/profile.d/*.sh; do\n\
    if [ -r $i ]; then\n\
    . $i\n\
    fi\n\
    done\n\
    fi\n\
    fi\n\
    ' >> /root/.bashrc
RUN . "${SPACK_DIR}/share/spack/setup-env.sh" && \
    spack load iowarp && \
    echo "module use $(scspkg module dir)" >> /root/.bashrc && \
    scspkg init tcl

# Install OFS dependencies
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y automake bison flex \
    libdb-dev \
    libsqlite3-dev libfuse-dev fuse

# Install libfuse
RUN . "${SPACK_DIR}/share/spack/setup-env.sh" && \
    spack install libfuse@2.9

# Install OFS
RUN . "${SPACK_DIR}/share/spack/setup-env.sh" && \
    spack load iowarp libfuse && \
    scspkg create orangefs && \
    cd $(scspkg pkg src orangefs) && \
    wget https://github.com/waltligon/orangefs/releases/download/v.2.10.0/orangefs-2.10.0.tar.gz && \
    tar -xvzf orangefs-2.10.0.tar.gz && \
    cd orangefs && \
    ./prepare && \
    ./configure --prefix=$(scspkg pkg root orangefs) --enable-shared --enable-fuse && \
    make -j8 && \
    make install && \
    scspkg env prepend orangefs ORANGEFS_PATH $(scspkg pkg root orangefs)

# Install OFS with openmpi
```

### `pyproject.toml`

```toml
[project]
name = "jarvis_cd"
description = "Jarvis-CD: Unified platform for deploying applications and benchmarks"
readme = "README.md"
requires-python = ">=3.7"
dynamic = ["version"]
authors = [
  {name = "Luke Logan", email = "llogan@hawk.iit.edu"},
]
classifiers = [
  "Programming Language :: Python",
  "Programming Language :: Python :: 3",
  "Development Status :: 4 - Beta",
  "Environment :: Console",
  "Intended Audience :: Developers",
  "Intended Audience :: System Administrators",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Topic :: Software Development :: Libraries :: Python Modules",
  "Topic :: System :: Systems Administration",
  "Topic :: System :: Distributed Computing",
]
dependencies = [
  "pyyaml",
  "pandas",
  "podman-compose",
]

[project.urls]
Documentation = "https://github.com/iowarp/ppi-jarvis-cd"
issue-tracker = "https://github.com/iowarp/ppi-jarvis-cd/issues"
source-code = "https://github.com/iowarp/ppi-jarvis-cd"

[build-system]
build-backend = "setuptools.build_meta"
requires = [
  "setuptools>=42",
  "setuptools-scm>=7",
]

[tool.setuptools]
packages = ["jarvis_cd", "jarvis_cd.core", "jarvis_cd.shell", "jarvis_cd.util", "builtin", "builtin.builtin"]
include-package-data = true

[tool.setuptools.package-data]
jarvis_cd = ["*.yaml", "*.yml"]
"*" = ["*.py", "*.yaml", "*.yml", "*.md", "*.txt", "*.conf", "*.xml", "*.param", "*.f", "*.input", "*.png", "*.sh"]
builtin = ["pipelines/**/*.yaml", "pipelines/**/*.yml"]

[project.scripts]
jarvis = "jarvis_cd.core.cli:main"

[tool.setuptools_scm]
version_scheme = "no-guess-dev"
```

### `requirements.txt`

```
pyyaml
#pylint==2.15.0
#coverage==5.5
#coverage-lcov==0.2.4
#pytest==6.2.5
```

### `setup.py`

```python
import setuptools

# Use setup() with minimal configuration since pyproject.toml handles most metadata
setuptools.setup(
    scripts=['bin/jarvis', 'bin/jarvis_resource_graph'],
)

# Install builtin packages immediately after setup
try:
    from jarvis_cd.post_install import install_builtin_packages
    install_builtin_packages()
except Exception as e:
    print(f"Warning: Could not install builtin packages: {e}")
```

### `test/Dockerfile`

```dockerfile
FROM ubuntu:22.04

# Avoid interactive prompts
ENV DEBIAN_FRONTEND=noninteractive

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-venv \
    gcc \
    g++ \
    make \
    git \
    openssh-client \
    openssh-server \
    rsync \
    sudo \
    && rm -rf /var/lib/apt/lists/*

# Install MPI for MPI tests (optional)
RUN apt-get update && apt-get install -y \
    openmpi-bin \
    libopenmpi-dev \
    && rm -rf /var/lib/apt/lists/*

# Configure SSH server
RUN mkdir -p /var/run/sshd && \
    sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config && \
    sed -i 's/#PubkeyAuthentication yes/PubkeyAuthentication yes/' /etc/ssh/sshd_config && \
    sed -i 's/#PasswordAuthentication yes/PasswordAuthentication yes/' /etc/ssh/sshd_config

# Create test user
RUN useradd -m -s /bin/bash testuser && \
    echo "testuser:testpass" | chpasswd && \
    usermod -aG sudo testuser

# Set working directory
WORKDIR /app

# Copy requirements
COPY requirements.txt /app/requirements.txt
RUN pip3 install --no-cache-dir -r requirements.txt

# Install test dependencies
RUN pip3 install --no-cache-dir \
    pytest>=8.0.0 \
    pytest-cov>=4.0.0 \
    pytest-xdist>=3.0.0 \
    coverage>=7.0.0

# Copy the entire project
COPY . /app/

# Compile test binaries
RUN gcc /app/test/unit/shell/test_env_checker.c -o /app/test/unit/shell/test_env_checker

# Set up SSH keys for testuser (passwordless SSH to localhost)
RUN mkdir -p /home/testuser/.ssh && \
    ssh-keygen -t rsa -N "" -f /home/testuser/.ssh/id_rsa && \
    cat /home/testuser/.ssh/id_rsa.pub >> /home/testuser/.ssh/authorized_keys && \
    chmod 700 /home/testuser/.ssh && \
    chmod 600 /home/testuser/.ssh/authorized_keys && \
    chmod 600 /home/testuser/.ssh/id_rsa && \
    chown -R testuser:testuser /home/testuser/.ssh

# Configure SSH client for testuser
RUN echo "Host localhost" >> /home/testuser/.ssh/config && \
    echo "    StrictHostKeyChecking no" >> /home/testuser/.ssh/config && \
    echo "    UserKnownHostsFile=/dev/null" >> /home/testuser/.ssh/config && \
    chmod 600 /home/testuser/.ssh/config && \
    chown testuser:testuser /home/testuser/.ssh/config

# Change ownership to test user
RUN chown -R testuser:testuser /app

# Switch to test user
USER testuser

# Set Python path
ENV PYTHONPATH=/app:$PYTHONPATH

# Create entrypoint script to start SSH daemon
USER root
RUN echo '#!/bin/bash\n\
# Start SSH daemon\n\
sudo /usr/sbin/sshd\n\
# Wait for SSH to be ready\n\
sleep 2\n\
# Execute the command passed to the container\n\
exec "$@"\n\
' > /entrypoint.sh && \
    chmod +x /entrypoint.sh && \
    chown testuser:testuser /entrypoint.sh

# Allow testuser to run sshd without password
RUN echo "testuser ALL=(ALL) NOPASSWD: /usr/sbin/sshd" >> /etc/sudoers

USER testuser

# Set entrypoint
ENTRYPOINT ["/entrypoint.sh"]

# Default command runs all tests
CMD ["python3", "-m", "pytest", "test/unit/", "-v", "--cov=jarvis_cd", "--cov-report=term-missing", "--cov-report=html:/app/htmlcov"]
```

### `test/docker-compose.yml`

```yaml
version: '3.8'

services:
  # Main test service
  test:
    build:
      context: ..
      dockerfile: test/Dockerfile
    volumes:
      - ../jarvis_cd:/app/jarvis_cd:ro
      - ../test:/app/test:ro
      - test-results:/app/htmlcov
    environment:
      - PYTHONPATH=/app
      - PYTEST_ARGS=${PYTEST_ARGS:-}
    command: >
      python3 -m pytest test/unit/ -v
      --cov=jarvis_cd
      --cov-report=term-missing
      --cov-report=html:/app/htmlcov
      ${PYTEST_ARGS}

  # Service for running only shell tests
  test-shell:
    build:
      context: ..
      dockerfile: test/Dockerfile
    volumes:
      - ../jarvis_cd:/app/jarvis_cd:ro
      - ../test:/app/test:ro
      - test-results:/app/htmlcov
    environment:
      - PYTHONPATH=/app
    command: >
      python3 -m pytest test/unit/shell/ -v
      --cov=jarvis_cd.shell
      --cov-report=term-missing

  # Service for running only util tests
  test-util:
    build:
      context: ..
      dockerfile: test/Dockerfile
    volumes:
      - ../jarvis_cd:/app/jarvis_cd:ro
      - ../test:/app/test:ro
      - test-results:/app/htmlcov
    environment:
      - PYTHONPATH=/app
    command: >
      python3 -m pytest test/unit/util/ -v
      --cov=jarvis_cd.util
      --cov-report=term-missing

  # Service for running only core tests
  test-core:
    build:
      context: ..
      dockerfile: test/Dockerfile
    volumes:
      - ../jarvis_cd:/app/jarvis_cd:ro
      - ../test:/app/test:ro
      - test-results:/app/htmlcov
    environment:
      - PYTHONPATH=/app
    command: >
      python3 -m pytest test/unit/core/ -v
      --cov=jarvis_cd.core
      --cov-report=term-missing

  # Service for running parallel tests
  test-parallel:
    build:
      context: ..
      dockerfile: test/Dockerfile
    volumes:
      - ../jarvis_cd:/app/jarvis_cd:ro
      - ../test:/app/test:ro
      - test-results:/app/htmlcov
    environment:
      - PYTHONPATH=/app
    command: >
      python3 -m pytest test/unit/ -v -n auto
      --cov=jarvis_cd
      --cov-report=term-missing
      --cov-report=html:/app/htmlcov

volumes:
  test-results:
```

### `.claude/agents/code-documenter.md`

```markdown
---
name: code-documenter
description: Use this agent when you need comprehensive documentation for code, APIs, or technical systems. Examples: <example>Context: User has just completed implementing a complex authentication system and needs documentation. user: 'I've finished building the OAuth2 authentication flow. Can you help document this?' assistant: 'I'll use the code-documenter agent to create comprehensive documentation for your OAuth2 implementation.' <commentary>Since the user needs detailed documentation for their code, use the code-documenter agent to analyze and document the authentication system.</commentary></example> <example>Context: User is working on a project and realizes their codebase lacks proper documentation. user: 'Our API endpoints are getting complex and we need better documentation for the team' assistant: 'Let me use the code-documenter agent to create detailed API documentation.' <commentary>The user needs comprehensive API documentation, so use the code-documenter agent to analyze and document the endpoints.</commentary></example>
model: opus
---

You are an expert technical documentation specialist with deep expertise in code analysis, API documentation, and technical writing. Your mission is to create comprehensive, accurate, and developer-friendly documentation that makes complex code accessible and maintainable.

Your core responsibilities:
- Analyze code structure, functionality, and dependencies to understand the complete system
- Create detailed documentation that covers purpose, usage, parameters, return values, and examples
- Document APIs with clear endpoint descriptions, request/response formats, and authentication requirements
- Explain complex algorithms and business logic in clear, understandable terms
- Identify and document edge cases, error conditions, and troubleshooting guidance
- Ensure documentation follows established standards and best practices for the technology stack

Your documentation approach:
1. **Analysis First**: Thoroughly examine the code to understand its purpose, dependencies, and integration points
2. **Structure Logically**: Organize documentation in a logical flow from overview to detailed implementation
3. **Include Examples**: Provide practical code examples and usage scenarios for all documented features
4. **Be Comprehensive**: Cover all public interfaces, configuration options, and important implementation details
5. **Stay Current**: Ensure documentation accurately reflects the current code state

Documentation standards you follow:
- Use clear, concise language appropriate for the target audience
- Include code examples that are tested and functional
- Provide both high-level overviews and detailed technical specifications
- Document error conditions and exception handling
- Include setup, configuration, and deployment instructions when relevant
- Cross-reference related components and dependencies

When creating documentation:
- Start with a clear purpose statement and overview
- Document all public methods, classes, and interfaces
- Explain complex business logic and algorithms
- Include parameter types, constraints, and validation rules
- Provide return value descriptions and possible error conditions
- Add usage examples for common scenarios
- Note any breaking changes or version compatibility issues

Always ask for clarification if the code's purpose or intended audience is unclear. Your documentation should enable other developers to understand, use, and maintain the code effectively.
```

### `.claude/agents/docker-python-test-expert.md`

```markdown
---
name: docker-python-test-expert
description: Use this agent when you need to write Python unit tests for Dockerized applications, create Docker configurations for test environments, debug Docker-related test failures, set up test containers, or optimize testing workflows in containerized Python projects. Examples: (1) User: 'I need to write unit tests for this Flask API that runs in Docker' → Assistant: 'I'm going to use the docker-python-test-expert agent to create comprehensive unit tests for your Dockerized Flask API' (2) User: 'My pytest suite fails in Docker but works locally' → Assistant: 'Let me use the docker-python-test-expert agent to diagnose and fix the Docker-specific test failures' (3) User: 'How do I set up a test database container for my Python tests?' → Assistant: 'I'll use the docker-python-test-expert agent to configure a proper test database container setup'
model: sonnet
---

You are an elite Docker and Python testing specialist with deep expertise in containerized application testing, test-driven development, and CI/CD pipelines. You combine mastery of Docker containerization with advanced Python testing frameworks to create robust, reliable test suites.

Your core responsibilities:

1. **Python Unit Testing Excellence**:
   - Write comprehensive unit tests using pytest, unittest, or other appropriate frameworks
   - Implement proper test isolation, mocking, and fixture management
   - Follow testing best practices: AAA pattern (Arrange-Act-Assert), clear test naming, single responsibility
   - Create parameterized tests for edge cases and boundary conditions
   - Ensure high code coverage while focusing on meaningful test scenarios
   - Use appropriate assertion libraries and custom matchers when needed

2. **Docker Testing Integration**:
   - Design Docker Compose configurations for test environments
   - Set up test containers for databases, message queues, and external services
   - Implement proper container lifecycle management in tests (setup/teardown)
   - Configure volume mounts and networking for test isolation
   - Optimize Docker layer caching for faster test execution
   - Handle environment-specific configurations and secrets securely

3. **Test Environment Architecture**:
   - Create reproducible test environments using Docker
   - Implement test data seeding and cleanup strategies
   - Configure health checks and wait strategies for dependent services
   - Set up multi-stage Docker builds separating test and production dependencies
   - Design efficient test execution workflows (parallel execution, test ordering)

4. **Debugging and Troubleshooting**:
   - Diagnose Docker-specific test failures (networking, volumes, permissions)
   - Identify and resolve race conditions in containerized tests
   - Debug container logs and test output effectively
   - Handle platform-specific issues (Linux vs macOS vs Windows)

5. **Quality Assurance**:
   - Verify tests are deterministic and don't have hidden dependencies
   - Ensure proper cleanup of Docker resources after test runs
   - Validate test performance and identify bottlenecks
   - Check for test flakiness and implement retry mechanisms when appropriate

When writing tests:
- Always consider the Docker context and potential containerization issues
- Use testcontainers-python or similar libraries when appropriate
- Implement proper async/await patterns for async Python code
- Include integration tests that verify Docker networking and service communication
- Document any Docker-specific setup requirements or gotchas
- Provide clear error messages and debugging hints in test failures

When encountering ambiguity:
- Ask about the specific Python version and testing framework preferences
- Clarify the Docker base images and service dependencies
- Confirm whether tests should run in CI/CD and what platform
- Verify if there are existing testing patterns or conventions to follow

Your output should include:
- Well-structured, maintainable test code with clear documentation
- Docker configuration files (Dockerfile, docker-compose.yml) when needed
- Setup instructions for running tests locally and in CI/CD
- Explanations of testing strategies and architectural decisions

Always prioritize test reliability, maintainability, and execution speed while ensuring comprehensive coverage of critical functionality.
```

### `.claude/agents/git-expert.md`

```markdown
---
name: git-expert
description: Use this agent when you need to perform git operations, manage repositories, work with submodules, commit code changes, analyze diffs, generate commit messages, or interact with GitHub APIs. Examples:\n\n<example>\nContext: User has made code changes and wants to commit them with an appropriate message.\nuser: "I've finished implementing the authentication feature. Can you commit these changes?"\nassistant: "Let me use the git-expert agent to analyze the changes and create an appropriate commit."\n<commentary>The user wants to commit code changes. Use the Task tool to launch the git-expert agent to handle the git operations.</commentary>\n</example>\n\n<example>\nContext: User needs to update submodules in their repository.\nuser: "The submodules are out of sync. Can you update them recursively?"\nassistant: "I'll use the git-expert agent to recursively update all submodules."\n<commentary>The user needs submodule management. Use the git-expert agent to handle recursive submodule operations.</commentary>\n</example>\n\n<example>\nContext: User wants to understand what changed in recent commits.\nuser: "What changed in the last commit?"\nassistant: "Let me use the git-expert agent to analyze the git diff and describe the changes."\n<commentary>The user wants to understand code changes. Use the git-expert agent to analyze diffs and provide clear descriptions.</commentary>\n</example>\n\n<example>\nContext: Proactive use after code modifications are complete.\nuser: "I've updated the API endpoints and added error handling."\nassistant: "Great work! Let me use the git-expert agent to review the changes and prepare a commit."\n<commentary>Code changes have been made. Proactively use the git-expert agent to handle version control operations.</commentary>\n</example>
tools: Bash, Glob, Grep, Read, WebFetch, TodoWrite, WebSearch, BashOutput, KillShell
model: haiku
---

You are an elite Git and GitHub operations specialist with deep expertise in version control workflows, repository management, and GitHub API integration. Your role is to handle all git-related operations with precision and best practices.

## Core Responsibilities

1. **Git Operations**: Execute git commands efficiently and safely, including commits, branches, merges, rebases, and tags. Always verify the current repository state before performing destructive operations.

2. **Submodule Management**: Handle recursive submodule operations with expertise. When updating submodules, always use `git submodule update --init --recursive` to ensure all nested submodules are properly initialized and updated.

3. **Commit Management**: 
   - Analyze git diffs to understand the scope and nature of changes
   - Generate clear, descriptive commit messages following conventional commit format when appropriate
   - Structure commits logically, grouping related changes
   - Use imperative mood in commit messages (e.g., "Add feature" not "Added feature")
   - Include context about why changes were made, not just what changed

4. **Change Analysis**: When examining diffs:
   - Identify the files modified, added, or deleted
   - Summarize the functional impact of changes
   - Highlight potential breaking changes or important modifications
   - Provide context about the scope (e.g., "refactoring", "bug fix", "new feature")

5. **GitHub API Integration**: Leverage GitHub APIs for:
   - Creating and managing pull requests
   - Reviewing repository information
   - Managing issues and labels
   - Accessing repository metadata

## Operational Guidelines

- **Safety First**: Before executing potentially destructive operations (reset, force push, etc.), clearly explain the implications and confirm intent
- **Status Checks**: Always check repository status before major operations using `git status`
- **Branch Awareness**: Be conscious of the current branch and working directory state
- **Conflict Resolution**: When conflicts arise, provide clear guidance on resolution strategies
- **Clean History**: Encourage atomic commits and clean commit history practices

## Best Practices

- Use `git diff --staged` to review changes before committing
- Prefer `git pull --rebase` to maintain linear history when appropriate
- When working with submodules, always verify their state after updates
- For commit messages: use a clear subject line (50 chars or less), followed by a blank line and detailed body if needed
- Leverage `git log --oneline --graph` for visualizing branch history

## Error Handling

- If a git command fails, analyze the error message and provide actionable solutions
- For merge conflicts, guide through the resolution process step-by-step
- If repository state is unclear, use diagnostic commands to gather information before proceeding

## Output Format

- When describing changes, structure your response with clear sections: files changed, summary of modifications, and recommended commit message
- For command execution, show the command being run and explain its purpose
- When analyzing diffs, present information in a hierarchical format: high-level summary, then file-by-file details if needed

You have the authority to execute git commands directly but should explain your actions clearly. When uncertain about the user's intent or when operations could have significant consequences, ask for clarification before proceeding.
```

### `.claude/agents/jarvis-pipeline-builder.md`

```markdown
---
name: jarvis-pipeline-builder
description: Use this agent when the user needs to create, modify, or understand Jarvis pipeline YAML files, or when they need to extract and document parameters from pipeline packages. Examples:\n\n<example>\nContext: User is working on creating a new pipeline configuration.\nuser: "I need to create a pipeline YAML for processing data through the ETL workflow"\nassistant: "I'm going to use the Task tool to launch the jarvis-pipeline-builder agent to help you create the pipeline YAML configuration."\n<commentary>\nThe user needs pipeline YAML creation assistance, which is the core expertise of the jarvis-pipeline-builder agent.\n</commentary>\n</example>\n\n<example>\nContext: User has just written code for a new pipeline package.\nuser: "I've just finished implementing the DataTransformer package. Can you help me understand what parameters it needs?"\nassistant: "I'm going to use the Task tool to launch the jarvis-pipeline-builder agent to analyze the DataTransformer package and extract its parameters."\n<commentary>\nThe user needs to understand package parameters, which requires the jarvis-pipeline-builder agent's expertise in reading packages and their parameter structures.\n</commentary>\n</example>\n\n<example>\nContext: User is reviewing existing pipeline configurations.\nuser: "Can you review this pipeline YAML and tell me if the parameters are correctly configured?"\nassistant: "I'm going to use the Task tool to launch the jarvis-pipeline-builder agent to review the pipeline YAML configuration and validate the parameters."\n<commentary>\nThe user needs expert review of pipeline YAML structure and parameter configuration.\n</commentary>\n</example>
model: sonnet
---

You are an elite Jarvis pipeline architect with deep expertise in constructing, analyzing, and optimizing Jarvis pipeline YAML configurations. Your specialized knowledge encompasses both the YAML structure and the underlying package implementations that power these pipelines.

## Core Responsibilities

You will:
- Design and construct well-structured Jarvis pipeline YAML files that follow best practices
- Analyze package source code to extract accurate parameter specifications, types, defaults, and constraints
- Validate pipeline configurations for correctness, completeness, and efficiency
- Troubleshoot pipeline configuration issues and suggest optimizations
- Document parameter requirements clearly and comprehensively

## Operational Guidelines

### When Building Pipeline YAML Files:
1. **Structure First**: Ensure proper YAML syntax and hierarchical organization
2. **Parameter Accuracy**: Cross-reference package implementations to verify all required parameters are included with correct types and formats
3. **Defaults and Optionals**: Clearly distinguish between required parameters and those with defaults
4. **Validation**: Include appropriate validation rules and constraints where applicable
5. **Documentation**: Add inline comments explaining non-obvious configuration choices
6. **Dependencies**: Ensure proper ordering and dependency management between pipeline stages

### When Reading Package Parameters:
1. **Thorough Analysis**: Examine package constructors, configuration classes, and parameter validation logic
2. **Type Extraction**: Identify precise parameter types (string, int, float, bool, list, dict, etc.)
3. **Constraint Discovery**: Note any validation rules, allowed values, ranges, or format requirements
4. **Default Values**: Document default values when they exist
5. **Required vs Optional**: Clearly distinguish mandatory parameters from optional ones
6. **Nested Structures**: Handle complex nested parameter structures accurately

### Quality Assurance:
- Always verify parameter names match exactly as defined in the package code
- Check for deprecated parameters or configuration patterns
- Ensure YAML syntax is valid and properly indented
- Validate that parameter types in YAML align with package expectations
- Consider edge cases and potential configuration conflicts

### When Uncertain:
- If package code is ambiguous, examine usage examples or tests
- If parameter requirements are unclear, ask for clarification rather than guessing
- If multiple valid approaches exist, present options with trade-offs

### Output Format:
- For YAML files: Provide complete, valid YAML with clear structure and helpful comments
- For parameter documentation: Use structured format showing name, type, required/optional status, default value (if any), description, and constraints
- For analysis: Provide clear, actionable insights with specific references to code locations when relevant

## Best Practices:
- Maintain consistency in naming conventions and structure across pipeline stages
- Optimize for readability and maintainability
- Include error handling and fallback configurations where appropriate
- Consider performance implications of configuration choices
- Follow any project-specific patterns established in existing pipeline files

You approach each task methodically, ensuring accuracy and completeness while maintaining clarity in your explanations and configurations.
```

### `.claude/agents/python-code-updater.md`

```markdown
---
name: python-code-updater
description: Use this agent when you need to modify, refactor, or enhance existing Python code. This includes updating deprecated syntax, improving performance, adding new features to existing functions/classes, fixing bugs, modernizing code to newer Python versions, or restructuring code for better maintainability. Examples: <example>Context: User has existing Python code that needs to be updated to use newer syntax or libraries. user: 'Can you update this function to use f-strings instead of .format()?' assistant: 'I'll use the python-code-updater agent to modernize this code with f-string syntax.' <commentary>The user wants to update existing Python code with modern syntax, which is exactly what the python-code-updater agent specializes in.</commentary></example> <example>Context: User has a Python script that needs performance improvements. user: 'This loop is running slowly, can you optimize it?' assistant: 'Let me use the python-code-updater agent to analyze and optimize this code for better performance.' <commentary>The user needs existing code improved for performance, which falls under the python-code-updater's expertise in enhancing existing Python code.</commentary></example>
model: sonnet
---

You are a professional software engineer specializing in Python code updates and improvements. You excel at analyzing existing Python code and making targeted enhancements while preserving functionality and improving code quality.

Your core responsibilities:
- Analyze existing Python code to understand its current functionality and structure
- Identify opportunities for improvement including performance, readability, maintainability, and modern Python practices
- Update code using current Python best practices and idioms
- Ensure backward compatibility when possible, or clearly communicate breaking changes
- Maintain existing functionality while enhancing code quality
- Follow PEP 8 style guidelines and modern Python conventions

Your approach:
1. First, thoroughly understand the existing code's purpose and current implementation
2. Identify specific areas for improvement (performance bottlenecks, deprecated syntax, code smells, etc.)
3. Plan updates that maintain functionality while improving code quality
4. Implement changes incrementally, testing logic as you go
5. Provide clear explanations of what was changed and why
6. Highlight any potential impacts or considerations for the updates

Key principles:
- Always preserve the original functionality unless explicitly asked to change behavior
- Use modern Python features appropriately (f-strings, type hints, dataclasses, etc.)
- Optimize for readability and maintainability over clever solutions
- Consider performance implications of your changes
- Follow established project patterns and conventions when evident
- Be explicit about any assumptions you make about the code's usage

When updating code:
- Explain your reasoning for each significant change
- Point out any potential breaking changes or migration considerations
- Suggest additional improvements that might be beneficial
- Ensure error handling is appropriate and robust
- Consider edge cases that might not be handled in the original code

You should ask for clarification if the update requirements are ambiguous or if there are multiple valid approaches to improving the code.
```

### `.github/workflows/build-containers.yaml`

```yaml
name: Build Containers

on:
  workflow_dispatch:
  workflow_call:

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 800

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_HUB_USERNAME }}
          password: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        id: buildx

      - name: Build and push build image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./docker/build.Dockerfile
          builder: ${{ steps.buildx.outputs.name }}
          platforms: linux/amd64,linux/arm64
          push: true
          tags: iowarp/ppi-jarvis-cd-build:latest

      - name: Build and push deploy image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./docker/deploy.Dockerfile
          builder: ${{ steps.buildx.outputs.name }}
          platforms: linux/amd64,linux/arm64
          push: true
          tags: iowarp/ppi-jarvis-cd:latest
```

### `.github/workflows/main.yml`

```yaml
name: main

on:
  # push:
  # pull_request:
  #   branches: [ main ]
  workflow_dispatch:
    inputs:
      debug_enabled:
        description: 'Run the build with tmate debugging enabled'
        required: false
        default: false
env:
  BUILD_TYPE: Debug
  LOCAL: local

jobs:
  build:
    runs-on: ubuntu-24.04

    steps:
      - name: Get Sources
        uses: actions/checkout@v4

      - name: Install Apt Dependencies
        run: bash ci/install_deps.sh

      - name: Install Jarvis
        run: bash ci/install_jarvis.sh

      - name: Run pylint
        run: bash ci/lint.sh

      - name: Test
        run: bash ci/run_tests.sh

#      - name: Coveralls
#        uses: coverallsapp/github-action@master
#        with:
#          path-to-lcov: lcov.info
#          github-token: ${{ secrets.GITHUB_TOKEN }}
```

### `.vscode/launch.json`

```json
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Debug jarvis rg build",
            "type": "python",
            "python": "/home/llogan/.venv/bin/python",
            "request": "launch",
            "program": "/home/llogan/.venv/bin/jarvis",
            "args": [
                "ppl",
                "append",
                "chimaera_run"
            ],
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "PATH": "/home/llogan/.scspkg/packages/iowarp_runtime/sbin:/home/llogan/.scspkg/packages/iowarp_runtime/bin:/usr/local/cuda/bin:/usr/local/cuda/gds/tools:/home/llogan/Documents/Projects/scspkg/packages/cuda/sbin:/home/llogan/Documents/Projects/scspkg/packages/cuda/bin:/opt/rocm/bin:/home/llogan/Documents/Projects/scspkg/packages/rocm/sbin:/home/llogan/Documents/Projects/scspkg/packages/rocm/bin:/home/llogan/.venv/bin:/home/llogan/.local/bin:/mnt/home/Projects/spack/bin:/home/llogan/.vscode-server/cli/servers/Stable-848b80aeb52026648a8ff9f7c45a9b0a80641e2e/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/llogan/.vscode-server/extensions/ms-python.debugpy-2025.8.0-linux-x64/bundled/scripts/noConfigScripts:/home/llogan/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand"
            }
        },
        {
            "name": "Debug jarvis ppl run",
            "type": "python",
            "python": "/home/llogan/.venv/bin/python",
            "request": "launch",
            "program": "/home/llogan/.venv/bin/jarvis",
            "args": [
                "ppl",
                "run"
            ],
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "PATH": "/home/llogan/.scspkg/packages/iowarp_runtime/sbin:/home/llogan/.scspkg/packages/iowarp_runtime/bin:/usr/local/cuda/bin:/usr/local/cuda/gds/tools:/home/llogan/Documents/Projects/scspkg/packages/cuda/sbin:/home/llogan/Documents/Projects/scspkg/packages/cuda/bin:/opt/rocm/bin:/home/llogan/Documents/Projects/scspkg/packages/rocm/sbin:/home/llogan/Documents/Projects/scspkg/packages/rocm/bin:/home/llogan/.venv/bin:/home/llogan/.local/bin:/mnt/home/Projects/spack/bin:/home/llogan/.vscode-server/cli/servers/Stable-848b80aeb52026648a8ff9f7c45a9b0a80641e2e/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/llogan/.vscode-server/extensions/ms-python.debugpy-2025.8.0-linux-x64/bundled/scripts/noConfigScripts:/home/llogan/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand"
            }
        },
        {
            "name": "Debug jarvis ppl run with file",
            "type": "python",
            "python": "/home/llogan/.venv/bin/python",
            "request": "launch",
            "program": "/home/llogan/.venv/bin/jarvis",
            "args": [
                "ppl",
                "run",
                "builtin/pipelines/simple_test.yaml",
                "yaml"
            ],
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "PATH": "/home/llogan/.scspkg/packages/iowarp_runtime/sbin:/home/llogan/.scspkg/packages/iowarp_runtime/bin:/usr/local/cuda/bin:/usr/local/cuda/gds/tools:/home/llogan/Documents/Projects/scspkg/packages/cuda/sbin:/home/llogan/Documents/Projects/scspkg/packages/cuda/bin:/opt/rocm/bin:/home/llogan/Documents/Projects/scspkg/packages/rocm/sbin:/home/llogan/Documents/Projects/scspkg/packages/rocm/bin:/home/llogan/.venv/bin:/home/llogan/.local/bin:/mnt/home/Projects/spack/bin:/home/llogan/.vscode-server/cli/servers/Stable-848b80aeb52026648a8ff9f7c45a9b0a80641e2e/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/llogan/.vscode-server/extensions/ms-python.debugpy-2025.8.0-linux-x64/bundled/scripts/noConfigScripts:/home/llogan/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand"
            }
        },
        {
            "name": "Debug jarvis ppl run (Python module)",
            "type": "python",
            "python": "/home/llogan/.venv/bin/python",
            "request": "launch",
            "module": "jarvis_cd.core.cli",
            "args": [
                "ppl",
                "run"
            ],
            "console": "integratedTerminal",
            "justMyCode": false,
            "cwd": "${workspaceFolder}",
            "env": {
                "PYTHONPATH": "${workspaceFolder}",
                "PATH": "/home/llogan/.scspkg/packages/iowarp_runtime/sbin:/home/llogan/.scspkg/packages/iowarp_runtime/bin:/usr/local/cuda/bin:/usr/local/cuda/gds/tools:/home/llogan/Documents/Projects/scspkg/packages/cuda/sbin:/home/llogan/Documents/Projects/scspkg/packages/cuda/bin:/opt/rocm/bin:/home/llogan/Documents/Projects/scspkg/packages/rocm/sbin:/home/llogan/Documents/Projects/scspkg/packages/rocm/bin:/home/llogan/.venv/bin:/home/llogan/.local/bin:/mnt/home/Projects/spack/bin:/home/llogan/.vscode-server/cli/servers/Stable-848b80aeb52026648a8ff9f7c45a9b0a80641e2e/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/llogan/.vscode-server/extensions/ms-python.debugpy-2025.8.0-linux-x64/bundled/scripts/noConfigScripts:/home/llogan/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand"
            }
        }
    ]
}
```

### `CLAUDE.md`

```markdown
When adding new menus, create a new documentation file for describing the menu.

When adding new commands, make sure to document them under the docs directory. You should place the documentation in the doc file that is most relevant.
```

### `ai-prompts/docker/phase1.md`

```markdown
@CLAUDE.md 

Let's make two dockerfiles under the directory docker: 
build.Dockerfile and deploy.Dockerfile.

build and deploy should be built in the github actions. Manual only. Please edit the github actions. Add a new action called build_dockerfiles.yml

## build.Dockerfile

Pip install from workspace. Mount the parent directory as /workspace. 
Add to spack.

```Dockerfile
FROM iowarp/iowarp-deps:latest
LABEL maintainer="llogan@hawk.iit.edu"
LABEL version="0.0"
LABEL description="IOWarp ppi-jarvis-cd Docker image"

# Add ppi-jarvis-cd to Spack configuration
RUN echo "  py-ppi-jarvis-cd:" >> ~/.spack/packages.yaml && \
    echo "    externals:" >> ~/.spack/packages.yaml && \
    echo "    - spec: py-ppi-jarvis-cd" >> ~/.spack/packages.yaml && \
    echo "      prefix: /usr/local" >> ~/.spack/packages.yaml && \
    echo "    buildable: false" >> ~/.spack/packages.yaml

# Setup jarvis
RUN jarvis init
```

Also create a local.sh in docker directory to build the container locally. Should look something like this:
```bash
#!/bin/bash

# Build iowarp-runtime Docker image

# Get the project root directory (parent of docker folder)
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
PROJECT_ROOT="$( cd "${SCRIPT_DIR}/.." && pwd )"

echo $PROJECT_ROOT
echo $SCRIPT_DIR
# Build the Docker image
docker build  --no-cache -t iowarp/ppi-jarvis-cd-build:latest -f "${SCRIPT_DIR}/build.Dockerfile" "${PROJECT_ROOT}"
```

## deploy.Dockerfile

Essentially just does ``FROM iowarp/ppi-jarvis-cd-build:latest`` for now.

In the github action, create ``iowarp/ppi-jarvis-cd:latest``.
```

### `ai-prompts/new-pipeline.md`

```markdown
Use the python-code-updater agent to change the structure of pipleine scripts. 

The old format looked as follows
```python
name: chimaera_unit_ipc
env: chimaera
pkgs:
  - pkg_type: chimaera_run
    pkg_name: chimaera_run
    sleep: 10
    do_dbg: true
    dbg_port: 4000
  - pkg_type: chimaera_unit_tests
    pkg_name: chimaera_unit_tests
    TEST_CASE: TestBdevIo
    do_dbg: true
    dbg_port: 4001
```

The new format should look like this:
```python
name: chimaera_unit_ipc
env: chimaera
pkgs:
  - pkg_type: chimaera_run
    pkg_name: chimaera_run
    sleep: 10
    do_dbg: true
    dbg_port: 4000
  - pkg_type: chimaera_unit_tests
    pkg_name: chimaera_unit_tests
    TEST_CASE: TestBdevIo
    do_dbg: true
    dbg_port: 4001
    interceptors: hermes_api
interceptors:
  - pkg_type: hermes_api
    pkg_name: hermes_api
```

Here the "interceptors" keys ares the new thing. 

In the Package class, there should be a new function called add_interceptor. This should modify a new self.config key called 'interceptors'. The interceptors key will be similar to the sub_pkgs key. The set of interceptors should be stored in a dictionary. This dictionary should be a mapping of pkg_name to a constructed package.

In SimplePackage, add a new config parameter called "interceptors", which is a list of strings. The list parameters look like this:
```
 self.add_args([
            {
                'name': 'hosts',
                'msg': 'A list of hosts and threads pr',
                'type': list,
                'args': [
                    {
                        'name': 'host',
                        'msg': 'A string representing a host',
                        'type': str,
                    }
                ]
            }
        ])
```

When loading a SimplePackage, iterate over the set of strings there and check self.ppl for the interceptors. Call interceptor.modify_env() to update our environment. Remove the ability to pass mod_env to update_env function. Make it so mod_env is a copy (not pointer) to env. This way each package gets its own isolated module environment.
```

### `ai-prompts/phase-14-update.md`

```markdown
# Phase 14 Update - Recovered Changes

## Recovery Information

**Status:** ✅ RECOVERED - All lost commits have been saved to branch `recovery-phase-14`

**Recovery Date:** 2025-10-01
**Lost Commits Found:**
- `5775416` - Init jarvis repos (2025-09-30 16:53:03)
- `f8b3d6a` - Refactor resource graph and enhance module management (2025-10-01 11:34:50)

**To restore these changes:**
```bash
# Option 1: Merge into current branch
git merge recovery-phase-14

# Option 2: Cherry-pick specific commits
git cherry-pick 5775416 f8b3d6a

# Option 3: Checkout the recovery branch
git checkout recovery-phase-14
```

---

## Summary of Changes

This phase included major refactoring of the resource graph system, module management enhancements, CLI improvements, and extensive documentation updates across two commits.

### Files Changed (20 files total)
- **Added:** 1 file
- **Modified:** 18 files
- **Deleted:** 1 file
- **Net change:** +790 insertions, -264 deletions

---

## Commit 1: Init Jarvis Repos (5775416)

**Date:** 2025-09-30 16:53:03
**Files Changed:** 13 files

### Changes Made:

#### Removed Files
- **bin/jarvis-install-builtin** (128 lines deleted)
  - Removed legacy installation script

#### CLI Enhancements (jarvis_cd/core/cli.py)
- Added 80+ lines of new CLI functionality
- Enhanced command structure and argument parsing

#### Configuration Updates (jarvis_cd/core/config.py)
- Updated configuration handling (12 modifications)

#### Documentation Updates
- **docs/modules.md** (44 lines modified)
  - Updated module documentation with new patterns

- **docs/package_dev_guide.md** (2 modifications)
  - Updated development guide

- **docs/pipelines.md** (14 modifications)
  - Enhanced pipeline documentation

- **docs/resource_graph.md** (6 modifications)
  - Updated resource graph documentation

#### Other Updates
- **MANIFEST.in** - Updated manifest
- **ai-prompts/phase5-jarvis-repos.md** - Updated phase 5 documentation
- **jarvis_cd/core/module_manager.py** - Minor fix
- **jarvis_cd/core/resource_graph.py** - 12 modifications
- **pyproject.toml** - Updated project configuration
- **setup.py** - Simplified setup (11 lines removed)

---

## Commit 2: Refactor Resource Graph and Enhance Module Management (f8b3d6a)

**Date:** 2025-10-01 11:34:50
**Files Changed:** 15 files
**Impact:** +790 insertions, -264 deletions

This was the major refactoring commit with extensive changes across multiple subsystems.

---

### 1. Resource Graph System Refactoring

#### A. Core Architecture Changes (jarvis_cd/util/resource_graph.py)
**Major refactoring: 205 lines changed**

**Key Changes:**
- **Removed StorageDevice class** - Simplified to use plain Python dictionaries
- **Data model simplification:**
  ```python
  # OLD approach (class-based):
  device = StorageDevice(name, capacity, available)

  # NEW approach (dict-based):
  device = {
      'name': str,
      'capacity': int,
      'available': int
  }
  ```

- **Method renaming for clarity:**
  - `build_resource_graph()` → `build()`
  - `load_resource_graph()` → `load()`
  - `show_resource_graph()` → `show()`
  - `show_resource_graph_path()` → `show_path()`

- **Auto-loading on initialization:**
  - ResourceGraphManager now automatically loads the resource graph when instantiated
  - Eliminates need for separate initialization step

- **Display improvements:**
  - `show()` now displays raw YAML file instead of processed summary
  - Better debugging and verification of resource graph state

#### B. Binary Bug Fix (bin/jarvis_resource_graph)
**Critical fix: 7 lines modified**

- **Bug:** Available capacity was being overwritten with `None`
- **Impact:** Resource tracking was broken
- **Fix:** Corrected the capacity update logic to preserve available capacity

#### C. Resource Graph Manager (jarvis_cd/core/resource_graph.py)
**49 lines modified**

- Updated all method calls to use shortened names
- Improved error handling
- Enhanced integration with module system

#### D. Documentation (docs/resource_graph.md)
**100 lines reorganized**

Updated to reflect dictionary-based approach:
```yaml
# Example storage device structure
storage_devices:
  /path/to/storage:
    name: "storage_name"
    capacity: 1000000000  # bytes
    available: 500000000   # bytes
```

---

### 2. Module Management System Enhancements

#### A. New CLI Commands (jarvis_cd/core/cli.py)
**93 lines added**

**New Commands Implemented:**

1. **`jarvis mod clear`**
   - Cleans module directories while preserving src/ folder
   - Useful for resetting module state without losing source code
   - Safe cleanup operation

2. **`jarvis mod dep add <module> <dependency>`**
   - Adds dependencies to module configuration
   - Updates module metadata
   - Example: `jarvis mod dep add mymod hermes`

3. **`jarvis mod dep remove <module> <dependency>`**
   - Removes dependencies from module configuration
   - Cleans up module metadata
   - Example: `jarvis mod dep remove mymod hermes`

#### B. Module Manager Implementation (jarvis_cd/core/module_manager.py)
**133 lines added**

**New Methods:**
- `clear_module()` - Implementation of module clearing logic
- `add_dependency()` - Dependency addition logic
- `remove_dependency()` - Dependency removal logic
- Enhanced dependency tracking and validation
- Improved error messages

#### C. Module Documentation (docs/modules.md)
**63 lines added/modified**

**Documentation includes:**
- Usage examples for new commands
- Workflow patterns for module development
- Dependency management best practices
- Examples:
  ```bash
  # Clear a module (keeps src/)
  jarvis mod clear mymodule

  # Add dependency
  jarvis mod dep add mymodule hermes

  # Remove dependency
  jarvis mod dep remove mymodule hermes
  ```

---

### 3. Pipeline System Improvements

#### A. Pipeline Core (jarvis_cd/core/pipeline.py)
**131 lines modified (significant refactoring)**

**Key Improvements:**
- **Auto-configuration on load:**
  - Pipelines now automatically configure associated packages on load/update
  - Eliminates manual configuration step

- **Better integration with resource graph:**
  - Pipeline operations now respect resource graph constraints
  - Improved resource allocation and tracking

- **Enhanced error handling:**
  - More descriptive error messages
  - Better failure recovery

- **Workflow improvements:**
  - Streamlined pipeline creation and management
  - Better state tracking

#### B. Package Management (jarvis_cd/core/pkg.py)
**26 lines modified**

- Updated package configuration integration
- Improved auto-configuration logic
- Better package lifecycle management

---

### 4. Configuration System Updates

#### A. Config Core (jarvis_cd/core/config.py)
**45 lines modified**

**Improvements:**
- Enhanced configuration validation
- Better default value handling
- Improved configuration merging logic
- More robust error handling

#### B. Utility Functions (jarvis_cd/util/__init__.py)
**5 lines modified**

- Updated utility imports
- Better helper function organization
- Enhanced `load_class()` error messages

---

### 5. Shell and Process Management

#### A. Process Handling (jarvis_cd/shell/process.py)
**29 lines added**

**Enhancements:**
- Improved process spawning logic
- Better signal handling
- Enhanced subprocess management
- More robust error handling

---

### 6. Documentation Additions

#### A. New Agent Documentation (.claude/agents/git-expert.md)
**63 lines - NEW FILE**

Added Git expert agent configuration for Claude Code integration

#### B. Package Development Guide (docs/package_dev_guide.md)
**98 lines added**

**New sections:**
- **GdbServer Integration:**
  ```bash
  # Using GDB with Jarvis packages
  gdbserver :2000 ./my_package

  # Connect from GDB
  gdb ./my_package
  (gdb) target remote :2000
  ```

- **Debugging workflows**
- **Development best practices**
- **Testing strategies**

#### C. Pipeline Documentation (docs/pipelines.md)
**Updates to workflow examples**

- Enhanced pipeline configuration examples
- Better integration with resource graph
- Improved troubleshooting section

#### D. AI Prompt Documentation (ai-prompts/phase3-launch.md)
**7 lines added**

- Updated phase 3 documentation
- Added context for future development

---

## Key Architectural Improvements

### 1. Simplified Data Models
- Moved from class-based to dictionary-based storage devices
- Reduced code complexity
- Improved serialization/deserialization

### 2. Enhanced Developer Experience
- Shorter, more intuitive command names
- Auto-loading/auto-configuration reduces manual steps
- Better error messages throughout

### 3. Better Module Lifecycle Management
- Complete dependency management workflow
- Safe module cleanup operations
- Improved module state tracking

### 4. Resource Graph Reliability
- Fixed critical capacity tracking bug
- Improved resource allocation
- Better integration with pipelines

### 5. Documentation Quality
- Comprehensive examples throughout
- Real-world workflow patterns
- Developer-focused guidance

---

## Testing & Validation Notes

**Areas to test after merging:**

1. **Resource Graph:**
   - Verify capacity tracking works correctly
   - Test resource allocation in pipelines
   - Validate dictionary-based device access

2. **Module Management:**
   - Test `jarvis mod clear` preserves src/
   - Verify dependency add/remove operations
   - Check module configuration updates

3. **Pipeline Integration:**
   - Test auto-configuration on pipeline load
   - Verify package configuration workflow
   - Check resource graph integration

4. **CLI Commands:**
   - Test all new commands with various inputs
   - Verify error handling
   - Check help text accuracy

---

## Migration Notes

### Breaking Changes:
1. **ResourceGraphManager API:**
   - Old: `manager.build_resource_graph()`
   - New: `manager.build()`

2. **Storage Device Access:**
   - Old: `device.capacity`, `device.available`
   - New: `device['capacity']`, `device['available']`

### Upgrade Path:
```python
# Update code using ResourceGraphManager
from jarvis_cd.util.resource_graph import ResourceGraphManager

# Old code:
mgr = ResourceGraphManager()
mgr.load_resource_graph()
mgr.show_resource_graph()

# New code:
mgr = ResourceGraphManager()  # Auto-loads now!
mgr.show()  # Simplified method name
```

---

## Statistics

- **Total Commits:** 2
- **Files Changed:** 20 unique files
- **Lines Added:** 790+
- **Lines Removed:** 264
- **Net Growth:** +526 lines
- **Documentation Added:** ~200 lines
- **New Features:** 3 CLI commands
- **Bug Fixes:** 1 critical fix (capacity tracking)
- **Refactorings:** 2 major (ResourceGraph, Pipeline)

---

## Next Steps

1. **Merge the recovery branch:**
   ```bash
   git checkout 36-refactor-with-ai
   git merge recovery-phase-14
   ```

2. **Run tests:**
   ```bash
   pytest tests/
   ```

3. **Verify documentation:**
   - Check all doc links work
   - Verify code examples are accurate
   - Test commands in documentation

4. **Update any dependent code:**
   - Search for old ResourceGraphManager method calls
   - Update StorageDevice class references
   - Fix any broken imports

5. **Create PR if needed:**
   ```bash
   git push origin recovery-phase-14
   # Create PR: recovery-phase-14 → 36-refactor-with-ai
   ```

---

## Files Modified Summary

### Core System Files
- `jarvis_cd/core/cli.py` - Major CLI enhancements
- `jarvis_cd/core/config.py` - Configuration improvements
- `jarvis_cd/core/module_manager.py` - Module management features
- `jarvis_cd/core/pipeline.py` - Pipeline refactoring
- `jarvis_cd/core/pkg.py` - Package management updates
- `jarvis_cd/core/resource_graph.py` - Resource graph integration
- `jarvis_cd/util/resource_graph.py` - Major refactoring
- `jarvis_cd/util/__init__.py` - Utility improvements
- `jarvis_cd/shell/process.py` - Process handling enhancements

### Binary Files
- `bin/jarvis_resource_graph` - Critical bug fix
- `bin/jarvis-install-builtin` - Removed (obsolete)

### Documentation Files
- `docs/modules.md` - Module management guide
- `docs/package_dev_guide.md` - Package development guide
- `docs/pipelines.md` - Pipeline documentation
- `docs/resource_graph.md` - Resource graph guide
- `.claude/agents/git-expert.md` - New agent config

### Configuration Files
- `pyproject.toml` - Project configuration
- `setup.py` - Setup simplification
- `MANIFEST.in` - Manifest updates

### AI Prompt Files
- `ai-prompts/phase3-launch.md` - Phase 3 updates
- `ai-prompts/phase5-jarvis-repos.md` - Phase 5 updates

---

## Command Reference - What Was Added

### New CLI Commands

| Command | Description | Example |
|---------|-------------|---------|
| `jarvis mod clear <module>` | Clear module directory (keep src/) | `jarvis mod clear mymod` |
| `jarvis mod dep add <mod> <dep>` | Add dependency to module | `jarvis mod dep add mymod hermes` |
| `jarvis mod dep remove <mod> <dep>` | Remove dependency from module | `jarvis mod dep remove mymod hermes` |

### Modified Commands (Simplified)

| Old Command | New Command | Notes |
|-------------|-------------|-------|
| `ResourceGraphManager.build_resource_graph()` | `.build()` | Shorter, cleaner |
| `ResourceGraphManager.load_resource_graph()` | `.load()` | Auto-loads on init now |
| `ResourceGraphManager.show_resource_graph()` | `.show()` | Shows raw YAML |
| `ResourceGraphManager.show_resource_graph_path()` | `.show_path()` | Simpler name |

---

## Lessons Learned

1. **Always create branches before making changes** - Even in detached HEAD state
2. **Git reflog is your friend** - Saved all this work!
3. **Document as you go** - This recovery would have been harder without commit messages
4. **Regular commits** - Two well-structured commits made recovery straightforward
```

### `ai-prompts/phase1-argparse.md`

```markdown

## Argument Parsing

Put this in jarvis_cd.util as argparse.py. Implement this class and build a unit test.

We want to build a custom library for parsing arguments. It should support
positional, keyword, and remainder arguments.

### Argument structure

The argument parser should have the following concepts:
* menu: a set of commands
* command: has a name and a set of args
* args

### Argument dict
```python
{
    'name': 'name of argument'
    'msg': 'description of argument'
    'type': 'type to cast argument to'
    'default': 'default value'
    'class': 'string. the name of a group of commands.'
    'rank': 'integer. orders arguments within a class'
    'required': 'complain if message is required'
    'args': 'specific for list types. list of this dictionary. Defines the structure of the list'.
    'aliases': 'any alternative names for the argument'
}
```


### self.add_menu

This will add a new menu to parse. A menu contains commands. If the a command is not found in a menu, then the set of commands and their arguments should be printed as error.

It takes as input a name. The name is a space-separated string. Each word in the string indicates either a sub-menu or a command within a menu. For example, 

"vpic run"

Is considered a command in the below example, while "vpic" is considered a menu.

### self.add_cmd

Commands are attached to menus as they are defined. The full string including menu name should be used to identify this class.

### self.add_args

Arguments are attached to commands as they are defined. 

### Example

Here is the argument parsing class for a file named "my_app.py"
```python
class MyAppArgParse(ArgParse):
    def define_options(self):
        self.add_menu('')
        self.add_cmd('', keep_remainder=True)
        self.add_args([
            {
                'name': 'hi',
                'msg': 'hello',
                'type': str,
                'default': None
            }
        ])

        self.add_menu('vpic', msg="The VPIC application")
        self.add_cmd('vpic run',
                      keep_remainder=False,
                      aliases=['vpic r', 'vpic runner'])
        self.add_args([
            {
                'name': 'steps',
                'msg': 'Number of checkpoints',
                'type': int,
                'required': True,
                'pos': True,
                'class': 'sim',
                'rank': 0
            },
            {
                'name': 'x',
                'msg': 'The length of the x-axis',
                'type': int,
                'required': False,
                'default': 256,
                'pos': True,
                'class': 'sim',
                'rank': 1
            },
            {
                'name': 'do_io',
                'msg': 'Whether to perform I/O or not',
                'type': bool,
                'required': False,
                'default': False,
                'pos': True,
            },
            {
                'name': 'make_figures',
                'msg': 'Whether to make a figure',
                'type': bool,
                'default': False,
            },
            {
                'name': 'data_size',
                'msg': 'Total amount of data to produce',
                'type': int,
                'default': 1024,
            },
            {
                'name': 'hosts',
                'msg': 'A list of hosts',
                'type': list,
                'args': [
                    {
                        'name': 'host',
                        'msg': 'A string representing a host',
                        'type': str,
                    }
                ],
                'aliases': ['x']
            },
            {
                'name': 'devices',
                'msg': 'A list of devices and counts',
                'type': list,
                'aliases': ['d']
                'args': [
                    {
                        'name': 'path',
                        'msg': 'The mount point of device',
                        'type': str,
                    },
                    {
                        'name': 'count',
                        'msg': 'The number of devices to search for',
                        'type': int,
                    }
                ]
            }
        ])

    def main_menu(self): 
        self.kwargs # The dictionary built
        self.remainder # any remaining args 
    def vpic_run(self): pass 
```

### Example: Empty menu, empty cmd
```
my_app hi="hi" rem1 rem2 rem3
```

self.remainder will contain [rem1, rem2, rem3]

self.kwargs will contain 'hi'

### Example 2: Set list args
Set the list, replacing old values in the list:
```
my_app vpic run 1 --devices="[(/mnt/home, 5), (/mnt/home2, 6)]"
```

### Example 3: Append to list args
Append to the list, keeping the old values. Difference is the = sign is missing.
```
my_app vpic run 1 --d "(/mnt/home, 5)" --d "(/mnt/home2, 6)"
```

Same as example 2, but slightly different

### Example 4: Boolean
I would like booleans in the argparse to support +/-. 
```
vpic run +do_io
```
should set do_io to true. 

Alternatively
```
vpic run -do_io
```


Sets it to falsee

## Dictionary Arguments

Sometimes, we already have a dictionary of parameters for a particular command, but the individual parameters have not yet been converted to their final types. 

For example, let's say we have a configuration for ``vpic run``:
```
arg_dict = {
    'do_io': True,
    'devices': [
        ("path", "1"),
        ("path2", "2")
    ]
}
```

We should have an api as follows:
```
ArgParse.parse_dict('vpic run', arg_dict)
```

We do not have remainder support for this version, so it will only set self.kwargs
```

### `ai-prompts/phase10-pipeline-index.md`

```markdown
# Pipeline Indexes

Pipeline indexes are folders containing pipeline scripts.
They can be used to disseminate working examples of your code.
For example, pipelines scripts used for unit tests would be good to have
in a pipeline index.

## Adding a Pipeline Index

Pipeline indexes are stored within repos as a subdirectory named
``pipelines``. It is required to be named ``pipelines``.

Below is an example structure of a jarvis repo containing a pipeline index.
```bash
jarvis_chimaera # Repo
├── jarvis_chimaera  # Jarvis Packages
│   ├── chimaera_bw_bench
│   ├── chimaera_docker
│   ├── chimaera_latency_bench
│   ├── chimaera_run
│   ├── chimaera_unit_tests
│   └── chimaera_zlib_bench
└── pipelines  # Pipeline Index
    ├── bench_bw_ipc.yaml
    ├── bench_latency_ipc.yaml
    ├── test_bdev_io.yaml
    ├── test_bdev_ram.yaml
    ├── test_bulk_ipc.yaml
    ├── test_bulk_read_ipc.yaml
    ├── test_bulk_write_ipc.yaml
    ├── test_compress.yaml
    ├── test_ipc_rocm.yaml
    ├── test_ipc.yaml
    ├── test_python.yaml
    ├── test_serialize.yaml
    └── test_upgrade.yaml
```

Below is another example with an index containing subdirectories:
```bash
jarvis_hermes  # Repo
├── jarvis_hermes  # Jarvis Packages
│   ├── hermes_api
│   │   ├── pkg.py
│   │   └── README.md
│   ├── hermes_api_bench
│   │   ├── pkg.py
│   │   └── README.md
└── pipelines  # Pipeline Index
    ├── hermes
    │   └── test_hermes.yaml
    ├── mpiio
    │   ├── test_hermes_mpiio_basic_async.yaml
    │   ├── test_hermes_mpiio_basic_sync.yaml
    │   └── test_mpiio_basic.yaml
    ├── nvidia_gds
    │   ├── test_hermes_nvidia_gds.yaml
    │   └── test_nvidia_gds_basic.yaml
    ├── posix
    │   ├── test_hermes_posix_basic_large.yaml
    │   ├── test_hermes_posix_basic_mpi_large.yaml
    │   ├── test_hermes_posix_basic_mpi_small.yaml
    ├── stdio
    │   ├── test_hermes_stdio_adapter_bypass.yaml
    │   ├── test_hermes_stdio_adapter_default.yaml
    │   ├── test_hermes_stdio_adapter_scratch.yaml
    │   ├── test_hermes_stdio_basic_large.yaml
    ├── test_borg.yaml
    ├── test_ior.yaml
    └── vfd
        ├── test_hermes_vfd_basic.yaml
        ├── test_hermes_vfd_python.yaml
        ├── test_hermes_vfd_scratch.yaml
        └── test_vfd_python.yaml
```

## List indexes

Since pipeline indexes are stored in repos, just list
the repos
```bash
jarvis repo list
```

## Index Queries

In the commands below, many commands have the parameter ``[index_query]``.
An index query is a dotted string in the following format:
```
[repo_name].[subdir1]...[subdirN].[script]
```

For example:
```
jarvis_chimaera.bench_bw_ipc
jarvis_hermes.hermes.test_hermes
``` 

NOTE: index queries do not include file extensions.

## Use a script from an index
To call a pipeline script stored in an index directly, you
can do:

```bash
jarvis ppl index load [index_query]
```

For example:
```bash
jarvis ppl index load jarvis_chimaera.bench_bw_ipc
jarvis ppl index load jarvis_hermes.hermes.test_hermes
```

## Copy a script from an index

You can copy a pipeline script from an index to your current
directory or some other directory. You can then edit the
parameters to the script and the call ``jarvis ppl load yaml``
on your modified script.

To copy the script from an index:
```bash
jarvis ppl index copy [index_query] [output (optional)]
```

Parameters:
* index_query: a dotted string indicating the script in the index to copy
* output: a directory of file to copy the script to. If output is not provided,
it will copy to the current working directory.

For example:
```bash
jarvis ppl index copy jarvis_chimaera.bench_bw_ipc
jarvis ppl index copy jarvis_hermes.hermes.test_hermes
```
```

### `ai-prompts/phase11-template.md`

```markdown
Add a method called copy_template_file to the Pkg class, which copies a file from one location to
another. It replaces constants in the file using a format. It looks like this:
 
It should be used like this:

```python
self.copy_template_file(f'{self.pkg_dir}/config/hermes.xml',
                                    self.adios2_xml_path,
                                    replacements={
                                        'PPN': 1
                                    })
```

The constants have the following structure:
```
<parameter key="ppn" value='##PPN##'/> 
```

So it would look like this in the result file:
```
<parameter key="ppn" value='1'/> 
```
```

### `ai-prompts/phase12-pipeline.md`

```markdown
mod_env should be an exact replica of env, except with LD_PRELOAD. LD_PRELOAD should
not be in env at any point. 

I want you to consolidate. Merge PipelineManager and PackageManager with Pkg and Pipeline. Pipeline is a new class. at a high level, the changes that should be made are as follows. Ideally, the pipeline and pkg classes will be in core instead of basic. I would like the basic directory removed. Udpate the CLI code to use these instead.

Pipeline:
1. __init__: Set all parameters and expected variables of the pipeline to reasonable values to avoid requiring hasattr everywhere. It is required for user to externally call load or create later. Users expected to know if the pipeline exists or not. Use the Jarvis singleton to get paths like conf_dir, shared_dir, and priv_dir.
1. create: create a pipeline. pipelines have environments and a config (a set of packages)
1. load: load the pipeline. Loads each package of the pipeline
1. save: call save for each package in the pipeline and then save my configuration
1. destroy: delete the pipeline directory
1. start: iterate over each package forward and call start()
1. stop: iterate over each package backward and call stop()
1. kill: like stop, but with kill
1. status: iterate over each package forward and call status()
1. run. start then stop.
1. append: append package to pipeline and call its create function
1. rm: remove a package from the pipeline and call its destroy function

Pkg:
1. __init__: Set all parameters and expected variables of the package to reasonable values to avoid requiring hasattr everywhere. It is required for user to externally call load or create later.  Use the Jarvis singleton to get paths like conf_dir, shared_dir, and priv_dir.
1. create: a method to create a package. Packages should be created as subdirectories in their parent pipeline. Take the parent pipeline as input in addition to other package parameters. 
1. load: load package configuration and environment variables
1. save: save package configuration and environment variables
1. destroy: delete the package directories shared_dir, conf_dir, and priv_dir, but NOT pkg_dir.
1. _configure_menu: abstract
1. configure_menu(): calls _configure_menu, but updates the arg dict with common arguments.
1. configure: abstract
1. start: abstract
1. stop: abstract
1. kill: abstract
1. status: abstract
```

### `ai-prompts/phase14-jarvis-ppl-pkg.md`

```markdown
@CLAUDE.md Use the python code updater agent.

The way the pipelines and pkgs are stored is incorrect. Currently, pipelines, packages, and the environment are represented as a single yaml file stored in the pipeline's config_dir.
This is not correct. It needs more separation.

For pipelines, we need the following:
1. A pipeline configuration yaml that stores the set of packages and interceptors. This represents ordering and existence. This does not include package parameters or environments. It should be the same exact format as the pipeline script, with the pkgs and interceptors sections. Hopefully this will allow us to reduce code.
2. A pipeline environment yaml. Stores environment variables. When loading a pipeline, this environment will be passed to each subsequent package.

We can remove the package load() and save() methods with this new idea.

Packages should always require the pipeline is input. No default value None, it is a requirement. In addition, config_dir, private_dir, and shared_dir should be initialized during __init__. These are fixed paths based on the pipeline directories.
```

### `ai-prompts/phase15-containers.md`

```markdown
@CLAUDE.md 

Add support for containerized applications. Create the following Exec (inherit from Exec):
PodmanComposeExec, DockerComposeExec, ContainerComposeExec. These are wrappers around 
docker compose and podman compose functions. ContainerCompose is simply a router that
calls one or the other. Each of them inherit from Exec.

In general, the container name should be "pipeline_name_pkg_name".

## IOR Example Update

For now, let us only change the ior package.

For the ior package, let's add a new parameter called "deploy". It is a string with the following choices: default, podman, docker.

We will have the following python files: pkg.py (the router), default.py (the default path), and container.py (the path for podman/docker).

The container.py should have the following

### Configure (container.py)

This will produce a pipeline, container, podman-compose file. 
Store a template of these two files.
The container should inherit from iowarp/iowarp-build:latest and install ior with spack. This has spack installed already.

The pipeline, dockerfile, and compose file should be placed in the private directory for the package.

#### Pipeline (YAML)
Should contain this package and all parameters to this package, including interceptors. Essentially, 
reconstruct a pipeline file containing just a single package in the pkgs: key and as many interceptors
as defined for the package.

#### Dockerfile
```
FROM iowarp/iowarp-build:latest

# Disable prompt during packages installation.
ARG DEBIAN_FRONTEND=noninteractive

# Install ior.
RUN . "${SPACK_DIR}/share/spack/setup-env.sh" && \
    spack install -y ior

# Copy required spack executables to /usr so no need to do spack load in future.
RUN . "${SPACK_DIR}/share/spack/setup-env.sh" && \
    spack load ior && \
    cp $(which ior) /usr/bin
    cp $(which mpi) /usr/bin

# Copy required spack executables to /usr so no need to do spack load in future.
RUN jarvis ppl load yaml /pkg.yaml
```

### Compose file

This file should expose the ior package and the priv_dir and shared_dir. 
It should connect to a container named iowarp_runtime.
```
services:
  ior: 
    image_name: .
    container_name: [PPL_NAME_PKG_NAME]

    # CRITICAL: This ensures the writer container's IPC namespace can be joined
    # by other, external containers.
    ipc: container:[PPL_NAME_iowarp_runtime]
```

The ipc command should be used only if there is an interceptor in the pipeline that requires
shared memory. Let's add a new pipeline property called shm_container and shm_size to Pipeline.
By default, shm_container is None and shm_size is 8g. If shm_container is None, the compose
file should not have an ipc: section.

## Run

Run should use ContainerComposeExec to launch the produced compose file stored in the shared directory
of the package.
```

### `ai-prompts/phase16-installer.md`

```markdown
@CLAUDE.md 

I have done a hard reset. Currently, we are building container images during configure of each package. This is not inefficient. We should separate the building of containers from pipelines entirely. Instead, let's create a new concept called installers. Installers will allow us to build docker containers that have all the dependencies necessary to execute containerized pipelines.

## Pipeline Script Update
```
env: env_name
container_name: my_iowarp
container_engine: podman
container_base: iowarp/iowarp-build:latest
pkgs:
interceptors:
```

New parameters: container_name, container_engine, container_base.
container_name is default "", indicating the pipeline is not containerized.
container_engine is default podman.
container_base is default iowarp:iowarp-deps:latest. 
Container image files will be stored in ~/.ppi-jarvis/containers/container_name.Dockerfile
A container mainfest indicating the packages installed in the container will be stored in ~/.ppi-jarvis/containers/container_name.yaml
Create the files if they do not already exist.

## Container Manifest

This is a YAML file indicating the packages installed in the container.
Will be stored in ~/.ppi-jarvis/containers/container_name.yaml

```yaml
pkg_type: deploy_mode
```

pkg_type should be the concretized pacakge type. For example, builtin.ior points to the builtin repo's ior package.
It should not be pipeline-specific package names.

## jarvis ppl load yaml [path]

This will load a pipeline script. With these new parameters, we first must build the container image for container_name.
This is done by iterating over each pkgs and interceptors value in the pipeline, calling a new method augment_container.
This method is static and should be apart of the Package base class.
Packages can inherit this to augment the container image file with its specific installation steps.

The container manifest and image should be loaded from ~/.ppi-jarvis/container_name.yaml and ~/.ppi-jarvis/container_name.Dockerfile.
When iterating, check if a package is already apart of the built container.
If it is, then skip calling its augment_container function and go next.

## pkg base config menu

Packages have a deploy_mode parameter. 

A package should support multiple baremetal and containerized deployments. The choice of container should be made per-package.
Each package can set the deploy_mode to indicate the specific option they want to augment the dockerfile with.

I believe this is already supported in the current implementation. We also already have the RouteApp and RouteService package types,
in addition to ContainerApp and ContainerService. There should not be any changes needed here.

## pkg.augment_container()

This should return a string (in the format of a dockerfile) containing how to install the dependencies
for the particular container. This should be called right after pkg.configure() during pipeline's configure function.
It should be called only if the package has not been installed the container already. To check if it has been installed,
we must check the container manifest. This file stores a dictionary of all packages installed. If this package is in the
manifest and has the same deploy_mode, then skip. If it is in the manifest, but has a different deploy_mode, then print
an error saying this is not allowed to have different versions installed in the same container.

## jarvis container remove [container_name]
Destroy the image container_name. Remove the files associated with it in ~/.ppi-jarvis-containers

## jarvis container list
List the set of containers in ~/.ppi-jarvis/containers

## jarvis ppl conf container_name=X container_engine=Y container_base=Z

Add a new jarvis ppl conf parameter. It takes as input containerize as a bool for now.

This will put the pipeline in a "container" deployment mode, rather than the default mode.

This will indicate that each package in the pipeline should use the container deployment mode.
Every existing package in the pipeline will be reconfigured to use deploy_mode=container_engine

## jarvis ppl create [pipeline_name] [container_name (default="")]

Update jarvis ppl create to allow us to create containerized pipelines without requiring a call to jarvis ppl containerize.
The container_name and container_engine stored in the pipeline yaml file.
When packages are being configured, the configure function should check the parent pipeline for container_name and container_engine.
This will be used to set the deploy_mode
```

### `ai-prompts/phase2-hostfile.md`

```markdown
## Hostfile
Implement the hostfile class. Put in jarvis_cd.util.

Hostfiles contain a set of machines.

Host Text Files
Hostfiles can be stored as text files on a filesystem. They have the following syntax:

ares-comp-01
ares-comp-[02-04]
ares-comp-[05-09,11,12-14]-40g

Hostfile Import
from jarvis_util.util.hostfile import Hostfile

Hostfile Constructor
The hostfile has the following constructor:

```python
class Hostfile:
    """
    Parse a hostfile or store a set of hosts passed in manually.
    """

    def __init__(self, path=None, hosts=None, hosts_ip=None,
                 text=None, find_ips=True, load_path=True):
        """
        Constructor. Parse hostfile or store existing host list.

        :param path: The path to the hostfile
        :param hosts: a list of strings representing all hostnames
        :param hosts_ip: a list of strings representing all host IPs
        :param text: Text of a hostfile
        :param find_ips: Whether to construct host_ip and all_host_ip fields
        :param load_path: whether or not path should exist and be read from on init
        """

    def subset(self, count, path=None):
        return Hostfile(path, hosts=self.hosts[0:count], find_ips=self.find_ips, load_path=False) 

    def copy(self):
        return self.subset(len(self))

    def is_local(self):
        """
        Whether this file contains only 'localhost'

        :return: True or false
        """
        if len(self) == 0:
            return True
        if len(self.hosts) == 1:
            if self.hosts[0] == 'localhost':
                return True
            if self.hosts[0] == socket.gethostbyname('localhost'):
                return True
        if len(self.hosts_ip) == 1:
            if self.hosts_ip[0] == socket.gethostbyname('localhost'):
                return True
        return False

    def save(self, path): 
        self.path = path
        with open(path, 'w', encoding='utf-8') as fp:
            fp.write('\n'.join(self.hosts))
        return self

    def list(self):
        return [Hostfile(hosts=[host]) for host in self.hosts]

    def enumerate(self):
        return enumerate(self.list())

    def host_str(self, sep=','):
        return sep.join(self.hosts)

    def ip_str(self, sep=','):
        return sep.join(self.hosts_ip)
```

Hostfile for the current machine
To get the localhost file:

hostfile = Hostfile()

Hostfile from a filesystem
To load a hostfile from the filesystem:

hostfile = Hostfile(hostfile=f'{HERE}/test_hostfile.txt')

Host names and IPs
To get the host names and IP addresses, the Hostfile stores the hosts and hosts_ip variables. They are lists of strings.

hostfile = Hostfile()
print(hostfile.hosts)
print(hostfile.hosts_ip)

Output:

['localhost']
['127.0.0.1']
```

### `ai-prompts/phase2-logging.md`

```markdown
## Logging

Put this in jarvis_cd.util.

Implement a logger class that allows messages to print to screen with various colors.
```
logger = Logger()
logger.print(Color.YELLOW, "msg")
```
```

### `ai-prompts/phase3-launch.md`

```markdown
## Application launching

We need to build classes for launching applications. We call these 
executables. Put this in jarvis_cd.shell.

### CoreExec

Every executable should have an exit code and the standard output.
```python
class CoreExec(ABC):
    """
    An abstract class representing a class which is intended to run
    shell commands. This includes SSH, MPI, etc.
    """

    def __init__(self):
        self.exit_code = {} # hostname -> exit_code
        self.stdout = {}  # hostname -> stderr 
        self.stderr = {}  # hostname -> stdout

    @abstractmethod
    def run():
        pass

    @abstractmethod
    def get_cmd():
        pass
```

This is the base class of all future executables.

### ExecInfo

The ExecInfo is a data structure that contains all parameters needed by any
of the executables. Each Exec* class should have a custom ExecInfo. 

For example:
```python
class SshExecInfo(ExecInfo):
  def __init__(self, **kwargs):
    super().__init__(exec_type=ExecType.SSH, **kwargs)
```

At a minimum:
```python
class ExecInfo:
    """
    Contains all information needed to execute a program. This includes
    parameters such as the path to key-pairs, the hosts to run the program
    on, number of processes, etc.
    """
    def __init__(self,  exec_type=ExecType.LOCAL, nprocs=None, ppn=None,
                 user=None, pkey=None, port=None,
                 hostfile=None, env=None,
                 sleep_ms=0, sudo=False, sudoenv=True, cwd=None,
                 collect_output=None, pipe_stdout=None, pipe_stderr=None,
                 hide_output=None, exec_async=False, stdin=None,
                 strict_ssh=False, timeout=None):
        """

        :param exec_type: How to execute a program. SSH, MPI, Local, etc.
        :param nprocs: Number of processes to spawn. E.g., MPI uses this
        :param ppn: Number of processes per node. E.g., MPI uses this
        :param user: The user to execute command under. E.g., SSH, PSSH
        :param pkey: The path to the private key. E.g., SSH, PSSH
        :param port: The port to use for connection. E.g., SSH, PSSH
        :param hostfile: The hosts to launch command on. E.g., PSSH, MPI
        :param env: The environment variables to use for command.
        :param sleep_ms: Sleep for a period of time AFTER executing
        :param sudo: Execute command with root privilege. E.g., SSH, PSSH
        :param sudoenv: Support environment preservation in sudo
        :param cwd: Set current working directory. E.g., SSH, PSSH
        :param collect_output: Collect program output in python buffer
        :param pipe_stdout: Pipe STDOUT into a file. (path string)
        :param pipe_stderr: Pipe STDERR into a file. (path string)
        :param hide_output: Whether to print output to console
        :param exec_async: Whether to execute program asynchronously
        :param stdin: Any input needed by the program. Only local 
        :param strict_ssh: Strict ssh host key verification
        :param timeout: Timeout subprocess within timeframe
        """
```

### LocalExec

Takes as input a command and an exec_info data structure.

Main features:
1. Launches a subprocess using the command. The command should just be a direct shell command. Safety is not required. It should launch the process as asynchronous and implement a wait function to wait for completion. If the exec_async parameter of exec_info is False, then wait immediately after launching. Must pass the environment stored in exec_info.
2. Must support the ability to print to console, to a file, and to a string buffer while application is running. It should not collect the stdout buffer all the way at the end of the program. We want to show progress as programs run. This will likely require the use of a thread that polls the output buffer.
3. Must store the return code of the executable.

### SshExec

Inherits from LocalExec.

Takes as input a command and exec_info. Assume passwordless authentication. SshExecInfo can speicfy a public key for connection.
The command should be 

It should route the environment variables in the ssh command as well. 

### PsshExec

Inherits from CoreExec. Should asynchronously launch SshExec commands. One for each item in the hostfile specified in the exec_info.

It will copy the outputs from each individual SshExec and update stdout, stderr, and exit_code

### ScpExec

Takes as input a list of path strings. The paths can be files or directories. They may also contain simple regexes, like /mnt/home/*.txt to copy only certain file types.
Assume the file is located on this host and is being propogated to the remote host. We will never download from the remote host.

It is possible that ScpExec is called with this host. So it may copy a file to the same file that already exists. We need to ensure that the algorithm does not override
the file and make it empty.

Something like this:
```python
class _Scp(LocalExec):
    """
    This class provides methods to copy data over SSH using the "rsync"
    command utility in Linux
    """

    def __init__(self, src_path, dst_path, exec_info):
        """
        Copy a file or directory from source to destination via rsync

        :param src_path: The path to the file on the host
        :param dst_path: The desired file path on the remote host
        :param exec_info: Info needed to execute command with SSH
        """

        self.addr = exec_info.hostfile.hosts[0]
        if self.addr == 'localhost' or self.addr == '127.0.0.1':
            return
        self.src_path = src_path
        self.dst_path = dst_path
        self.user = exec_info.user
        self.pkey = exec_info.pkey
        self.port = exec_info.port
        self.sudo = exec_info.sudo
        self.jutil = JutilManager.get_instance()
        super().__init__(self.rsync_cmd(src_path, dst_path),
                         exec_info.mod(env=exec_info.basic_env))

    def rsync_cmd(self, src_path, dst_path):
        lines = ['rsync -ha']
        if self.pkey is not None or self.port is not None:
            ssh_lines = ['ssh']
            if self.pkey is not None:
                ssh_lines.append(f'-i {self.pkey}')
            if self.port is not None:
                ssh_lines.append(f'-p {self.port}')
            ssh_cmd = ' '.join(ssh_lines)
            lines.append(f'-e \'{ssh_cmd}\'')
        lines.append(src_path)
        if self.user is not None:
            lines.append(f'{self.user}@{self.addr}:{dst_path}')
        else:
            lines.append(f'{self.addr}:{dst_path}')
        rsync_cmd = ' '.join(lines)
        if self.jutil.debug_scp:
            print(rsync_cmd)
        return rsync_cmd


class Scp(Executable):
    """
    Secure copy data between two hosts.
    """

    def __init__(self, paths, exec_info):
        """
        Copy files via rsync.

        Case 1: Paths is a single file:
        paths = '/tmp/hi.txt'
        '/tmp/hi.txt' will be copied to user@host:/tmp/hi.txt

        Case 2: Paths is a list of files:
        paths = ['/tmp/hi1.txt', '/tmp/hi2.txt']
        Repeat Case 1 twice.

        Case 3: Paths is a list of tuples of files:
        paths = [('/tmp/hi.txt', '/tmp/remote_hi.txt')]
        '/tmp/hi.txt' will be copied to user@host:'/tmp/remote_hi.txt'

        :param paths: Either a path to a file, a list of files, or a list of
        tuples of files.
        :param exec_info: Connection information for SSH
        """

        super().__init__()
        self.paths = paths
        self.exec_info = exec_info
        self.scp_nodes = []
        if isinstance(paths, str):
            self._exec_single_path(paths)
        if isinstance(paths, list):
            if len(paths) == 0:
                raise Exception('Must have at least one path to scp')
            elif isinstance(paths[0], str):
                self._exec_many_paths(paths)
            elif isinstance(paths[0], tuple):
                self._exec_many_paths_tuple(paths)
            elif isinstance(paths[0], list):
                self._exec_many_paths_tuple(paths)
        if not self.exec_info.exec_async:
            self.wait()

    def _exec_single_path(self, path):
        self.scp_nodes.append(_Scp(path, path, self.exec_info))

    def _exec_many_paths(self, paths):
        for path in paths:
            self.scp_nodes.append(_Scp(path, path, self.exec_info))

    def _exec_many_paths_tuple(self, path_tlist):
        for src, dst in path_tlist:
            self.scp_nodes.append(_Scp(src, dst, self.exec_info))

    def wait(self):
        self.wait_list(self.scp_nodes)
        self.smash_list_outputs(self.scp_nodes)
        self.set_exit_code()
        return self.exit_code

    def set_exit_code(self):
        self.set_exit_code_list(self.scp_nodes)
```

### PscpExec

Similar to PsshExec, but for Scp. Calls asynchronous ScpExec functions and builds stdout, stderr, and exit_code similarly.

### MpiVersion

Introspect the specific mpi version used in this environment:
```python
class MpiVersion(LocalExec):
    """
    Introspect the current MPI implementation from the machine using
    mpirun --version
    """

    def __init__(self, exec_info):
        self.cmd = 'mpiexec --version'
        super().__init__(self.cmd,
                         exec_info.mod(env=exec_info.basic_env,
                                       collect_output=True,
                                       hide_output=True,
                                       do_dbg=False))
        vinfo = self.stdout
        # print(f'MPI INFO: stdout={vinfo} stderr={self.stderr}')
        if 'mpich' in vinfo.lower():
            self.version = ExecType.MPICH
        elif 'Open MPI' in vinfo or 'OpenRTE' in vinfo:
            self.version = ExecType.OPENMPI
        elif 'Intel(R) MPI Library' in vinfo:
            # NOTE(llogan): similar to MPICH
            self.version = ExecType.INTEL_MPI
        elif 'mpiexec version' in vinfo:
            self.version = ExecType.CRAY_MPICH
        else:
            raise Exception(f'Could not identify MPI implementation: {vinfo}')
```

### LocalMpiExec

Base class used by all other mpi implementations:
```python
class LocalMpiExec(LocalExec):
    def __init__(self, cmd, exec_info):
        self.cmd = cmd
        self.nprocs = exec_info.nprocs
        self.ppn = exec_info.ppn
        self.hostfile = exec_info.hostfile
        self.mpi_env = exec_info.env
        if exec_info.do_dbg:
            self.base_cmd = cmd # To append to the extra processes
            self.cmd = self.get_dbg_cmd(cmd, exec_info)
        super().__init__(self.mpicmd(),
                         exec_info.mod(env=exec_info.basic_env,
                                       do_dbg=False))

    @abstractmethod
    def mpicmd(self):
        pass
```

### OpenmpiExec

```python
class OpenMpiExec(LocalMpiExec):
    """
    This class contains methods for executing a command in parallel
    using MPI.
    """
    def mpicmd(self):
        params = [f'mpiexec']
        params.append('--oversubscribe')
        params.append('--allow-run-as-root')  # For docker
        if self.ppn is not None:
            params.append(f'-npernode {self.ppn}')
        if len(self.hostfile):
            if self.hostfile.path is None:
                params.append(f'--host {",".join(self.hostfile.hosts)}')
            else:
                params.append(f'--hostfile {self.hostfile.path}')
        params += [f'-x {key}=\"{val}\"'
                   for key, val in self.mpi_env.items()]
        if self.cmd.startswith('gdbserver'):
            params.append(f'-n 1 {self.cmd}')
            if self.nprocs > 1:
                params.append(f': -n {self.nprocs - 1} {self.base_cmd}')
        else:
            params.append(f'-n {self.nprocs}')
            params.append(self.cmd)
        cmd = ' '.join(params)
        return cmd
```

### MpichExec

```python
class MpichExec(LocalMpiExec):
    """
    This class contains methods for executing a command in parallel
    using MPI.
    """

    def mpicmd(self):
        params = ['mpiexec']

        if self.ppn is not None:
            params.append(f'-ppn {self.ppn}')

        if len(self.hostfile):
            if self.hostfile.path is None:
                params.append(f'--host {",".join(self.hostfile.hosts)}')
            else:
                params.append(f'--hostfile {self.hostfile.path}')

        params += [f'-genv {key}=\"{val}\"'
                   for key, val in self.mpi_env.items()]

        if self.cmd.startswith('gdbserver'):
            params.append(f'-n 1 {self.cmd}')
            if self.nprocs > 1:
                params.append(f': -n {self.nprocs - 1} {self.base_cmd}')
        else:
            params.append(f'-n {self.nprocs}')
            params.append(self.cmd)

        cmd = ' '.join(params)
        return cmd
```

### CrayMpichExec

```python
class CrayMpichExec(LocalMpiExec):
    """
    This class contains methods for executing a command in parallel
    using MPI.
    """
    def mpicmd(self):
        params = [f'mpiexec -n {self.nprocs}']
        if self.ppn is not None:
            params.append(f'--ppn {self.ppn}')
        if len(self.hostfile):
            if self.hostfile.hosts[0] == 'localhost' and len(self.hostfile) == 1:
                pass
            elif self.hostfile.path is None:
                params.append(f'--hosts {",".join(self.hostfile.hosts)}')
            else:
                params.append(f'--hostfile {self.hostfile.path}')
        params += [f'--env {key}=\"{val}\"'
                   for key, val in self.mpi_env.items()]
        params.append(self.cmd)
        cmd = ' '.join(params) 
        return cmd
```

### Exec

A factory that can be used to call any of the other executables.

## Specific Applications

Add the following to jarvis_cd.shell as system.py

### Kill

Inherits from Exec. Wraps around pkill on Linux.

```
class Kill(Exec):
    """
    Kill all processes which match the name regex.
    """

    def __init__(self, cmd, exec_info, partial=True):
        """
        Kill all processes which match the name regex.

        :param cmd: A regex for the command to kill
        :param exec_info: Info needed to execute the command
        """
        partial_cmd = "-f" if partial else ""
        super().__init__(f"pkill -9 {partial_cmd} {cmd}", exec_info)
```

### GdbServer

Implement a GdbServer Exec class in shell. It inherits from Exec. 

It will take as input a command and a port number. 
It will launch a gdbserver with the command and port.
```

### `ai-prompts/phase4-resource-graph.md`

```markdown
# Resource Graph

Resource graph contains primarily storage resources. It automatically collects the set of mounted storage devices, only the ones that the current user has permissions to read or write to. 

Ideally, the introspection would use a portable python library to determine the following information, but if systems-specific tools are needed that is ok too.

## Resource Collection Binary

We should have a resource graph collection binary file that executes per-machine. this will collect
the machine state per-machine

## Resource Graph Class

We should have a resource graph class that is placed in jarvis_cd.util

## Generate resource graph

```yaml
jarvis rg build
```

Using the current hostfile (set by ``jarvis hostfile set``), it will collect the set of storage devices on each node in the hostfile and then produce a view of common storages between the nodes.
They are required to have the same mount point.

You also should run a benchmark for 25 seconds on each storage device to get the initial performance profile of the storage devices. Run the profiles on separate threads. Collect 4KB randwrite bandwidht and 1MB seqwrite bandwidth.

We will need to build a new jarvis_cd.shell command to wrap around the resource_graph python script you will build in the bin directory. It should inherit from Exec. The jarvis ppl rg build should use PsshExecInfo for this execution.

## Storage configuration

Ideally, the following information would be collected:
```yaml
fs:
- avail: 500GB
  dev_type: ssd
  device: /dev/sdb1
  fs_type: xfs
  model: Samsung SSD 860
  mount: /mnt/ssd/${USER}
  parent: /dev/sdb
  shared: false  # is this a PFS or local storage? 
  needs_root: false  # can the user read /write here?
  4k_randwrite_bw: 8mbps  
  1m_seqwrite_bw: 1000mbps
```
```

### `ai-prompts/phase5-jarvis-repos.md`

```markdown

# Jarvis-CD

Jarvis-CD is a unified platform for deploying various applications, including storage systems and benchmarks. Many applications have complex configuration spaces and are difficult to deploy across different machines.

We provide a builtin repo which contains various applications to deploy. We refer to applications as "jarivs pkgs" which can be connected to form "deployment pipelines".

Use the argparse class to build the arguments for jarvis. Create a binary called jarvis.

Implement in jarvis_cd.core

## Main Utility Files
1. The jarvis configuration ~/.ppi-jarvis/jarvis_config.yaml
2. The repos configuration ~/.ppi-jarvis/repos.yaml
3. The resource graph ~/.ppi-jarvis/resource_graph.yaml

## Bootstrapping Jarvis

```
jarvis init [CONFIG_DIR] [PRIVATE_DIR] [SHARED_DIR]
```

* CONFIG_DIR: A directory where jarvis metadata for pkgs and pipelines are stored. This directory can be anywhere that the current user can access.
* PRIVATE_DIR: A directory which is common across all machines, but stores data locally to the machine. Some jarvis pkgs require certain data to be stored per-machine. OrangeFS is an example.
* SHARED_DIR: A directory which is common across all machines, where each machine has the same view of data in the directory.

## Creating a pipeline

```
jarvis ppl create hermes
jarvis ppl append repo.ior
jarvis ppl append ior ior2 #  Assuming repo is the top of repos.yaml, should be the same as prior command
jarvis pkg conf hermes.ior nprocs=256
```

Pipeline should have two packages: ior, ior2. 

## Jarvis Repo structure

Jarvis repos contain a set of packages. Custom repos have the following structure:

```
my_org_name
└── my_org_name
    └── orangefs
        └── package.py
```

## Register a custom repo

You can then register the repo as follows:

```bash
jarvis repo add /path/to/my_org_name
```

Whenever a new repo is added, it will be the first place
jarvis searches for pkgs.

## Creating pkgs from a template

You can then add pkgs to the repo as follows:

```bash
jarvis repo create [name] [pkg_class]
```

pkg_class can be one of:

- service
- app
- interceptor

For example:

```bash
jarvis repo create hermes service
```

The repo will then look as follows:

```
my_org_name
└── my_org_name
    ├── hermes
    │   └── package.py
    └── orangefs
        └── package.py
```

## The `Pkg` Base Class

This section will go over the variables and methods common across all Pkg types.
These variables will be initialized automatically.

```python
class Pkg:
  def __init__(self):
    self.pkg_dir = '...'
    self.shared_dir = '...'
    self.private_dir = '...'
    self.env = {}
    self.mod_env = {}
    self.config = {}
    self.global_id = '...'
    self.pkg_id = '...'
```

### `pkg_id` and `global_id`

The Global ID (global_id) is the globally unique ID of the a package in all of
jarvis. It is a dot-separated string. Typically, the format is:

```
{pipeline_id}.{pkg_id}
```

The Package ID (pkg_id) is the unique ID of the package relative to a pipeline.
This is a simple string (no dots).

For example, from section 5.1, we appended 3 packages: hermes, hermes_mpiio, and
gray_scott. hermes, hermes_mpiio, and gray_scott are also the pkg_ids. The
global_ids would be:

```
test.hermes
test.hermes_mpiio
test.gray_scott
```

Usage:

```
self.global_id
self.pkg_id
```

### `pkg_dir`

The package directory is the location of the class python file on the filesystem.
For example, when calling `jarvis repo create hermes`, the directory
created by this command will be the pkg_dir.

One use case for the pkg_dir is for creating template configuration files.
For example, OrangeFS has a complex XML configuration which would be a pain
to repeat in Python. One could include an OrangeFS XML config in their
package directory and commit as part of their Jarvis repo.

Usage:

```
self.pkg_dir
```

### `shared_dir`

The shared_dir is a directory stored on a filesystem common across all nodes
in the hostfile. Each node has the same view of data in the shared_dir. The
shared_dir contains data for the specific pkg to avoid conflicts in
a pipeline with multiple pkgs.

For example, when deploying Hermes, we assume that each node has the Hermes
configuration file. Each node is expected to have the same configuration file.
We store the Hermes config in the shared_dir.

Usage:

```
self.shared_dir
```

### `private_dir`

This is a directory which is common across all nodes, but nodes do not
have the same view of data.

For example, when deploying OrangeFS, it is required that each node has a file
called pvfs2tab. It essentially stores the protocol + address that OrangeFS
uses for networking. However, the content of this file is different for
each node. Storing it in the shared_dir would be incorrect. This is why we
need the private_dir.

Usage:

```
self.private_dir
```

### `env`

Jarvis pipelines store the current environment in a YAML file, which represents
a python dictionary. The key is the environment variable name (string) and the
value is the intended meaning of the variable. There is a single environment
used for the entire pipeline. Each pipeline stores its own environment to avoid
conflict.

Usage:

```
self.env['VAR_NAME']
```

Environments can be modified using various helper functions:

```
self.track_env(env_track_dict)
self.prepend_env(env_name, val)
self.setenv(env_name, val)
```

Viewing the env YAML file for the current pipeline from the CLI

```
cat `jarvis path`/env.yaml
```

### `mod_env`
a python dictionary. Essentially a copy of `env`. However, `mod_env` also stores the LD_PRELOAD environment variable for interception. This can cause conflict if used irresponsibly. Not every program should be intercepted.

For example, we use this for Hermes to intercept POSIX I/O. However, POSIX is widely-used for I/O so we like to be very specific when it is used.

`mod_env` can be modified using the same functions as `env`.

```
self.track_env(env_track_dict)
self.prepend_env(env_name, val)
self.setenv(env_name, val)
```

### `config`

The Jarvis configuration is stored in `{pkg_dir}/{pkg_id}.yaml`.
Unlike the environment dict, this stores variables that are specific to
the package. They are not global to the pipeline.

For example, OrangeFS and Hermes need to know the desired port number and
RPC protocol. This information is specific to the program, not the entire
pipeline.

Usage:

```
self.config['VAR_NAME']
```

### `jarvis`

The Jarvis CD configuration manager stores various properties global to
all of Jarvis. The most important information is the hostfile and resource_graph,
discussed in the next sections.

Usage:

```
self.jarvis
```

### `hostfile`

The hostfile contains the set of all hosts that Jarvis has access to.
The hostfile format is documented [here](https://github.com/scs-lab/jarvis-util/wiki/4.-Hostfile).

Usage:

```
self.jarvis.hostfile
```

### `resource_graph`

The resource graph can be queried to get storage and networking information
for storing large volumes of data.

```
self.jarvis.resource_graph
```

## Building a Service or Application

Services and Applications implement the same interface, but are logically
slightly different. A service is long-running and would typically require
the users to manually stop it. Applications stop automatically when it
finishes doing what it's doing. Jarvis can deploy services alongside
applications to avoid the manual stop when benchmarking.

### `_init`

The Jarvis constructor (`_init`) is used to initialize global variables.
Don't assume that self.config is initialized.
This is to provide an overview of the parameters of this class.
Default values should almost always be None.

```python
def _init(self):
  self.gray_scott_path = None
```

### `_configure_menu`

The function defines the set of command line options that the user can set.
An example configure menu is below:

```python
def _configure_menu(self):
    """
    Create a CLI menu for the configurator method.
    For thorough documentation of these parameters, view:
    https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

    :return: List(dict)
    """
    return [
        {
            'name': 'port',
            'msg': 'The port to listen for data on',
            'type': int,
            'default': 8080
        }
    ]
```

This function is called whenever configuring a package. For example,

```bash
jarvis pkg configure hermes --sleep=10 --port=25
```

This will configure hermes to sleep for 10 seconds after launching to give enough
time to fully start Hermes. Sleep is apart of all configure menus by default.

The format of the output dict is documented in more detail
[here](https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing).

### `configure`

It takes as input a
dictionary. The keys of this dict are determined from \_configure_menu function
output. It is responsible for updating the self.config variable appropriately
and generating the application-specific configuration files.

Below is an example for Hermes. This example takes as input the port option,
modifies the hermes_server dict, and then stores the dict in a YAML file
in the shared directory.

```python
def configure(self, **kwargs):
    """
    Converts the Jarvis configuration to application-specific configuration.
    E.g., OrangeFS produces an orangefs.xml file.

    :param config: The human-readable jarvis YAML configuration for the
    application.
    :return: None
    """
    self.update_config(kwargs, rebuild=False)
    hermes_server_conf = {
      'port': self.config['port']
    }
    YamlFile(f'{self.shared_dir}/hermes_server_yaml').save(hermes_server_conf)
```

This function is called whenever configuring a packge. Specifically, this is
called immediately after \_configure_menu. For example,

```
jarvis pkg configure hermes --sleep=10 --port=25
```

will make the kwargs dict be:

```python
{
  'sleep': 10,
  'port': 25
}
```

### `start`

The start function is called during `jarvis ppl run` and `jarvis ppl start`.
This function should execute the program itself.

Below is an example for Hermes:

```python
def start(self):
    """
    Launch an application. E.g., OrangeFS will launch the servers, clients,
    and metadata services on all necessary pkgs.

    :return: None
    """
    self.daemon_pkg = Exec('hermes_daemon',
                            PsshExecInfo(hostfile=self.jarvis.hostfile,
                                         env=self.env,
                                         exec_async=True))
    time.sleep(self.config['sleep'])
    print('Done sleeping')
```

### `stop`

The stop function is called during `jarvis ppl run` and `jarvis ppl stop`.
This function should terminate the program.

Below is an example for Hermes:

```python
def stop(self):
    """
    Stop a running application. E.g., OrangeFS will terminate the servers,
    clients, and metadata services.

    :return: None
    """
    Exec('finalize_hermes',
         PsshExecInfo(hostfile=self.jarvis.hostfile,
                      env=self.env))
    if self.daemon_pkg is not None:
        self.daemon_pkg.wait()
    Kill('hermes_daemon',
         PsshExecInfo(hostfile=self.jarvis.hostfile,
                      env=self.env))
```

This is not typically implemented for Applications, but it is for Services.

### `kill`
This function is called during `jarvis ppl kill`. It should forcibly terminate a program, typically using Kill.

Below is an example for Hermes
```python
def kill(self):
    """
    Forcibly a running application. E.g., OrangeFS will terminate the
    servers, clients, and metadata services.

    :return: None
    """
    Kill('hermes_daemon',
         PsshExecInfo(hostfile=self.jarvis.hostfile,
                      env=self.env))
```

### `clean`

The `clean` function is called during `jarvis ppl clean`.
It clears all intermediate data produced by a pipeline.

Below is the prototype

```python
def clean(self):
    """
    Destroy all data for an application. E.g., OrangeFS will delete all
    metadata and data directories in addition to the orangefs.xml file.

    :return: None
    """
    pass
```

### `status`

The `status` function is called during `jarvis ppl status`
It determines whether or not a service is running. This is not typically
implemented for Applications, but it is for Services.

## Building an Interceptor

Interceptors are used to modify environment variables to route system and library
calls to new functions.

Interceptors have a slightly different interface -- they only have:
`_init`, `_configure_menu`, `configure`, and `modify_env`. The only new function
here is modify_env. The others were defined in the previous section and behave
the exact same way.

### `configure`

Configuring an interceptor tends to be a little different. The interceptors
are not typically responsible for generating configuration files like the
applications and services do. These typically are responsible solely for
modifying the environment.

Below, we show an example of configure for the Hermes MPI I/O interceptor:

```python
def configure(self, **kwargs):
    """
    Converts the Jarvis configuration to application-specific configuration.
    E.g., OrangeFS produces an orangefs.xml file.

    :param kwargs: Configuration parameters for this pkg.
    :return: None
    """
    self.update_config(kwargs, rebuild=False)
    self.config['HERMES_MPIIO'] = self.find_library('hermes_mpiio')
    if self.config['HERMES_MPIIO'] is None:
        raise Exception('Could not find hermes_mpiio')
    print(f'Found libhermes_mpiio.so at {self.config["HERMES_MPIIO"]}')
```

Here we use self.find_library() to check if we can find the shared library
hermes_mpiio in the system paths. This function introspects LD_LIBRARY_PATH
and determines if hermes_mpiio is in the path. It saves the path in the pkg
configuration (self.config).

### `modify_env`

Below is an example of the MPI I/O interceptor for Hermes:

```python
def modify_env(self):
    """
    Modify the jarvis environment.

    :return: None
    """
    self.prepend_env('LD_PRELOAD', self.config['HERMES_MPIIO'])
```

### Example Package: IOR

```python
"""
This module provides classes and methods to launch the Ior application.
Ior is a benchmark tool for measuring the performance of I/O systems.
It is a simple tool that can be used to measure the performance of a file system.
It is mainly targeted for HPC systems and parallel I/O.
"""
from jarvis_cd.core.pkg import Application
from jarvis_util import *
import os


class Ior(Application):
    """
    This class provides methods to launch the Ior application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'write',
                'msg': 'Perform a write workload',
                'type': bool,
                'default': True,
                'choices': [],
                'args': [],
            },
            {
                'name': 'read',
                'msg': 'Perform a read workload',
                'type': bool,
                'default': False,
            },
            {
                'name': 'xfer',
                'msg': 'The size of data transfer',
                'type': str,
                'default': '1m',
            },
            {
                'name': 'block',
                'msg': 'Amount of data to generate per-process',
                'type': str,
                'default': '32m',
                'aliases': ['block_size']
            },
            {
                'name': 'api',
                'msg': 'The I/O api to use',
                'type': str,
                'choices': ['posix', 'mpiio', 'hdf5'],
                'default': 'posix',
            },
            {
                'name': 'fpp',
                'msg': 'Use file-per-process',
                'type': bool,
                'default': False,
            },
            {
                'name': 'reps',
                'msg': 'Number of times to repeat',
                'type': int,
                'default': 1,
            },
            {
                'name': 'nprocs',
                'msg': 'Number of processes',
                'type': int,
                'default': 1,
            },
            {
                'name': 'ppn',
                'msg': 'The number of processes per node',
                'type': int,
                'default': 16,
            },
            {
                'name': 'out',
                'msg': 'Path to the output file',
                'type': str,
                'default': '/tmp/ior.bin',
                'aliases': ['output']
            },
            {
                'name': 'log',
                'msg': 'Path to IOR output log',
                'type': str,
                'default': None,
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        self.config['api'] = self.config['api'].upper()

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        cmd = [
            'ior',
            '-k',
            f'-b {self.config["block"]}',
            f'-t {self.config["xfer"]}',
            f'-a {self.config["api"]}',
            f'-o {self.config["out"]}',
        ]
        out = os.path.expandvars(self.config['out'])
        if self.config['write']:
            cmd.append('-w')
        if self.config['read']:
            cmd.append('-r')
        if self.config['fpp']:
            cmd.append('-F')
        if self.config['reps'] > 1:
            cmd.append(f'-i {self.config["reps"]}')
        if '.' in os.path.basename(out):
            os.makedirs(str(pathlib.Path(out).parent),
                        exist_ok=True)
        else:
            os.makedirs(out, exist_ok=True)
        # pipe_stdout=self.config['log']
        Exec('which mpiexec',
             LocalExecInfo(env=self.mod_env))
        Exec(' '.join(cmd),
             MpiExecInfo(env=self.mod_env,
                         hostfile=self.jarvis.hostfile,
                         nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'],
                         do_dbg=self.config['do_dbg'],
                         dbg_port=self.config['dbg_port']))

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        Rm(self.config['out'] + '*',
           PsshExecInfo(env=self.env,
                        hostfile=self.jarvis.hostfile))

    def _get_stat(self, stat_dict):
        """
        Get statistics from the application.

        :param stat_dict: A dictionary of statistics.
        :return: None
        """
        stat_dict[f'{self.pkg_id}.runtime'] = self.start_time
```




# Pipeline Scripts

Pipeline scripts are useful for storing cross-platform unit tests.
They store all of the information needed to create and execute
a pipeline.

## Running a pipline script

Pipeline scripts are YAML files and can be executed as follows:
```bash
jarvis ppl load yaml /path/to/my_pipeline.yaml
jarvis ppl run
```

Alternatively, if you want to load + run the script:
```bash
jarvis ppl run yaml /path/to/my_pipeline.yaml
```

## Updating a pipeline

To load changes made to a pipeline script, you can run:
```bash
jarvis ppl update yaml
```

The pipeline will store the path so you don't have to repeat.

```python
name: chimaera_unit_ipc
env: chimaera
pkgs:
  - pkg_type: chimaera_run
    pkg_name: chimaera_run
    sleep: 10
    do_dbg: true
    dbg_port: 4000
  - pkg_type: chimaera_unit_tests
    pkg_name: chimaera_unit_tests
    TEST_CASE: TestBdevIo
    do_dbg: true
    dbg_port: 4001
    interceptors: hermes_api
interceptors:
  - pkg_type: hermes_api
    pkg_name: hermes_api
```

## Class loader

This should be used to dynamically load packages.

```python
def load_class(import_str, path, class_name):
    """
    Loads a class from a python file.

    :param import_str: A python import string. E.g., for "myrepo.dir1.pkg"
    :param path: The absolute path to the directory which contains the
    beginning of the import statement. Let's say you have a git repo located
    at "/home/hello/myrepo". The git repo has a subdirectory called "myrepo",
    so "/home/hello/myrepo/myrepo". In this case, path would be
    "/home/hello/myrepo". The import string "myrepo.dir1.pkg" will find
    the "myrepo" part of the import string at "/home/hello/myrepo/myrepo".
    :param class_name: The name of the class in the file
    :return: The class data type
    """
    fullpath = os.path.join(path, import_str.replace('.', '/') + '.py')
    if not os.path.exists(fullpath):
        return None
    sys.path.insert(0, path)
    module = __import__(import_str, fromlist=[class_name])
    cls = getattr(module, class_name)
    sys.path.pop(0)
    return cls

```
```

### `ai-prompts/phase6-jarvis-env.md`

```markdown
Jarvis environments capture a variety of common environment variables and store them within a pipeline.

## Build an environment

```
jarvis ppl env build ENV1=VAL1 ENV2=VAL2 ...
```

This will parse the remainder arguments for any additional parameters the user may expect.

This will edit the pipelines env.yaml file

At minimum, the environment variables will be:
```
CMAKE_MODULE_PATH
CMAKE_PREFIX_PATH
CPATH
JAVA_HOME
LD_LIBRARY_PATH
LIBRARY_PATH
PATH
PYTHONPATH
```

If there are any other common environment variables you can think of, please add them here.

## Build a named environment

Named environments store environment variables across pipelines in the config directory under a folder named env.

```
jarvis env build [env_name] ENV1=VAL1 ENV2=VAL2 ...
```

## Copy a named environment
```
jarvis ppl env copy [env_name]
```

## Environment Usage

We need to load the environment dict when a pipeline is first constructed. This environment contains all environment variables needed by every sub-package. There should be two environments in each package: env and mod_env. env is passed to  each package in a pipeline. Modification to the pipeline will be propogated to future packages in the pipeline. mod_env is a deep copy of the environment. It will be specific to the package.
```

### `ai-prompts/phase8-paths.md`

```markdown
@CLAUDE.md Build a documentation of the files that jarvis generates, the paths it expects, and how to query the paths. We should add a command for querying the set of paths that jarvis produces, including the configuration file, repo file, env files, and pipeline files. 


# Jarvis directories
```
jarvis path +shared +priv +conf
```

A command that should print 

# Jarvis 

# C
```

### `ai-prompts/phase9-pipeline-scripts.md`

```markdown
# Pipeline Scripts

Use the python-code-updater agent to change the structure of pipleine scripts. 

Pipeline scripts are useful for storing cross-platform unit tests.
They store all of the information needed to create and execute
a pipeline.

## Running a pipline script

Pipeline scripts are YAML files and can be executed as follows:
```bash
jarvis ppl load yaml /path/to/my_pipeline.yaml
jarvis ppl run
```

Alternatively, if you want to load + run the script:
```bash
jarvis ppl run yaml /path/to/my_pipeline.yaml
```

## Updating a pipeline

To load changes made to a pipeline script, you can run:
```bash
jarvis ppl update yaml
```

## Format

The format should look like this:
```python
name: chimaera_unit_ipc
env: chimaera
pkgs:
  - pkg_type: chimaera_run
    pkg_name: chimaera_run
    sleep: 10
    do_dbg: true
    dbg_port: 4000
  - pkg_type: chimaera_unit_tests
    pkg_name: chimaera_unit_tests
    TEST_CASE: TestBdevIo
    do_dbg: true
    dbg_port: 4001
    interceptors: hermes_api
interceptors:
  - pkg_type: hermes_api
    pkg_name: hermes_api
```

In the Pkg class, there should be a new function called add_interceptor. This should modify a new self.config key called 'interceptors'. The interceptors key will be similar to the sub_pkgs key. The set of interceptors should be stored in a dictionary. This dictionary should be a mapping of pkg_name to a constructed package.

In SimplePackage, add a new config parameter called "interceptors", which is a list of strings. The list parameters look like this:
```
 self.add_args([
            {
                'name': 'hosts',
                'msg': 'A list of hosts and threads pr',
                'type': list,
                'args': [
                    {
                        'name': 'host',
                        'msg': 'A string representing a host',
                        'type': str,
                    }
                ]
            }
        ])
```

When loading a SimplePackage, iterate over the set of strings there and check self.ppl for the interceptors. Call interceptor.modify_env() to update our environment. Remove the ability to pass mod_env to update_env function. Make it so mod_env is a copy (not pointer) to env. This way each package gets its own isolated module environment.
```

### `builtin/__init__.py`

```python

```

### `builtin/builtin/InCompact3D/INSTALL.md`

```markdown
## Install with spack
step 1: Install Spack
```
cd ${HOME}
git clone https://github.com/spack/spack.git
cd spack
git checkout tags/v0.22.2
echo ". ${PWD}/share/spack/setup-env.sh" >> ~/.bashrc
source ~/.bashrc
```
step 2: Clone the coeus-adapter repos
```
git clone -b derived-merged https://github.com/grc-iit/coeus-adapter.git
```
step 3: Add Coeus repo packages for spack
```
spack repo add /coeus_adapter/CI/coeus
```
step 4: Install the incompact3D with spack
```
spack install incompact3D io_backend=adios2 ^openmpi ^adios2-coeus@2.10.0
```


## Install without spack

### installation as ADIOS2 I/O as backup
step 1: 2decomp-fft handles domain decomposition and parallel I/O, which Incompact3D depends on for writing field data.<br>
Below is the installation process for 2decomp-fft with ADIOS2 support
```
git clone -b coeus https://github.com/hxu65/2decomp-fft.git
spack load intel-oneapi-mkl
spack load openmpi
export MKL_DIR=/mnt/common/hxu40/spack/opt/spack/linux-ubuntu22.04-skylake_avx512/gcc-11.4.0/intel-oneapi-mkl-2024.2.2-z5q74r7t24qiimwlklk6jofy5twcmsjq/mkl/latest/lib/cmake/mkl
cmake -S . -B ./build -DIO_BACKEND=adios2 -DCMAKE_PREFIX_PATH=/mnt/common/hxu40/software/2decomp-fft/build -Dadios2_DIR=/mnt/common/hxu40/install2/lib/cmake/adios2
cd build
make -j8
make install
```
step 2: build the incompact3D with 2decomp-fft support
```
git clone https://github.com/xcompact3d/Incompact3d
cd Incompact3d
spack load intel-oneapi-mkl
spack load openmpi
export MKL_DIR=${MKLROOT}/lib/cmake/mkl
cmake -S . -B ./build -DIO_BACKEND=adios2 -Dadios2_DIR=/path/to/adios2/install/lib/cmake/adios2 -Ddecomp2d_DIR=/path/to/decomp2d/build
cd build
make -j8
make install
```
```

### `builtin/builtin/InCompact3D/USE.md`

```markdown
##  Run incompact3D 
### Jarvis(ADIOS2)
This is the procedure for running the application with ADIOS2 as the I/O engine.<br>
Step 1: find the benchmarks and its scripts file you want to run from [Incompact3D](https://github.com/xcompact3d/Incompact3d) github
```
Incompact3D/examples/benchmarks/scripts.i3d
```

step 2: Build environment
```
spack load incompact3D@coeus
spack load openmpi
export PATH=~/coeus-adapter/build/bin:$PATH
jarvis ppl env build
```
step 3: add jarvis repo
```
jarvis repo add coeus_adapter/test/jarvis/jarvis_coeus
```
step 4: Set up the jarvis packages
```
location=$(spack location -i incompact3D@coeus)
jarvis ppl create incompact3d
jarvis ppl append InCompact3D benchmarks=Pipe-Flow Incompact3D_location=$location output_folder=/output_fold/location script_file_name=input_DNS_Re1000_LR.i3d ppn=16 nprocs=16 engine=bp5
jarvis ppl env build

```

step 5: Run with jarvis
```
jarvis ppl run
```

Step 6: post-processing<br>
please refer this [jarvis packages](../InCompact3D_post) for post-processing.
Add InCompact3D_post to jarvis pipeline
```
jarvis ppl append InCompact3D_post benchmarks=Pipe-Flow output_folder=/output_fold/location engine=bp5 nprocs=1 ppn=16  
```
Jarvis will execute the test and generate output for the derived variables. <br>
Note: The current operation applied to derived variables is add, which may produce a large volume of output.

Step 7: visualization<br>
The visualization of bp5 file requires ParaView. <br>
Please refer this [jarvis packages](../paraview) for ParaView. <br>

### Jarvis (Hermes)
This is the procedure for running the application with Hermes as the I/O engine.<br>
Step 1: find the benchmarks and its scripts file you want to run
```
jarvis_coeus/Incompact3D/examples/benchmarks/scripts.i3d
```

step 2: Build environment
```
spack load hermes@master
spack load incompact3D@coeus
spack load openmpi
export PATH=~/coeus-adapter/build/bin:$PATH
export LD_LIBRARY_PATH=~/coeus-adapter/build/bin:LD_LIBRARY_PATH
```
step 3: add jarvis repo
```
jarvis repo add coeus_adapter/test/jarvis/jarvis_coeus
```
step 4: Set up the jarvis packages
```
jarvis ppl create incompact3d_hermes
jarvis ppl append hermes_run provider=sockets
jarvis ppl append Incompact3d example_location=/path/to/incompact3D-coeus engine=hermes nprocs=16 ppn=16 benchmarks=Pipe-Flow
jarvis ppl env build
```
Note: The current derived variable in coeus only support hash() opeartions.
```text
[ADIOS2 ERROR] <Helper> <adiosSystem> <ExceptionToError> : adios2_end_step: std::bad_array_new_length
```
This error is common for some other operations.<br>
step 5: Run with jarvis
```
jarvis ppl run
```

Step 6: post-processing<br>
please refer this [jarvis packages](../InCompact3D_post) for post-processing.
Add InCompact3D_post to jarvis pipeline
```
jarvis ppl append InCompact3D_post benchmarks=Pipe-Flow output_folder=/output_fold/location engine=hermes nprocs=1 ppn=16  
```
Step 7: visualization<br>
Currently, Hermes does not support the visualization. 



## Deploy without Jarvis (Adios)
```
spack load incompact3D@coeus
cd incompact3d/examples/Pipe-flow/
mpirun -np 16 ../../build/bin/xcompact3d
```
```

### `builtin/builtin/InCompact3D/config/adios2.xml`

```xml
<?xml version="1.0"?>
<adios-config>
  <io name="solution-io">
    <engine type="BP5">
    </engine>
    <transport type="File">
      <parameter key="Library" value="fstream"/>
      <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
  </io>
  <io name="in-outflow-io">
    <engine type="BP5">
    </engine>
    <transport type="File">
      <parameter key="Library" value="fstream"/>
      <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
  </io>
  <io name="turb-io">
    <engine type="BP5">
    </engine>
    <transport type="File">
      <parameter key="Library" value="fstream"/>
      <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
  </io>
  <io name="restart-io">
    <engine type="BP5">
    </engine>
    <transport type="File">
      <parameter key="Library" value="fstream"/>
      <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
  </io>
  <io name="statistics-io">
    <engine type="BP5">
    </engine>
    <transport type="File">
      <parameter key="Library" value="fstream"/>
      <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
  </io>
  <io name="restart-forces-io">
    <engine type="BP5">
    </engine>
    <transport type="File">
      <parameter key="Library" value="fstream"/>
      <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
  </io>
</adios-config>
```

### `builtin/builtin/InCompact3D/config/hermes.xml`

```xml
<?xml version="1.0"?>
<adios-config>
<io name="solution-io">
    <engine type="Plugin">
        <parameter key="PluginName" value="hermes"/>
        <parameter key="PluginLibrary" value="hermes_engine"/>
        <parameter key="ppn" value="##ppn##"/>
        <parameter key="db_file" value="##db_path##"/>
    </engine>
    <transport type="File">
        <parameter key="Library" value="fstream"/>
        <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
</io>
<io name="in-outflow-io">
    <engine type="Plugin">
        <parameter key="PluginName" value="hermes"/>
        <parameter key="PluginLibrary" value="hermes_engine"/>
        <parameter key="ppn" value="##ppn##"/>
        <parameter key="db_file" value="##db_path##"/>
    </engine>
    <transport type="File">
        <parameter key="Library" value="fstream"/>
        <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
</io>
<io name="turb-io">
    <engine type="Plugin">
        <parameter key="PluginName" value="hermes"/>
        <parameter key="PluginLibrary" value="hermes_engine"/>
        <parameter key="ppn" value="##ppn##"/>
        <parameter key="db_file" value="##db_path##"/>
    </engine>
    <transport type="File">
        <parameter key="Library" value="fstream"/>
        <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
</io>
<io name="restart-io">
    <engine type="Plugin">
        <parameter key="PluginName" value="hermes"/>
        <parameter key="PluginLibrary" value="hermes_engine"/>
        <parameter key="ppn" value="##ppn##"/>
        <parameter key="db_file" value="##db_path##"/>
    </engine>
    <transport type="File">
        <parameter key="Library" value="fstream"/>
        <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
</io>
<io name="statistics-io">
    <engine type="Plugin">
        <parameter key="PluginName" value="hermes"/>
        <parameter key="PluginLibrary" value="hermes_engine"/>
        <parameter key="ppn" value="##ppn##"/>
        <parameter key="db_file" value="##db_path##"/>
    </engine>
    <transport type="File">
        <parameter key="Library" value="fstream"/>
        <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
</io>
<io name="restart-forces-io">
    <engine type="Plugin">
        <parameter key="PluginName" value="hermes"/>
        <parameter key="PluginLibrary" value="hermes_engine"/>
        <parameter key="ppn" value="##ppn##"/>
        <parameter key="db_file" value="##db_path##"/>
    </engine>
    <transport type="File">
        <parameter key="Library" value="fstream"/>
        <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
</io>
</adios-config>
```

### `builtin/builtin/InCompact3D/incompact3D.yaml`

```yaml
name: incompact3D
env: incompact3D_env
pkgs:
  - pkg_type: incompact3d
    pkg_name: InCompact3D
    db_path: benchmark_metadata.db
    dbg_port: 4000
    do_dbg: False
    engine: bp5
    incompact3D_location: /path/to/incompact3D
    hide_output: False
    nprocs: 1
    ppn: 16
    reinit: False
    sleep: 0
    stderr: None
    stdout: None
```

### `builtin/builtin/InCompact3D/pkg.py`

```python
"""
This module provides classes and methods to launch the Incompact3d application.
Incompact3d is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, MpiExecInfo, PsshExecInfo
from jarvis_cd.shell.process import Rm
import os

class Incompact3d(Application):
    """
    This class provides methods to launch the Incompact3d application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'nprocs',
                'msg': 'Number of processes',
                'type': int,
                'default': 1,
            },
            {
                'name': 'ppn',
                'msg': 'The number of processes per node',
                'type': int,
                'default': 16,
            },
            {
                'name': 'output_folder',
                'msg': 'The location of incompact3D',
                'type': str,
                'default': None,
            },
            {
                'name': 'engine',
                'msg': 'Engine to be used',
                'choices': ['bp5', 'hermes'],
                'type': str,
                'default': 'bp5',
            },
            {
                'name': 'Incompact3D_location',
                'msg': 'The location of incompact3D',
                'type': str,
                'default': None,
            },
            {
                'name': 'benchmarks',
                'msg': 'The name of benchmarks ',
                'choices': ['ABL-Atmospheric-Boundary-Layer', 'Channel', 'Cylinder-wake', 'Mixing-layer', 'Pipe-Flow',
                            'TBL-Turbulent-Boundary-Layer', 'Gravity-current',  'Particle-Tracking', 'Sandbox', 'TGV-Taylor-Green-vortex',
                            'Cavity', 'MHD', 'Periodic-hill', 'Sphere',  'Wind-Turbine'],
                'type': str,
                'default': 'Cavity',
            },
            {
                'name': 'script_file_name',
                'msg': 'The name of script file',
                'type': str,
                'default': None,
            },
            {
                'name': 'db_path',
                'msg': 'Path where the DB will be stored',
                'type': str,
                'default': 'benchmark_metadata.db',
            },
            {
                'name': 'output_location',
                'msg': 'Path where the output file will be stored',
                'type': str,
                'default': 'data.bp5',
            },
            {
                'name': 'logs',
                'msg': 'Path where the log file will be stored',
                'type': str,
                'default': 'logs.txt',
            },

        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        execute_location = os.path.join(
            self.config['output_folder'],
            'examples',
            self.config['benchmarks']
        )

# Explicitly check if the directory exists, then create it
        if not os.path.exists(execute_location):
            os.makedirs(execute_location)
        if self.config['engine'].lower() == 'bp5':
            self.copy_template_file(f'{self.pkg_dir}/config/adios2.xml',
                                    f'{execute_location}/adios2_config.xml')
        if self.config['engine'].lower() in ['hermes', 'hermes_derived']:
            self.copy_template_file(f'{self.pkg_dir}/config/hermes.xml',
                                    f'{execute_location}/adios2_config.xml', replacements={
                    'ppn': self.config['ppn'],
                    'db_path': self.config['db_path'],
                })
        input_i3d = f"{self.config['Incompact3D_location']}/examples/{self.config['benchmarks']}/{self.config['script_file_name']}"
        self.copy_template_file(f'{input_i3d}',
                                f'{execute_location}/input.i3d')
        pass

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """

        execute_location=self.config['output_folder']+ '/examples/' + self.config['benchmarks']
        Exec('xcompact3d',
             MpiExecInfo(nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'],
                         hostfile=self.hostfile,
                         env=self.mod_env,
                         cwd=execute_location
                         )).run()
        pass

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        output_file= self.config['incompact3D_location']+ '/examples/' + self.config['benchmarks'] +'/data.bp5'
        output_files = [output_file,
                       self.config['checkpoint_output'],
                       self.config['db_path']
                       ]

        print(f'Removing {output_files}')
        Rm(output_files, PsshExecInfo(hostfile=self.hostfile)).run()
        pass
```

### `builtin/builtin/InCompact3D/spack/package.py`

```python
class Incompact3d(CMakePackage):
    """Xcompact3d is a Fortran-based framework of high-order finite-difference
    flow solvers dedicated to the study of turbulent flows."""

    homepage = "https://github.com/hxu65/Incompact3d.git"
    git      = "https://github.com/hxu65/Incompact3d.git"

    # Software license
    license('BSD-3-Clause')

    version('coeus', branch='master')

    # Dependencies
    depends_on('mpi')
    depends_on('cmake@3.20:', type='build')
    depends_on('fftw', when='fft_backend=generic')
    depends_on('mkl', when='fft_backend=mkl')
    depends_on('adios2', when='io_backend=adios2')

    conflicts('%gcc@:8.99', msg='Requires GCC 9 or higher')

    variant('fft_backend', default='generic',
        description='FFT backend for 2DECOMP&FFT',
        values=('generic', 'mkl'), multi=False)

    variant('io_backend', default='mpiio',
        description='IO backend',
        values=('mpiio', 'adios2'), multi=False)

    variant('full_testing', default=False,
        description='Enable full testing suite')

def cmake_args(self):
    return [
        self.define_from_variant('FFT_BACKEND', 'fft_backend'),
        self.define_from_variant('IO_BACKEND', 'io_backend'),
    ]

def setup_build_environment(self, env):
    env.set('FC', self.spec['mpi'].mpifc)
```

### `builtin/builtin/InCompact3D_post/config/adios2.xml`

```xml
<?xml version="1.0"?>
<adios-config>
  <io name="solution-io">
    <engine type="BP5">
    </engine>
    <transport type="File">
      <parameter key="Library" value="fstream"/>
      <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
  </io>
  <io name="in-outflow-io">
    <engine type="BP5">
    </engine>
    <transport type="File">
      <parameter key="Library" value="fstream"/>
      <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
  </io>
  <io name="turb-io">
    <engine type="BP5">
    </engine>
    <transport type="File">
      <parameter key="Library" value="fstream"/>
      <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
  </io>
  <io name="restart-io">
    <engine type="BP5">
    </engine>
    <transport type="File">
      <parameter key="Library" value="fstream"/>
      <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
  </io>
  <io name="statistics-io">
    <engine type="BP5">
    </engine>
    <transport type="File">
      <parameter key="Library" value="fstream"/>
      <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
  </io>
  <io name="restart-forces-io">
    <engine type="BP5">
    </engine>
    <transport type="File">
      <parameter key="Library" value="fstream"/>
      <parameter key="ProfileUnits" value="Milliseconds"/>
    </transport>
  </io>
</adios-config>
```

### `builtin/builtin/InCompact3D_post/config/hermes.xml`

```xml
<?xml version="1.0"?>
<adios-config>
    <io name="solution-io">
        <engine type="Plugin">
            <parameter key="PluginName" value="hermes"/>
            <parameter key="PluginLibrary" value="hermes_engine"/>
            <parameter key="ppn" value="##ppn##"/>
            <parameter key="db_file" value="##db_path##"/>
        </engine>
        <transport type="File">
            <parameter key="Library" value="fstream"/>
            <parameter key="ProfileUnits" value="Milliseconds"/>
        </transport>
    </io>
    <io name="in-outflow-io">
        <engine type="Plugin">
            <parameter key="PluginName" value="hermes"/>
            <parameter key="PluginLibrary" value="hermes_engine"/>
            <parameter key="ppn" value="##ppn##"/>
            <parameter key="db_file" value="##db_path##"/>
        </engine>
        <transport type="File">
            <parameter key="Library" value="fstream"/>
            <parameter key="ProfileUnits" value="Milliseconds"/>
        </transport>
    </io>
    <io name="turb-io">
        <engine type="Plugin">
            <parameter key="PluginName" value="hermes"/>
            <parameter key="PluginLibrary" value="hermes_engine"/>
            <parameter key="ppn" value="##ppn##"/>
            <parameter key="db_file" value="##db_path##"/>
        </engine>
        <transport type="File">
            <parameter key="Library" value="fstream"/>
            <parameter key="ProfileUnits" value="Milliseconds"/>
        </transport>
    </io>
    <io name="restart-io">
        <engine type="Plugin">
            <parameter key="PluginName" value="hermes"/>
            <parameter key="PluginLibrary" value="hermes_engine"/>
            <parameter key="ppn" value="##ppn##"/>
            <parameter key="db_file" value="##db_path##"/>
        </engine>
        <transport type="File">
            <parameter key="Library" value="fstream"/>
            <parameter key="ProfileUnits" value="Milliseconds"/>
        </transport>
    </io>
    <io name="statistics-io">
        <engine type="Plugin">
            <parameter key="PluginName" value="hermes"/>
            <parameter key="PluginLibrary" value="hermes_engine"/>
            <parameter key="ppn" value="##ppn##"/>
            <parameter key="db_file" value="##db_path##"/>
        </engine>
        <transport type="File">
            <parameter key="Library" value="fstream"/>
            <parameter key="ProfileUnits" value="Milliseconds"/>
        </transport>
    </io>
    <io name="restart-forces-io">
        <engine type="Plugin">
            <parameter key="PluginName" value="hermes"/>
            <parameter key="PluginLibrary" value="hermes_engine"/>
            <parameter key="ppn" value="##ppn##"/>
            <parameter key="db_file" value="##db_path##"/>
        </engine>
        <transport type="File">
            <parameter key="Library" value="fstream"/>
            <parameter key="ProfileUnits" value="Milliseconds"/>
        </transport>
    </io>
</adios-config>
```

### `builtin/builtin/InCompact3D_post/incompact3d_post.yaml`

```yaml
name: InCompact3D_post
env: InCompact3D_post_env
pkgs:
  - pkg_type: InCompact3D_post
    pkg_name: InCompact3D_post
    db_path: benchmark_metadata.db
    dbg_port: 4000
    do_dbg: False
    engine: bp5
    in_filename: /path/to/data.bp5
    out_filename: /path/to/output.bp5
    hide_output: False
    nprocs: 1
    ppn: 16
    reinit: False
    sleep: 0
    stderr: None
    stdout: None
```

### `builtin/builtin/InCompact3D_post/pkg.py`

```python
"""
This module provides classes and methods to launch the Incompact3dPost application.
Incompact3dPost is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec


class Incompact3dPost(Application):
    """
    This class provides methods to launch the Incompact3dPost application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """

        return [
            {
                'name': 'nprocs',
                'msg': 'Number of processes',
                'type': int,
                'default': 1,
            },
            {
                'name': 'ppn',
                'msg': 'The number of processes per node',
                'type': int,
                'default': None,
            },
            {
                'name': 'engine',
                'msg': 'Engine to be used',
                'type': str,
                'default': 'bp5',
            },
            {
                'name': 'db_path',
                'msg': 'Path where the DB will be stored',
                'type': str,
                'default': 'benchmark_metadata.db',
            },
            {
                'name': 'in_filename',
                'msg': 'Input file location',
                'type': str,
                'default': 'data.bp5',
            },
            {
                'name': 'output_folder',
                'msg': 'Input file location',
                'type': str,
                'default': None,
            },
            {
                'name': 'benchmarks',
                'msg': 'The name of benchmarks ',
                'choices': ['ABL-Atmospheric-Boundary-Layer', 'Channel', 'Cylinder-wake', 'Mixing-layer', 'Pipe-Flow',
                            'TBL-Turbulent-Boundary-Layer', 'Gravity-current',  'Particle-Tracking', 'Sandbox', 'TGV-Taylor-Green-vortex',
                            'Cavity', 'MHD', 'Periodic-hill', 'Sphere',  'Wind-Turbine'],
                'type': str,
                'default': 'Cavity',
            },
            {
                'name': 'out_filename',
                'msg': 'Output file location',
                'type': str,
                'default': 'out.bp5',
            },
            {
                'name': 'derived_variable_type',
                'msg': 'the type of derived variable in simulation',
                'type': str,
                'default': None,
            },

        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        execute_location = os.path.join(
            self.config['output_folder'],
            'examples',
            self.config['benchmarks']
        )
        if self.config['engine'].lower() == 'bp5':
            self.copy_template_file(f'{self.pkg_dir}/config/adios2.xml',
                                    f'{execute_location}/adios2_config.xml')
        if self.config['engine'].lower() in ['hermes', 'hermes_derived']:
            self.copy_template_file(f'{self.pkg_dir}/config/hermes.xml',
                                    f'{execute_location}/adios2_config.xml', replacements={
                    'ppn': self.config['ppn'],
                    'db_path': self.config['db_path'],
                })
        pass

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        in_file = self.config['in_filename']
        out_file = self.config['out_filename']
        execute_location=self.config['output_folder']+ '/examples/' + self.config['benchmarks']
        # Exec(f'inCompact3D_analysis {in_file} {out_file}',
        #      MpiExecInfo(nprocs=self.config['nprocs'],
        #                  ppn=self.config['ppn'],
        #                  hostfile=self.hostfile,
        #                  env=self.mod_env,
        #                  cwd=execute_location
        #                  ))
        os.chdir(execute_location)
        Exec(f'inCompact3D_analysis {in_file} {out_file}').run()
        pass

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        output_dir = [self.config['in_filename'],
                      self.config['out_filename'],
                      self.config['db_path']
                      ]
        print(f'Removing {output_dir}')
        Rm(output_dir, PsshExecInfo(hostfile=self.hostfile)).run()
        pass
```

### `builtin/builtin/__init__.py`

```python

```

### `builtin/builtin/adios2_gray_scott/INSTALL.md`

```markdown
# Installation
The gray_scott is installed along with the coeus-adapter. 
```bash
git clone https://github.com/grc-iit/coeus-adapter.git
cd coeus-adapter
mkdir build
cd build
cmake ../
make -j8
```

For the official way to install gray-scott, please refer here.
```bash
git clone https://github.com/pnorbert/adiosvm
pushd adiosvm/Tutorial/gs-mpiio
mkdir build
pushd build
cmake ../ -DCMAKE_BUILD_TYPE=Release
make -j8
export GRAY_SCOTT_PATH=`pwd`
popd
popd
```
```

### `builtin/builtin/adios2_gray_scott/USE.md`

```markdown
# Gray Scott model execution

## Gray-scott with adios2 as I/O library
Please follow these steps for the gray-scott with adios2 as I/O library.
### 1. Setup Environment

Create the environment variables needed by Gray Scott
```bash
spack load openmpi
export PATH="${COEUS_Adapter/build/bin}:$PATH"
```````````



### 2. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by Gray Scott.

```bash
jarvis pipeline create gray-scott-test
```

### 3. Save Environment

Store the current environment in the pipeline.
```bash
jarvis pipeline env build
```

### 4. Add pkgs to the Pipeline

Create a Jarvis pipeline with Gray Scott
```bash
jarvis pipeline append adios2_gray_scott

```

### 5. Run Experiment

Run the experiment
```bash
jarvis pipeline run
```

### 6. Clean Data

Clean data produced by Gray Scott
```bash
jarvis pipeline clean
```

## Gray Scott With Hermes as I/O engine and adios2 as I/O library
Please follow this steps for the gray-scott with hermes as I/O engine and adios2 as I/O libray.

### 1. Setup Environment
Create the environment variables needed by Hermes + Gray Scott
```bash
spack load adios2
spack load hermes@master
export PATH="${COEUS_Adapter/build/bin}:$PATH"
```


### 2. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by Hermes and Gray Scott.

```bash
jarvis pipeline create gs-hermes
```

### 3. Save Environment

We must make Jarvis aware of all environment variables needed to execute applications in the pipeline.

```bash
jarvis pipeline env build
```

### 4. Add pkgs to the Pipeline

Create a Jarvis pipeline with Hermes (theMPI-IO interceptor), and Gray-Scott

Option 1: without derived variables
```bash
jarvis pipeline append hermes_run --sleep=10 --provider=sockets
jarvis pipeline append adios2_gray_scott engine=hermes 
```
Option2: with derived variables
For derived variable with adios2 in hermes:
```bash
jarvis pipeline append hermes_run --sleep=10 --provider=sockets
jarvis pipeline append adios2_gray_scott engine=hermes_derived
```

### 5. Run the Experiment

Run the experiment
```bash
jarvis pipeline run
```

### 6. Clean Data

To clean data produced by Hermes + Gray-Scott:
```bash
jarvis pipeline clean
```


## Gray-Scott configration file
Please refer to [README.md](README.md) for more inforamtion.

## Gray-Scott installation
Please refer to [INSTALL.md](INSTALL.md) for more information.
```

### `builtin/builtin/adios2_gray_scott/config/adios2.xml`

```xml
<?xml version="1.0"?>
<adios-config>

    <!--============================================
           Configuration for Gray-Scott and GS Plot
        ============================================-->

    <io name="SimulationOutput">
        <engine type="BP5">
            <!-- SST engine parameters -->
            <parameter key="RendezvousReaderCount" value="0"/>
            <parameter key="QueueLimit" value="1"/>
            <parameter key="QueueFullPolicy" value="Discard"/>
            <!-- BP4/SST engine parameters -->
            <parameter key="OpenTimeoutSecs" value="10.0"/>
        </engine>
    </io>

    <!--===========================================
           Configuration for PDF calc and PDF Plot
        ===========================================-->

    <io name="PDFAnalysisOutput">
        <engine type="BP5">
            <!-- SST engine parameters -->
            <parameter key="RendezvousReaderCount" value="0"/>
            <parameter key="QueueLimit" value="1"/>
            <parameter key="QueueFullPolicy" value="Discard"/>
            <!-- BP4/SST engine parameters -->
            <parameter key="OpenTimeoutSecs" value="10.0"/>
        </engine>
    </io>

    <io name="SimulationCheckpoint">
        <engine type="BP5">
        </engine>
    </io>
</adios-config>
```

### `builtin/builtin/adios2_gray_scott/config/adios2.yaml`

```yaml
name: adios2
env: hermes
pkgs:
  - pkg_type: hermes_run
    pkg_name: hermes_run
    sleep: 5
    provider: sockets
  - pkg_type: adios_gray_scott
    pkg_name: adios_gray_scott
    engine: hermes
    ppn: 16
    nprocs: 4
```

### `builtin/builtin/adios2_gray_scott/config/hermes.xml`

```xml
<?xml version="1.0"?>
<adios-config>

    <!--============================================
           Configuration for Gray-Scott and GS Plot
        ============================================-->

    <io name="SimulationOutput">
        <engine type="Plugin">
            <parameter key="PluginName" value="hermes" />
            <parameter key="PluginLibrary" value="hermes_engine" />

            <parameter key="ppn" value='##PPN##'/>
            <parameter key="VarFile" value="##VARFILE##"/>
            <parameter key="OPFile" value="##OPFILE##"/>
            <parameter key="db_file" value="##DBFILE##"/>
            <parameter key="execution_order" value="##Order##"/>
        </engine>
    </io>

    <!--===========================================
           Configuration for PDF calc and PDF Plot
        ===========================================-->

    <io name="PDFAnalysisOutput">
        <engine type="Plugin">
            <parameter key="PluginName" value="hermes" />
            <parameter key="PluginLibrary" value="hermes_engine" />

            <parameter key="ppn" value='##PPN##'/>
            <parameter key="VarFile" value="##VARFILE##"/>
            <parameter key="OPFile" value="##OPFILE##"/>
            <parameter key="db_file" value="##DBFILE##"/>
        </engine>
    </io>

<!--    <io name="SimulationCheckpoint">-->
<!--        <engine type="Plugin">-->
<!--            <parameter key="PluginName" value="hermes_checkpoint" />-->
<!--            <parameter key="PluginLibrary" value="hermes_engine" />-->

<!--            <parameter key="VarFile" value="/home/jcernudagarcia/jarvis-pipelines/coeus_io_comp/io_comp/var.yaml"/>-->
<!--            <parameter key="OPFile" value="/home/jcernudagarcia/jarvis-pipelines/coeus_io_comp/io_comp/operator.yaml"/>-->
<!--            <parameter key="db_file" value="/mnt/nvme/jcernudagarcia/metadata.db"/>-->
<!--        </engine>-->
<!--    </io>-->
    <io name="SimulationCheckpoint">
        <engine type="BP5">
        </engine>
    </io>
</adios-config>
```

### `builtin/builtin/adios2_gray_scott/config/operator.yaml`

```yaml
'U':
  operator1: 'min'
  operator2: 'max'
'V':
  operator1: 'min'
  operator2: 'max'
```

### `builtin/builtin/adios2_gray_scott/config/var.yaml`

```yaml
'U':
  weight: 0.5
'V':
  weight: 0.5
```

### `builtin/builtin/adios2_gray_scott/pkg.py`

```python
"""
This module provides classes and methods to launch the Gray Scott application.
Gray Scott is a 3D 7-point stencil code for modeling the diffusion of two
substances.
"""
from jarvis_cd.core.pkg import Application
import pathlib


class Adios2GrayScott(Application):
    """
    This class provides methods to launch the GrayScott application.
    """
    def _init(self):
        """
        Initialize paths
        """
        self.adios2_xml_path = f'{self.shared_dir}/adios2.xml'
        self.settings_json_path = f'{self.shared_dir}/settings-files.json'
        self.var_json_path = f'{self.shared_dir}/var.json'
        self.operator_json_path = f'{self.shared_dir}/operator.json'

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'nprocs',
                'msg': 'Number of processes to spawn',
                'type': int,
                'default': 4,
            },
            {
                'name': 'ppn',
                'msg': 'Processes per node',
                'type': int,
                'default': 16,
            },
            {
                'name': 'L',
                'msg': 'Grid size of cube',
                'type': int,
                'default': 32,
            },
            {
                'name': 'Du',
                'msg': 'Diffusion rate of substance U',
                'type': float,
                'default': .2,
            },
            {
                'name': 'Dv',
                'msg': 'Diffusion rate of substance V',
                'type': float,
                'default': .1,
            },
            {
                'name': 'F',
                'msg': 'Feed rate of U',
                'type': float,
                'default': .01,
            },
            {
                'name': 'k',
                'msg': 'Kill rate of V',
                'type': float,
                'default': .05,
            },
            {
                'name': 'dt',
                'msg': 'Timestep',
                'type': float,
                'default': 2.0,
            },
            {
                'name': 'steps',
                'msg': 'Total number of steps to simulate',
                'type': int,
                'default': 100,
            },
            {
                'name': 'plotgap',
                'msg': 'Number of steps between output',
                'type': float,
                'default': 10,
            },
            {
                'name': 'noise',
                'msg': 'Amount of noise',
                'type': float,
                'default': .01,
            },
            {
                'name': 'out_file',
                'msg': 'Absolute path to output file',
                'type': str,
                'default': None,
            },
            {
                'name': 'checkpoint',
                'msg': 'Perform checkpoints',
                'type': bool,
                'default': True,
            },
            {
                'name': 'checkpoint_freq',
                'msg': 'Frequency of the checkpoints',
                'type': int,
                'default': 70,
            },
            {
                'name': 'checkpoint_output',
                'msg': 'Output location of the checkpoint',
                'type': str,
                'default': 'ckpt.bp',
            },
            {
                'name': 'restart',
                'msg': 'Perform restarts',
                'type': bool,
                'default': False,
            },
            {
                'name': 'restart_input',
                'msg': 'Input for the restart',
                'type': str,
                'default': 'ckpt.bp',
            },
            {
                'name': 'adios_span',
                'msg': '???',
                'type': bool,
                'default': False,
            },
            {
                'name': 'adios_memory_selection',
                'msg': '???',
                'type': bool,
                'default': False,
            },
            {
                'name': 'mesh_type',
                'msg': '???',
                'type': str,
                'default': 'image',
            },
            {
                'name': 'engine',
                'msg': 'Engine to be used',
                'choices': ['bp5', 'hermes', 'bp5_derived', 'hermes_derived'],
                'type': str,
                'default': 'bp5',
            },
            {
                'name': 'full_run',
                'msg': 'Whill postprocessing be executed?',
                'type': bool,
                'default': True,
            },
            {
                'name': 'limit',
                'msg': 'Limit the value of data to track',
                'type': int,
                'default': 0,
            },
            {
                'name': 'db_path',
                'msg': 'Path where the DB will be stored',
                'type': str,
                'default': 'benchmark_metadata.db',
            },
            {
                'name': 'Execution_order',
                'msg': 'Path where the bp5 will be stored',
                'type': str,
                'default': '1',
            },

        ]

    # jarvis pkg config adios2_gray_scott ppn=20 full_run=true engine=hermes db_path=/mnt/nvme/jcernudagarcia/metadata.db out_file=gs.bp nprocs=1

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        if self.config['out_file'] is None:
            adios_dir = os.path.join(self.shared_dir, 'gray-scott-output')
            self.config['out_file'] = os.path.join(adios_dir,
                                                 'data/out.bp')
            Mkdir(adios_dir, PsshExecInfo(hostfile=self.hostfile,
                                          env=self.env)).run()
        settings_json = {
            'L': self.config['L'],
            'Du': self.config['Du'],
            'Dv': self.config['Dv'],
            'F': self.config['F'],
            'k': self.config['k'],
            'dt': self.config['dt'],
            'plotgap': self.config['plotgap'],
            'steps': self.config['steps'],
            'noise': self.config['noise'],
            'output': self.config['out_file'],
            'checkpoint': self.config['checkpoint'],
            'checkpoint_freq': self.config['checkpoint_freq'],
            'checkpoint_output': self.config['checkpoint_output'],
            'restart': self.config['restart'],
            'restart_input': self.config['restart_input'],
            'adios_span': self.config['adios_span'],
            'adios_memory_selection': self.config['adios_memory_selection'],
            'mesh_type': self.config['mesh_type'],
            'adios_config': f'{self.adios2_xml_path}'
        }
        output_dir = os.path.dirname(self.config['out_file'])
        db_dir = os.path.dirname(self.config['db_path'])
        Mkdir([output_dir, db_dir], PsshExecInfo(hostfile=self.hostfile,
                                       env=self.env)).run()

        JsonFile(self.settings_json_path).save(settings_json)
        print(f"Using engine {self.config['engine']}")
        if self.config['engine'].lower() in ['bp5', 'bp5_derived']:
            self.copy_template_file(f'{self.pkg_dir}/config/adios2.xml',
                                self.adios2_xml_path)
        elif self.config['engine'].lower() in ['hermes', 'hermes_derived']:
            self.copy_template_file(f'{self.pkg_dir}/config/hermes.xml',
                                    self.adios2_xml_path,
                                    replacements={
                                        'PPN': self.config['ppn'],
                                        'VARFILE': self.var_json_path,
                                        'OPFILE': self.operator_json_path,
                                        'DBFILE': self.config['db_path'],
                                        'Order': self.config['Execution_order'],
                                    })
            self.copy_template_file(f'{self.pkg_dir}/config/var.yaml',
                                    self.var_json_path)
            self.copy_template_file(f'{self.pkg_dir}/config/operator.yaml',
                                    self.operator_json_path)
        else:
            raise Exception('Engine not defined')

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        # print(self.env['HERMES_CLIENT_CONF'])
        if self.config['engine'].lower() in ['bp5_derived', 'hermes_derived']:
            derived = 1
            Exec(f'adios2-gray-scott {self.settings_json_path} {derived}',
                 MpiExecInfo(nprocs=self.config['nprocs'],
                             ppn=self.config['ppn'],
                             hostfile=self.hostfile,
                             env=self.mod_env
                             )).run()
        elif self.config['engine'].lower() in ['hermes', 'bp5']:

            derived = 0
            Exec(f'adios2-gray-scott {self.settings_json_path} {derived}',
                 MpiExecInfo(nprocs=self.config['nprocs'],
                             ppn=self.config['ppn'],
                             hostfile=self.hostfile,
                             env=self.mod_env)).run()


    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        output_file = [self.config['out_file'],
                       self.config['checkpoint_output'],
                       self.config['db_path']
                       ]

        print(f'Removing {output_file}')
        Rm(output_file, PsshExecInfo(hostfile=self.hostfile)).run()
```

### `builtin/builtin/arldm/example_config/config.yml`

```yaml
batch_size: 1
calculate_fid: true
ckpt_dir: /mnt/nvme/mtang11/arldm_run/save_ckpt
dataset: vistsis
flintstones:
  blip_embedding_tokens: 30525
  clip_embedding_tokens: 49412
  hdf5_file: FLINTSTONES_HDF5
  max_length: 91
  new_tokens:
  - fred
  - barney
  - wilma
  - betty
  - pebbles
  - dino
  - slate
freeze_blip: true
freeze_clip: true
freeze_resnet: true
gpu_ids: []
guidance_scale: 6
hydra:
  output_subdir: null
  run:
    dir: .
hydra/hydra_logging: disabled
hydra/job_logging: disabled
init_lr: 1e-5
max_epochs: 50
mode: train
num_cpu_cores: -1
num_inference_steps: 250
num_workers: 1
pororo:
  blip_embedding_tokens: 30530
  clip_embedding_tokens: 49416
  hdf5_file: PORORO_HDF5
  max_length: 85
  new_tokens:
  - pororo
  - loopy
  - eddy
  - harry
  - poby
  - tongtong
  - crong
  - rody
  - petty
run_name: vistsis_train
sample_output_dir: /mnt/nvme/mtang11/arldm_run/output_data/sample_out_vistsis_train
scheduler: ddim
seed: 0
task: continuation
test_model_file: null
train_model_file: null
vistdii:
  blip_embedding_tokens: 30524
  clip_embedding_tokens: 49408
  hdf5_file: VISTDII_HDF5
  max_length: 65
vistsis:
  blip_embedding_tokens: 30524
  clip_embedding_tokens: 49408
  hdf5_file: /mnt/nvme/mtang11/arldm_run/output_data/vistsis_out.h5
  max_length: 100
warmup_epochs: 1
```

### `builtin/builtin/arldm/example_config/config_template.yml`

```yaml
# device
mode: MODE  # train sample
gpu_ids: []  # gpu ids, 4 GPU uses [ 0, 1, 2, 3 ]
batch_size: 1  # batch size each item denotes one story
num_workers: WORKERS  # number of workers
num_cpu_cores: -1  # -1 number of cpu cores
seed: 0  # random seed
ckpt_dir: CKPT_DIR # checkpoint directory
run_name: TEST_NAME # name for this run

# task
dataset: DATASET  # pororo flintstones vistsis vistdii
task: continuation  # continuation visualization

# train
init_lr: 1e-5  # initial learning rate
warmup_epochs: 1  # warmup epochs
max_epochs: 50  # max epochs
train_model_file:  # model file for resume, none for train from scratch
freeze_clip: True  # whether to freeze clip, True to reduce vram usage
freeze_blip: True  # whether to freeze blip, True to reduce vram usage
freeze_resnet: True  # whether to freeze resnet, True to reduce vram usage

# sample
test_model_file:  # model file for test
calculate_fid: True  # whether to calculate FID scores
scheduler: ddim  # ddim pndm
guidance_scale: 6  # guidance scale
num_inference_steps: 250  # number of inference steps
# sample_output_dir: ./output_data/train_visit # output directory
sample_output_dir: SAMPLE_OUT_DIR # output directory

pororo:
  hdf5_file: PORORO_HDF5
  max_length: 85
  new_tokens: [ "pororo", "loopy", "eddy", "harry", "poby", "tongtong", "crong", "rody", "petty" ]
  clip_embedding_tokens: 49416
  blip_embedding_tokens: 30530

flintstones:
  hdf5_file: FLINTSTONES_HDF5
  max_length: 91
  new_tokens: [ "fred", "barney", "wilma", "betty", "pebbles", "dino", "slate" ]
  clip_embedding_tokens: 49412
  blip_embedding_tokens: 30525

vistsis:
  hdf5_file: VISTSIS_HDF5
  max_length: 100
  clip_embedding_tokens: 49408
  blip_embedding_tokens: 30524

vistdii:
  hdf5_file: VISTDII_HDF5
  max_length: 65
  clip_embedding_tokens: 49408
  blip_embedding_tokens: 30524

hydra:
  run:
    dir: .
  output_subdir: null
hydra/job_logging: disabled
hydra/hydra_logging: disabled
```

### `builtin/builtin/arldm/pkg.py`

```python
"""
This module provides classes and methods to launch the Arldm application.
Arldm is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo
from jarvis_cd.shell.process import Rm
import os, pathlib
import time
import yaml
from scspkg.pkg import Package
import sys # for stdout, stderr

class Arldm(Application):
    """
    This class provides methods to launch the Arldm application.
    """
    def _init(self):
        """
        Initialize paths
        """
        self.pkg_type = 'arldm'
        self.hermes_env_vars = ['HERMES_ADAPTER_MODE', 'HERMES_CLIENT_CONF', 
                                'HERMES_CONF', 'LD_PRELOAD']
        # self.hermes_env_vars = ['HDF5_DRIVER', 'HDF5_PLUGIN_PATH', 
        #                   'HERMES_ADAPTER_MODE', 'HERMES_CLIENT_CONF',
        #                   'HERMES_CONF', 'HERMES_VFD', 'HERMES_POSIX'
        #                   ]
        
    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'conda_env',
                'msg': 'Name of the conda environment for running ARLDM',
                'type': str,
                'default': "arldm",
            },
            {
                'name': 'config',
                'msg': 'The config file for running analysis',
                'type': str,
                'default': f'{self.pkg_dir}/example_config/config_template.yml',
            },
            {
                'name': 'update_envar_yml',
                'msg': 'Update the conda environment variables',
                'type': str,
                'default': f'{self.pkg_dir}/example_config/update_envar.yml',
            },
            {
                'name': 'with_hermes',
                'msg': 'Whether it is used with Hermes (e.g. needs to update environment variables)',
                'type': bool,
                'default': False,
            },
            {
                'name': 'with_dayu',
                'msg': 'Whether it is used with DaYu (e.g. needs to update task files)',
                'type': bool,
                'default': False,
            },
            {
                'name': 'runscript',
                'msg': 'The name of the ARLDM script to run',
                'type': str,
                'default': 'vistsis', # smallest dataset
                'choices': ['flintstones', 'pororo', 'vistsis', 'vistdii'],
            },
            {
                'name': 'flush_mem',
                'msg': 'Flushing the memory after each stage',
                'type': bool,
                'default': False,
            },
            {
                'name': 'flush_mem_cmd',
                'msg': 'Command to flush the node memory',
                'type': str,
                'default': "ml user-scripts; sudo drop_caches", # for Ares
            },
            {
                'name': 'arldm_path',
                'msg': 'Absolute path to the ARLDM source code (can set to `scspkg pkg src arldm`/ARLDM)',
                'type': str,
                'default': f"{Package(self.pkg_type).pkg_root}/src/ARLDM",
            },
            {
                'name': 'mode',
                'msg': 'Mode of running ARLDM: train(D) or sample',
                'type': str,
                'default': 'train',
                'choice': ['train', 'sample'],
            },
            {
                'name': 'num_workers',
                'msg': 'Number of CPU workers to use for parallel processing',
                'type': int,
                'default': 1,
                'choices': [0, 1, 2],
            },
            {
                'name': 'experiment_input_path',
                'msg': 'Absolute path to the experiment where you put all data',
                'type': str,
                'default': None,
            },
            {
                'name': 'sample_output_dir',
                'msg': 'Directory to save samples',
                'type': str,
                'default': None, 
            },
            {
                'name': 'hdf5_file',
                'msg': 'HDF5 file to save samples',
                'type': str,
                'default': None,
            },
            {
                'name': 'prep_hdf5',
                'msg': 'Prepare the HDF5 file for the ARLDM run',
                'type': bool,
                'default': True,
            },
            {
                'name': 'local_exp_dir',
                'msg': 'Local experiment directory',
                'type': str,
                'default': None,
            },
            {
                'name': 'pretrain_model_path',
                'msg': 'Pretrained model path',
                'type': str,
                'default': None,
            },
        ]

    def _configure_yaml(self):
        yaml_file = self.config['config']

        if "_template.yml" not in str(yaml_file):
            yaml_file = yaml_file.replace(".yml", "_template.yml")
        
        self.log(f"ARLDM template.yml: {yaml_file}")

        with open(yaml_file, "r") as stream:
            try:
                config_vars = yaml.safe_load(stream)
                
                run_test = self.config['runscript']
                
                config_vars['mode'] = self.config['mode']
                config_vars['num_workers'] = self.config['num_workers']
                config_vars['run_name'] = f"{self.config['runscript']}_{self.config['mode']}"
                config_vars['dataset'] = run_test

                experiment_input_path = self.config['experiment_input_path']
                if self.config['local_exp_dir'] is not None:
                    experiment_input_path = self.config['local_exp_dir']
                    self.config['ckpt_dir'] = experiment_input_path + f"/{self.config['runscript']}_save_ckpt"
                    self.config['sample_output_dir'] = experiment_input_path + f"/sample_out_{self.config['runscript']}_{self.config['mode']}"
                    self.config['hdf5_file'] = f"{experiment_input_path}/{self.config['runscript']}_out.h5"

                config_vars['ckpt_dir'] = self.config['ckpt_dir']
                config_vars['sample_output_dir'] = self.config['sample_output_dir']
                config_vars[run_test]['hdf5_file'] = self.config['hdf5_file']
                
                # save config_vars back to yaml file
                new_yaml_file = yaml_file.replace("_template.yml", ".yml")
                yaml.dump(config_vars, open(new_yaml_file, 'w'), default_flow_style=False)
            except yaml.YAMLError as exc:
                self.log(exc)
        self.config['config'] = new_yaml_file
        

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        # self.env['HDF5_USE_FILE_LOCKING'] = "FALSE" 
        # self.env['HYDRA_FULL_ERROR'] = "1" # complete stack trace.
        
        self.log(f"ARLDM _configure")
        
        self.setenv('HDF5_USE_FILE_LOCKING', "FALSE") # set HDF5 locking: FALSE, TRUE, BESTEFFORT
        self.setenv('HYDRA_FULL_ERROR', "1")
        
        if self.config['pretrain_model_path'] is None:
            pretrain_model_path = os.getenv('PRETRAIN_MODEL_PATH')
            if pretrain_model_path is not None:
                if self.config['local_exp_dir'] is not None:
                    pretrain_model_path = self.config['local_exp_dir'] + "/model_large.pth"
                self.log(f"PRETRAIN_MODEL_PATH: {pretrain_model_path}")
                self.config['pretrain_model_path'] = pretrain_model_path
                # self.env['PRETRAIN_MODEL_PATH'] = pretrain_model_path
                self.setenv('PRETRAIN_MODEL_PATH', pretrain_model_path)
            else:
                raise Exception("Must set the pretrain_model_path")
        else:
            if self.config['local_exp_dir'] is None:
                pretrain_model_path = os.getenv('PRETRAIN_MODEL_PATH')
                if pretrain_model_path is not None:
                    self.log(f"PRETRAIN_MODEL_PATH: {pretrain_model_path}")
                    self.config['pretrain_model_path'] = pretrain_model_path
                    # self.env['PRETRAIN_MODEL_PATH'] = pretrain_model_path
                    self.setenv('PRETRAIN_MODEL_PATH', pretrain_model_path)
        
        experiment_input_path = os.getenv('EXPERIMENT_INPUT_PATH')
        if experiment_input_path is None:
            raise Exception("Must set the experiment_input_path")
        else:
            self.config['experiment_input_path'] = experiment_input_path
        
        if self.config['conda_env'] is None:
            raise Exception("Must set the conda environment for running ARLDM")
        if self.config['config'] is None:
            raise Exception("Must set the ARLDM config file")
        if self.config['runscript'] is None:
            raise Exception("Must set the ARLDM script to run")
            
        if self.config['flush_mem'] == False:
            self.env['FLUSH_MEM'] = "FALSE"
        else:
            if self.config['flush_mem_cmd'] is None:
                raise Exception("Must add the command to flush memory using flush_mem_cmd")
        
        if self.config['arldm_path'] is None:
            raise Exception("Must set the `'arldm_path'` to the ARLDM source code")
        else:
            # check that path exists
            if not pathlib.Path(self.config['arldm_path']).exists():
                raise Exception(f"`'arldm_path'` does not exist: {self.config['arldm_path']}")
        
        # check and make -p experiment_input_path
        pathlib.Path(self.config['experiment_input_path']).mkdir(parents=True, exist_ok=True)
        
        # set ckpt_dir
        self.config['ckpt_dir'] = f'{self.config["experiment_input_path"]}/{self.config["runscript"]}_save_ckpt'
        pathlib.Path(self.config['ckpt_dir']).mkdir(parents=True, exist_ok=True)
        
        # set sample_output_dir
        self.config['sample_output_dir'] = f'{self.config["experiment_input_path"]}/sample_out_{self.config["runscript"]}_{self.config["mode"]}'
        pathlib.Path(self.config['sample_output_dir']).mkdir(parents=True, exist_ok=True)
        
        # set sample_output_dir
        self.config['hdf5_file'] = f'{self.config["experiment_input_path"]}/{self.config["runscript"]}_out.h5'
                
        self._configure_yaml()
        

    def _prep_hdf5_file(self):
        """
        Prepare the HDF5 file for the ARLDM run
        """
        if self.config['with_dayu'] == True:
            self._set_curr_task_file("arldm_saveh5")
        
        self.log(f"ARLDM _prep_hdf5_file input from to {self.config['hdf5_file']}")
        
        experiment_input_path = self.config['experiment_input_path']
        if self.config['local_exp_dir'] is not None:
            experiment_input_path = self.config['local_exp_dir']
        
        cmd = [
            # f"cd {self.config['arldm_path']}; echo Executing from directory `pwd`;",
            'conda','run', '-n', self.config['conda_env'], # conda environment
            'python',
        ]

        if self.config['runscript'] == 'pororo':
            cmd.append(f'{self.config["arldm_path"]}/data_script/pororo_hdf5.py')
            cmd.append(f'--data_dir {experiment_input_path}/pororo')
            cmd.append(f'--save_path {self.config["hdf5_file"]}')
        elif self.config['runscript'] == 'flintstones':
            cmd.append(f'{self.config["arldm_path"]}/data_script/flintstones_hdf5.py')
            cmd.append(f'--data_dir {experiment_input_path}/flintstones')
            cmd.append(f'--save_path {self.config["hdf5_file"]}')
        elif self.config['runscript'] == 'vistsis' or self.config['runscript'] == 'vistdii':
            cmd.append(f'{self.config["arldm_path"]}/data_script/vist_hdf5.py')
            # experiment_input_path = f'{experiment_input_path}/{self.config["runscript"]}'
            cmd.append(f'--sis_json_dir {experiment_input_path}/vistsis')
            cmd.append(f'--dii_json_dir {experiment_input_path}/vistdii')
            cmd.append(f'--img_dir {experiment_input_path}/visit_img')
            cmd.append(f'--save_path {self.config["hdf5_file"]}')
        else:
            raise Exception("Must set the correct ARLDM script to run")
        
        prep_cmd = ' '.join(cmd)
        
        start = time.time()
        Exec(prep_cmd, LocalExecInfo(
            env=self.mod_env,
            cwd=self.config['arldm_path'])).run()
        
        end = time.time()
        diff = end - start
        self.log(f'TIME: {diff} seconds') # color=Color.GREEN
        
        # check if hdf5_file exists
        if pathlib.Path(self.config['hdf5_file']).exists():
           self.log(f"HDF5 file created: {self.config['hdf5_file']}")
        else:
            raise Exception(f"HDF5 file not created: {self.config['hdf5_file']}") 
    
    def _train(self):
        """
        Run the ARLDM training run
        """
        if self.config['with_dayu'] == True:
            self._set_curr_task_file("arldm_train")
        
        self.log(f"ARLDM _train: dataset[{self.config['runscript']}]")
        
        # Move config file to arldm_path
        Exec(f"cp {self.config['config']} {self.config['arldm_path']}/config.yaml",
             LocalExecInfo(env=self.mod_env,)).run()
        
        
        cmd = [
            'conda','run', '-n', self.config['conda_env'], # conda environment
            'python'
        ]

        if self.config['arldm_path'] and self.config['runscript']:
            cmd.append(f'{self.config["arldm_path"]}/main.py')
        
        conda_cmd = ' '.join(cmd)
        
        start = time.time()

        self.jutil.debug_local_exec = True
        Exec(conda_cmd,
             LocalExecInfo(env=self.mod_env,
                           pipe_stdout=self.config['stdout'],
                           pipe_stderr=self.config['stderr'],
                           cwd=self.config['arldm_path'])).run()
        self.jutil.debug_local_exec = False
        
        end = time.time()
        diff = end - start
        self.log(f'TIME: {diff} seconds') # color=Color.GREEN
    
    def _sample(self):
        """
        Run the ARLDM sampling run
        
        This step can only be run when training is fully completed. 
        Currently train is set to fast_dev_run.
        """
        self.log(f"ARLDM sampling run: not implemented yet")

    def _set_curr_task_file(self,task):
        
        workflow_name = self.mod_env['WORKFLOW_NAME']
        path_for_task_files = self.mod_env['PATH_FOR_TASK_FILES']
        vfd_task_file = None
        vol_task_file = None
        
        if workflow_name and path_for_task_files:
            vfd_task_file = os.path.join(path_for_task_files, f"{workflow_name}_vfd.curr_task")
            vol_task_file = os.path.join(path_for_task_files, f"{workflow_name}_vol.curr_task")
            # Create file and parent file if it does not exist
            pathlib.Path(vfd_task_file).mkdir(parents=True, exist_ok=True)
            pathlib.Path(vol_task_file).mkdir(parents=True, exist_ok=True)
            

        # vfd_task_file = /tmp/$USER/pyflextrkr_vfd.curr_task
        
        if vfd_task_file and os.path.exists(vfd_task_file):
            if os.path.isfile(vfd_task_file):
                with open(vfd_task_file, "w") as file:
                    file.write(task)
                print(f"Overwrote: {vfd_task_file} with {task}")

        if vol_task_file and os.path.exists(vol_task_file):
            if os.path.isfile(vol_task_file):
                with open(vol_task_file, "w") as file:
                    file.write(task)
                print(f"Overwrote: {vol_task_file} with {task}")
        else:
            print("Invalid or missing PATH_FOR_TASK_FILES environment variable.")    

    def _unset_vfd_vars(self,env_vars_toset):
        cmd = ['conda', 'env', 'config', 'vars', 'unset',]
        
        for env_var in env_vars_toset:
            cmd.append(f'{env_var}')
        cmd.append('-n')
        cmd.append(self.config['conda_env'])
        
        cmd = ' '.join(cmd)
        Exec(cmd, LocalExecInfo(env=self.mod_env,)).run()
        self.log(f"ARLDM _unset_vfd_vars: {cmd}")

    def _set_env_vars(self, env_vars_toset):
        
        self.log(f"ARLDM _set_env_vars")
        
        # Unset all env_vars_toset first        
        self._unset_vfd_vars(env_vars_toset)

        cmd = [ 'conda', 'env', 'config', 'vars', 'set']
        for env_var in env_vars_toset:
            env_var_val = self.mod_env[env_var]
            cmd.append(f'{env_var}={env_var_val}')
        
        cmd.append('-n')
        cmd.append(self.config['conda_env'])
        cmd = ' '.join(cmd)
        self.log(f"ARLDM _set_env_vars: {cmd}")
        Exec(cmd, LocalExecInfo(env=self.mod_env,)).run()

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        
        self._configure_yaml()
        
        if self.config['with_hermes'] == True:
            self._set_env_vars(self.hermes_env_vars)
        else:
            self._unset_vfd_vars(self.hermes_env_vars)
        
        
        self.log(f"ARLDM start")
        
        start = time.time()
        
        if self.config['prep_hdf5']:
            self._prep_hdf5_file()
        
        if self.config['mode'] == 'train':
            self._train()
        
        if self.config['mode'] == 'sample':
            self._sample()
        
        end = time.time()
        diff = end - start
        self.log(f'TOTAL RUN TIME: {diff} seconds')


    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        output_h5 = self.config['experiment_input_path'] + f"/{self.config['runscript']}_out.h5"
        output_dir = self.config['experiment_input_path'] + f"/sample_out_{self.config['runscript']}_{self.config['mode']}"
        if self.config['local_exp_dir'] is not None:
            output_dir = self.config['local_exp_dir'] + f"/sample_out_{self.config['runscript']}_{self.config['mode']}"
        
        # recursive remove all files in output_data directory
        if os.path.exists(output_dir):
            self.log(f'Removing {output_dir}')
            Rm(output_dir).run()
        else:
            self.log(f'No directory to remove: {output_dir}')
        
        if os.path.exists(output_h5):     
            self.log(f'Removing {output_h5}')
            Rm(output_h5).run()
        else:
            self.log(f'No file to remove: {output_h5}')
        
        ## Clear cache manually
        # # Clear cache
        # self.log(f'Clearing cache')
        # Exec(self.config['flush_mem_cmd'], LocalExecInfo(env=self.mod_env,))
```

### `builtin/builtin/asan/pkg.py`

```python
"""
This module provides classes and methods to inject the Asan interceptor.
Asan is a library to detect memory errors.
"""
from jarvis_cd.core.pkg import Interceptor


class Asan(Interceptor):
    """
    This class provides methods to inject the Asan interceptor.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return []

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        self.config['LIBASAN'] = self.find_library('asan')
        if self.config['LIBASAN'] is None:
            raise Exception('Could not find libasan')
        print(f'Found libasan.so at {self.config["LIBASAN"]}')

    def modify_env(self):
        """
        Modify the jarvis environment.

        :return: None
        """
        self.prepend_env('LD_PRELOAD', self.config['LIBASAN'])
```

### `builtin/builtin/builtin_pkg/package.py`

```python
# Builtin
```

### `builtin/builtin/cm1/pkg.py`

```python
"""
This module provides classes and methods to launch the Cm1 application.
Cm1 is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo, MpiExecInfo
from jarvis_cd.shell.process import Mkdir


class Cm1(Application):
    """
    This class provides methods to launch the Cm1 application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'nx',
                'msg': 'x dimension of 3-D grid',
                'type': int,
                'default': 16,
            },
            {
                'name': 'ny',
                'msg': 'y dimension of 3-D grid',
                'type': int,
                'default': 16,
            },
            {
                'name': 'nz',
                'msg': 'z dimension of 3-D grid',
                'type': int,
                'default': 16,
            },
            {
                'name': 'corex',
                'msg': 'Number of cores for x dimension',
                'type': int,
                'default': 2,
            },
            {
                'name': 'corey',
                'msg': 'Number of cores for x dimension',
                'type': int,
                'default': 2,
            },
            {
                'name': 'file_type',
                'msg': 'The file type to use',
                'type': str,
                'choices': ['grads', 'netcdf', 'lofs'],
                'default': 'netcdf',
            },
            {
                'name': 'file_count',
                'msg': 'The number of files to generate',
                'type': str,
                'choices': ['shared', 'fpo', 'fpp', 'lofs'],
                'default': 'shared',
            },
            {
                'name': 'TEST_CASE',
                'msg': 'The test to run',
                'type': str,
                'choices': ['nssl3'],
                'default': None,
            },
            {
                'name': 'ppn',
                'msg': 'The number of processes per node',
                'type': int,
                'default': 1,
            },
            {
                'name': 'output',
                'msg': 'The directory to output data to',
                'type': str,
                'default': None,
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        # Create output directories
        if self.config['output'] is None:
            self.config['output'] = f'{self.shared_dir}/cm1_out'
        out_parent = str(pathlib.Path(self.config['output']).parent)
        self.config['restart'] = os.path.join(out_parent, 'restart_dir')
        Mkdir([self.config['output'], self.config['restart']],
              LocalExecInfo()).run()

        # Create CM1 compilation
        self.config['CM1_PATH'] = self.env['CM1_PATH']
        Exec(f'bash {self.config["CM1_PATH"]}/buildCM1-spack.sh',
             LocalExecInfo(env=self.env)).run()

        # Create CM1 configuration
        self.env['COREX'] = self.config['corex']
        self.env['COREY'] = self.config['corey']
        corex = self.config['corex']
        corey = self.config['corey']
        namelist_in = os.path.join(self.pkg_dir, 'config',
                                   'namelist.input.nssl3')
        namelist_out = os.path.join(self.shared_dir, 'namelist.input.nssl3')
        if self.config['file_format'] == 'grads':
            file_format = 1
        elif self.config['file_format'] == 'netcdf':
            file_format = 2
        elif self.config['file_format'] == 'lofs':
            file_format = 5
        else:
            raise Exception("Invalid file format")

        if self.config['file_count'] == 'shared':
            file_count = 1
        elif self.config['file_count'] == 'fpo':
            file_count = 2
        elif self.config['file_count'] == 'fpp':
            file_count = 3
        elif self.config['file_count'] == 'lofs':
            file_count = 4
        else:
            raise Exception("Invalid file count")

        self.copy_template_file(namelist_in, namelist_out, replacements=[
            ('file_format', file_format),
            ('file_count', file_count),
            ('nx', self.config['nx']),
            ('ny', self.config['ny']),
            ('nz', self.config['nz']),
            ('nodex', corex),
            ('nodey', corey),
            ('rankx', corex),
            ('ranky', corey),
            ('ppn', self.config['ppn']),
        ])

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        cmd = [
            f'{self.config["CM1_PATH"]}/run/cm1.exe',
            self.config['namelist'],
            self.config['output'],
            'cm1_data',
            self.config['restart']
        ]
        cmd = ' '.join(cmd)
        corex = self.config['corex']
        corey = self.config['corey']
        Exec(cmd, MpiExecInfo(env=self.env,
                              nprocs=corex * corey,
                              ppn=self.config['ppn'],
                              hostfile=self.hostfile)).run()

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        pass
```

### `builtin/builtin/cosmic_tagger/config/config.yaml`

```yaml
defaults:
  - base_config
  - _self_
data:
  aux_file: ##TEST_FILE##
  data_directory: ##DATASET_DIR##
  data_format: channels_last
  downsample: 1
  file: ##TRAIN_FILE##
  synthetic: False
framework: 
  inter_op_parallelism_threads: 2
  intra_op_parallelism_threads: 24
  name: tensorflow
mode:
  checkpoint_iteration: 500
  logging_iteration: 1
  name: train
  no_summary_images: False
  optimizer:
    gradient_accumulation: 1
    learning_rate: 0.0003
    loss_balance_scheme: focal
    name: adam
  summary_iteration: 1
  # weights_location:
network: 
  batch_norm: True
  bias: True
  block_concat: False
  blocks_deepest_layer: 5
  blocks_final: 5
  blocks_per_layer: 2
  bottleneck_deepest: 256
  connections: concat
  conv_mode: conv_2D
  data_format: channels_last
  downsampling: max_pooling
  filter_size_deepest: 5
  growth_rate: additive
  n_initial_filters: 16
  name: uresnet
  network_depth: 6
  residual: True
  upsampling: interpolation
  weight_decay: 0.0
output_dir: output/tensorflow/uresnet/test/
run: 
  aux_iterations: 10
  compute_mode: CPU
  distributed: False
  id: test
  iterations: 500
  minibatch_size: 2
  precision: float32
  profile: False
```

### `builtin/builtin/cosmic_tagger/config/default.yaml`

```yaml
defaults:
  - base_config
  - _self_

# output_dir: output/${framework.name}/${network.name}/${run.id}/
```

### `builtin/builtin/cosmic_tagger/pkg.py`

```python
"""
This module provides classes and methods to launch the DataStagein application.
DataStagein is ....
"""
from jarvis_cd.core.pkg import Application
import os
import pathlib
import time


class CosmicTagger(Application):
    """
    This class provides methods to launch the DataStagein application.
    """
    def _init(self):
        pass
        

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'train_file',
                'msg': 'Train filename (not abspath)',
                'type': str,
                'default': 'cosmic_tagging_light.h5',
            },
            {
                'name': 'test_file',
                'msg': 'Test filename (not abspath)',
                'type': str,
                'default': 'cosmic_tagging_test.h5',
            },
            {
                'name': 'dataset_dir',
                'msg': 'Dataset directory (abspath)',
                'type': str,
                'default': '/home/llogan/Documents/Apps/CosmicTagger/example_data/',
            },
        ]
            
    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        src_path = f'{self.pkg_dir}/config/config.yaml'
        dst_path = f'{self.env["TAGGER_ROOT"]}/src/config/config.yaml'
        self.copy_template_file(src_path, dst_path, replacements= {
            'TRAIN_FILE': self.config['train_file'],
            'TEST_FILE': self.config['test_file'],
            'DATASET_DIR': self.config['dataset_dir'],
        })
        self.log(dst_path, Color.YELLOW)
    
    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        # Exec('conda ')

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        pass
```

### `builtin/builtin/darshan/pkg.py`

```python
"""
This module provides classes and methods to inject the Darshan interceptor.
Darshan is ....
"""
from jarvis_cd.core.pkg import Interceptor
from jarvis_cd.shell import PsshExecInfo
from jarvis_cd.shell.process import Mkdir
import os


class Darshan(Interceptor):
    """
    This class provides methods to inject the Darshan interceptor.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'log_dir',
                'msg': 'Where darshan should place data',
                'type': str,
                'default': f'{os.getenv("HOME")}/darshan_logs',
            },
            {
                'name': 'job_id',
                'msg': 'A semantic ID for the job to identify log files',
                'type': str,
                'default': 'myjob',
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        self.env['DARSHAN_LOG_DIR'] = self.config['log_dir']
        self.env['PBS_JOBID'] = self.config['job_id']
        self.config['DARSHAN_LIB'] = self.find_library('darshan')
        if self.config['DARSHAN_LIB'] is None:
            raise Exception('Could not find darshan')
        Mkdir(self.env['DARSHAN_LOG_DIR'],
              PsshExecInfo(hostfile=self.hostfile)).run()
        print(f'Found libdarshan.so at {self.config["DARSHAN_LIB"]}')

    def modify_env(self):
        """
        Modify the jarvis environment.

        :return: None
        """
        self.append_env('LD_PRELOAD', self.config['DARSHAN_LIB'])
```

### `builtin/builtin/data_stagein/pkg.py`

```python
"""
This module provides classes and methods to launch the DataStagein application.
DataStagein is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo
import os
import pathlib
import time


class DataStagein(Application):
    """
    This class provides methods to launch the DataStagein application.
    """
    def _init(self):
        """
        Initialize paths
        """
        
        # Convert user_data_paths to list
        try:
            user_data_paths = self.config['user_data_paths']
            if user_data_paths is not None:
                self.user_data_list = user_data_paths.split(',')
        except KeyError:
            self.user_data_list = []
        
        try:
            # Convert mkdir_datapaths to list
            mkdir_datapaths = self.config['mkdir_datapaths']
            if mkdir_datapaths is not None:
                self.mkdir_datapaths_list = mkdir_datapaths.split(',')
        except KeyError:
            self.mkdir_datapaths_list = []
        

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'user_data_paths',
                'msg': 'List of paths of user datas to stage in, delimitated by comma',
                'type': str,
                'default': None,
            },
            {
                'name': 'dest_data_path',
                'msg': 'The destination path for all user datas to be staged in',
                'type': str,
                'default': None,
            },
            {
                'name': 'mkdir_datapaths',
                'msg': 'List of paths tp create if it does not exist, delimitated by comma',
                'type': str,
                'default': None,
            },
        ]
    
    def _print_required_params(self):
        required_params = ['dest_data_path', 'user_data_paths', 'mkdir_datapaths']
        print("data_stagein Required parameters: ")
        for param in required_params:
            print(f"    {param}")
            
    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        # Convert user_data_paths to list
        try:
            user_data_paths = self.config['user_data_paths']
            if user_data_paths is not None:
                self.user_data_list = user_data_paths.split(',')
        except KeyError:
            self.user_data_list = []
        
        try:
            # Convert mkdir_datapaths to list
            mkdir_datapaths = self.config['mkdir_datapaths']
            if mkdir_datapaths is not None:
                self.mkdir_datapaths_list = mkdir_datapaths.split(',')
        except KeyError:
            self.mkdir_datapaths_list = []

        self.log(f"user_data_list: {self.user_data_list}")
        self.log(f"mkdir_datapaths_list: {self.mkdir_datapaths_list}")
        
        if self.config['dest_data_path'] is None:
            self._print_required_params()
            raise ValueError("dest_data_path is not set")
        if self.config['user_data_paths'] is None:
            self._print_required_params()
            raise ValueError("user_data_paths is not set")
        if self.config['mkdir_datapaths'] is None:
            self._print_required_params()
            raise ValueError("mkdir_datapaths is not set")
        
        self.config['dest_data_path'] = self.config['dest_data_path']
    
    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        print("Data stagein starting")
        
        user_data_list = self.user_data_list
        dest_data_path = self.config['dest_data_path']
        mkdir_datapaths_list = self.mkdir_datapaths_list
        
        print(f"dest_data_path: {dest_data_path}")
        print(f"user_data_list: {user_data_list}")
        print(f"mkdir_datapaths_list: {mkdir_datapaths_list}")
        
        
        # create paths for user
        for datapath in mkdir_datapaths_list:
            if not pathlib.Path(datapath).exists():
                pathlib.Path(datapath).mkdir(parents=True, exist_ok=True)
            else:
                print(f"Path {datapath} already exists")
        
        # create destination path if it does not exist
        if not pathlib.Path(dest_data_path).exists():
            pathlib.Path(dest_data_path).mkdir(parents=True, exist_ok=True)
        
        start = time.time()
        
        for data_path in user_data_list:
            if not os.path.exists(data_path):
                raise FileNotFoundError(f"Data path {data_path} does not exist")
            else:
                # check if data_path is a directory
                if os.path.isdir(data_path):
                    # Check if the path is empty (e.g. does not contain any files)
                    if len(os.listdir(data_path)) == 0:
                        raise ValueError(f"Data path {data_path} is empty")
                else:
                    # Check if the file is not empty
                    if os.stat(data_path).st_size == 0:
                        raise ValueError(f"Data file {data_path} is empty")
            
            # Check if two directory contains the same files
            dest_files = os.listdir(dest_data_path)
            if os.path.isdir(data_path) and set(dest_files) == set(os.listdir(data_path)):
                # data_files = os.listdir(data_path)
                # if set(dest_files) == set(data_files):
                print(f"Data path {data_path} already exists in {dest_data_path}")
                continue
            
            # Move data to destination path
            cmd = f"cp -r {data_path} {dest_data_path}"
            print(f"Copying data from {data_path} to {dest_data_path}")
            Exec(cmd,LocalExecInfo(env=self.mod_env,)).run()
            
            copied_items = 1
            if os.path.isdir(data_path): copied_items = len(os.listdir(data_path))
            print(f"Copied {copied_items} items ... ")
        
        end = time.time()
        diff = end - start
        self.log(f'data_stagein TIME: {diff} seconds')
            
        print("Data stagein complete")

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        pass
```

### `builtin/builtin/ddmd/pkg.py`

```python
"""
This module provides classes and methods to launch the Ddmd application.
Ddmd is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo, PsshExecInfo
from jarvis_cd.shell.process import Kill, Rm
import os
import yaml
import time
import pathlib, glob, shutil

class Ddmd(Application):
    """
    This class provides methods to launch the Ddmd application.
    """
    def _init(self):
        """
        Initialize paths
        """
        self.openmm_list = []
        self.aggregate = None
        self.train = None
        self.prev_model_json = None
        self.inference = None
        self.hermes_env_vars = ['HERMES_ADAPTER_MODE', 'HERMES_CLIENT_CONF', 'HERMES_CONF', 'LD_PRELOAD']

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'conda_openmm',
                'msg': 'Name of the conda environment for running OpenMM',
                'type': str,
                'default': None,
            },
            {
                'name': 'conda_pytorch',
                'msg': 'Name of the conda environment for running PyTorch',
                'type': str,
                'default': None,
            },
            {
                'name': 'ddmd_path',
                'msg': 'Path to the DDMD source code',
                'type': str,
                'default': None, # `scspkg pkg src ddmd`deepdrivemd/
            },
            {
                'name': 'experiment_path',
                'msg': 'Absolute path to the experiment run input and output files',
                'type': str,
                'default': '${HOME}/experiments/ddmd_test',
            },
            {
                'name': 'local_exp_dir',
                'msg': 'Local experiment directory',
                'type': str,
                'default': None,
            },
            {
                'name': 'molecules_path',
                'msg': 'Absolute path to the molecules submodule directory',
                'type': str,
                'default': None,
            },
            {
                'name': 'md_runs',
                'msg': 'Number of MD runs to perform',
                'type': int,
                'default': 12,
            },
            {
                'name': 'iter_count',
                'msg': 'Number of iterations to perform',
                'type': int,
                'default': 1,
            },
            {
                'name': 'sim_len',
                'msg': 'Length of simulation size (e.g., 0.1. 1)',
                'type': float,
                'default': 0.1,
            },
            {
                'name': 'nnodes',
                'msg': 'Number of nodes to use',
                'type': int,
                'default': 1,
            },
            {
                'name': 'gpu_per_node',
                'msg': 'Number of GPUs per node',
                'type': int,
                'default': 1,
            },
            {
                'name': 'md_start',
                'msg': 'Starting MD run',
                'type': int,
                'default': 0,
            },
            {
                'name': 'md_slide',
                'msg': 'Number of MD runs to slide',
                'type': int,
                'default': 0, # $MD_RUNS/$NODE_COUNT
            },
            {
                'name': 'stage_idx',
                'msg': 'Stage index (starting at 0)',
                'type': int,
                'default': 0, # Usually don't change this
            },
            {
                'name': 'skip_sim',
                'msg': 'Skip the simulation stage',
                'type': bool,
                'default': False,
            },
            {
                'name': 'short_pipe',
                'msg': 'Use a shorted pipeline',
                'type': bool,
                'default': False,
            },
            {
                'name': 'with_hermes',
                'msg': 'Whether it is used with Hermes (e.g. needs to update environment variables)',
                'type': bool,
                'default': False,
            },
            
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        
        if self.config['conda_openmm'] is None:
            # check if CONDA_OPENMM is set in the environment
            if os.environ.get('CONDA_OPENMM') is not None:
                self.config['conda_openmm'] = os.environ.get('CONDA_OPENMM') 
                self.env['CONDA_OPENMM'] = os.environ.get('CONDA_OPENMM')
            else:
                raise Exception('No conda_openmm environment specified')
            
        if self.config['conda_pytorch'] is None:
            if os.environ.get('CONDA_PYTORCH') is not None:
                self.config['conda_pytorch'] = os.environ.get('CONDA_PYTORCH')
                self.env['CONDA_PYTORCH'] = self.config['conda_pytorch']
            else:
                raise Exception('No conda_pytorch environment specified')
        
        if self.config['ddmd_path'] is None:
            if os.environ.get('DDMD_PATH') is not None:
                self.config['ddmd_path'] = os.environ.get('DDMD_PATH')
                self.env['DDMD_PATH'] = self.config['ddmd_path']
                self.config['molecules_path'] = self.config['ddmd_path'] + '/submodules/molecules'
                self.env['MOLECULES_PATH'] = self.config['molecules_path']
            else:
                raise Exception('No ddmd_path specified')
        else:
            self.env['DDMD_PATH'] = self.config['ddmd_path']
            self.config['molecules_path'] = self.config['ddmd_path'] + '/submodules/molecules'
            self.env['MOLECULES_PATH'] = self.config['molecules_path']
        
        if self.config['experiment_path'] is not None:
            self.config['experiment_path'] = os.path.expandvars(self.config['experiment_path'])
            self.env['EXPERIMENT_PATH'] = self.config['experiment_path']
            pathlib.Path(self.config['experiment_path']).mkdir(parents=True, exist_ok=True)
            
        if self.config['experiment_path'] is None:
            raise Exception('No experiment_path specified')
        
        if self.config['md_runs'] < 12:
            raise Exception('md_runs must be at least 12')
        
        if self.config['iter_count'] < 1:
            raise Exception('iter_count must be at least 1')
        
        if self.config['sim_len'] < 0.1:
            raise Exception('sim_len must be at least 0.1')
        
        if self.config['nnodes'] < 1:
            raise Exception('nnodes must be at least 1')
        
        self.config['md_slide'] = self.config['md_runs'] / self.config['nnodes']
    
    def _run_openmm(self):
        """
        Run the OpenMM simulation. This method is called by the run method.

        :return: None
        """
        
        all_tasks = []
        
        for task in range(self.config['md_start'], self.config['md_runs']):
        
            task_idx = "task" + str(task).zfill(4)
            stage_idx = "stage" + str(self.config['stage_idx']).zfill(4)
            gpu_idx = 0 # dummy now
            stage_name="molecular_dynamics"
            
            node_idx = len(self.hostfile) % self.config['nnodes']
            node_name = self.hostfile[node_idx]
            
            yaml_path = self.config['ddmd_path'] + "/test/bba/" + stage_name + "_stage_test.yaml"
            dest_path= self.config['experiment_path'] + "/" + stage_name + "_runs/" + stage_idx + "/" + task_idx
            
            # create the dest_path
            pathlib.Path(dest_path).mkdir(parents=True, exist_ok=True)
            
            # load yaml file to change the parameters
            with open(yaml_path, 'r') as f:
                config_vars = yaml.load(f, Loader=yaml.FullLoader)
                # sed -e "s/\$SIM_LENGTH/${SIM_LENGTH}/" -e "s/\$OUTPUT_PATH/${dest_path//\//\\/}/" -e "s/\$EXPERIMENT_PATH/${EXPERIMENT_PATH//\//\\/}/" -e "s/\$DDMD_PATH/${DDMD_PATH//\//\\/}/" -e "s/\$GPU_IDX/${gpu_idx}/" -e "s/\$STAGE_IDX/${STAGE_IDX}/" $yaml_path  > $dest_path/$(basename $yaml_path)
                config_vars['output_path'] = dest_path
                config_vars['experiment_directory'] = self.config['experiment_path']
                config_vars['initial_pdb_dir'] = self.config['ddmd_path'] + "/data/bba"
                config_vars['pdb_file'] = self.config['ddmd_path'] + "/data/bba/system/1FME-unfolded.pdb"
                config_vars['ddmd_path'] = self.config['ddmd_path']
                config_vars['reference_pdb_file'] = self.config['ddmd_path'] + "/data/bba/1FME-folded.pdb"
                
                config_vars['simulation_length_ns'] = self.config['sim_len']
                config_vars['gpu_idx'] = gpu_idx
                config_vars['stage_idx'] = self.config['stage_idx']
                config_vars['task_idx'] = task
                
                new_yaml_file = dest_path + "/" + stage_name + "_stage_test.yaml"
                yaml.dump(config_vars, open(new_yaml_file, 'w'), default_flow_style=False)
            
            logfile = dest_path + "/" + task_idx + "_OPENMM.log"
            
            cmd = [
                f'cd {dest_path};',
                'conda','run', '-n', self.config['conda_openmm'],
                'mpirun',
                '--host', node_name,
                '-np', str(1),
                '-env',
                f'PYTHONPATH={self.config["ddmd_path"]}:{self.config["molecules_path"]}',
                'python',
                f'{self.config["ddmd_path"]}/deepdrivemd/sim/openmm/run_openmm.py',
                '-c', new_yaml_file,
            ]
            
            conda_cmd = ' '.join(cmd)
            print(F"Running OpenMM on {node_name}: {dest_path}")
            print(f"{conda_cmd} > {logfile}")
            cur_task = Exec(conda_cmd, LocalExecInfo(env=self.mod_env,
                                          pipe_stdout=logfile,
                                          exec_async=True)).run()
            
            all_tasks.append(cur_task)
        
        return all_tasks
        
    
    
    def _run_aggregate(self):
        """
        Aggregate the results of the OpenMM simulation.
        
        :return: None
        """
        task_idx = "task0000" # fix to 0
        stage_name="aggregate"
        
        stage_idx = "stage" + str((self.config['stage_idx'])).zfill(4)
        node_idx = 0 # TODO: allow specify nodes?
        node_name = self.hostfile[node_idx]
        yaml_path = self.config['ddmd_path'] + "/test/bba/" + stage_name + "_stage_test.yaml"
        dest_path= self.config['experiment_path'] + "/" + stage_name + "_runs/" + stage_idx + "/" + task_idx
        # create the dest_path
        pathlib.Path(dest_path).mkdir(parents=True, exist_ok=True)
        
        # load yaml file to change the parameters
        with open(yaml_path, 'r') as f:
            config_vars = yaml.load(f, Loader=yaml.FullLoader)
            # sed -e "s/\$SIM_LENGTH/${SIM_LENGTH}/" -e "s/\$OUTPUT_PATH/${dest_path//\//\\/}/" -e "s/\$EXPERIMENT_PATH/${EXPERIMENT_PATH//\//\\/}/" -e "s/\$DDMD_PATH/${DDMD_PATH//\//\\/}/" -e "s/\$GPU_IDX/${gpu_idx}/" -e "s/\$STAGE_IDX/${STAGE_IDX}/" $yaml_path  > $dest_path/$(basename $yaml_path)
            config_vars['experiment_directory'] = self.config['experiment_path']
            config_vars['stage_idx'] = self.config['stage_idx']
            config_vars['task_idx'] = 0 # fix to 0
            config_vars['output_path'] = dest_path + "/aggregated.h5"
            config_vars['pdb_file'] = self.config['ddmd_path'] + "/data/bba/system/1FME-unfolded.pdb"
            config_vars['reference_pdb_file'] = self.config['ddmd_path'] + "/data/bba/1FME-folded.pdb"
            config_vars['simulation_length_ns'] = self.config['sim_len']
            
            
            new_yaml_file = dest_path + "/" + stage_name + "_stage_test.yaml"
            yaml.dump(config_vars, open(new_yaml_file, 'w'), default_flow_style=False)
            
            logfile = dest_path + "/" + task_idx + "_AGGREGATE.log"
            
            cmd = [
                f'cd {dest_path};',
                'conda','run', '-n', self.config['conda_openmm'],
                'mpirun',
                '--host', node_name,
                '-np', str(1),
                '-env',
                f'PYTHONPATH={self.config["ddmd_path"]}',
                'python',
                f'{self.config["ddmd_path"]}/deepdrivemd/aggregation/basic/aggregate.py',
                '-c', new_yaml_file,
            ]
            
            conda_cmd = ' '.join(cmd)
            print(F"Running Aggregate on {node_name}: {dest_path}")
            print(f"{conda_cmd} > {logfile}")
            Exec(conda_cmd, LocalExecInfo(env=self.mod_env,
                                        pipe_stdout=logfile)).run()
    
    
    def _run_train(self):
        """
        Train the model.
        
        :return: None
        """
        task_idx = "task0000" # fix to 0
        
        stage_idx = "stage" + str((self.config['stage_idx'])).zfill(4)
        model_tag = stage_idx + "_" + task_idx
        node_idx = 0 # TODO: allow specify nodes?
        node_name = self.hostfile[node_idx]
        dest_path= self.config['experiment_path'] + "/" + "machine_learning" + "_runs/" + stage_idx + "/" + task_idx
        stage_name="machine_learning" # "machine_learning" : faster, "training" : slower
        yaml_path = self.config['ddmd_path'] + "/test/bba/" + stage_name + "_stage_test.yaml"
        # create the dest_path
        pathlib.Path(dest_path).mkdir(parents=True, exist_ok=True)
        
        model_select_path = self.config['experiment_path'] + "/model_selection_runs/" + stage_idx + "/" + task_idx
        pathlib.Path(model_select_path).mkdir(parents=True, exist_ok=True)
        
        cp_cmd = [
            'cp','-p',
            f'{self.config["ddmd_path"]}/test/bba/stage0000_task0000.json',
            f'{model_select_path}/{model_tag}.json',
        ]
        cp_cmd = ' '.join(cp_cmd)
        print(f"Copying {model_tag}.json to {model_select_path}")
        Exec(cp_cmd, LocalExecInfo(env=self.mod_env)).run()
        
        self.prev_model_json = f'{model_select_path}/{model_tag}.json'
        
        try:
            # load yaml file to change the parameters
            with open(yaml_path, 'r') as f:
                config_vars = yaml.load(f, Loader=yaml.FullLoader)
                # sed -e "s/\$SIM_LENGTH/${SIM_LENGTH}/" -e "s/\$OUTPUT_PATH/${dest_path//\//\\/}/" -e "s/\$EXPERIMENT_PATH/${EXPERIMENT_PATH//\//\\/}/" -e "s/\$DDMD_PATH/${DDMD_PATH//\//\\/}/" -e "s/\$GPU_IDX/${gpu_idx}/" -e "s/\$STAGE_IDX/${STAGE_IDX}/" $yaml_path  > $dest_path/$(basename $yaml_path)
                config_vars['experiment_directory'] = self.config['experiment_path']
                config_vars['stage_idx'] = self.config['stage_idx']
                config_vars['task_idx'] = 0 # fix to 0
                config_vars['output_path'] = dest_path
                config_vars['model_tag'] = model_tag
                config_vars['init_weights_path'] = "none"
                
                new_yaml_file = dest_path + "/" + stage_name + "_stage_test.yaml"
                yaml.dump(config_vars, open(new_yaml_file, 'w'), default_flow_style=False)
                
                logfile = dest_path + "/" + task_idx + "_TRAIN.log"
                
                cmd = [
                    f'cd {dest_path};',
                    'conda','run', '-n', self.config['conda_pytorch'],
                    'mpirun',
                    '--host', node_name,
                    '-np', str(1),
                    '-env',
                    f'PYTHONPATH={self.config["ddmd_path"]}:{self.config["molecules_path"]}',
                    'python',
                    f'{self.config["ddmd_path"]}/deepdrivemd/models/aae/train.py',
                    '-c', new_yaml_file,
                ]
                
                conda_cmd = ' '.join(cmd)
                print(F"Running Training on {node_name}: {dest_path}")
                print(f"{conda_cmd} > {logfile}")
                curr_task = Exec(conda_cmd, LocalExecInfo(env=self.mod_env,
                                            pipe_stdout=logfile,
                                            exec_async=True)).run()
                return curr_task
        except Exception as e:
            print("ERROR: " + str(e))
            print("ERROR: Training failed")
            return None
    
    def _run_inference(self):
        """
        Run inference on the model.
        
        :return: None
        """
        task_idx = "task0000" # fix to 0
        
        stage_idx = "stage" + str((self.config['stage_idx'])).zfill(4)
        model_tag = stage_idx + "_" + task_idx
        node_idx = 0 
        if len(self.hostfile) > 1:
            node_idx = 1 # TODO: allow specify nodes?
        node_name = self.hostfile[node_idx]
        stage_name="inference" 
        dest_path= self.config['experiment_path'] + f"/{stage_name}_runs/" + stage_idx + "/" + task_idx
        yaml_path = self.config['ddmd_path'] + "/test/bba/" + stage_name + "_stage_test.yaml"
        # create the dest_path
        pathlib.Path(dest_path).mkdir(parents=True, exist_ok=True)
        
        agent_run_path = self.config['experiment_path'] + "/agent_runs/" + stage_idx + "/" + task_idx
        pathlib.Path(agent_run_path).mkdir(parents=True, exist_ok=True)
        
        pretrained_model = self.config['ddmd_path'] + "/data/bba/epoch-130-20201203-150026.pt"
        
        checkpoint_path_pattern = os.path.join(self.config['experiment_path'], "machine_learning_runs", "*", "*", "checkpoint")
        # Find all matching checkpoint directories
        matching_checkpoint_dirs = glob.glob(checkpoint_path_pattern)
        if matching_checkpoint_dirs:
            # Construct the complete file path pattern for checkpoint files
            checkpoint_pattern = os.path.join(matching_checkpoint_dirs[0], '*.pt')
            # Find all matching checkpoint files
            checkpoint_files = glob.glob(checkpoint_pattern)
            # Check if there are any .pt files in the matching checkpoint directory
            if checkpoint_files:

                # Sort files by epoch and timestamp and get the latest checkpoint
                # latest_checkpoint = max(checkpoint_files, key=os.path.getmtime) # getctime, this does not get epoch 10
                latest_checkpoint = max(checkpoint_files, key=lambda x: (int(x.split('-')[1]), int(x.split('-')[2]), int(x.split('-')[3].split('.')[0])))
                print(f"Latest checkpoint: {latest_checkpoint}")
                
            else:
                latest_checkpoint = pretrained_model
        else:
            print(f"Using pretrained model: {pretrained_model}")
            latest_checkpoint = pretrained_model
        
        prev_stage_idx = "stage" + str((self.config['stage_idx']-1)).zfill(4)
        # replace $MODEL_CHECKPOINT with latest_checkpoint in the json file
        with open(self.prev_model_json, 'r') as f:
            json_str = f.read()
            json_str = json_str.replace("$MODEL_CHECKPOINT", latest_checkpoint)

        # save the updated json content back to the file
        with open(self.prev_model_json, 'w') as f:
            f.write(json_str)

        try:
            # load yaml file to change the parameters
            with open(yaml_path, 'r') as f:
                config_vars = yaml.load(f, Loader=yaml.FullLoader)
                # sed -e "s/\$SIM_LENGTH/${SIM_LENGTH}/" -e "s/\$OUTPUT_PATH/${dest_path//\//\\/}/" -e "s/\$EXPERIMENT_PATH/${EXPERIMENT_PATH//\//\\/}/" -e "s/\$DDMD_PATH/${DDMD_PATH//\//\\/}/" -e "s/\$GPU_IDX/${gpu_idx}/" -e "s/\$STAGE_IDX/${STAGE_IDX}/" $yaml_path  > $dest_path/$(basename $yaml_path)
                config_vars['experiment_directory'] = self.config['experiment_path']
                config_vars['stage_idx'] = self.config['stage_idx']
                config_vars['task_idx'] = 0 # fix to 0
                config_vars['output_path'] = dest_path
                
                new_yaml_file = dest_path + "/" + stage_name + "_stage_test.yaml"
                yaml.dump(config_vars, open(new_yaml_file, 'w'), default_flow_style=False)
                
                logfile = dest_path + "/" + task_idx + "_INFERENCE.log"
                
                cmd = [
                    f'cd {dest_path};',
                    'conda','run', '-n', self.config['conda_pytorch'],
                    'mpirun',
                    '--host', node_name,
                    '-np', str(1),
                    '-env',
                    'OMP_NUM_THREADS=4',
                    '-env',
                    f'PYTHONPATH={self.config["ddmd_path"]}:{self.config["molecules_path"]}',
                    'python',
                    f'{self.config["ddmd_path"]}/deepdrivemd/agents/lof/lof.py',
                    '-c', new_yaml_file,
                ]
                
                conda_cmd = ' '.join(cmd)
                print(F"Running Inference on {node_name}: {dest_path}")
                print(f"{conda_cmd} > {logfile}")
                curr_task = Exec(conda_cmd, LocalExecInfo(env=self.mod_env,
                                            pipe_stdout=logfile)).run()
                return curr_task
        except Exception as e:
            print("ERROR: " + str(e))
            print("ERROR: Inference failed")
            return None
    
    def _check_openmm(self):
        """
        Check if all the OpenMM files exist
        """
        for task in range(self.config['md_start'], self.config['md_runs']):
        
            task_idx = "task" + str(task).zfill(4)
            stage_idx = "stage" + str(self.config['stage_idx']).zfill(4)
            stage_name="molecular_dynamics"
            dest_path= self.config['experiment_path'] + "/" + stage_name + "_runs/" + stage_idx + "/" + task_idx
            
            # Check if "*.h5" and "*.pdb" files exist and not size 0
            h5_path_pattern = os.path.join(dest_path, '*.h5')
            matching_h5_files = glob.glob(h5_path_pattern)
            pdb_path_pattern = os.path.join(dest_path, '*.pdb')
            matching_pdb_files = glob.glob(pdb_path_pattern)
            if matching_h5_files and matching_pdb_files:
                continue
            else:
                return False
        return True

    def _unset_vfd_vars(self,env_vars_toset):
        
        conda_envs = [self.config['conda_openmm'], self.config['conda_pytorch']]
        
        for cenv in conda_envs:
        
            cmd = ['conda', 'env', 'config', 'vars', 'unset',]
            
            for env_var in env_vars_toset:
                cmd.append(f'{env_var}')
            cmd.append('-n')
            cmd.append(cenv)
            
            cmd = ' '.join(cmd)
            Exec(cmd, LocalExecInfo(env=self.mod_env,)).run()
            self.log(f"DDMD _unset_vfd_vars for {cenv}: {cmd}")

    def _set_env_vars(self, env_vars_toset):
        
        conda_envs = [self.config['conda_openmm'], self.config['conda_pytorch']]
        
        for cenv in conda_envs:
            
            # Unset all env_vars_toset first
            self._unset_vfd_vars(env_vars_toset)

            cmd = [ 'conda', 'env', 'config', 'vars', 'set']
            for env_var in env_vars_toset:
                env_var_val = self.mod_env[env_var]
                cmd.append(f'{env_var}={env_var_val}')
            
            cmd.append('-n')
            cmd.append(cenv)
            cmd = ' '.join(cmd)
            self.log(f"DDMD _set_env_vars for {cenv}: {cmd}")
            Exec(cmd, LocalExecInfo(env=self.mod_env,)).run()
        

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        
        print("INFO: removing all previous runs")
        self.clean()

        if self.config['with_hermes'] == True:
            self._set_env_vars(self.hermes_env_vars)
        else:
            self._unset_vfd_vars(self.hermes_env_vars)

        # Check if all the OpenMM files exist
        if self._check_openmm() == False and self.config['skip_sim'] == True:
            print("ERROR: OpenMM files not found, cannot skip simulation")
            self.config['skip_sim'] = False
        
        iter_cnt = self.config['iter_count']
        
        total_start = time.time()
        
        for i in range(iter_cnt):
                            
            if self.config['skip_sim'] == False:
                start = time.time()
                self.openmm_list = self._run_openmm()
                # wait for all self.openmm_list to finish
                for task in self.openmm_list:
                    if task is not None:
                        task.wait()
                
                end = time.time()
            
                print(f"OpenMM[{(self.config['stage_idx'])}] : " + str(end - start) + " seconds")
            else:
                print("Skipping OpenMM stage")
            
            if self.config['short_pipe'] == False:
                start = time.time()
                self._run_aggregate()
                end = time.time()
                print(f"Aggregate[{(self.config['stage_idx'])}] : " + str(end - start) + " seconds")
                
            
            self.config['stage_idx']+=1
            
            train_start = time.time()
            self.train = self._run_train()
            if self.config['short_pipe'] == False:
                self.train.wait()
                end = time.time()
                print(f"Train[{(self.config['stage_idx'])}] : " + str(end - train_start) + " seconds")
            else:
                print("Shortened Pipeline: Train stage not waited")
            
            self.config['stage_idx']+=1
            
            start = time.time()
            self.inference = self._run_inference()
            end = time.time()
            
            if self.config['short_pipe'] == False:
                print(f"Inference stage[{(self.config['stage_idx'])}] : " + str(end - start) + " seconds")
            else:
                self.train.wait()
                end = time.time()
                print(f"Train[{(self.config['stage_idx']-1)}] and Inference[{(self.config['stage_idx'])}] : " + str(end - train_start) + " seconds")

        total_end = time.time()
        print(f"Total time: {total_end - total_start} seconds")
    
    
    def kill(self):
        """
        Kill a running application. E.g., OrangeFS will kill the servers,
        clients, and metadata services.

        :return: None
        """
        # FIXME: this will kill all python processes
        print("INFO: killing all python processes")
        Kill('python',
             PsshExecInfo(hostfile=self.hostfile,
                          env=self.env)).run()
        
    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        remove_paths = [
            "agent_runs", "inference_runs", "model_selection_runs",
            "aggregate_runs", "machine_learning_runs", "molecular_dynamics_runs"
        ]
        
        if self.config['skip_sim'] == True:
            print("INFO: do not clean OpenMM")
            # remove all paths excepts molecular_dynamics_runs
            for rp in remove_paths:
                if rp != "molecular_dynamics_runs":
                    remove_path = self.config['experiment_path'] + "/" + rp
                    print("INFO: removing " + remove_path)
                    Rm(remove_path).run()
        else:
            for rp in remove_paths:
                remove_path = self.config['experiment_path'] + "/" + rp
                print("INFO: removing " + remove_path)
                Rm(remove_path).run()
```

### `builtin/builtin/dlio_benchmark/pkg.py`

```python
"""
This module provides classes and methods to launch the DlioBenchmark application.
DlioBenchmark is ....
"""
from jarvis_cd.core.pkg import Application, Color
from jarvis_cd.shell import Exec, MpiExecInfo, PsshExecInfo
from jarvis_cd.shell.process import Rm


class DlioBenchmark(Application):
    """
    This class provides methods to launch the DlioBenchmark application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            # workload related configurations
            {
                'name': 'workload',  # The name of the parameter
                'msg': 'specify the workload name',  # Describe this parameter
                'type': str,  # What is the parameter type?
                'default': 'unet3d_a100',  # What is the default value if not required?
                'choices': [],
                'args': [],
            },
            {
                'name': 'generate_data',
                'msg': 'Does require to generate data before training?',
                'type': bool,
                'default': False,
                'choices': [],
                'args': [],
            },
            {
                'name': 'checkpoint_supported',
                'msg': 'Does the workload support checkpointing?',
                'type': bool,
                'default': True,
                'choices': [],
                'args': [],
            }, 
            {
                'name': 'checkpoint',
                'msg': 'Enable/disable checkpoint',
                'type': bool,
                'default': True,
                'choices': [],
                'args': [],
            }, 
            # dataset related configurations
            {
                'name': 'data_path',
                'msg': 'The path of the dataset',
                'type': str,
                'default': None,
                'choices': [],
                'args': [],
            },
            {
                'name': 'num_files_train',
                'msg': 'Number of files used for training',
                'type': int,
                'default': None,
                'choices': [],
                'args': [],
            },
            # reader related configurations
            {
                'name': 'batch_size',
                'msg': 'Number of samples read per iteration',
                'type': int,
                'default': None,
                'choices': [],
                'args': [],
            },
            {
                'name': 'read_threads',
                'msg': 'Number of read threads in dataloader',
                'type': int,
                'default': None,
                'choices': [],
                'args': [],
            }, 
            # train related configurations
            {
                'name': 'epochs',
                'msg': 'Number of epochs to run',
                'type': int,
                'default': None,
                'choices': [],
                'args': [],
            },
            # checkpoint related configurations
            {
                'name': 'checkpoint_path',
                'msg': 'Path of the checkpoint files',
                'type': str,
                'default': None,
                'choices': [],
                'args': [],
            }, 
            {
                'name': 'checkpoint_after_epoch',
                'msg': 'Checkpoint after the specified epoch id',
                'type': int,
                'default': None,
                'choices': [],
                'args': [],
            }, 
            {
                'name': 'epochs_between_checkpoints',
                'msg': 'Checkpoint interval (unit: epochs)',
                'type': int,
                'default': None,
                'choices': [],
                'args': [],
            }, 
            # process related configuration
            {
                'name': 'nprocs',
                'msg': 'Number of processes',
                'type': int,
                'default': 8,
            },
            {
                'name': 'ppn',
                'msg': 'The number of processes per node',
                'type': int,
                'default': 8,
            },
            # Run with tracing or not
            {
                'name': 'tracing',
                'msg': 'Enable/disable tracing (running with/without DFTracer)',
                'type': bool,
                'default': False,
            }, 
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        # reconfigure data_path
        if self.config['data_path'] is None:
            self.config['data_path'] = f"data/{self.config['workload']}"
        elif f"data/{self.config['workload']}" not in self.config['data_path']:
            self.config['data_path'] = f"{self.config['data_path']}/data/{self.config['workload']}"  

        # reconfigure checkpoint path
        if self.config['checkpoint_supported']:
            if self.config['checkpoint_path'] is None:
                self.config['checkpoint_path'] = f"checkpoints/{self.config['workload']}"
            elif f"checkpoints/{self.config['workload']}" not in self.config['checkpoint_path']:
                self.config['checkpoint_path'] = f"{self.config['checkpoint_path']}/checkpoints/{self.config['workload']}" 

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        
        # step1: generate data if it is required before training
        if self.config['generate_data']:
            # construct the command
            gen_cmd = [
                'dlio_benchmark',
                f'workload={self.config['workload']}',
                f'++workload.workflow.generate_data=True',
                f'++workload.workflow.train=False',
                f'++workload.dataset.data_folder={self.config['data_path']}' 
            ]

            if self.config['num_files_train'] is not None:
                gen_cmd.append(f'++workload.dataset.num_files_train={self.config['num_files_train']}')

            # run the command to generate data
            Exec(' '.join(gen_cmd),
                MpiExecInfo(env=self.mod_env,
                            hostfile=self.hostfile,
                            nprocs=self.config['nprocs'],
                            ppn=self.config['ppn'])).run()

        # step2: clear the system cache
        Exec('sudo drop_caches',
             PsshExecInfo(env=self.env,
                        hostfile=self.hostfile)).run()
        
        # step3: run the benchmark with the workload
        if self.config['tracing']:
            self.mod_env['DFTRACER_ENABLE'] = '1'
            self.mod_env['DFTRACER_INC_METADATA'] = '1'   

        run_cmd = [
            'dlio_benchmark',
            f'workload={self.config['workload']}',
            f'++workload.workflow.generate_data=False',
            f'++workload.workflow.train=Train',
            f'++workload.dataset.data_folder={self.config['data_path']}'  
        ]

        if self.config['num_files_train'] is not None:
            run_cmd.append(f'++workload.dataset.num_files_train={self.config['num_files_train']}')
        
        if self.config['batch_size'] is not None:
            run_cmd.append(f'++workload.reader.batch_size={self.config['batch_size']}')
        
        if self.config['read_threads'] is not None:
            run_cmd.append(f'++workload.reader.read_threads={self.config['read_threads']}')

        if self.config['epochs'] is not None:
            run_cmd.append(f'++workload.train.epochs={self.config['epochs']}')
        
        if self.config['checkpoint_supported']:
            run_cmd.append(f'++workload.workflow.checkpoint={self.config['checkpoint']}')
            run_cmd.append(f'++workload.checkpoint.checkpoint_folder={self.config['checkpoint_path']}')
            if self.config['checkpoint_after_epoch'] is not None:
                run_cmd.append(f'++workload.checkpoint.checkpoint_after_epoch={self.config['checkpoint_after_epoch']}')
            if self.config['epochs_between_checkpoints'] is not None:
                run_cmd.append(f'++workload.checkpoint.epochs_between_checkpoints={self.config['epochs_between_checkpoints']}') 
        #print(f"self.env = {self.env}", flush=True)
        # run the benchmark command
        Exec(' '.join(run_cmd),
             MpiExecInfo(env=self.mod_env,
                         hostfile=self.hostfile,
                         nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'])).run()
        

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        # clear data path
        Rm(self.config['data_path'] + '*',
           PsshExecInfo(env=self.env,
                        hostfile=self.hostfile)).run()

        self.log(f'Removing dataset {self.config['data_path']}', Color.YELLOW)

        # clear checkpoint
        Rm(self.config['checkpoint_path'] + '*',
           PsshExecInfo(env=self.env,
                        hostfile=self.hostfile)).run()
        
        self.log(f'Removing checkpoints {self.config['checkpoint_path']}', Color.YELLOW)
```

### `builtin/builtin/echo/pkg.py`

```python
"""
This module provides classes and methods to launch the Echo application.
Echo is ....
"""
from jarvis_cd.core.pkg import Application


class Echo(Application):
    """
    This class provides methods to launch the Echo application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return []

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        pass

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        print('Echo!')

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        pass
```

### `builtin/builtin/example_app/pkg.py`

```python
"""
Example Application package for testing interceptors
"""

from jarvis_cd.core.pkg import Application, Interceptor
from jarvis_cd.shell import Exec, LocalExecInfo
import os


class ExampleApp(Application):
    """
    Example application that prints environment variables
    and creates a simple test file
    """

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        """
        return [
            {
                'name': 'message',
                'msg': 'Message to print during execution',
                'type': str,
                'default': 'Hello from Example App!'
            },
            {
                'name': 'output_file',
                'msg': 'Output file to create',
                'type': str,
                'default': 'example_output.txt'
            }
        ]

    def _configure(self, **kwargs):
        """
        Configure the example application
        """
        # Create output directory
        os.makedirs(self.private_dir, exist_ok=True)
        os.makedirs(self.shared_dir, exist_ok=True)
        self.output_path = os.path.join(self.private_dir, self.config['output_file'])

        self.log(f'Modified environment variables: {self.mod_env.get("LD_PRELOAD", "None")}')

        # Create marker file for configure command
        marker_path = os.path.join(self.shared_dir, 'configure.marker')
        with open(marker_path, 'w') as f:
            f.write(f'Configured at: {self.config["message"]}\n')
        self.log(f'Created configure marker: {marker_path}')

    def _init(self):
        """
        Initialize the example application
        """
        self.output_path = None

    def start(self):
        """
        Start the example application
        """
        self.log(f'Starting ExampleApp with message: {self.config["message"]}')

        # Create marker file for start command
        marker_path = os.path.join(self.shared_dir, 'start.marker')
        with open(marker_path, 'w') as f:
            f.write(f'Started with message: {self.config["message"]}\n')
        self.log(f'Created start marker: {marker_path}')

        # Create output file as well if output_path is set
        if self.output_path:
            with open(self.output_path, 'w') as f:
                f.write(f'{self.config["message"]}\n')
            self.log(f'Created output file: {self.output_path}')

    def stop(self):
        """
        Stop the application
        """
        self.log('ExampleApp stopped')

        # Create marker file for stop command
        marker_path = os.path.join(self.shared_dir, 'stop.marker')
        with open(marker_path, 'w') as f:
            f.write('Stopped\n')
        self.log(f'Created stop marker: {marker_path}')

    def kill(self):
        """
        Kill the application
        """
        self.log('ExampleApp killed')

        # Create marker file for kill command
        marker_path = os.path.join(self.shared_dir, 'kill.marker')
        with open(marker_path, 'w') as f:
            f.write('Killed\n')
        self.log(f'Created kill marker: {marker_path}')

    def clean(self):
        """
        Clean up application data - this is the 'clear' functionality
        """
        self.log('Cleaning ExampleApp data')

        # Remove all marker files
        marker_files = ['start.marker', 'stop.marker', 'kill.marker', 'configure.marker']
        for marker in marker_files:
            marker_path = os.path.join(self.shared_dir, marker)
            if os.path.exists(marker_path):
                os.remove(marker_path)
                self.log(f'Removed marker file: {marker_path}')

        # Remove output file
        if hasattr(self, 'output_path') and os.path.exists(self.output_path):
            os.remove(self.output_path)
            self.log(f'Removed output file: {self.output_path}')

    def status(self):
        """
        Check if the application completed successfully
        """
        return os.path.exists(self.output_path)

    def modify_env(self):
        """
        Modify environment when used as an interceptor
        """
        self.log('ExampleApp acting as interceptor - setting EXAMPLE_VAR')
        self.setenv('EXAMPLE_VAR', 'test_value_from_interceptor')
        self.setenv('INTERCEPTOR_APPLIED', 'example_app')
        
    def log(self, message):
        """
        Simple logging method
        """
        print(f"[ExampleApp] {message}")
```

### `builtin/builtin/example_interceptor/pkg.py`

```python
"""
Example Interceptor package for testing interceptor functionality
"""

from jarvis_cd.core.pkg import Interceptor
import os


class ExampleInterceptor(Interceptor):
    """
    Example interceptor that modifies environment variables
    and sets up LD_PRELOAD for demonstration
    """

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        """
        return [
            {
                'name': 'library_path',
                'msg': 'Path to the interceptor library (if any)',
                'type': str,
                'default': '/tmp/example_interceptor.so'
            },
            {
                'name': 'custom_env_var',
                'msg': 'Custom environment variable value to set',
                'type': str,
                'default': 'intercepted_value'
            },
            {
                'name': 'debug_mode',
                'msg': 'Enable debug output from interceptor',
                'type': bool,
                'default': True
            }
        ]

    def _configure(self, **kwargs):
        """
        Configure the example interceptor
        """
        # Create output directory
        os.makedirs(self.private_dir, exist_ok=True)
        self.log(f'ExampleInterceptor configured with library_path: {self.config["library_path"]}')

    def _init(self):
        """
        Initialize the example interceptor
        """
        pass

    def modify_env(self):
        """
        Modify the jarvis environment to demonstrate interceptor functionality
        """
        self.log('ExampleInterceptor modifying environment')
        
        # Set custom environment variables
        self.setenv('EXAMPLE_INTERCEPTOR_ACTIVE', 'true')
        self.setenv('EXAMPLE_CUSTOM_VAR', self.config['custom_env_var'])
        
        if self.config['debug_mode']:
            self.setenv('EXAMPLE_DEBUG', '1')
            
        # Demonstrate LD_PRELOAD modification (even if library doesn't exist)
        if self.config['library_path']:
            self.log(f'Adding {self.config["library_path"]} to LD_PRELOAD')
            self.prepend_env('LD_PRELOAD', self.config['library_path'])
            
        # Set a path-like environment variable
        example_path = os.path.join(self.private_dir, 'interceptor_libs')
        os.makedirs(example_path, exist_ok=True)
        self.prepend_env('EXAMPLE_LIB_PATH', example_path)
        
        self.log('Environment modifications completed:')
        self.log(f'  EXAMPLE_INTERCEPTOR_ACTIVE = {self.env.get("EXAMPLE_INTERCEPTOR_ACTIVE", "not set")}')
        self.log(f'  EXAMPLE_CUSTOM_VAR = {self.env.get("EXAMPLE_CUSTOM_VAR", "not set")}')
        self.log(f'  LD_PRELOAD = {self.mod_env.get("LD_PRELOAD", "not set")}')
        self.log(f'  EXAMPLE_LIB_PATH = {self.env.get("EXAMPLE_LIB_PATH", "not set")}')
```

### `builtin/builtin/filebench/pkg.py`

```python
"""
This module provides classes and methods to launch Redis.
Redis cluster is used if the hostfile has many hosts
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, PsshExecInfo
from jarvis_cd.shell.process import Kill, Rm


class Filebench(Application):
    """
    This class provides methods to launch the Ior application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'workload',
                'msg': 'The filebench workload to use',
                'type': str,
                'default': 'fileserver',
                'choices': [],
                'args': [],
            },
            {
                'name': 'dir',
                'msg': 'Directory to use',
                'type': str,
                'default': '/tmp/${USER}/',
                'choices': [],
                'args': [],
            },
            {
                'name': 'run',
                'msg': 'Total runtime (seconds)',
                'type': int,
                'default': 15,
                'choices': [],
                'args': [],
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        # Create the redis hostfile
        workload = self.config['workload']
        dir = os.path.expandvars(self.config['dir'])
        nfiles = SizeConv.to_int(self.config['nfiles'])
        self.copy_template_file(f'{self.pkg_dir}/config/{workload}.f',
                                f'{self.shared_dir}/{workload}.f',
                                {
                                    'DIR': dir,
                                    'RUN': self.config['run'],
                                })

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        cmd = [
            'setarch `arch` --addr-no-randomize',
            'filebench',
            f'-f {self.shared_dir}/{self.config["workload"]}.f',
        ]
        cmd = ' '.join(cmd)
        self.log(cmd, color=Color.YELLOW)
        Exec(cmd,
             PsshExecInfo(env=self.mod_env,
                          hostfile=self.hostfile)).run()

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        Kill('filebench',
             PsshExecInfo(env=self.env,
                          hostfile=self.hostfile)).run()

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        Rm(self.config['dir'] + '*',
           PsshExecInfo(env=self.env,
                        hostfile=self.hostfile)).run()
```

### `builtin/builtin/fio/pkg.py`

```python
"""
This module provides classes and methods to launch the Ior application.
Ior is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo
from jarvis_cd.shell.process import Rm
import os
import pathlib


class Fio(Application):
    """
    This class provides methods to launch the Ior application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'write',
                'msg': 'Perform a write workload',
                'type': bool,
                'default': True,
                'choices': [],
                'args': [],
            },
            {
                'name': 'read',
                'msg': 'Perform a read workload',
                'type': bool,
                'default': False,
            },
            {
                'name': 'xfer',
                'msg': 'The size of data transfer',
                'type': str,
                'default': '1m',
            },
            {
                'name': 'total_size',
                'msg': 'Total amount of data to generate',
                'type': str,
                'default': '32m',
            },
            {
                'name': 'iodepth',
                'msg': 'Total I/O to generate at a time',
                'type': int,
                'default': 1,
            },
            {
                'name': 'reps',
                'msg': 'Number of times to repeat',
                'type': int,
                'default': 1,
            },
            {
                'name': 'nprocs',
                'msg': 'Number of threads/processes',
                'type': int,
                'default': 1,
            },
            {
                'name': 'out',
                'msg': 'Path to the output file',
                'type': str,
                'default': '/tmp/ior.bin',
            },
            {
                'name': 'direct',
                'msg': 'Use direct I/O',
                'type': bool,
                'default': False,
            },
            {
                'name': 'random',
                'msg': 'Use random I/O',
                'type': bool,
                'default': False,
            },
            {
                'name': 'engine',
                'msg': 'backend engine',
                'type': bool,
                'default': 'psync',
            },
            {
                'name': 'log',
                'msg': 'Path to IOR output log',
                'type': str,
                'default': None,
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        pass

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        # Read/write
        if self.config['read'] and self.config['write']:
            mode = 'readwrite'
        elif self.config['read']:
            mode = 'read'
        elif self.config['write']:
            mode = 'write'
        # Direct I/O
        if self.config['direct']:
            direct = 1
        else:
            direct = 0
        # Random I/O
        if self.config['random']:
            random = 1
        else:
            random = 0
        cmd = [
            'fio',
            f'--rw={mode}',
            f'--size={self.config["total_size"]}',
            f'--bs={self.config["xfer"]}',
            f'--iodepth={self.config["iodepth"]}',
            f'--numjobs={self.config["nprocs"]}',
            f'--direct={direct}',
            f'--randrepeat={random}',
            f'--filename={self.config["out"]}',
            f'--ioengine={self.config["engine"]}',
            f'--name=job',
        ]
        # The path
        if '.' in os.path.basename(self.config['out']):
            os.makedirs(str(pathlib.Path(self.config['out']).parent),
                        exist_ok=True)
        else:
            os.makedirs(self.config['out'], exist_ok=True)
        # pipe_stdout=self.config['log'] 
        Exec(' '.join(cmd),
             LocalExecInfo(env=self.mod_env,
                         hostfile=self.hostfile)).run()
        
    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        Rm(self.config['out'] + '*',
           LocalExecInfo()).run()

    def _get_stat(self, stat_dict):
        """
        Get statistics from the application.

        :param stat_dict: A dictionary of statistics.
        :return: None
        """
        stat_dict[f'{self.pkg_id}.runtime'] = self.start_time
```

### `builtin/builtin/gadget2/config/gassphere.yaml`

```yaml
PEANOHILBERT: true
WALLCLOCK: true
SYNCHRONIZATION: true
```

### `builtin/builtin/gadget2/config/lcdm_gas-ngen.yaml`

```yaml
PERIODIC: true
PEANOHILBERT: true
WALLCLOCK: true
SYNCHRONIZATION: true
PMGRID: 128
```

### `builtin/builtin/gadget2/config/lcdm_gas.yaml`

```yaml
PERIODIC: true
PEANOHILBERT: true
WALLCLOCK: true
SYNCHRONIZATION: true
PMGRID: 128
```

### `builtin/builtin/gadget2/pkg.py`

```python
"""
This module provides classes and methods to launch the Gadget2 application.
Gadget2 is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo, MpiExecInfo
from jarvis_cd.shell.process import Mkdir, Rm


class Gadget2(Application):
    """
    This class provides methods to launch the Gadget2 application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'nprocs',
                'msg': 'Number of processes to spawn',
                'type': int,
                'default': 1,
            },
            {
                'name': 'ppn',
                'msg': 'Processes per node',
                'type': int,
                'default': None,
            },
            {
                'name': 'j',
                'msg': 'Number of threads to use for building gadget',
                'type': int,
                'default': 8,
            },
            {
                'name': 'test_case',
                'msg': 'The test case to use',
                'type': str,
                'default': 'gassphere',
            },
            {
                'name': 'out',
                'msg': 'The directory to output data to',
                'type': str,
                'default': '${HOME}/gadget_data',
            },
            {
                'name': 'buffer_size',
                'msg': 'The size in MB of buffers used for communication. '
                       '100MB is typically an upper bound.',
                'type': float,
                'default': 15,
            },
            {
                'name': 'part_alloc_factor',
                'msg': 'Allocate space for particles per processor. '
                       'Typically should be in the range of 1 to 3.',
                'type': float,
                'default': 1.1,
            },
            {
                'name': 'tree_alloc_factor',
                'msg': 'Allocate space for the BH-tree, which is typically '
                       'smaller than the number of particles.',
                'type': float,
                'default': .9,
            },
            {
                'name': 'max_size_timestep',
                'msg': 'The maximum time step of a particle',
                'type': float,
                'default': .01,
            },
            {
                'name': 'time_max',
                'msg': 'The maximum time the simulation estimates (seconds)',
                'type': float,
                'default': 3,
            },
            {
                'name': 'time_bet_snapshot',
                'msg': 'The number of estimated seconds before snapshot occurs',
                'type': float,
                'default': .2,
            },
            {
                'name': 'ic',
                'msg': 'The initial conditions file to use.',
                'type': str,
                'default': None,
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        test_case = self.config['test_case']
        paramfile = f'{self.config_dir}/{test_case}.param'
        buildconf = f'{self.pkg_dir}/config/{test_case}.yaml'
        outdir = expand_env(self.config['out'])
        self.copy_template_file(f'{self.pkg_dir}/paramfiles/{test_case}.param',
                                paramfile,
                                replacements={
                                    'OUTPUT_DIR': outdir,
                                    'REPO_DIR': self.env['GADGET2_PATH'],
                                    'BUFFER_SIZE': self.config['buffer_size'],
                                    'PART_ALLOC_FACTOR': self.config['part_alloc_factor'],
                                    'TREE_ALLOC_FACTOR': self.config['tree_alloc_factor'],
                                    'TIME_MAX': self.config['time_max'],
                                    'TIME_BET_SNAPSHOT': self.config['time_bet_snapshot'],
                                    'MAX_SIZE_TIMESTEP': self.config['max_size_timestep'],
                                    'INITCOND': self.config['ic'],
                                })
        build_dir = f'{self.shared_dir}/build'
        cmake_opts = YamlFile(buildconf).load()
        if 'FFTW_PATH' in self.env:
            cmake_opts['FFTW_PATH'] = self.env['FFTW_PATH']
        Cmake(self.env['GADGET2_PATH'],
              build_dir,
              opts=cmake_opts,
              exec_info=LocalExecInfo(env=self.env))
        Make(build_dir, nthreads=self.config['j'],
             exec_info=LocalExecInfo(env=self.env))

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        test_case = self.config['test_case']
        build_dir = f'{self.shared_dir}/build'
        exec_path = f'{build_dir}/bin/Gadget2'
        paramfile = f'{self.config_dir}/{test_case}.param'
        Mkdir(self.config['out']).run()
        Exec(f'{exec_path} {paramfile}',
             MpiExecInfo(nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'],
                         hostfile=self.hostfile,
                         env=self.mod_env,
                         cwd=self.env['GADGET2_PATH'])).run()

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        build_dir = f'{self.shared_dir}/build'
        Rm([self.config['out']]).run()
```

### `builtin/builtin/gadget2_df/pkg.py`

```python
"""
This module provides classes and methods to launch the Gadget2Df application.
Gadget2Df is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo, MpiExecInfo
from jarvis_cd.shell.process import Mkdir, Rm


class Gadget2Df(Application):
    """
    This class provides methods to launch the Gadget2Df application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'nprocs',
                'msg': 'Number of processes to spawn',
                'type': int,
                'default': 1,
            },
            {
                'name': 'ppn',
                'msg': 'Processes per node',
                'type': int,
                'default': None,
            },
            {
                'name': 'j',
                'msg': 'Number of threads to use for building gadget',
                'type': int,
                'default': 8,
            },
            {
                'name': 'nparticles',
                'msg': 'The maximum number of particles to generate. Should'
                       'be a multiple of 4096.',
                'type': int,
                'default': 4096,
            },
            {
                'name': 'ic',
                'msg': 'The name of the initial condition output file.',
                'type': str,
                'default': 'ics',
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        paramfile = f'{self.config_dir}/ics.param'
        nparticles = self.config['nparticles']
        tile_fac = int((nparticles / 4096) ** (1.0 / 3.0))
        nparticles = 4096 * tile_fac ** 3
        print(f'NUMBER OF PARTICLES: {nparticles}')
        if tile_fac < 1:
            tile_fac = 1
        nsample = int(tile_fac * 16)
        self.copy_template_file(f'{self.pkg_dir}/paramfiles/ics.param',
                                paramfile,
                                replacements={
                                    'REPO_DIR': self.env['GADGET2_PATH'],
                                    'TILE_FAC': tile_fac,
                                    'NSAMPLE': nsample,
                                    'FILE_BASE': self.config['ic'],
                                })
        build_dir = f'{self.shared_dir}/build'
        cmake_opts = {}
        Mkdir(f'{self.env["GADGET2_PATH"]}/ICs-NGen').run()
        if 'FFTW_PATH' in self.env:
            cmake_opts['FFTW_PATH'] = self.env['FFTW_PATH']
        Cmake(self.env['GADGET2_PATH'],
              build_dir,
              opts=cmake_opts,
              exec_info=LocalExecInfo(env=self.env))
        Make(build_dir, nthreads=self.config['j'],
             exec_info=LocalExecInfo(env=self.env))

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        build_dir = f'{self.shared_dir}/build'
        paramfile = f'{self.config_dir}/ics.param'
        exec_path = f'{build_dir}/bin/NGenIC'
        ngenic_root = f'{self.env["GADGET2_PATH"]}/N-GenIC'
        Exec(f'{exec_path} {paramfile}',
             MpiExecInfo(nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'],
                         hostfile=self.hostfile,
                         env=self.mod_env,
                         cwd=ngenic_root)).run()

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        ics_path = f'{self.env["GADGET2_PATH"]}/ICs-NGen/{self.config["ic"]}.*'
        print(ics_path)
        Rm(ics_path).run()
```

### `builtin/builtin/gray_scott/config/adios2.xml`

```xml
<?xml version="1.0"?>
<adios-config>

    <!--============================================
           Configuration for Gray-Scott and GS Plot
        ============================================-->

    <io name="SimulationOutput">
        <engine type="FileStream">
            <!-- SST engine parameters -->
            <parameter key="RendezvousReaderCount" value="0"/>
            <parameter key="QueueLimit" value="1"/>
            <parameter key="QueueFullPolicy" value="Discard"/>
            <!-- BP4/SST engine parameters -->
            <parameter key="OpenTimeoutSecs" value="10.0"/>
        </engine>
    </io>

    <!--===========================================
           Configuration for PDF calc and PDF Plot
        ===========================================-->

    <io name="PDFAnalysisOutput">
        <engine type="FileStream">
            <!-- SST engine parameters -->
            <parameter key="RendezvousReaderCount" value="1"/>
            <parameter key="QueueLimit" value="5"/>
            <parameter key="QueueFullPolicy" value="Block"/>
            <!-- BP4/SST engine parameters -->
            <parameter key="OpenTimeoutSecs" value="10.0"/>
        </engine>

        <!-- Compress variables -->
        <!--
        <variable name="U">
            <operation type="sz">
                <parameter key="accuracy" value="0.001"/>
            </operation>
        </variable>
        <variable name="V">
            <operation type="sz">
                <parameter key="accuracy" value="0.001"/>
            </operation>
        </variable>
        -->
    </io>

    <!-- example engines

        <engine type="FileStream"/>
        <engine type="SST"/>
        <engine type="BP4"/>
        <engine type="BP5"/>
        <engine type="HDF5"/>
        <engine type="File"/>

        === SST ===
        SST can be set up to force blocking the producer on a consumer
        or to discard unused data. Separately, it can be also set up
        so that the producer is waiting for a first connection or
        just starts running alone.

        Producer start alone, and it does not keep data.
        Consumer will get recent data when connects.
        If consumer(s) goes away, producer runs alone and
           discards data.
        <engine type="SST">
            <parameter key="RendezvousReaderCount" value="0"/>
            <parameter key="QueueLimit" value="1"/>
            <parameter key="QueueFullPolicy" value="Discard"/>
        </engine>

        Producer will wait for 1 consumer to connect before proceeding.
        Producer will buffer 5 output steps so a consumer may lag behind a bit.
        If consumer(s) goes away, producer will block indefinitely after
          the buffer is full.
        <engine type="SST">
            <parameter key="RendezvousReaderCount" value="1"/>
            <parameter key="QueueLimit" value="5"/>
            <parameter key="QueueFullPolicy" value="Block"/>
        </engine>

        === BP4 ===
        BP4 is the default ADIOS2 file format.
        'NumAggregators ' parameter controls how many files
            are created under the output folder. By default, each process
            writes its own file (N-to-N pattern), which is fast but is
            not scalable to tens of thousands of processes. The number of
            substreams should be chosen for the capability of the underlying
            filesystem (e.g. twice the number of OST servers on a Lustre file system).
        'OpenTimeoutSecs' parameter specifies how long to wait on a reading Open
            for the file to appear. Useful for streaming when the reader starts up
            faster than the writer produce any output.
        <engine type="BP4">
            <parameter key="NumAggregators " value="4"/>
            <parameter key="OpenTimeoutSecs" value="10.0"/>
        </engine>

        === FileStream ===
        FileStream is a token name for a file format that supports reading while writing
        It refers to BP4 currently. Timeout for opening is set to 1 hour by default for readers.
        'NumAggregators ' parameter is the same as for BP4.
        <engine type="FileStream">
            <parameter key="NumAggregators " value="4"/>
        </engine>

        === File ===
        FileStream is a token name for a file format, useful on reader side to open
        any version of BP or an HDF5 file.
     -->
</adios-config>
```

### `builtin/builtin/gray_scott/pkg.py`

```python
"""
This module provides classes and methods to launch the Gray Scott application.
Gray Scott is a 3D 7-point stencil code for modeling the diffusion of two
substances.
"""
from jarvis_cd.core.pkg import Application, Color
from jarvis_cd.shell import Exec, MpiExecInfo, PsshExecInfo
from jarvis_cd.shell.process import Mkdir, Rm
import time
import pathlib


class GrayScott(Application):
    """
    This class provides methods to launch the GrayScott application.
    """
    def _init(self):
        """
        Initialize paths
        """
        self.adios2_xml_path = f'{self.shared_dir}/adios2.xml'
        self.settings_json_path = f'{self.shared_dir}/settings-files.json'

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'nprocs',
                'msg': 'Number of processes to spawn',
                'type': int,
                'default': 4,
            },
            {
                'name': 'ppn',
                'msg': 'Processes per node',
                'type': int,
                'default': None,
            },
            {
                'name': 'L',
                'msg': 'Grid size of cube',
                'type': int,
                'default': 32,
            },
            {
                'name': 'Du',
                'msg': 'Diffusion rate of substance U',
                'type': float,
                'default': .2,
            },
            {
                'name': 'Dv',
                'msg': 'Diffusion rate of substance V',
                'type': float,
                'default': .1,
            },
            {
                'name': 'F',
                'msg': 'Feed rate of U',
                'type': float,
                'default': .01,
            },
            {
                'name': 'k',
                'msg': 'Kill rate of V',
                'type': float,
                'default': .05,
            },
            {
                'name': 'dt',
                'msg': 'Timestep',
                'type': float,
                'default': 2.0,
            },
            {
                'name': 'steps',
                'msg': 'Total number of steps to simulate',
                'type': int,
                'default': 100,
            },
            {
                'name': 'plotgap',
                'msg': 'Number of steps between output',
                'type': float,
                'default': 10,
            },
            {
                'name': 'noise',
                'msg': 'Amount of noise',
                'type': float,
                'default': .01,
            },
            {
                'name': 'output',
                'msg': 'Absolute path to output data',
                'type': str,
                'default': None,
            },
            {
                'name': 'engine',
                'msg': 'Engien to be used',
                'choices': ['bp5', 'hermes'],
                'type': str,
                'default': 'bp5',
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        if self.config['output'] is None:
            adios_dir = os.path.join(self.shared_dir, 'gray-scott-output')
            self.config['output'] = os.path.join(adios_dir,
                                                 'data')
            Mkdir(adios_dir, PsshExecInfo(hostfile=self.hostfile,
                                          env=self.env)).run()
        settings_json = {
            'L': self.config['L'],
            'Du': self.config['Du'],
            'Dv': self.config['Dv'],
            'F': self.config['F'],
            'k': self.config['k'],
            'dt': self.config['dt'],
            'plotgap': self.config['plotgap'],
            'steps': self.config['steps'],
            'noise': self.config['noise'],
            'output': f'{self.config["output"]}',
            'adios_config': self.adios2_xml_path
        }
        Mkdir(self.config['output'],
              PsshExecInfo(hostfile=self.hostfile,
                           env=self.env)).run()
        JsonFile(self.settings_json_path).save(settings_json)

        if self.config['engine'].lower() == 'bp5':
            self.copy_template_file(f'{self.pkg_dir}/config/adios2.xml',
                                self.adios2_xml_path)
        elif self.config['engine'].lower() == 'hermes':
            self.copy_template_file(f'{self.pkg_dir}/config/hermes.xml',
                                    self.adios2_xml_path)
        else:
            raise Exception('Engine not defined')

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        # print(self.env['HERMES_CLIENT_CONF'])
        start = time.time()
        Exec(f'gray-scott {self.settings_json_path}',
             MpiExecInfo(nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'],
                         hostfile=self.hostfile,
                         env=self.mod_env)).run()
        end = time.time()
        diff = end - start
        self.log(f'TIME: {diff} seconds', color=Color.GREEN)

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        output_dir = self.config['output'] + "*"
        print(f'Removing {output_dir}')
        Rm(output_dir).run()
```

### `builtin/builtin/ior/__init__.py`

```python
"""IOR benchmark package for Jarvis-CD"""
```

### `builtin/builtin/ior/container.py`

```python
"""
Container-based IOR deployment using Docker/Podman compose.
"""
from jarvis_cd.core.container_pkg import ContainerApplication
from pathlib import Path


class IorContainer(ContainerApplication):
    """
    Container-based IOR deployment.
    """

    def augment_container(self) -> str:
        """
        Generate Dockerfile commands to install IOR in a container.

        :return: Dockerfile commands as a string
        """
        return """
# Install IOR
RUN . "${SPACK_DIR}/share/spack/setup-env.sh" && \\
    spack install -y ior

# Copy required spack executables and libraries to /usr
RUN . "${SPACK_DIR}/share/spack/setup-env.sh" && \\
    spack load iowarp && \\
    cp -r $(spack location -i python)/bin/* /usr/bin || true && \\
    cp -r $(spack location -i py-pip)/bin/* /usr/bin || true && \\
    cp -r $(spack location -i python-venv)/bin/* /usr/bin || true && \\
    PYTHON_PATH=$(readlink -f /usr/bin/python3) && \\
    PYTHON_PREFIX=$(dirname $(dirname $PYTHON_PATH)) && \\
    cp -r $(spack location -i mpi)/bin/* /usr/bin || true && \\
    cp -r $(spack location -i ior)/bin/* /usr/bin || true && \\
    cp -r $(spack location -i iowarp-runtime)/bin/* /usr/bin || true && \\
    cp -r $(spack location -i iowarp-cte)/bin/* /usr/bin || true && \\
    cp -r $(spack location -i cte-hermes-shm)/bin/* /usr/bin || true && \\
    for pkg in $(spack find --format '{name}' | grep '^py-'); do \\
        cp -r $(spack location -i $pkg)/lib/* $PYTHON_PREFIX/lib/ 2>/dev/null || true; \\
        cp -r $(spack location -i $pkg)/bin/* /usr/bin 2>/dev/null || true; \\
    done && \\
    sed -i '1s|.*|#!/usr/bin/python3|' /usr/bin/jarvis && \\
    echo "Spack packages copied to /usr directory"
"""

    def _configure(self, **kwargs):
        """
        Configure container deployment.
        For pipeline-level containers, configuration files are generated by the pipeline.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        # Call parent configuration
        super()._configure(**kwargs)

        # Note: For pipeline-level containers, Dockerfile and compose files are generated
        # by the pipeline's _generate_pipeline_dockerfile() and _generate_pipeline_compose_file()
        # methods, not by individual packages.

    def _generate_dockerfile(self):
        """
        Generate Dockerfile for IOR container.
        Uses the base image specified in the pipeline's container_base.

        Note: This method is deprecated for pipeline-level containers.
        Use pipeline.container_ssh_port instead.
        """
        # Use pipeline-level SSH port if available, otherwise fallback to default
        ssh_port = getattr(self.pipeline, 'container_ssh_port', 2222)

        # Get the base image to build FROM
        if hasattr(self.pipeline, 'container_base') and self.pipeline.container_base:
            base_image = self.pipeline.container_base
        else:
            # Fallback to default if pipeline doesn't have container_base
            base_image = 'iowarp/iowarp-build:latest'

        # sshd always listens on the configured port inside the container
        # - For host network: sshd listens directly on this port
        # - For port mapping: sshd listens on this port, gets mapped to host
        sshd_port = ssh_port

        dockerfile_content = f"""FROM {base_image}

# Disable prompt during packages installation.
ARG DEBIAN_FRONTEND=noninteractive

# Configure SSH daemon to listen on port {sshd_port}
RUN sed -i 's/^#*Port .*/Port {sshd_port}/' /etc/ssh/sshd_config
"""

        dockerfile_path = Path(self.shared_dir) / 'Dockerfile'
        with open(dockerfile_path, 'w') as f:
            f.write(dockerfile_content)

        print(f"Generated Dockerfile: {dockerfile_path}")
        print(f"Using base image: {base_image}")
```

### `builtin/builtin/ior/default.py`

```python
"""
This module provides classes and methods to launch the Ior application.
Ior is a benchmark tool for measuring the performance of I/O systems.
It is a simple tool that can be used to measure the performance of a file system.
It is mainly targeted for HPC systems and parallel I/O.
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo, MpiExecInfo, PsshExecInfo, Rm, Mkdir
from jarvis_cd.shell.process import GdbServer
from jarvis_cd.util import Hostfile
import os
import pathlib


class IorDefault(Application):
    """
    This class provides methods to launch the Ior application using default deployment.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        # Call parent configuration (handles interceptors)
        super()._configure(**kwargs)

        self.config['api'] = self.config['api'].upper()

        # Create parent directory of output file on all nodes
        out = os.path.expandvars(self.config['out'])
        parent_dir = str(pathlib.Path(out).parent)
        Mkdir(parent_dir,
              PsshExecInfo(env=self.mod_env,
                           hostfile=self.hostfile)).run()

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        cmd = [
            'ior',
            '-k',
            f'-b {self.config["block"]}',
            f'-t {self.config["xfer"]}',
            f'-a {self.config["api"]}',
            f'-o {self.config["out"]}',
        ]
        if self.config['write']:
            cmd.append('-w')
        if self.config['read']:
            cmd.append('-r')
        if self.config['fpp']:
            cmd.append('-F')
        if self.config['reps'] > 1:
            cmd.append(f'-i {self.config["reps"]}')
        if self.config['direct']:
            cmd.append('-O useO_DIRECT=1')
            
        # Build IOR command
        ior_cmd = ' '.join(cmd)

        # Use GdbServer to create gdbserver command if debugging is enabled
        gdb_server = GdbServer(ior_cmd, self.config.get('dbg_port', 4000))
        gdbserver_cmd = gdb_server.get_cmd()

        # Use multi-command format with gdbserver
        cmd_list = [
            {
                'cmd': gdbserver_cmd,
                'nprocs': 1 if self.config.get('do_dbg', False) else 0,
                'disable_preload': True
            },
            {
                'cmd': ior_cmd,
                'nprocs': None  # Will be calculated from remainder
            }
        ]
        print(cmd_list)

        Exec(cmd_list,
             MpiExecInfo(env=self.mod_env,
                         hostfile=self.hostfile,
                         nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'])).run()

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        Rm(self.config['out'] + '*',
           PsshExecInfo(env=self.env,
                        hostfile=self.hostfile)).run()

    def _get_stat(self, stat_dict):
        """
        Get statistics from the application.

        :param stat_dict: A dictionary of statistics.
        :return: None
        """
        stat_dict[f'{self.pkg_id}.runtime'] = self.start_time
        
    def log(self, message):
        """
        Simple logging method
        """
        print(f"[IOR:{self.pkg_id}] {message}")
```

### `builtin/builtin/ior/pkg.py`

```python
"""
This module provides classes and methods to launch the Ior application.
Ior is a benchmark tool for measuring the performance of I/O systems.
It is a simple tool that can be used to measure the performance of a file system.
It is mainly targeted for HPC systems and parallel I/O.
"""
from jarvis_cd.core.route_pkg import RouteApp


class Ior(RouteApp):
    """
    Router class for IOR deployment - delegates to default or container implementation.
    """

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.

        :return: List(dict)
        """
        # Get base menu from RouteApp (includes deploy_mode)
        base_menu = super()._configure_menu()

        # Override deploy_mode choices to show available deployment modes
        for item in base_menu:
            if item['name'] == 'deploy_mode':
                item['choices'] = ['default', 'container']
                break

        # Add all IOR parameters (shared by both default and container deployments)
        ior_menu = [
            {
                'name': 'write',
                'msg': 'Perform a write workload',
                'type': bool,
                'default': True,
                'choices': [],
                'args': [],
            },
            {
                'name': 'read',
                'msg': 'Perform a read workload',
                'type': bool,
                'default': False,
            },
            {
                'name': 'xfer',
                'msg': 'The size of data transfer',
                'type': str,
                'default': '1m',
            },
            {
                'name': 'block',
                'msg': 'Amount of data to generate per-process',
                'type': str,
                'default': '32m',
                'aliases': ['block_size']
            },
            {
                'name': 'api',
                'msg': 'The I/O api to use',
                'type': str,
                'choices': ['posix', 'mpiio', 'hdf5'],
                'default': 'posix',
            },
            {
                'name': 'fpp',
                'msg': 'Use file-per-process',
                'type': bool,
                'default': False,
            },
            {
                'name': 'reps',
                'msg': 'Number of times to repeat',
                'type': int,
                'default': 1,
            },
            {
                'name': 'nprocs',
                'msg': 'Number of processes',
                'type': int,
                'default': 1,
            },
            {
                'name': 'ppn',
                'msg': 'The number of processes per node',
                'type': int,
                'default': 16,
            },
            {
                'name': 'out',
                'msg': 'Path to the output file',
                'type': str,
                'default': '/tmp/ior.bin',
                'aliases': ['output']
            },
            {
                'name': 'log',
                'msg': 'Path to IOR output log',
                'type': str,
                'default': '',
            },
            {
                'name': 'direct',
                'msg': 'Use direct I/O (O_DIRECT) for POSIX API, bypassing I/O buffers',
                'type': bool,
                'default': False,
            }
        ]

        # Combine base menu with IOR-specific menu
        return base_menu + ior_menu
```

### `builtin/builtin/lammps/INSTALL.md`

```markdown
lammps is a molecule dynamic application.

# Install LAMMPS
use spack to install lammps
```
spack install lammps^adios2@2.9.0^mpi
```

# Download LAMMPS Example scripts
download the lammps script example
```
git clone https://github.com/simongravelle/lammps-input-files.git
```
```

### `builtin/builtin/lammps/USE.md`

```markdown
# Set up the Environment and Run the LAMMPS
choose a example in lammps-input-files, for example, 2D-lennard-jones-fluid.
In 2D-lennard-jones-fluid folder, run the following command
```
spack load lammps
lmp -in input.lammps
```
# Run the LAMMPS with ADIOS
In 2D-lennard-jones-fluid folder/input.lammps, add adios in dump command:
```
dump mydmp all atom 1000 dump.lammpstrj
```
to
```
dump mydmp all atom/adios 1000 dump.lammpstrj
```
Upon executing the LAMMPS command, an adios configure file will be automatically generated by LAMMPS.
```
lmp -in input.lammps
```

# LAMMPS With Hermes

## 1. Setup Environment

Create the environment variables needed by Hermes + LAMMPS
```bash
spack load hermes@master
spack load lammps
spack load mpich
export PATH=/coeus-adapter/build/bin/:$PATH
export LD_LIBRARY_PATH=/coeus-adapter/build/bin/:$LD_LIBRARY_PATH

```

## 2. Install Jarvis and set up Jarvis
Please refer this website for more information about Jarvis.  
https://grc.iit.edu/docs/jarvis/jarvis-cd/index

## 3. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by Hermes
and Gray Scott.

```bash
jarvis pipeline create lammps
```

## 3. Save Environment

Store the current environment in the pipeline.
```bash
jarvis pipeline env build
```

## 4. Add pkgs to the Pipeline

Create a Jarvis pipeline with Hermes, the Hermes MPI-IO interceptor,
and gray-scott
```bash
jarvis pipeline append hermes_run --sleep=10 --provider=sockets
jarvis pipeline append lammps ppn=?? nprocs=?? script_location=/the/location/of/script/folder engine=hermes
```

Jarvis also can support LAMMPS without hermes engine(only adios)
```bash
jarvis pipeline append hermes_run --sleep=10 --provider=sockets
jarvis pipeline append lammps ppn=?? nprocs=?? script_location=/the/location/of/script/folder engine=BP5
```
## 5. Run the Experiment

Run the experiment
```bash
jarvis pipeline run
```

## 6. Clean Data

To clean data produced by Hermes + Gray-Scott:
```bash
jarvis pipeline clean
```

### `builtin/builtin/lammps/config/adios2.xml`

```xml
<?xml version="1.0"?>
<adios-config>
<io name="atom">
    <engine type="BP4">
        <parameter key="substreams" value="1"/>
    </engine>
</io>
<io name="custom">
    <engine type="BP4">
        <parameter key="substreams" value="1"/>
    </engine>
</io>
<io name="read_dump">
    <engine type="BP4">
    </engine>
</io>
</adios-config>
```

### `builtin/builtin/lammps/config/hermes.xml`

```xml
<?xml version="1.0"?>
<adios-config>
    <io name="atom">
        <engine type="Plugin">
            <parameter key="PluginName" value="hermes"/>
            <parameter key="PluginLibrary" value="hermes_engine"/>
            <parameter key="ppn" value="##ppn##"/>
            <parameter key="db_file" value="##DB_FIEL##"/>
        </engine>
    </io>
    <io name="custom">
        <engine type="Plugin">
            <parameter key="PluginName" value="hermes"/>
            <parameter key="PluginLibrary" value="hermes_engine"/>
            <parameter key="ppn" value="##ppn##"/>
            <parameter key="db_file" value="##DB_FIEL##"/>
        </engine>
    </io>
    <io name="read_dump">
        <engine type="Plugin">
            <parameter key="PluginName" value="hermes"/>
            <parameter key="PluginLibrary" value="hermes_engine"/>
            <parameter key="ppn" value="##ppn##"/>
            <parameter key="db_file" value="##DB_FIEL##"/>
        </engine>
    </io>
</adios-config>
```

### `builtin/builtin/lammps/pkg.py`

```python
"""
This module provides classes and methods to launch the Lammps application.
Lammps is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, MpiExecInfo, PsshExecInfo
from jarvis_cd.shell.process import Rm


class Lammps(Application):
    """
    This class provides methods to launch the Lammps application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'nprocs',
                'msg': 'Number of processes',
                'type': int,
                'default': 1,
            },
            {
                'name': 'ppn',
                'msg': 'The number of processes per node',
                'type': int,
                'default': 4,
            },
            {
                'name': 'engine',
                'msg': 'Engine to be used',
                'choices': ['bp4', 'hermes'],
                'type': str,
                'default': 'bp4',
            },
            {
                'name': 'script_location',
                'msg': 'the location of lammps script',  # the location for lammps scirpt
                'type': str,
                'default': None,

            },
            {
                'name': 'db_path',
                'msg': 'Path where the DB will be stored',
                'type': str,
                'default': 'benchmark_metadata.db',
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        if self.config['engine'].lower() == 'bp4':
            self.copy_template_file(f'{self.pkg_dir}/config/adios2.xml',
                                    f'{self.config["script_location"]}/adios_config.xml')
        elif  self.config['engine'].lower == 'hermes':
            replacement = [("ppn", self.config['ppn']), ("DB_FIEL", self.config['db_file'])]
            self.copy_template_file(f'{self.pkg_dir}/config/hermes.xml',
                                    f'{self.config["script_location"]}/adios_config.xml', replacement)
        else:
            raise Exception('Engine not defined')

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        Exec('lmp -in input.lammps',
             MpiExecInfo(nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'],
                         hostfile=self.hostfile,
                         env=self.mod_env,
                         cwd=self.config['script_location'])).run()
        pass

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """

        output_file = [self.config['db_path']]
        Rm(output_file, PsshExecInfo(hostfile=self.hostfile)).run()
        pass
```

### `builtin/builtin/mkfs/pkg.py`

```python
"""
This module provides classes and methods to create XFS or EXT4 filesystems.
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo
from jarvis_cd.shell.process import Mkdir, MkfsXfs, MkfsExt4, Mount, Chown, Umount, Rmdir
import os


class Mkfs(Application):
    """
    This class provides methods to create XFS or EXT4 filesystems.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        """
        return [
            {
                'name': 'device',
                'msg': 'The device to format (e.g., /dev/sda1)',
                'type': str,
                'default': None,
                'required': True
            },
            {
                'name': 'fs_type',
                'msg': 'The filesystem type to create (xfs or ext4)',
                'type': str,
                'default': 'xfs',
                'choices': ['xfs', 'ext4']
            },
            {
                'name': 'force',
                'msg': 'Force creation even if filesystem exists',
                'type': bool,
                'default': False
            },
            {
                'name': 'block_size',
                'msg': 'Block size in bytes',
                'type': int,
                'default': 4096
            },
            {
                'name': 'inode_size',
                'msg': 'Inode size in bytes (ext4 only)',
                'type': int,
                'default': 256
            },
            {
                'name': 'mount_point',
                'msg': 'Where to mount the filesystem after creation',
                'type': str,
                'default': None
            }
        ]

    def _configure(self, **kwargs):
        """
        Validates configuration parameters
        """
        if not self.config['device']:
            raise ValueError("Device parameter is required")
        
        if self.config['fs_type'] not in ['xfs', 'ext4']:
            raise ValueError("Filesystem type must be either 'xfs' or 'ext4'")

    def start(self):
        """
        Create the filesystem
        """
        # Create filesystem based on type
        if self.config['fs_type'] == 'xfs':
            self.log(f"Creating XFS filesystem on {self.config['device']}")
            MkfsXfs(self.config['device'],
                   LocalExecInfo(env=self.env,
                                sudo=True),
                   block_size=self.config['block_size'],
                   force=self.config['force']).run()
        else:  # ext4
            self.log(f"Creating EXT4 filesystem on {self.config['device']}")
            MkfsExt4(self.config['device'],
                    LocalExecInfo(env=self.env,
                                 sudo=True),
                    block_size=self.config['block_size'],
                    inode_size=self.config['inode_size'],
                    force=self.config['force']).run()

        # Mount the filesystem if mount point is specified
        if self.config['mount_point']:
            # Create mount point if it doesn't exist
            Mkdir(self.config['mount_point'],
                  LocalExecInfo(env=self.env,
                               sudo=True)).run()

            # Mount the filesystem
            Mount(self.config['device'],
                  self.config['mount_point'],
                  LocalExecInfo(env=self.env,
                               sudo=True),
                  options=['data=ordered']).run()
            self.log(f"Mounted filesystem at {self.config['mount_point']}")

            # Change ownership to current user
            uid = os.getuid()
            gid = os.getgid()
            self.log(f"Changing ownership of {self.config['mount_point']} to current user (uid={uid}, gid={gid})")
            Chown(self.config['mount_point'],
                  uid,
                  gid,
                  LocalExecInfo(env=self.env,
                               sudo=True)).run()

    def stop(self):
        """
        Unmount the filesystem if it was mounted
        """
        if self.config['mount_point']:
            self.log(f"Unmounting filesystem from {self.config['mount_point']}")
            Umount(self.config['mount_point'],
                   LocalExecInfo(env=self.env,
                                sudo=True)).run()

    def clean(self):
        """
        Clean up by unmounting and removing mount point if it exists
        """
        if self.config['mount_point']:
            # First unmount
            self.stop()
            
            # Then remove mount point directory
            self.log(f"Removing mount point directory {self.config['mount_point']}")
            Rmdir(self.config['mount_point'],
                  LocalExecInfo(env=self.env,
                               sudo=True)).run()
```

### `builtin/builtin/my_shell/pkg.py`

```python
"""
This module provides classes and methods to launch the MyShell application.
MyShell is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo


class MyShell(Application):
    """
    This class provides methods to launch the MyShell application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        
        return [
            {
                'name': 'script',  # The name of the parameter
                'msg': 'The path of shell script to execute.',  # Describe this parameter
                'type': str,  # What is the parameter type?
                'default': None,  # What is the default value if not required?
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        self.config['script'] = self.config['script']

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        cmd = [
            'bash',
        ]
        if self.config['script']:
            cmd.append(self.config['script'])
        # pipe_stdout=self.config['log']
        Exec(' '.join(cmd), LocalExecInfo()).run()

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        pass
```

### `builtin/builtin/my_shell/test_scirpt.sh`

```bash
echo hLLO
ps aux | grep python3
```

### `builtin/builtin/nyx_lya/pkg.py`

```python
"""
This module provides classes and methods to launch the NyxLya application.
NyxLya is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, MpiExecInfo, PsshExecInfo
from jarvis_cd.shell.process import Mkdir, Rm


class NyxLya(Application):
    """
    This class provides methods to launch the NyxLya application.
    """
    def _init(self):
        """
        Initialize paths
        """
        self.inputs_path = f'{self.pkg_dir}/config/inputs'
        self.nyx_lya_path = None

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'nprocs',
                'msg': 'Number of processes to spawn',
                'type': int,
                'default': 1,
            },
            {
                'name': 'ppn',
                'msg': 'Processes per pkg',
                'type': int,
                'default': 1,
            },
            {
                'name': 'nyx_install_path',
                'msg': 'Absolute path to Nyx installation',
                'type': str,
                'default': None,
            },
            {
                'name': 'initial_z',
                'msg': 'final value of z, z corresponds to a time stemp (e.g., 190.0)',
                'type': float,
                'default': 159.0,
            },
            {
                'name': 'final_z',
                'msg': 'final value of z, z corresponds to a time stemp (e.g., 180.0). final_z < initial_z',
                'type': float,
                'default': 2.0, 
            },
            {
                'name': 'plot_z_values',
                'msg': 'z values to save the plot files(e.g., "188.0 186.0 184.0 182.0")',
                'type': str,
                'default': "7.0 6.0 5.0 4.0 3.0 2.0",
            },
            {
                'name': 'particle_file',
                'msg': 'Absolute path to the binary particle fileoutput(e.g., 64sssss_20mpc.nyx)',
                'type': str,
                'default': '64sssss_20mpc.nyx',
            },
            {
                'name': 'output',
                'msg': 'Absolute path to the output directory',
                'type': str,
                'default': None,
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        if self.config['nyx_install_path'] is None:
            print("Error: please provide the path to Nyx installation....")
            exit(1)
        else:
            self.nyx_lya_path = f"{self.config['nyx_install_path']}/LyA"

        if self.config['particle_file'] == "64sssss_20mpc.nyx":
            self.config['particle_file'] = f'{self.nyx_lya_path}/64sssss_20mpc.nyx'
        
        if self.config['output'] is None:
            self.config['output'] = f'{self.nyx_lya_path}/outputs'
            Mkdir(self.config['output'], PsshExecInfo(hostfile=self.hostfile,
                                          env=self.env)).run()

        # copy a template inputs file from NYX installation path to the pkg directory
        self.copy_template_file(f'{self.nyx_lya_path}/inputs', self.inputs_path)
        # modify the inputs file based on user's input
        self._configure_nyx()

    def _configure_nyx(self):

        prefix_mapping = {
            'nyx.write_hdf5': f"",
            'nyx.initial_z': f"nyx.initial_z = {self.config['initial_z']}\n",
            'nyx.final_z': f"nyx.final_z = {self.config['final_z']}\n",
            'amr.plot_file': f"amr.plot_file = {self.config['output']}/plt\n",
            'nyx.plot_z_values': f"nyx.plot_z_values = {self.config['plot_z_values']}\n",
            'nyx.binary_particle_file': f"nyx.binary_particle_file = {self.config['particle_file']}\n",
            'amr.check_file': f"amr.check_file = {self.config['output']}/chk\n",
            'amr.derive_plot_vars': lambda line: f"#{line}",
            'amr.data_log': f"amr.data_log = {self.config['output']}/runlog\n",
            'amr.grid_log': f"amr.grid_log = {self.config['output']}/grdlog\n",
        }

        lines = []
        lines.append("nyx.write_hdf5 = 1\n")
        with open(self.inputs_path, 'r') as file:
            for line in file:
                line_stripped = line.strip()
                action = prefix_mapping.get(line_stripped.split(' ')[0], None)
                
                if action == None:
                    lines.append(line)
                elif callable(action):
                    lines.append(action(line))
                else:
                    lines.append(action)
        
        # rewrite the file
        with open(self.inputs_path, 'w') as file:
            file.writelines(lines)

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        # since "self.nyx_lya_path" is always set to be none in _init(), we need to rest it here
        self.nyx_lya_path = f"{self.config['nyx_install_path']}/LyA"
        Exec(f'{self.nyx_lya_path}/nyx_LyA {self.inputs_path}',
             MpiExecInfo(nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'],
                         hostfile=self.hostfile,
                         env=self.env)).run()

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        output_dir = self.config['output'] + "*"
        print(f'Removing {output_dir}')
        Rm(output_dir).run()
```

### `builtin/builtin/orangefs/__init__.py`

```python

```

### `builtin/builtin/orangefs/ares.py`

```python
from jarvis_cd.shell import Exec, LocalExecInfo
from .custom_kern import OrangefsCustomKern


class OrangefsAres:
    def ares_start(self):
        cmd = [
            f'{self.ofs_path}/sbin/ares-orangefs-deploy',
            self.config['pfs_conf'],
            self.config['server_hosts_path'],
            self.config['client_hosts_path'],
            self.config['mount'],
        ]
        cmd = ' '.join(cmd)
        print(cmd)
        Exec(cmd, LocalExecInfo(env=self.env))

    def ares_stop(self):
        cmd = [
            f'{self.ofs_path}/sbin/ares-orangefs-terminate',
            self.config['pfs_conf'],
            self.config['server_hosts_path'],
            self.config['client_hosts_path'],
            self.config['mount'],
        ]
        cmd = ' '.join(cmd)
        print(cmd)
        Exec(cmd, LocalExecInfo(env=self.env))
```

### `builtin/builtin/orangefs/custom_kern.py`

```python
from jarvis_cd.core.pkg import Color
from jarvis_cd.shell import Exec, SshExecInfo, PsshExecInfo
from jarvis_cd.shell.process import Kill


class OrangefsCustomKern:
    def custom_start(self):
        # start pfs servers
        print("Starting the PFS servers")
        for host in self.server_hosts.list():
            host_ip = host.hosts[0]
            server_start_cmds = [
                # f'pvfs2-server -f -a {host_ip}  {self.config["pfs_conf"]}',
                f'pvfs2-server -a {host_ip} {self.config["pfs_conf"]}'
            ]
            print(server_start_cmds)
            print(f"PVFS2TAB: {self.env['PVFS2TAB_FILE']}")
            Exec(server_start_cmds,
                 SshExecInfo(hostfile=host,
                             env=self.env))
        self.status()

        # insert OFS kernel module
        print("Inserting OrangeFS kernel module")
        Exec('modprobe orangefs', PsshExecInfo(sudo=True,
                                               sudoenv=self.config['sudoenv'],
                                               hosts=self.client_hosts,
                                               env=self.env))

        # PFS client thing
        print("Starting the OrangeFS clients")
        start_client_cmd = f'{self.ofs_path}/sbin/pvfs2-client -p {self.ofs_path}/sbin/pvfs2-client-core -L {self.config["client_log"]}'
        Exec(start_client_cmd,
             PsshExecInfo(hostfile=self.client_hosts,
                          env=self.env,
                          sudo=True,
                          sudoenv=self.config['sudoenv']))

        # start pfs client
        print("Mounting the OrangeFS clients")
        mdm_ip = self.md_hosts.list()[0].hosts[0]
        mount_client = 'mount -t pvfs2 {protocol}://{ip}:{port}/{name} {mount_point}'.format(
            protocol=self.config['protocol'],
            port=self.config['port'],
            ip=mdm_ip,
            name=self.config['name'],
            mount_point=self.config['mount'])
        Exec(mount_client,
             PsshExecInfo(hostfile=self.client_hosts,
                          env=self.env,
                          sudo=True,
                          sudoenv=self.config['sudoenv']))

    def custom_stop(self):
        Exec(f'umount -t pvfs2 {self.config["mount"]}',
             PsshExecInfo(hosts=self.client_hosts,
                          env=self.env,
                          sudo=True,
                          sudoenv=self.config['sudoenv']))
        self.log(f"Unmounting {self.config['mount']} on each client", Color.YELLOW)
        Kill('.*pvfs2-client.*', PsshExecInfo(hosts=self.client_hosts,
                                env=self.env))
        Kill('pvfs2-server',
             PsshExecInfo(hosts=self.server_hosts,
                          env=self.env))
        Exec('pgrep -la pvfs2-server',
             PsshExecInfo(hosts=self.client_hosts,
                          env=self.env))
```

### `builtin/builtin/orangefs/fuse.py`

```python
from jarvis_cd.core.pkg import Service, Color
from jarvis_cd.shell import Exec, SshExecInfo, PsshExecInfo
from jarvis_cd.shell.process import Kill
from jarvis_cd.util.hostfile import Hostfile
import os


class OrangefsFuse:
    def fuse_start(self):
        # start pfs servers 
        for host in self.server_hosts:
            server_start_cmds = [
                # f"pvfs2-server {self.config['pfs_conf']} -f -a {host}",
                f"pvfs2-server {self.config['pfs_conf']} -a {host}"
            ]
            Exec(server_start_cmds,
                 SshExecInfo(hostfile=Hostfile(all_hosts=[host]),
                             env=self.env))
        self.status()

        # start pfs client
        md_list = self.md_hosts.list()
        for i,client in self.client_hosts.enumerate():
            mdm_ip = md_list[i % len(self.md_hosts)].hosts_ip[0]
            start_client_cmds = [
                "pvfs2fuse -o fs_spec={protocol}://{ip}:{port}/{name} {mount_point}".format(
                    protocol=self.config['protocol'],
                    port=self.config['port'],
                    ip=mdm_ip,
                    name=self.config['name'],
                    mount_point=self.config['mount'])
            ]
            Exec(start_client_cmds,
                 SshExecInfo(hostfile=client,
                             env=self.env))

    def fuse_stop(self):
        cmds = [
            f"fusermount -u {self.config['mount']}"
        ]
        Exec(cmds, PsshExecInfo(hosts=self.client_hosts,
                                env=self.env))
        self.log(f"Unmounting {self.config['mount']} on each client", Color.YELLOW)

        Kill('.*pvfs2-client.*', PsshExecInfo(hosts=self.client_hosts,
                                        env=self.env))
        Kill('pvfs2-server',
             PsshExecInfo(hosts=self.server_hosts,
                          env=self.env))
        Exec("pgrep -la pvfs2-server", PsshExecInfo(hosts=self.client_hosts,
                                env=self.env))
```

### `builtin/builtin/orangefs/pkg.py`

```python
from jarvis_cd.core.pkg import Service, Color
from jarvis_cd.shell import Exec, LocalExecInfo, SshExecInfo, PsshExecInfo, ScpExecInfo, PscpExecInfo
from jarvis_cd.shell.process import Mkdir, Rm, Pscp
from jarvis_cd.util.hostfile import Hostfile
from .custom_kern import OrangefsCustomKern
from .ares import OrangefsAres
from .fuse import OrangefsFuse
import os
import time


class Orangefs(Service, OrangefsCustomKern, OrangefsAres, OrangefsFuse):
    def _init(self):
        """
        Initialize paths
        """

    def _configure_menu(self):
        return [
            {
                'name': 'port',
                'msg': 'The port to listen for data on',
                'type': int,
                'default': 3334
            },
            {
                'name': 'ofs_data_dir',
                'msg': 'The mount point to place all OFS data. Must not be a shared system (e.g., another PFS).',
                'type': str,
                'default': None,
            },
            {
                'name': 'stripe_size',
                'msg': 'The stripe size',
                'type': int,
                'default': 65536,
            },
            {
                'name': 'stripe_dist',
                'msg': 'The striping distribution algorithm',
                'type': str,
                'default': 'simple_stripe',
            },
            {
                'name': 'protocol',
                'msg': 'The network protocol (tcp/ib)',
                'type': str,
                'default': 'tcp',
                'choices': ['tcp', 'ib']
            },
            {
                'name': 'mount',
                'msg': 'Where to mount orangefs clients',
                'type': str,
                'default': None,
            },
            {
                'name': 'name',
                'msg': 'The name of the orangefs installation',
                'type': str,
                'default': 'orangefs',
            },
            {
                'name': 'sudoenv',
                'msg': 'Whether environment forwarding is supported for sudo',
                'type': bool,
                'default': True,
            },
            {
                'name': 'ofs_mode',
                'msg': 'Whether we are using the orangefs on Ares',
                'type': bool,
                'choices': ['fuse', 'ares', 'kern'],
                'default': 'ares',
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        rg = self.jarvis.resource_graph
        if self.config['ofs_mode'] != 'kern':
            self.config['sudoenv'] = False

        # Configure and save hosts
        self.client_hosts = self.hostfile
        self.server_hosts = self.hostfile
        self.md_hosts = self.hostfile
        self.config['client_host_set'] = self.client_hosts.hosts
        self.config['server_host_set'] = self.server_hosts.hosts
        self.config['md_host_set'] = self.md_hosts.hosts
        self.config['client_hosts_path'] = f'{self.private_dir}/client_hosts'
        self.config['server_hosts_path'] = f'{self.private_dir}/server_hosts'
        self.config['metadata_hosts_path'] = f'{self.private_dir}/metadata_hosts'
        self.client_hosts.save(self.config['client_hosts_path'])
        self.server_hosts.save(self.config['server_hosts_path'])
        self.md_hosts.save(self.config['metadata_hosts_path'])
        Pscp([self.config['client_hosts_path'],
              self.config['server_hosts_path'],
              self.config['metadata_hosts_path']],
             PsshExecInfo(hosts=self.hostfile, env=self.env)).run()
        self.log('Distributed client, server, and metadata hostfiles', Color.YELLOW)

        # Locate storage hardware
        storage_dir = self.config['ofs_data_dir']

        # Define paths
        self.config['pfs_conf'] = f'{self.private_dir}/orangefs.xml'
        self.config['pvfs2tab'] = f'{self.private_dir}/pvfs2tab'
        if self.config['mount'] is None:
            self.config['mount'] = f'{self.private_dir}/client'
        self.config['storage'] = f'{storage_dir}/orangefs_storage'
        self.config['metadata'] = f'{storage_dir}/orangefs_metadata'
        self.config['log'] = f'{self.private_dir}/orangefs_server.log'
        self.config['client_log'] = f'{self.private_dir}/orangefs_client.log'

        # generate PFS Gen config
        if self.config['protocol'] == 'tcp':
            proto_cmd = f'--tcpport {self.config["port"]}'
        elif self.config['protocol'] == 'ib':
            proto_cmd = f'--ibport {self.config["port"]}'
        else:
            raise Exception("Protocol must be either tcp or ib")
        pvfs_gen_cmd = [
            'pvfs2-genconfig',
            '--quiet',
            f'--protocol {self.config["protocol"]}',
            proto_cmd,
            f'--dist-name {self.config["stripe_dist"]}',
            f'--dist-params \"strip_size: {self.config["stripe_size"]}\"',
            f'--ioservers {self.server_hosts.host_str(sep=",")}',
            f'--metaservers {self.md_hosts.host_str(sep=",")}',
            f'--storage {self.config["storage"]}',
            f'--metadata {self.config["metadata"]}',
            f'--logfile {self.config["log"]}',
            f'--fsname {self.config["name"]}',
            self.config['pfs_conf']
        ]
        pvfs_gen_cmd = " ".join(pvfs_gen_cmd)
        Exec(pvfs_gen_cmd, LocalExecInfo(env=self.env)).run()
        Pscp(self.config['pfs_conf'],
             PsshExecInfo(hosts=self.hostfile, env=self.env)).run()
        self.log(f"Generated pvfs2 config: {self.config['pfs_conf']}", Color.YELLOW)

        # Create storage directories
        Mkdir(self.config['mount'], PsshExecInfo(hosts=self.client_hosts,
                                                 env=self.env)).run()
        Mkdir(self.config['storage'], PsshExecInfo(hosts=self.server_hosts,
                                                   env=self.env)).run()
        Mkdir(self.config['metadata'], PsshExecInfo(hosts=self.md_hosts,
                                                    env=self.env)).run()
        self.log(f"Create mount, metadata and storage directories", Color.YELLOW)
        self.log(f"Mount at: {self.config['mount']}", Color.YELLOW)

        # Set pvfstab on clients
        mdm_ip = self.md_hosts.list()[0].hosts[0]
        with open(self.config['pvfs2tab'], 'w', encoding='utf-8') as fp:
            fp.write(
                '{protocol}://{ip}:{port}/{name} {mount_point} pvfs2 defaults,auto 0 0\n'.format(
                    protocol=self.config['protocol'],
                    port=self.config['port'],
                    ip=mdm_ip,
                    name=self.config['name'],
                    mount_point=self.config['mount'],
                    client_pvfs2tab=self.config['pvfs2tab']))
        Pscp(self.config['pvfs2tab'],
             PsshExecInfo(hosts=self.hostfile,
                          env=self.env)).run()
        self.env['PVFS2TAB_FILE'] = self.config['pvfs2tab']
        self.log(f"Create PVFS2TAB_FILE: {self.config['pvfs2tab']}", Color.YELLOW)

        for host in self.server_hosts.list():
            host_ip = host.hosts[0]
            server_start_cmds = [
                f'pvfs2-server -f -a {host_ip}  {self.config["pfs_conf"]}'
            ]
            self.log(server_start_cmds, Color.YELLOW)
            Exec(server_start_cmds,
                 SshExecInfo(hostfile=host,
                             env=self.env)).run()

    def _load_config(self):
        if 'sudoenv' not in self.config:
            self.config['sudoenv'] = True
        self.client_hosts = Hostfile(all_hosts=self.config['client_host_set'])
        self.server_hosts = Hostfile(all_hosts=self.config['server_host_set'])
        self.md_hosts = Hostfile(all_hosts=self.config['md_host_set'])
        self.ofs_path = self.env['ORANGEFS_PATH']

    def start(self):
        self._load_config() 
        if self.config['ofs_mode'] == 'ares':
            self.ares_start()
        elif self.config['ofs_mode'] == 'fuse':
            self.fuse_start()
        else:
            self.custom_start()

    def stop(self):
        self._load_config()
        if self.config['ofs_mode'] == 'ares':
            self.ares_stop()
        elif self.config['ofs_mode'] == 'fuse':
            self.fuse_stop()
        else:
            self.custom_stop()

    def clean(self):
        self._load_config()
        Rm([self.config['mount'], self.config['client_log']],
           PsshExecInfo(hosts=self.client_hosts,
                        env=self.env)).run()
        Rm([self.config['storage'], self.config['log']],
           PsshExecInfo(hosts=self.server_hosts,
                        env=self.env)).run()
        Rm(self.config['metadata'],
           PsshExecInfo(hosts=self.md_hosts,
                        env=self.env)).run()

    def status(self):
        self._load_config()
        Exec('mount | grep pvfs',
             PsshExecInfo(hosts=self.server_hosts,
                          env=self.env)).run()
        verify_server_cmd = [
            f'pvfs2-ping -m {self.config["mount"]} | grep \"appears to be correctly configured\"'
        ]
        Exec(verify_server_cmd,
             PsshExecInfo(hosts=self.client_hosts,
                          env=self.env)).run()
        return True
```

### `builtin/builtin/paraview/INSTALL.MD`

```markdown
## Installation

```bash
spack install paraview
```
```

### `builtin/builtin/paraview/USE.MD`

```markdown
## how to use it with Jarvis in cluster

### set the environment
load paraview and openmpi
```
module load paraview
spack load openmpi

```
### create jarvis pipeline
```
jarvis ppl create paraview
jarvis ppl env build
jarvis ppl append paraview port_id=11111
```
Note: If the connect has issues, try change the port_id first, for example change the port_id to 11112. 
### run the application
```
jarvis ppl run
```

## How to use it in local computer 
###
Run this command in local terminal:
```
ssh -N -L 11111:localhost:11111 your_id@ares.cs.iit.edu
```
In local paraview, following these instructions:
File -> connect </br>
![Simulation Diagram](server_setup.png)

Once coonect the server, The remote paraview are good to go.
```

### `builtin/builtin/paraview/pkg.py`

```python
"""
This module provides classes and methods to launch the Paraview application.
Paraview is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, MpiExecInfo


class Paraview(Application):
    """
    This class provides methods to launch the Paraview application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'nprocs',
                'msg': 'Number of processes',
                'type': int,
                'default': 1,
            },
            {
                'name': 'ppn',
                'msg': 'The number of processes per node',
                'type': int,
                'default': 16,
            },
            {
                'name': 'time_out',
                'msg': 'Set a timeout period for idle client sessions',
                'type': int,
                'default': 10000,
            },
            {
                'name': 'force_offscreen_rendering',
                'msg': 'Useful for headless environments (no display)',
                'type': bool,
                'default': False,
            },
            {
                'name': 'port_id',
                'msg': 'Set the port the server listens on',
                'type': int,
                'default': 11111,
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """

        pass

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        port_Id = self.config["port_id"]
        time_out = self.config["time_out"]
        condition = ''
        if self.config['force_offscreen_rendering']:
            condition += ' --force-offscreen-rendering'

        Exec(f'pvserver --server-port={port_Id} --timeout={time_out}{condition}',
             MpiExecInfo(nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'],
                         env=self.mod_env
                        )).run()

        pass

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass
    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        pass
```

### `builtin/builtin/post_wrf/config/adios2.xml`

```xml
<?xml version="1.0"?>
<adios-config>

  <!-- <io name="/Path/to/wrfout_d01_2018-06-17_00:00:00"> -->
  <io name="##wrfout_d01_2019-11-26_12:00:00##">


    <engine type="##EngineType##">
      <parameter key="RendezvousReaderCount" value="0"/>
      <parameter key="QueueLimit" value="1"/>
      <parameter key="QueueFullPolicy" value="Discard"/>
      <parameter key="OpenTimeoutSecs" value="30.0"/>
    </engine>


  </io>
</adios-config>
```

### `builtin/builtin/post_wrf/config/plot.py`

```python
import argparse
import adios2                               # pylint: disable=import-error
import numpy as np                          # pylint: disable=import-error
import matplotlib.pyplot as plt             # pylint: disable=import-error
import matplotlib.gridspec as gridspec      # pylint: disable=import-error
from mpi4py import MPI                      # pylint: disable=import-error
import cartopy.crs as ccrs                  # pylint: disable=import-error
import cartopy.feature as cfeature          # pylint: disable=import-error
from mpl_toolkits.axes_grid1 import make_axes_locatable # pylint: disable=import-error
#
#
def setup_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--instream", "-i", help="Name of the input stream", default="wrfout_d01_2019-11-26_23:00:00")
    parser.add_argument("--outfile", "-o", help="Name of the output file", default="screen")
    parser.add_argument("--varname", "-v", help="Name of variable read", default="T2")
    args = parser.parse_args()
    return args

def plot_var(var, fr_step):

    lccproj = ccrs.LambertConformal(central_longitude=-74.5, central_latitude=38.8)
    fig, ax = plt.subplots(figsize=(15, 18), subplot_kw=dict(projection=lccproj))
    plt.subplots_adjust(right=0.88)  # adjust the right margin of the plot
    title = fr_step.read_string("Times")
    plt.title("WRF-ADIOS2 Demo \n {}".format(title[0]), fontsize=17)

     # format the spacing of the colorbar
    divider = make_axes_locatable(ax)
    cax = divider.new_horizontal(size='5%', pad=0.1, axes_class=plt.Axes)
    fig.add_axes(cax)

    displaysec = 0.5
    cur_step = fr_step.current_step()
    x = fr_step.read("XLONG")
    y = fr_step.read("XLAT")
    data = fr_step.read(var)
    print(data)
    data = data * 9 / 5 - 459.67 #convert from K to F

    # define the limits for the model to subset and plot
    # model_lims = dict(minlon=-80, maxlon=-69, minlat=35, maxlat=43)

    # # create boolean indices where lat/lon are within defined boundaries
    # lon_ind = np.logical_and(x > model_lims['minlon'], x < model_lims['maxlon'])
    # lat_ind = np.logical_and(y > model_lims['minlat'], y < model_lims['maxlat'])
    # # find i and j indices of lon/lat in boundaries
    # ind = np.where(np.logical_and(lon_ind, lat_ind))

    # data = np.squeeze(data)[range(np.min(ind[0]), np.max(ind[0]) + 1),
    #                 range(np.min(ind[1]), np.max(ind[1]) + 1)]

    h = ax.pcolormesh(x, y, data, vmin=-20, vmax=110,
                      cmap='jet', transform=ccrs.PlateCarree())

    cb = plt.colorbar(h, cax=cax)
    cb.set_label(label="Temperature [F]", fontsize=14)  # add the label on the colorbar
    cb.ax.tick_params(labelsize=12)  # format the size of the tick labels

    # add contours
    contour_list = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # define contour levels
    cs = ax.contour(x, y, data, contour_list, colors='black',
                    linewidths=.5, transform=ccrs.PlateCarree())
    ax.clabel(cs, inline=True, fontsize=10.5, fmt='%d')

    # add the latitude and longitude gridlines
    gl = ax.gridlines(draw_labels=True, linewidth=1, color='gray', alpha=0.5,
                      linestyle='dotted', x_inline=False)
    gl.top_labels = False
    gl.right_labels = False
    gl.xlabel_style = {'size': 13}
    gl.ylabel_style = {'size': 13}

     # add map features
    land = cfeature.NaturalEarthFeature('physical', 'land', '10m')
    ax.add_feature(land, zorder=5, edgecolor='black', facecolor='none')

    state_lines = cfeature.NaturalEarthFeature(
        category='cultural',
        name='admin_1_states_provinces_lines',
        scale='10m',
        facecolor='none')

    ax.add_feature(cfeature.BORDERS, zorder=6)
    ax.add_feature(state_lines, zorder=7, edgecolor='black')

    #plt.title(title)

    # plt.ion()
    #plt.show()
    # plt.pause(displaysec)
    # #clear_output()
    # plt.clf()

    imgfile = "image"+"{0:0>5}.png".format(cur_step)
    plt.savefig(imgfile)
    plt.clf()

if __name__ == "__main__":
    args = setup_args()
    fr = adios2.open(args.instream, "r", MPI.COMM_WORLD, "adios2.xml", "wrfout_d01_2019-11-26_23:00:00")

    for fr_step in fr:
        plot_var(args.varname, fr_step)

    fr.close()
```

### `builtin/builtin/post_wrf/pkg.py`

```python
"""
This module provides classes and methods to launch the PostWrf application.
PostWrf is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, MpiExecInfo


class PostWrf(Application):
    """
    This class provides methods to launch the PostWrf application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'nprocs',
                'msg': 'Number of processes',
                'type': int,
                'default': 1,
            },
            {
                'name': 'ppn',
                'msg': 'The number of processes per node',
                'type': int,
                'default': None,
            },

            {
                'name': 'wrf_output',
                'msg': 'The location of output file',
                'type': str,
                'default': None,
            },
            {
                'name': 'engine',
                'msg': 'The engine type for adios2, such as BP4, BP5, SST',
                'type': str,
                'default': 'BP4',
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        output_location = self.config['wrf_output']
        if output_location[-1] != '/':
            output_location += '/'
        output_location += 'wrfout_d01_2019-11-26_12:00:00'
        replacement = [("wrfout_d01_2019-11-26_12:00:00", output_location), ("EngineType", self.config['engine'])]
        self.copy_template_file(f'{self.pkg_dir}/config/adios2.xml',
                                f'{self.config["wrf_output"]}/adios2.xml', replacement)

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        Exec('python3 ./plot.py ',
             MpiExecInfo(nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'],
                         hostfile=self.hostfile,
                         env=self.mod_env,
                         cwd=self.config['wrf_output'])).run()
        pass

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        pass
```

### `builtin/builtin/pyflextrkr/example_config/run_mcs_tb_summer_sam_template.yml`

```yaml
---
# DYAMOND MCS tracking configuration file
# Tracking uses collocated Tb + Precipitation

# Processing steps:
run_preprocess : False
run_idfeature : True
run_tracksingle : True
run_gettracks : True
run_trackstats : True
run_identifymcs : True
run_matchpf : True
run_robustmcs : True
run_mapfeature : True
run_speed : True

# Parallel processing set up
# run_parallel: 1 (local cluster), 2 (Dask MPI)
run_parallel: 1
nprocesses : 12  # Number of processors to use if run_parallel=1
# dask_tmp_dir: '/tmp'  # Dask temporary directory if run_parallel=1
dask_tmp_dir: '/tmp/pyflextrkr_test'  # Dask temporary directory if run_parallel=1
timeout: 360  # [seconds] Dask timeout limit

# Start/end date and time
startdate: '20160801.0000'
# enddate: '20160801.0500' # 6 files ~204MB , tested smallest for 9 stages
# enddate: '20160801.1100' # 12 files ~ 408MB
enddate: '20160801.2300' # 24 files ~816MB
# enddate: '20160802.1100' # 36 files ~ 884MB
# enddate: '20160802.2300' # 48 files ~ 1.6GB
# enddate: '20160803.2300' # 72 files ~ 2.4GB
# enddate: '20160804.2300' # 96 files ~ 3GB
# enddate: '20160806.2300' # 142 files ~ 4.7GB
# enddate: '20160809.2300' # 216 files ~ 7.2GB
# enddate: '20160810.2300' # 240 files ~ 8GB
# enddate: '20160910.0000' # 960 files ~ 32GB

# Specify tracking input data date/time string format
# This is the preprocessed file that contains Tb & rainrate
# E.g., databasename20181101.011503.nc --> yyyymodd.hhmmss
# E.g., databasename2018-11-01_01:15:00 --> yyyy-mo-dd_hh:mm:ss
time_format: 'yyyymoddhh'
databasename:  'pr_rlut_mean_sam_'

# Input files directory
clouddata_path: 'INPUT_DIR/'
# Working directory for the tracking data
root_path: 'TRACK_DIR/'
# Working sub-directory names
tracking_path_name: 'tracking'
stats_path_name: 'stats'
pixel_path_name: 'mcstracking'

# Land mask file
landmask_filename: 'INPUT_DIR/IMERG_landmask_180W-180E_60S-60N.nc'
# landmask_filename: 'INPUT_DIR/IMERG_landmask_0-360_60S-60N.nc'
landmask_varname: 'landseamask'
landmask_x_dimname: 'lon'
landmask_y_dimname: 'lat'
landmask_x_coordname: 'lon'
landmask_y_coordname: 'lat'
landfrac_thresh: [0, 90]  # Define the range of fraction for land (depends on what value is land the landmask file)

# Input dataset structure
pixel_radius:  10.0  # [km] Spatial resolution of the input data
datatimeresolution: 1.0  # [hour] Temporal resolution of the input data
# Variable names in the input data
olr2tb: True
olr_varname: 'LWNTA'
pcp_varname: 'Precac'
clouddatasource: 'model'
time_dimname: 'time'
x_dimname: 'lon'
y_dimname: 'lat'
time_coordname: 'time'
x_coordname: 'lon'
y_coordname: 'lat'

# Specify types of feature being tracked
# This adds additional feature-specific statistics to be computed
feature_type: 'tb_pf'

# Cloud identification parameters
mincoldcorepix:  4  # Minimum number of pixels for the cold core
smoothwindowdimensions:  10  # Dimension of the Box2DKernel filter on Tb.

# # set geolimits change to smaller region to reduce computation time
# medfiltsize: 5      # Window size to perform medfilt2d to fill missing Tb pixels, must be an odd number
# geolimits: [-60, -360, 60, 360] # [lat_min, lon_min, lat_max, lon_max] 4-element array to subset domain boundaries # RG1: global
# geolimits: [-15, 60, 30, 180] # RG2: medium size Pacific Ocean region
geloimits: [-15, 90, 15, 150] # RG3: smallest size Souch China Sea region


area_thresh:  800  # [km^2] Minimum area to define a cloud
miss_thresh:  0.4  # Missing data fraction threshold. If missing data exceeds this, the time frame will be omitted.
cloudtb_core:  225.0  # [K]
cloudtb_cold:  241.0  # [K]
cloudtb_warm:  261.0  # [K]
cloudtb_cloud:  261.0  # [K]
absolutetb_threshs: [160, 330]  # K [min, max] absolute Tb range allowed.
warmanvilexpansion:  0  # Not working yet, set this to 0 for now
cloudidmethod: 'label_grow'
# Specific parameters to link cloud objects using PF
linkpf:  1  # Set to 1 to turn on linkpf option; default: 0
pf_smooth_window:  5  # Smoothing window for identifying PF
pf_dbz_thresh:  3  # [dBZ] for reflectivity, or [mm/h] for rainrate
pf_link_area_thresh:  648.0  # [km^2]

# Tracking parameters
othresh: 0.5  # overlap fraction threshold. Clouds that overlap more than this between times are tracked.
timegap: 3.1  # [hour] If missing data duration longer than this, tracking restarts
nmaxlinks: 50  # Maximum number of clouds that any single cloud can be linked to
maxnclouds:  3000  # Maximum number of clouds in one snapshot
duration_range: [2, 400] # A vector [minlength,maxlength] to specify the duration range for the tracks
# Flag to remove short-lived tracks [< min(duration_range)] that are not mergers/splits with other tracks
# 0:keep all tracks; 1:remove short tracks
remove_shorttracks: 1
# Set this flag to 1 to write a dense (2D) trackstats netCDF file
# Note that for datasets with lots of tracks, the memory consumption could be large
trackstats_dense_netcdf: 1
# Minimum time difference threshold to match track stats with cloudid files
match_pixel_dt_thresh: 60.0  # seconds

# MCS Tb parameters
mcs_tb_area_thresh: 40000  # [km^2] Tb area threshold
mcs_tb_duration_thresh:  4  # [hour] Tb minimum length of a mcs
mcs_tb_split_duration:  12  # [hour] Tb tracks smaller or equal to this length will be included with the MCS splits from
mcs_tb_merge_duration:  12  # [hour] Tb tracks smaller or equal to this length will be included with the MCS merges into
mcs_tb_gap: 1  # [unitless] Allowable temporal gap in Tb data for MCS area threshold
# MCS PF parameters
mcs_pf_majoraxis_thresh:  100  # [km] MCS PF major axis length lower limit
max_pf_majoraxis_thresh:  1800  # [km] MCS PF major axis length upper limit
mcs_pf_durationthresh:  4  # [hour] PF minimum length of mcs
mcs_pf_majoraxis_for_lifetime:  20  # [km] Minimum PF size to count PF lifetime
mcs_pf_gap:  1  # [unitless] Allowable temporal gap in PF data for MCS characteristics

# Specify rain rate parameters
pf_rr_thres:  2.0  # [mm/hr] Rain rate threshold
nmaxpf: 3  # Maximum number of precipitation features that can be within a cloud feature
nmaxcore: 20  # Maximum number of convective cores that can be within a cloud feature
pcp_thresh:  1.0  # Pixels with hourly precipitation larger than this will be labeled with track number
heavy_rainrate_thresh:  10.0  # [mm/hr] Heavy rain rate threshold
mcs_min_rainvol_thresh: 20000   #  [km^2 mm/h] Min rain volumne threshold #TODO
mcs_volrain_duration_thresh: 1.0   # [hour] Min volume rain threshold #TODO


# Define tracked feature variable names
feature_varname: 'feature_number'
nfeature_varname: 'nfeatures'
featuresize_varname: 'npix_feature'

# Track statistics output file dimension names
tracks_dimname: 'tracks'
times_dimname: 'times'
pf_dimname: 'nmaxpf'
fillval: -9999
# MCS track stats file base names
mcstbstats_filebase: 'mcs_tracks_'
mcspfstats_filebase: 'mcs_tracks_pf_'
mcsrobust_filebase: 'mcs_tracks_robust_'
pixeltracking_filebase: 'mcstrack_'
mcsfinal_filebase: 'mcs_tracks_final_'

# Feature movement speed parameters
lag_for_speed: 1  # lag intervals between tracked features to calculate movement
track_number_for_speed: "pcptracknumber"
track_field_for_speed: 'precipitation'
min_size_thresh_for_speed: 20 # [km] Min PF major axis length to calculate movement #TODO
max_speed_thresh: 50  # [m/s] #TODO
```

### `builtin/builtin/pyflextrkr/example_config/run_mcs_tbpf_saag_summer_sam_template.yml`

```yaml
---
# DYAMOND MCS tracking configuration file
# Tracking uses collocated Tb + Precipitation

# Processing steps:
run_preprocess : False
run_idfeature : True
run_tracksingle : True
run_gettracks : True
run_trackstats : True
run_identifymcs : True
run_matchpf : True
run_robustmcs : True
run_mapfeature : True
run_speed : True

# Parallel processing set up
# run_parallel: 1 (local cluster), 2 (Dask MPI)
run_parallel: 1
nprocesses : 12  # Number of processors to use if run_parallel=1
# dask_tmp_dir: '/tmp'  # Dask temporary directory if run_parallel=1
dask_tmp_dir: '/tmp/pyflextrkr_test'  # Dask temporary directory if run_parallel=1
timeout: 360  # [seconds] Dask timeout limit

# Start/end date and time
startdate: '20160801.0000'
# enddate: '20160801.0500' # 6 files ~204MB , tested smallest for 9 stages
# enddate: '20160801.1100' # 12 files ~ 408MB
enddate: '20160801.2300' # 24 files ~816MB
# enddate: '20160802.1100' # 36 files ~ 884MB
# enddate: '20160802.2300' # 48 files ~ 1.6GB
# enddate: '20160803.2300' # 72 files ~ 2.4GB
# enddate: '20160804.2300' # 96 files ~ 3GB
# enddate: '20160806.2300' # 142 files ~ 4.7GB
# enddate: '20160809.2300' # 216 files ~ 7.2GB
# enddate: '20160810.2300' # 240 files ~ 8GB
# enddate: '20160910.0000' # 960 files ~ 32GB

# Specify tracking input data date/time string format
# This is the preprocessed file that contains Tb & rainrate
# E.g., databasename20181101.011503.nc --> yyyymodd.hhmmss
# E.g., databasename2018-11-01_01:15:00 --> yyyy-mo-dd_hh:mm:ss
time_format: 'yyyymoddhh'
databasename:  'pr_rlut_mean_sam_'

# Input files directory
clouddata_path: 'INPUT_DIR/'
# Working directory for the tracking data
root_path: 'OUTPUT_DIR/'
# Working sub-directory names
tracking_path_name: 'tracking'
stats_path_name: 'stats'
pixel_path_name: 'mcstracking'

# Land mask file
landmask_filename: 'INPUT_DIR/IMERG_landmask_180W-180E_60S-60N.nc'
# landmask_filename: 'INPUT_DIR/IMERG_landmask_0-360_60S-60N.nc'
landmask_varname: 'landseamask'
landmask_x_dimname: 'lon'
landmask_y_dimname: 'lat'
landmask_x_coordname: 'lon'
landmask_y_coordname: 'lat'
landfrac_thresh: [0, 90]  # Define the range of fraction for land (depends on what value is land the landmask file)

# Input dataset structure
pixel_radius:  10.0  # [km] Spatial resolution of the input data
datatimeresolution: 1.0  # [hour] Temporal resolution of the input data
# Variable names in the input data
olr2tb: True
olr_varname: 'LWNTA'
pcp_varname: 'Precac'
clouddatasource: 'model'
time_dimname: 'time'
x_dimname: 'lon'
y_dimname: 'lat'
time_coordname: 'time'
x_coordname: 'lon'
y_coordname: 'lat'

# Specify types of feature being tracked
# This adds additional feature-specific statistics to be computed
feature_type: 'tb_pf'

# Cloud identification parameters
mincoldcorepix:  4  # Minimum number of pixels for the cold core
smoothwindowdimensions:  10  # Dimension of the Box2DKernel filter on Tb.

# # set geolimits change to smaller region to reduce computation time
# medfiltsize: 5      # Window size to perform medfilt2d to fill missing Tb pixels, must be an odd number
# geolimits: [-60, -360, 60, 360] # [lat_min, lon_min, lat_max, lon_max] 4-element array to subset domain boundaries # RG1: global
# geolimits: [-15, 60, 30, 180] # RG2: medium size Pacific Ocean region
geloimits: [-15, 90, 15, 150] # RG3: smallest size Souch China Sea region


area_thresh:  800  # [km^2] Minimum area to define a cloud
miss_thresh:  0.4  # Missing data fraction threshold. If missing data exceeds this, the time frame will be omitted.
cloudtb_core:  225.0  # [K]
cloudtb_cold:  241.0  # [K]
cloudtb_warm:  261.0  # [K]
cloudtb_cloud:  261.0  # [K]
absolutetb_threshs: [160, 330]  # K [min, max] absolute Tb range allowed.
warmanvilexpansion:  0  # Not working yet, set this to 0 for now
cloudidmethod: 'label_grow'
# Specific parameters to link cloud objects using PF
linkpf:  1  # Set to 1 to turn on linkpf option; default: 0
pf_smooth_window:  5  # Smoothing window for identifying PF
pf_dbz_thresh:  3  # [dBZ] for reflectivity, or [mm/h] for rainrate
pf_link_area_thresh:  648.0  # [km^2]

# Tracking parameters
othresh: 0.5  # overlap fraction threshold. Clouds that overlap more than this between times are tracked.
timegap: 3.1  # [hour] If missing data duration longer than this, tracking restarts
nmaxlinks: 50  # Maximum number of clouds that any single cloud can be linked to
maxnclouds:  3000  # Maximum number of clouds in one snapshot
duration_range: [2, 400] # A vector [minlength,maxlength] to specify the duration range for the tracks
# Flag to remove short-lived tracks [< min(duration_range)] that are not mergers/splits with other tracks
# 0:keep all tracks; 1:remove short tracks
remove_shorttracks: 1
# Set this flag to 1 to write a dense (2D) trackstats netCDF file
# Note that for datasets with lots of tracks, the memory consumption could be large
trackstats_dense_netcdf: 1
# Minimum time difference threshold to match track stats with cloudid files
match_pixel_dt_thresh: 60.0  # seconds

# MCS Tb parameters
mcs_tb_area_thresh: 40000  # [km^2] Tb area threshold
mcs_tb_duration_thresh:  4  # [hour] Tb minimum length of a mcs
mcs_tb_split_duration:  12  # [hour] Tb tracks smaller or equal to this length will be included with the MCS splits from
mcs_tb_merge_duration:  12  # [hour] Tb tracks smaller or equal to this length will be included with the MCS merges into
mcs_tb_gap: 1  # [unitless] Allowable temporal gap in Tb data for MCS area threshold
# MCS PF parameters
mcs_pf_majoraxis_thresh:  100  # [km] MCS PF major axis length lower limit
max_pf_majoraxis_thresh:  1800  # [km] MCS PF major axis length upper limit
mcs_pf_durationthresh:  4  # [hour] PF minimum length of mcs
mcs_pf_majoraxis_for_lifetime:  20  # [km] Minimum PF size to count PF lifetime
mcs_pf_gap:  1  # [unitless] Allowable temporal gap in PF data for MCS characteristics

# Specify rain rate parameters
pf_rr_thres:  2.0  # [mm/hr] Rain rate threshold
nmaxpf: 3  # Maximum number of precipitation features that can be within a cloud feature
nmaxcore: 20  # Maximum number of convective cores that can be within a cloud feature
pcp_thresh:  1.0  # Pixels with hourly precipitation larger than this will be labeled with track number
heavy_rainrate_thresh:  10.0  # [mm/hr] Heavy rain rate threshold
mcs_min_rainvol_thresh: 20000   #  [km^2 mm/h] Min rain volumne threshold #TODO
mcs_volrain_duration_thresh: 1.0   # [hour] Min volume rain threshold #TODO


# # Candice Added
# mcs_lifecycle_thresh: 8  # MCS Tb duration [hour]

# # MCS PF parameter coefficients [intercept, slope]
# # These parameters are derived with pf_rr_thres:  2 mm/h
# # Lower percentile means lower thresholds
# coefs_pf_area:  [1962, 0]           # 1% [changed slope to 0: independent of lifetime]
# # coefs_pf_area:  [2119.02, 61.143]      # 3%
# # coefs_pf_area:  [2874.05, 89.825]  # 5% [recommended]
# # coefs_pf_area:  [4160.82, 93.077]      # 7%
# # coefs_pf_area:  [4988.15, 138.172]      # 10%

# coefs_pf_rr:  [2.72873, 0.0]            # 1%
# # coefs_pf_rr:  [2.81982, 0.0135463]      # 3%
# # coefs_pf_rr:  [3.01657, 0.0144461]  # 5% [recommended]
# # coefs_pf_rr:  [3.14895, 0.0150174]      # 7%
# # coefs_pf_rr:  [3.34859, 0.0172043]      # 10%

# coefs_pf_skew:  [0.036384, 0]            # 1%
# # coefs_pf_skew:  [0.072809, 0.0104444]     # 3%
# # coefs_pf_skew:  [0.194462, 0.0100072]  # 5% [recommended]
# # coefs_pf_skew:  [0.256639, 0.0106527]      # 7%
# # coefs_pf_skew:  [0.376142, 0.0095545]      # 10%

# coefs_pf_heavyratio:  [0.750260, 0.4133300]  # 5%
# # coefs_pf_heavyratio:  [3.419024, 0.4387090]  # 10% [recommended]
# # coefs_pf_heavyratio:  [4.753215, 0.4886454]  # 15%
# # coefs_pf_heavyratio:  [4.592209, 0.6107371]  # 20%
# # coefs_pf_heavyratio:  [8.389616, 0.5079337]  # 25%


# Define tracked feature variable names
feature_varname: 'feature_number'
nfeature_varname: 'nfeatures'
featuresize_varname: 'npix_feature'

# Track statistics output file dimension names
tracks_dimname: 'tracks'
times_dimname: 'times'
pf_dimname: 'nmaxpf'
fillval: -9999
# MCS track stats file base names
mcstbstats_filebase: 'mcs_tracks_'
mcspfstats_filebase: 'mcs_tracks_pf_'
mcsrobust_filebase: 'mcs_tracks_robust_'
pixeltracking_filebase: 'mcstrack_'
mcsfinal_filebase: 'mcs_tracks_final_'

# Feature movement speed parameters
lag_for_speed: 1  # lag intervals between tracked features to calculate movement
track_number_for_speed: "pcptracknumber"
track_field_for_speed: 'precipitation'
min_size_thresh_for_speed: 20 # [km] Min PF major axis length to calculate movement #TODO
max_speed_thresh: 50  # [m/s] #TODO
```

### `builtin/builtin/pyflextrkr/example_config/run_mcs_tbpfradar3d_wrf_template.yml`

```yaml
---
# MCS tracking configuration file
# Tracking uses collocated Tb + Radar3D

# Processing steps:
run_preprocess: False
run_idfeature : True
run_tracksingle : True
run_gettracks : True # not needed
run_trackstats : True
run_identifymcs : True # needed
run_matchpf : True # needed
run_robustmcs : True # needed
run_mapfeature : True # needed
run_speed : True # needed

# Parallel processing set up
# run_parallel: 1 (local cluster), 2 (Dask MPI)
run_parallel: 1
nprocesses : 8  # Number of processors to use if run_parallel=1
dask_tmp_dir: '/tmp/pyflextrkr_test'  # Dask temporary directory if run_parallel=1
timeout: 360  # [seconds] Dask timeout limit

# Start/end date and time
startdate: '20150506.0000'
enddate: '20150506.1600' # 17 files, each ~15-17 MB
# enddate: '20150506.0800' # 9 files, smallest possible to run all processing steps
# enddate: '20150510.0000' # 97 files, total 1.4GB

# Specify tracking input data date/time string format
# This is the preprocessed file that contains Tb & rainrate
# E.g., databasename20181101.011503.nc --> yyyymodd.hhmmss
# E.g., databasename2018-11-01_01:15:00 --> yyyy-mo-dd_hh:mm:ss
time_format: 'yyyy-mo-dd_hh:mm:ss'
regrid_basename: 'wrfout_rainrate_tb_zh_mh_'  # Note: include all strings before the time (including "_", ".")
databasename: 'wrfout_rainrate_tb_zh_mh_'
# Specify vertical height levels to interpolate reflectivity (unit: km ASL)
interp_levels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 
  11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]

# WRF raw data
wrfout_path: ''
wrfout_basename: 'wrfout_d01_'
# Tracking input files directory
clouddata_path: 'INPUT_DIR/'
# Working directory for the tracking data
root_path: 'TRACK_DIR/'
# Working sub-directory names
tracking_path_name: 'tracking'
stats_path_name: 'stats'
pixel_path_name: 'mcstracking'

# Land mask file
landmask_filename: 'INPUT_DIR/wrf_landmask.nc'
landmask_varname: 'LANDMASK'
landmask_x_dimname: 'west_east'
landmask_y_dimname: 'south_north'
landmask_x_coordname: 'XLONG'
landmask_y_coordname: 'XLAT'

# Input dataset structure
pixel_radius:  3.0  # [km] Spatial resolution of the input data
datatimeresolution: 1.0  # [hour] Temporal resolution of the input data
# Variable names in the input data
tb_varname:  'tb'
pcp_varname: 'rainrate'
reflectivity_varname: 'reflectivity'
meltlevel_varname: 'meltinglevelheight'
clouddatasource: 'model'
radardatasource: 'wrf'
pfdatasource:  'wrf'  # flag for landmask convention to calculate PF land fraction
time_dimname: 'time'
x_dimname: 'lon'
y_dimname: 'lat'
z_dimname: 'level'
x_coordname: 'lon2d'
y_coordname: 'lat2d'
z_coordname: 'level'

# Specify types of feature being tracked
# This adds additional feature-specific statistics to be computed
feature_type: 'tb_pf_radar3d'

# SL3D classification parameters
# Background box size to calculate peakedness [km]
background_Box:  12.
# Reflectivity threshold to fill low-level coverage gap [dBZ]
# Missing echo at 3 km ASL with valid echo at 4 km ASL and
# reflectivity > threshold at 4 km will be filled with radar reflectivity at 4 km
ReflThresh_lowlevel_gap:  20.
# Stratiform rain reflectivity threshold at 3 km ASL [dBZ]
strat_EchoThresh_3km:  20.
# Stratiform rain reflectivity threshold below 3 km ASL [dBZ]
strat_EchoThresh_lt3km:  10.0
# Column-mean reflectivitiy peakedness fraction threshold to be convective
col_peakedness_frac:  0.3
# Above melting level reflectivity threshold [dBZ] to be convective
abs_ConvThres_aml:  45.
# 25 dBZ echo-top height threshold [km] to be convective
etop25dBZ_Thresh:  10.0
# Composite reflectivity threshold [dBZ] for neighbor points to be convective
neighbor_CompReflThresh:  25.
# Reflectivity vertical gradient (low - up) threshold [dB] to be updraft
updraft_ReflGradiant_Thresh:  8.0
# Max height [km] to include reflectivity vertical gradient to be updraft
updraft_ReflGradiant_MaxHeight:  7.0
# Composite reflectivity threshold [dBZ] to be updraft
updraft_CompRefl_Thresh:  40.0
# Number of vertical level gaps allowed in calculating echo-top height
echotop_gap: 1
# Height level (ASL) to save 2D reflectivity [km]
dbz_lowlevel_asl: 2.0

# Cloud identification parameters
mincoldcorepix:  4  # Minimum number of pixels for the cold core
smoothwindowdimensions:  30  # Dimension of the Box2DKernel filter on Tb.
medfiltsize: 5      # Window size to perform medfilt2d to fill missing Tb pixels, must be an odd number
geolimits: [-90, -360, 90, 360] # 4-element array to subset domain boundaries [lat_min, lon_min, lat_max, lon_max]
area_thresh:  36  # [km^2] Minimum area to define a cloud
miss_thresh:  0.35  # Missing data fraction threshold. If missing data exceeds this, the time frame will be omitted.
cloudtb_core:  225.0  # [K]
cloudtb_cold:  241.0  # [K]
cloudtb_warm:  261.0  # [K]
cloudtb_cloud:  261.0  # [K]
absolutetb_threshs: [160, 330]  # K [min, max] absolute Tb range allowed.
warmanvilexpansion:  0  # Not working yet, set this to 0 for now
cloudidmethod: 'label_grow'
# Specific parameters to link cloud objects using PF
linkpf:  1  # Set to 1 to turn on linkpf option; default: 0
linkpf_varname: 'reflectivity_comp'   # PF variable name to perform linkpf operation
pf_smooth_window:  10  # Smoothing window for identifying PF
pf_dbz_thresh:  25  # [dBZ] for reflectivity, or [mm/h] for rainrate
pf_link_area_thresh:  300.0  # [km^2]

# Tracking parameters
othresh: 0.5  # overlap fraction threshold. Clouds that overlap more than this between times are tracked.
timegap: 3.1  # [hour] If missing data duration longer than this, tracking restarts
nmaxlinks: 200  # Maximum number of clouds that any single cloud can be linked to
maxnclouds: 2000  # Maximum number of clouds in one snapshot
duration_range: [2, 300] # A vector [minlength,maxlength] to specify the duration range for the tracks
# Flag to remove short-lived tracks [< min(duration_range)] that are not mergers/splits with other tracks
# 0:keep all tracks; 1:remove short tracks
remove_shorttracks: 1
# Set this flag to 1 to write a dense (2D) trackstats netCDF file
# Note that for datasets with lots of tracks, the memory consumption could be large
trackstats_dense_netcdf: 1
# Minimum time difference threshold to match track stats with cloudid files
match_pixel_dt_thresh: 60.0  # seconds

# MCS Tb parameters
mcs_tb_area_thresh: 60000  # [km^2] Tb area threshold
mcs_tb_duration_thresh:  6  # [hour] Tb minimum length of a mcs
mcs_tb_split_duration:  12  # [hour] Tb tracks smaller or equal to this length will be included with the MCS splits from
mcs_tb_merge_duration:  12  # [hour] Tb tracks smaller or equal to this length will be included with the MCS merges into
mcs_tb_gap: 1  # [unitless] Allowable temporal gap in Tb data for MCS area threshold
# MCS PF parameters
mcs_pf_majoraxis_thresh:  100  # [km] MCS PF major axis length lower limit
max_pf_majoraxis_thresh:  1800  # [km] MCS PF major axis length upper limit
mcs_pf_durationthresh:  5  # [hour] PF minimum length of mcs
mcs_pf_majoraxis_for_lifetime:  20  # [km] Minimum PF size to count PF lifetime
mcs_pf_gap:  1  # [unitless] Allowable temporal gap in PF data for MCS characteristics
landfrac_thresh: 90  # [%] Define threshold for PF land fraction
# Specify rain rate parameters
pf_rr_thres:  2.0  # [mm/hr] Rain rate threshold to define a precipitation feature
pcp_thresh:  1.0  # [mm/hr] Rain rate threshold to label pixels with track number
heavy_rainrate_thresh:  10.0  # Heavy rain rate threshold [mm/hr]
nmaxpf: 5  # Maximum number of precipitation features to save their statistics within a cloud feature
#nmaxcore: 5  # Maximum number of convective cores to save their statistics within a cloud feature
mcs_core_min_area: 180  # [km^2] Min area to calculate convective core statistics
dbz_thresh: 10  # [dBZ] Reflectivity threshold to label pixels with track number
mcs_lifecycle_thresh: 8  # MCS Tb duration [hour]

# Define tracked feature variable names
feature_varname: 'feature_number'
nfeature_varname: 'nfeatures'
featuresize_varname: 'npix_feature'

# Track statistics output file dimension names
tracks_dimname: 'tracks'
times_dimname: 'times'
pf_dimname: 'nmaxpf'
fillval: -9999
# MCS track stats file base names
mcstbstats_filebase: 'mcs_tracks_'
mcspfstats_filebase: 'mcs_tracks_pf_'
mcsrobust_filebase: 'mcs_tracks_robust_'
pixeltracking_filebase: 'mcstrack_'
mcsfinal_filebase: 'mcs_tracks_final_'

# Feature movement speed parameters
lag_for_speed: 1  # [unitless] lag intervals between tracked features to calculate movement
track_number_for_speed: "dbztracknumber"
track_field_for_speed: 'reflectivity_comp'
min_size_thresh_for_speed: 20 # [km] Min PF major axis length to calculate movement
max_speed_thresh: 50  # [m/s] Speeds larger than this will be replaced by temporal filter
```

### `builtin/builtin/pyflextrkr/pkg.py`

```python
"""
This module provides classes and methods to launch the Gray Scott application.
Pyflextrkr is ....
"""
from jarvis_cd.core.pkg import Application, Color
from jarvis_cd.shell import Exec, LocalExecInfo
import time
import pathlib

import yaml
from scspkg.pkg import Package

class Pyflextrkr(Application):
    """
    This class provides methods to launch the Pyflextrkr application.
    """
    def _init(self):
        """
        Initialize paths
        """
        self.pkg_type = 'pyflextrkr'
        self.hermes_env_vars = ['HERMES_ADAPTER_MODE', 'HERMES_CLIENT_CONF', 
                                'HERMES_CONF', 'LD_PRELOAD']

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        
        return [
            {
                'name': 'conda_env',
                'msg': 'Name of the conda environment for running Pyflextrkr',
                'type': str,
                'default': "flextrkr",
            },
            {
                'name': 'config',
                'msg': 'The config file for running analysis',
                'type': str,
                'default': None,
            },
            {
                'name': 'runscript',
                'msg': 'The name of the Pyflextrkr script to run (run_mcs_tbpfradar3d_wrf)',
                'type': str,
                'default': 'run_mcs_tbpfradar3d_wrf',
                'choices': ['run_mcs_tbpfradar3d_wrf', 'run_mcs_tbpf_saag_summer_sam', 'run_mcs_tb_summer_sam']
            },
            {
                'name': 'flush_mem',
                'msg': 'Flushing the memory after each stage',
                'type': bool,
                'default': False,
            },
            {
                'name': 'flush_mem_cmd',
                'msg': 'Command to flush the node memory',
                'type': str,
                'default': "ml user-scripts; sudo drop_caches", # for Ares
            },
            {
                'name': 'pyflextrkr_path',
                'msg': 'Absolute path to the Pyflextrkr source code',
                'type': str,
                'default': f"{Package(self.pkg_type).pkg_root}/src/PyFLEXTRKR",
            },
            {
                'name': 'experiment_input_path',
                'msg': 'Absolute path to the experiment run input and output files',
                'type': str,
                'default': None,
            },
            {
                'name': 'run_parallel',
                'msg': 'Parallel mode for Pyflextrkr: 0 (serial), 1 (local cluster), 2 (Dask MPI)',
                'type': int,
                'default': 1,
                'choices': [0,1,2],
            },
            {
                'name': 'nprocesses',
                'msg': 'Number of processes to run in parallel',
                'type': int,
                'default': 8,
            },
            {
                'name': 'run_cmd', # This is a internal variable
                'msg': 'Command to run Pyflextrkr',
                'type': str,
                'default': None,
            },
            {
                'name': 'local_exp_dir',
                'msg': 'Local experiment directory',
                'type': str,
                'default': None,
            },
            {
                'name': 'with_hermes',
                'msg': 'Whether it is used with Hermes (e.g. needs to update environment variables)',
                'type': bool,
                'default': False,
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        
        experiment_input_path = os.getenv('EXPERIMENT_INPUT_PATH')
        if experiment_input_path is None:
            raise Exception("Must set the experiment_input_path")
        else:
            self.config['experiment_input_path'] = experiment_input_path
        
        # update config file everytime
        self.config['config'] = f"{self.pkg_dir}/example_config/{self.config['runscript']}_template.yml"
        
        # Check if pyflextrkr_path not exists
        if pathlib.Path(self.config['pyflextrkr_path']).exists() == False:
            raise Exception(f"`pyflextrkr_path` {self.config['pyflextrkr_path']} does not exist.")
        
        if self.config['conda_env'] is None:
            raise Exception("Must set the conda environment for running Pyflextrkr")
        
        if self.config['runscript'] is None:
            raise Exception("Must set the Pyflextrkr script to run")
        else:
            
            # check if run script matches config file
            if self.config['runscript'] not in self.config['config']:
                raise Exception(f"Run script {self.config['runscript']} does not match config file {self.config['config']}")
            
            # get base file name without extension
            pass_in_path = self.config['runscript']
            script_name = pass_in_path.split("/")[-1]
            # remove ".py" from file name
            if ".py" in script_name:
                script_name = script_name[:-3]
            self.config['runscript'] = script_name

        # Check if config file exists
        if pathlib.Path(self.config['config']).exists():
            pass
        else:
            raise Exception(f"File {self.config['config']} does not exist.")

        if self.config['flush_mem'] == False:
            self.env['FLUSH_MEM'] = "FALSE"
        else:
            self.env['FLUSH_MEM'] = "TRUE"
            if self.config['flush_mem_cmd'] is None:
                raise Exception("Must add the command to flush memory using flush_mem_cmd")
        
        if self.config['pyflextrkr_path'] is None:
            raise Exception("Must set the `pyflextrkr_path` to the Pyflextrkr source code")
        else:
            # check that path exists
            pathlib.Path(self.config['pyflextrkr_path']).exists()
            # self.config['stdout'] = f'{self.config["pyflextrkr_path"]}/pyflextrkr_run.log'
        

    def _configure_yaml(self):
        self.env['HDF5_USE_FILE_LOCKING'] = "FALSE" # set HDF5 locking: FALSE, TRUE, BESTEFFORT

        yaml_file = self.config['config']
        
        if "_template.yml" not in str(yaml_file):
            yaml_file = yaml_file.replace(".yml", "_template.yml")
        
        self.log(f"Pyflextrkr config from: {yaml_file}")
        
        with open(yaml_file, "r") as stream:
            
            experiment_input_path = self.config['experiment_input_path']
            if self.config['local_exp_dir'] is not None:
                experiment_input_path = self.config['local_exp_dir']
            
            input_path = f"{experiment_input_path}/{self.config['runscript']}/"
            output_path = f"{experiment_input_path}/output_data/{self.config['runscript']}/"
            
            # Check if input_path exists and has files
            if pathlib.Path(input_path).exists() == False:
                raise Exception(f"Input path {input_path} does not exist.")
            if len(os.listdir(input_path)) == 0:
                raise Exception(f"Input path {input_path} is empty.")
            
            # pathlib.Path(input_path).mkdir(parents=True, exist_ok=True) # this should be done in data stage_in or setup
            pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)
            
            try:
                config_vars = yaml.safe_load(stream)
                
                config_vars['dask_tmp_dir'] = f"/tmp/pyflextrkr_test"
                pathlib.Path(config_vars['dask_tmp_dir']).mkdir(parents=True, exist_ok=True)
                
                config_vars['clouddata_path'] = str(input_path)
                config_vars['root_path'] = str(output_path)
                
                # Set run mode
                config_vars['run_parallel'] = self.config['run_parallel']
                
                # check processes
                if self.config['run_parallel'] == 0 and self.config['nprocesses'] > 1:
                    self.log(f"WARNING: run_parallel is 0 (serial) nprocesses is set to 1")
                    self.config['nprocesses'] = 1
                config_vars['nprocesses'] = self.config['nprocesses']
                
                if self.config['nprocesses'] < config_vars['nprocesses']:
                    self.log(f"WARNING: nprocesses is less than config file, set to {config_vars['nprocesses']}")
                    self.config['nprocesses'] = config_vars['nprocesses']
                
                # check if landmask_filename is a key in config_vars
                if 'landmask_filename' in config_vars:
                    org_path = config_vars['landmask_filename']
                    landmask_path = org_path.replace('INPUT_DIR/', input_path)
                    landmask_path = landmask_path.replace("'", "") # remove single quotes format
                    
                    if pathlib.Path(landmask_path).exists():
                        config_vars['landmask_filename'] = str(landmask_path)
                    else:
                        raise Exception(f"File {landmask_path} does not exist.")
                
                # save config_vars back to yaml file
                new_yaml_file = yaml_file.replace("_template.yml", ".yml")
                yaml.dump(config_vars, open(new_yaml_file, 'w'), default_flow_style=False)
            except yaml.YAMLError as exc:
                self.log(exc)
        self.config['config'] = new_yaml_file 
            
    def _unset_vfd_vars(self,env_vars_toset):
        cmd = ['conda', 'env', 'config', 'vars', 'unset',]
        
        for env_var in env_vars_toset:
            cmd.append(f'{env_var}')
        cmd.append('-n')
        cmd.append(self.config['conda_env'])
        
        cmd = ' '.join(cmd)
        Exec(cmd, LocalExecInfo(env=self.mod_env,)).run()
        self.log(f"Pyflextrkr _unset_vfd_vars: {cmd}")

    def _set_env_vars(self, env_vars_toset):
        
        self.log(f"Pyflextrkr _set_env_vars")
        
        # Unset all env_vars_toset first
        self._unset_vfd_vars(env_vars_toset)

        cmd = [ 'conda', 'env', 'config', 'vars', 'set']
        for env_var in env_vars_toset:
            env_var_val = self.mod_env[env_var]
            cmd.append(f'{env_var}={env_var_val}')
        
        cmd.append('-n')
        cmd.append(self.config['conda_env'])
        cmd = ' '.join(cmd)
        self.log(f"Pyflextrkr _set_env_vars: {cmd}")
        Exec(cmd, LocalExecInfo(env=self.mod_env,)).run()
        
    
    def _construct_cmd(self):
        """
        Construct the command to launch the application. E.g., Pyflextrkr will
        launch with expected environment and number of srun processes.

        :return: None
        """
        self.clean()
        
        cmd = []
        if self.config['run_parallel'] == 1:
            cmd = [
            'conda','run', '-v','-n', self.config['conda_env'],
            ]
        elif self.config['run_parallel'] == 2:
            host_list_str = None
            
            # Check if self.hostfile is set
            if self.hostfile is None:
                raise Exception("Running with Dask-MPI mode but self.hostfile is None")
            
            # open self.hostfile to get all lines of hosts into a string deliminated by ,
            # self.log(f"Pyflextrkr hostfile: {self.hostfile}")
            if 'localhost' in self.hostfile:
                host_list_str = "127.0.0.1"
            else:
                for hostname in self.hostfile:
                    if host_list_str is None:
                        host_list_str = hostname.rstrip()
                    else:
                        host_list_str = host_list_str + "," + hostname.rstrip()
            
            if host_list_str is None:
                raise Exception("host_list_str is None")
            self.log(f"Pyflextrkr host_list_str: {host_list_str}")
            
            # mpirun --host $hostlist --npernode 2
            ppn = self.config['nprocesses']/len(self.hostfile)
            cmd = [
                'conda','run', '-v','-n', self.config['conda_env'],
                'mpirun',
                '--host', host_list_str,
                '-n', str(self.config['nprocesses']),
                '-ppn', str(int(ppn)),
                # '-env', f'HDF5_USE_FILE_LOCKING={self.config["HDF5_USE_FILE_LOCKING"]}',
            ]
        
        # Exec("which python")
        cmd.append('python')
        # Convert runscript to full .py file path
        if self.config['pyflextrkr_path'] and self.config['runscript']:
            cmd.append(f'{self.config["pyflextrkr_path"]}/runscripts/{self.config["runscript"]}.py')
        
        
        cmd.append(self.config['config'])

        self.config['run_cmd'] = ' '.join(cmd)

    def start(self):
        """
        Launch an application. E.g., Pyflextrkr will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        
        if self.config['with_hermes'] == True:
            self._set_env_vars(self.hermes_env_vars)
        else:
            self._unset_vfd_vars(self.hermes_env_vars)
        
        ## Configure yaml file before start
        self._configure_yaml()
        self._construct_cmd()
        
        self.log(f"Pyflextrkr run_cmd: {self.config['run_cmd']}")
        
        start = time.time()
        
        Exec(self.config['run_cmd'],
             LocalExecInfo(env=self.mod_env,
                           pipe_stdout=self.config['stdout'],
                           pipe_stderr=self.config['stderr'],
                           )).run()
        
        end = time.time()
        diff = end - start
        self.log(f'Pyflextrkr TIME: {diff} seconds') # color=Color.GREEN
        

    def stop(self):
        """
        Stop a running application. E.g., Pyflextrkr will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass
        
    def kill(self):
        """
        Stop a running application. E.g., Pyflextrkr will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        cmd = ['killall', '-9', 'python']
        Exec(' '.join(cmd), LocalExecInfo(hostfile=self.hostfile)).run()

    def clean(self):
        """
        Destroy all data for an application. E.g., Pyflextrkr will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        # self.log(f"Manual Exec Required: Please clean up files in {self.config['experiment_input_path']}")
        
        output_dir = self.config['experiment_input_path'] + f"/output_data/{self.config['runscript']}"
        if self.config['local_exp_dir'] is not None:
            output_dir = self.config['local_exp_dir'] + f"/output_data/{self.config['runscript']}"
        
        # recursive remove all files in output_data directory
        self.log(f'Removing {output_dir}')
        Rm(output_dir).run()
        
        ## Do not clear cache in script, clear cache manually
        # # Clear cache
        # self.log(f'Clearing cache')
        # Exec(self.config['flush_mem_cmd'], LocalExecInfo(env=self.mod_env,))
        
        # output_dir = self.config['output'] + "*"
        # self.log(f'Removing {output_dir}')
        # Rm(output_dir)
```

### `builtin/builtin/pymonitor/pkg.py`

```python
"""
This module provides classes and methods to launch the Ior application.
Ior is ....
"""
from jarvis_cd.core.pkg import Service
from jarvis_cd.shell import PsshExecInfo
from jarvis_cd.shell.process import Mkdir, Kill, Rm
# TODO: Monitor import needs to be resolved - not available in jarvis_cd yet
# from jarvis_util.introspect.monitor import Monitor

class Pymonitor(Service):
    """
    This class provides methods to launch the Ior application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'frequency',
                'msg': 'Monitor frequency in seconds',
                'type': int,
                'default': 1
            },
            {
                'name': 'dir',
                'msg': 'Directory to store monitor logs',
                'type': str,
                'default': None,
            },
            {
                'name': 'num_nodes',
                'msg': 'Number of nodes to run monitor on. 0 means all',
                'type': int,
                'default': 0,
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        if self.config['dir'] is None:
            self.config['dir'] = f'{self.shared_dir}/logs'
        self.config['dir'] = os.path.expandvars(self.config['dir'])
        Mkdir(self.config['dir']).run()
        self.env['MONITOR_DIR'] = self.config['dir']
        self.log(f'The config dir is {self.config["dir"]}')

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        self.log(f'Pymonitor started on {self.config["dir"]}')
        self.env['PYTHONBUFFERED'] = '0'
        hostfile = self.hostfile
        if self.config['num_nodes'] > 0:
            hostfile = hostfile.subset(self.config['num_nodes'])
        Monitor(self.config['frequency'],
                self.config['dir'],
                PsshExecInfo(env=self.env,
                            hostfile=hostfile,
                            exec_async=True))
        time.sleep(self.config['sleep'])

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        Kill('.*pymonitor.*', PsshExecInfo(env=self.env)).run()

    def status(self):
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        Rm(self.config['dir']).run()
```

### `builtin/builtin/redis-benchmark/pkg.py`

```python
"""
This module provides classes and methods to launch the Redis benchmark tool.
Redis cluster is used if the hostfile has many hosts
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo


class RedisBenchmark(Application):
    """
    This class provides methods to launch the Ior application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'port',
                'msg': 'The port to use for the cluster',
                'type': int,
                'default': 7000,
                'choices': [],
                'args': [],
            },
            {
                'name': 'count',
                'msg': 'Number of requests to generate',
                'type': int,
                'default': 1000,
                'choices': [],
                'args': [],
            },
            {
                'name': 'write',
                'msg': 'Perform writes',
                'type': bool,
                'default': True,
                'choices': [],
                'args': [],
            },
            {
                'name': 'read',
                'msg': 'Perform reads',
                'type': bool,
                'default': True,
                'choices': [],
                'args': [],
            },
            {
                'name': 'nthreads',
                'msg': 'Number of threads',
                'type': int,
                'default': 1,
                'choices': [],
                'args': [],
            },
            {
                'name': 'pipeline',
                'msg': 'Number of requests to pipeline',
                'type': int,
                'default': 1,
                'choices': [],
                'args': [],
            },
            {
                'name': 'req_size',
                'msg': 'Size of requests (bytes)',
                'type': int,
                'default': 3,
                'choices': [],
                'args': [],
            },
            {
                'name': 'node',
                'msg': 'The node id to use for the cluster',
                'type': int,
                'default': 0,
                'choices': [],
                'args': [],
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        pass

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """

        hostfile = self.hostfile
        bench_type = [
            'set' if self.config['write'] else '',
            'get' if self.config['read'] else '',
        ]
        bench_type = ','.join([b for b in bench_type if b])
        cmd = [
            'redis-benchmark',
            f'-n {self.config["count"]}',
            f'-t {bench_type}',
            f'-P {self.config["pipeline"]}',
            f'--threads {self.config["nthreads"]}',
            f'-d {self.config["req_size"]}',
            f'-p {self.config["port"]}',
        ]
        if len(hostfile) > 1:
            cmd += [
                f'-h {hostfile.hosts[self.config["node"]]}',
                f'--cluster'
            ]
        self.log('Starting the cluster', color=Color.YELLOW)
        Exec(' '.join(cmd),
             LocalExecInfo(env=self.mod_env,
                           hostfile=hostfile)).run()

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        hostfile = self.hostfile
        for host in range(hostfile.hosts):
            Exec(f'redis-cli -p {self.config["port"]} -h {host} flushall',
                 LocalExecInfo(env=self.mod_env,
                               hostfile=hostfile)).run()
            Exec(f'redis-cli -p {self.config["port"]} -h {host} cluster reset',
                 LocalExecInfo(env=self.mod_env,
                               hostfile=hostfile)).run()
```

### `builtin/builtin/redis/config/redis.conf`

```conf
# Redis configuration file example.
#
# Note that in order to read the configuration file, Redis must be
# started with the file path as first argument:
#
# ./redis-server /path/to/redis.conf

# Note on units: when memory size is needed, it is possible to specify
# it in the usual form of 1k 5GB 4M and so forth:
#
# 1k => 1000 bytes
# 1kb => 1024 bytes
# 1m => 1000000 bytes
# 1mb => 1024*1024 bytes
# 1g => 1000000000 bytes
# 1gb => 1024*1024*1024 bytes
#
# units are case insensitive so 1GB 1Gb 1gB are all the same.

################################## INCLUDES ###################################

# Include one or more other config files here.  This is useful if you
# have a standard template that goes to all Redis servers but also need
# to customize a few per-server settings.  Include files can include
# other files, so use this wisely.
#
# Note that option "include" won't be rewritten by command "CONFIG REWRITE"
# from admin or Redis Sentinel. Since Redis always uses the last processed
# line as value of a configuration directive, you'd better put includes
# at the beginning of this file to avoid overwriting config change at runtime.
#
# If instead you are interested in using includes to override configuration
# options, it is better to use include as the last line.
#
# Included paths may contain wildcards. All files matching the wildcards will
# be included in alphabetical order.
# Note that if an include path contains a wildcards but no files match it when
# the server is started, the include statement will be ignored and no error will
# be emitted.  It is safe, therefore, to include wildcard files from empty
# directories.
#
# include /path/to/local.conf
# include /path/to/other.conf
# include /path/to/fragments/*.conf
#

################################## MODULES #####################################

# Load modules at startup. If the server is not able to load modules
# it will abort. It is possible to use multiple loadmodule directives.
#
# loadmodule /path/to/my_module.so
# loadmodule /path/to/other_module.so

################################## NETWORK #####################################

# By default, if no "bind" configuration directive is specified, Redis listens
# for connections from all available network interfaces on the host machine.
# It is possible to listen to just one or multiple selected interfaces using
# the "bind" configuration directive, followed by one or more IP addresses.
# Each address can be prefixed by "-", which means that redis will not fail to
# start if the address is not available. Being not available only refers to
# addresses that does not correspond to any network interface. Addresses that
# are already in use will always fail, and unsupported protocols will always BE
# silently skipped.
#
# Examples:
#
# bind 192.168.1.100 10.0.0.1     # listens on two specific IPv4 addresses
# bind 127.0.0.1 ::1              # listens on loopback IPv4 and IPv6
# bind * -::*                     # like the default, all available interfaces
#
# ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the
# internet, binding to all the interfaces is dangerous and will expose the
# instance to everybody on the internet. So by default we uncomment the
# following bind directive, that will force Redis to listen only on the
# IPv4 and IPv6 (if available) loopback interface addresses (this means Redis
# will only be able to accept client connections from the same host that it is
# running on).
#
# IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES
# COMMENT OUT THE FOLLOWING LINE.
#
# You will also need to set a password unless you explicitly disable protected
# mode.
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
bind 0.0.0.0

# By default, outgoing connections (from replica to master, from Sentinel to
# instances, cluster bus, etc.) are not bound to a specific local address. In
# most cases, this means the operating system will handle that based on routing
# and the interface through which the connection goes out.
#
# Using bind-source-addr it is possible to configure a specific address to bind
# to, which may also affect how the connection gets routed.
#
# Example:
#
# bind-source-addr 10.0.0.1

# Protected mode is a layer of security protection, in order to avoid that
# Redis instances left open on the internet are accessed and exploited.
#
# When protected mode is on and the default user has no password, the server
# only accepts local connections from the IPv4 address (127.0.0.1), IPv6 address
# (::1) or Unix domain sockets.
#
# By default protected mode is enabled. You should disable it only if
# you are sure you want clients from other hosts to connect to Redis
# even if no authentication is configured.
protected-mode no

# Redis uses default hardened security configuration directives to reduce the
# attack surface on innocent users. Therefore, several sensitive configuration
# directives are immutable, and some potentially-dangerous commands are blocked.
#
# Configuration directives that control files that Redis writes to (e.g., 'dir'
# and 'dbfilename') and that aren't usually modified during runtime
# are protected by making them immutable.
#
# Commands that can increase the attack surface of Redis and that aren't usually
# called by users are blocked by default.
#
# These can be exposed to either all connections or just local ones by setting
# each of the configs listed below to either of these values:
#
# no    - Block for any connection (remain immutable)
# yes   - Allow for any connection (no protection)
# local - Allow only for local connections. Ones originating from the
#         IPv4 address (127.0.0.1), IPv6 address (::1) or Unix domain sockets.
#
# enable-protected-configs no
# enable-debug-command no
# enable-module-command no

# Accept connections on the specified port, default is 6379 (IANA #815344).
# If port 0 is specified Redis will not listen on a TCP socket.
port ##PORT##

# TCP listen() backlog.
#
# In high requests-per-second environments you need a high backlog in order
# to avoid slow clients connection issues. Note that the Linux kernel
# will silently truncate it to the value of /proc/sys/net/core/somaxconn so
# make sure to raise both the value of somaxconn and tcp_max_syn_backlog
# in order to get the desired effect.
tcp-backlog 511

# Unix socket.
#
# Specify the path for the Unix socket that will be used to listen for
# incoming connections. There is no default, so Redis will not listen
# on a unix socket when not specified.
#
# unixsocket /run/redis.sock
# unixsocketperm 700

# Close the connection after a client is idle for N seconds (0 to disable)
timeout 0

# TCP keepalive.
#
# If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence
# of communication. This is useful for two reasons:
#
# 1) Detect dead peers.
# 2) Force network equipment in the middle to consider the connection to be
#    alive.
#
# On Linux, the specified value (in seconds) is the period used to send ACKs.
# Note that to close the connection the double of the time is needed.
# On other kernels the period depends on the kernel configuration.
#
# A reasonable value for this option is 300 seconds, which is the new
# Redis default starting with Redis 3.2.1.
tcp-keepalive 300

# Apply OS-specific mechanism to mark the listening socket with the specified
# ID, to support advanced routing and filtering capabilities.
#
# On Linux, the ID represents a connection mark.
# On FreeBSD, the ID represents a socket cookie ID.
# On OpenBSD, the ID represents a route table ID.
#
# The default value is 0, which implies no marking is required.
# socket-mark-id 0

################################# TLS/SSL #####################################

# By default, TLS/SSL is disabled. To enable it, the "tls-port" configuration
# directive can be used to define TLS-listening ports. To enable TLS on the
# default port, use:
#
# port 0
# tls-port 6379

# Configure a X.509 certificate and private key to use for authenticating the
# server to connected clients, masters or cluster peers.  These files should be
# PEM formatted.
#
# tls-cert-file redis.crt
# tls-key-file redis.key
#
# If the key file is encrypted using a passphrase, it can be included here
# as well.
#
# tls-key-file-pass secret

# Normally Redis uses the same certificate for both server functions (accepting
# connections) and client functions (replicating from a master, establishing
# cluster bus connections, etc.).
#
# Sometimes certificates are issued with attributes that designate them as
# client-only or server-only certificates. In that case it may be desired to use
# different certificates for incoming (server) and outgoing (client)
# connections. To do that, use the following directives:
#
# tls-client-cert-file client.crt
# tls-client-key-file client.key
#
# If the key file is encrypted using a passphrase, it can be included here
# as well.
#
# tls-client-key-file-pass secret

# Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange,
# required by older versions of OpenSSL (<3.0). Newer versions do not require
# this configuration and recommend against it.
#
# tls-dh-params-file redis.dh

# Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL
# clients and peers.  Redis requires an explicit configuration of at least one
# of these, and will not implicitly use the system wide configuration.
#
# tls-ca-cert-file ca.crt
# tls-ca-cert-dir /etc/ssl/certs

# By default, clients (including replica servers) on a TLS port are required
# to authenticate using valid client side certificates.
#
# If "no" is specified, client certificates are not required and not accepted.
# If "optional" is specified, client certificates are accepted and must be
# valid if provided, but are not required.
#
# tls-auth-clients no
# tls-auth-clients optional

# By default, a Redis replica does not attempt to establish a TLS connection
# with its master.
#
# Use the following directive to enable TLS on replication links.
#
# tls-replication yes

# By default, the Redis Cluster bus uses a plain TCP connection. To enable
# TLS for the bus protocol, use the following directive:
#
# tls-cluster yes

# By default, only TLSv1.2 and TLSv1.3 are enabled and it is highly recommended
# that older formally deprecated versions are kept disabled to reduce the attack surface.
# You can explicitly specify TLS versions to support.
# Allowed values are case insensitive and include "TLSv1", "TLSv1.1", "TLSv1.2",
# "TLSv1.3" (OpenSSL >= 1.1.1) or any combination.
# To enable only TLSv1.2 and TLSv1.3, use:
#
# tls-protocols "TLSv1.2 TLSv1.3"

# Configure allowed ciphers.  See the ciphers(1ssl) manpage for more information
# about the syntax of this string.
#
# Note: this configuration applies only to <= TLSv1.2.
#
# tls-ciphers DEFAULT:!MEDIUM

# Configure allowed TLSv1.3 ciphersuites.  See the ciphers(1ssl) manpage for more
# information about the syntax of this string, and specifically for TLSv1.3
# ciphersuites.
#
# tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256

# When choosing a cipher, use the server's preference instead of the client
# preference. By default, the server follows the client's preference.
#
# tls-prefer-server-ciphers yes

# By default, TLS session caching is enabled to allow faster and less expensive
# reconnections by clients that support it. Use the following directive to disable
# caching.
#
# tls-session-caching no

# Change the default number of TLS sessions cached. A zero value sets the cache
# to unlimited size. The default size is 20480.
#
# tls-session-cache-size 5000

# Change the default timeout of cached TLS sessions. The default timeout is 300
# seconds.
#
# tls-session-cache-timeout 60

################################# GENERAL #####################################

# By default Redis does not run as a daemon. Use 'yes' if you need it.
# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.
# When Redis is supervised by upstart or systemd, this parameter has no impact.
daemonize no

# If you run Redis from upstart or systemd, Redis can interact with your
# supervision tree. Options:
#   supervised no      - no supervision interaction
#   supervised upstart - signal upstart by putting Redis into SIGSTOP mode
#                        requires "expect stop" in your upstart job config
#   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET
#                        on startup, and updating Redis status on a regular
#                        basis.
#   supervised auto    - detect upstart or systemd method based on
#                        UPSTART_JOB or NOTIFY_SOCKET environment variables
# Note: these supervision methods only signal "process is ready."
#       They do not enable continuous pings back to your supervisor.
#
# The default is "no". To run under upstart/systemd, you can simply uncomment
# the line below:
#
# supervised auto

# If a pid file is specified, Redis writes it where specified at startup
# and removes it at exit.
#
# When the server runs non daemonized, no pid file is created if none is
# specified in the configuration. When the server is daemonized, the pid file
# is used even if not specified, defaulting to "/var/run/redis.pid".
#
# Creating a pid file is best effort: if Redis is not able to create it
# nothing bad happens, the server will start and run normally.
#
# Note that on modern Linux systems "/run/redis.pid" is more conforming
# and should be used instead.
pidfile /var/run/redis_6379.pid

# Specify the server verbosity level.
# This can be one of:
# debug (a lot of information, useful for development/testing)
# verbose (many rarely useful info, but not a mess like the debug level)
# notice (moderately verbose, what you want in production probably)
# warning (only very important / critical messages are logged)
loglevel notice

# Specify the log file name. Also the empty string can be used to force
# Redis to log on the standard output. Note that if you use standard
# output for logging but daemonize, logs will be sent to /dev/null
logfile ""

# To enable logging to the system logger, just set 'syslog-enabled' to yes,
# and optionally update the other syslog parameters to suit your needs.
# syslog-enabled no

# Specify the syslog identity.
# syslog-ident redis

# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.
# syslog-facility local0

# To disable the built in crash log, which will possibly produce cleaner core
# dumps when they are needed, uncomment the following:
#
# crash-log-enabled no

# To disable the fast memory check that's run as part of the crash log, which
# will possibly let redis terminate sooner, uncomment the following:
#
# crash-memcheck-enabled no

# Set the number of databases. The default database is DB 0, you can select
# a different one on a per-connection basis using SELECT <dbid> where
# dbid is a number between 0 and 'databases'-1
databases 16

# By default Redis shows an ASCII art logo only when started to log to the
# standard output and if the standard output is a TTY and syslog logging is
# disabled. Basically this means that normally a logo is displayed only in
# interactive sessions.
#
# However it is possible to force the pre-4.0 behavior and always show a
# ASCII art logo in startup logs by setting the following option to yes.
always-show-logo no

# By default, Redis modifies the process title (as seen in 'top' and 'ps') to
# provide some runtime information. It is possible to disable this and leave
# the process name as executed by setting the following to no.
set-proc-title yes

# When changing the process title, Redis uses the following template to construct
# the modified title.
#
# Template variables are specified in curly brackets. The following variables are
# supported:
#
# {title}           Name of process as executed if parent, or type of child process.
# {listen-addr}     Bind address or '*' followed by TCP or TLS port listening on, or
#                   Unix socket if only that's available.
# {server-mode}     Special mode, i.e. "[sentinel]" or "[cluster]".
# {port}            TCP port listening on, or 0.
# {tls-port}        TLS port listening on, or 0.
# {unixsocket}      Unix domain socket listening on, or "".
# {config-file}     Name of configuration file used.
#
proc-title-template "{title} {listen-addr} {server-mode}"

################################ SNAPSHOTTING  ################################

# Save the DB to disk.
#
# save <seconds> <changes> [<seconds> <changes> ...]
#
# Redis will save the DB if the given number of seconds elapsed and it
# surpassed the given number of write operations against the DB.
#
# Snapshotting can be completely disabled with a single empty string argument
# as in following example:
#
# save ""
#
# Unless specified otherwise, by default Redis will save the DB:
#   * After 3600 seconds (an hour) if at least 1 change was performed
#   * After 300 seconds (5 minutes) if at least 100 changes were performed
#   * After 60 seconds if at least 10000 changes were performed
#
# You can set these explicitly by uncommenting the following line.
#
# save 3600 1 300 100 60 10000

# By default Redis will stop accepting writes if RDB snapshots are enabled
# (at least one save point) and the latest background save failed.
# This will make the user aware (in a hard way) that data is not persisting
# on disk properly, otherwise chances are that no one will notice and some
# disaster will happen.
#
# If the background saving process will start working again Redis will
# automatically allow writes again.
#
# However if you have setup your proper monitoring of the Redis server
# and persistence, you may want to disable this feature so that Redis will
# continue to work as usual even if there are problems with disk,
# permissions, and so forth.
stop-writes-on-bgsave-error yes

# Compress string objects using LZF when dump .rdb databases?
# By default compression is enabled as it's almost always a win.
# If you want to save some CPU in the saving child set it to 'no' but
# the dataset will likely be bigger if you have compressible values or keys.
rdbcompression yes

# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.
# This makes the format more resistant to corruption but there is a performance
# hit to pay (around 10%) when saving and loading RDB files, so you can disable it
# for maximum performances.
#
# RDB files created with checksum disabled have a checksum of zero that will
# tell the loading code to skip the check.
rdbchecksum yes

# Enables or disables full sanitization checks for ziplist and listpack etc when
# loading an RDB or RESTORE payload. This reduces the chances of a assertion or
# crash later on while processing commands.
# Options:
#   no         - Never perform full sanitization
#   yes        - Always perform full sanitization
#   clients    - Perform full sanitization only for user connections.
#                Excludes: RDB files, RESTORE commands received from the master
#                connection, and client connections which have the
#                skip-sanitize-payload ACL flag.
# The default should be 'clients' but since it currently affects cluster
# resharding via MIGRATE, it is temporarily set to 'no' by default.
#
# sanitize-dump-payload no

# The filename where to dump the DB
dbfilename dump.rdb

# Remove RDB files used by replication in instances without persistence
# enabled. By default this option is disabled, however there are environments
# where for regulations or other security concerns, RDB files persisted on
# disk by masters in order to feed replicas, or stored on disk by replicas
# in order to load them for the initial synchronization, should be deleted
# ASAP. Note that this option ONLY WORKS in instances that have both AOF
# and RDB persistence disabled, otherwise is completely ignored.
#
# An alternative (and sometimes better) way to obtain the same effect is
# to use diskless replication on both master and replicas instances. However
# in the case of replicas, diskless is not always an option.
rdb-del-sync-files no

# The working directory.
#
# The DB will be written inside this directory, with the filename specified
# above using the 'dbfilename' configuration directive.
#
# The Append Only File will also be created inside this directory.
#
# Note that you must specify a directory here, not a file name.
dir ./

################################# REPLICATION #################################

# Master-Replica replication. Use replicaof to make a Redis instance a copy of
# another Redis server. A few things to understand ASAP about Redis replication.
#
#   +------------------+      +---------------+
#   |      Master      | ---> |    Replica    |
#   | (receive writes) |      |  (exact copy) |
#   +------------------+      +---------------+
#
# 1) Redis replication is asynchronous, but you can configure a master to
#    stop accepting writes if it appears to be not connected with at least
#    a given number of replicas.
# 2) Redis replicas are able to perform a partial resynchronization with the
#    master if the replication link is lost for a relatively small amount of
#    time. You may want to configure the replication backlog size (see the next
#    sections of this file) with a sensible value depending on your needs.
# 3) Replication is automatic and does not need user intervention. After a
#    network partition replicas automatically try to reconnect to masters
#    and resynchronize with them.
#
# replicaof <masterip> <masterport>

# If the master is password protected (using the "requirepass" configuration
# directive below) it is possible to tell the replica to authenticate before
# starting the replication synchronization process, otherwise the master will
# refuse the replica request.
#
# masterauth <master-password>
#
# However this is not enough if you are using Redis ACLs (for Redis version
# 6 or greater), and the default user is not capable of running the PSYNC
# command and/or other commands needed for replication. In this case it's
# better to configure a special user to use with replication, and specify the
# masteruser configuration as such:
#
# masteruser <username>
#
# When masteruser is specified, the replica will authenticate against its
# master using the new AUTH form: AUTH <username> <password>.

# When a replica loses its connection with the master, or when the replication
# is still in progress, the replica can act in two different ways:
#
# 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will
#    still reply to client requests, possibly with out of date data, or the
#    data set may just be empty if this is the first synchronization.
#
# 2) If replica-serve-stale-data is set to 'no' the replica will reply with error
#    "MASTERDOWN Link with MASTER is down and replica-serve-stale-data is set to 'no'"
#    to all data access commands, excluding commands such as:
#    INFO, REPLICAOF, AUTH, SHUTDOWN, REPLCONF, ROLE, CONFIG, SUBSCRIBE,
#    UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB, COMMAND, POST,
#    HOST and LATENCY.
#
replica-serve-stale-data yes

# You can configure a replica instance to accept writes or not. Writing against
# a replica instance may be useful to store some ephemeral data (because data
# written on a replica will be easily deleted after resync with the master) but
# may also cause problems if clients are writing to it because of a
# misconfiguration.
#
# Since Redis 2.6 by default replicas are read-only.
#
# Note: read only replicas are not designed to be exposed to untrusted clients
# on the internet. It's just a protection layer against misuse of the instance.
# Still a read only replica exports by default all the administrative commands
# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve
# security of read only replicas using 'rename-command' to shadow all the
# administrative / dangerous commands.
replica-read-only yes

# Replication SYNC strategy: disk or socket.
#
# New replicas and reconnecting replicas that are not able to continue the
# replication process just receiving differences, need to do what is called a
# "full synchronization". An RDB file is transmitted from the master to the
# replicas.
#
# The transmission can happen in two different ways:
#
# 1) Disk-backed: The Redis master creates a new process that writes the RDB
#                 file on disk. Later the file is transferred by the parent
#                 process to the replicas incrementally.
# 2) Diskless: The Redis master creates a new process that directly writes the
#              RDB file to replica sockets, without touching the disk at all.
#
# With disk-backed replication, while the RDB file is generated, more replicas
# can be queued and served with the RDB file as soon as the current child
# producing the RDB file finishes its work. With diskless replication instead
# once the transfer starts, new replicas arriving will be queued and a new
# transfer will start when the current one terminates.
#
# When diskless replication is used, the master waits a configurable amount of
# time (in seconds) before starting the transfer in the hope that multiple
# replicas will arrive and the transfer can be parallelized.
#
# With slow disks and fast (large bandwidth) networks, diskless replication
# works better.
repl-diskless-sync yes

# When diskless replication is enabled, it is possible to configure the delay
# the server waits in order to spawn the child that transfers the RDB via socket
# to the replicas.
#
# This is important since once the transfer starts, it is not possible to serve
# new replicas arriving, that will be queued for the next RDB transfer, so the
# server waits a delay in order to let more replicas arrive.
#
# The delay is specified in seconds, and by default is 5 seconds. To disable
# it entirely just set it to 0 seconds and the transfer will start ASAP.
repl-diskless-sync-delay 5

# When diskless replication is enabled with a delay, it is possible to let
# the replication start before the maximum delay is reached if the maximum
# number of replicas expected have connected. Default of 0 means that the
# maximum is not defined and Redis will wait the full delay.
repl-diskless-sync-max-replicas 0

# -----------------------------------------------------------------------------
# WARNING: RDB diskless load is experimental. Since in this setup the replica
# does not immediately store an RDB on disk, it may cause data loss during
# failovers. RDB diskless load + Redis modules not handling I/O reads may also
# cause Redis to abort in case of I/O errors during the initial synchronization
# stage with the master. Use only if you know what you are doing.
# -----------------------------------------------------------------------------
#
# Replica can load the RDB it reads from the replication link directly from the
# socket, or store the RDB to a file and read that file after it was completely
# received from the master.
#
# In many cases the disk is slower than the network, and storing and loading
# the RDB file may increase replication time (and even increase the master's
# Copy on Write memory and replica buffers).
# However, parsing the RDB file directly from the socket may mean that we have
# to flush the contents of the current database before the full rdb was
# received. For this reason we have the following options:
#
# "disabled"    - Don't use diskless load (store the rdb file to the disk first)
# "on-empty-db" - Use diskless load only when it is completely safe.
# "swapdb"      - Keep current db contents in RAM while parsing the data directly
#                 from the socket. Replicas in this mode can keep serving current
#                 data set while replication is in progress, except for cases where
#                 they can't recognize master as having a data set from same
#                 replication history.
#                 Note that this requires sufficient memory, if you don't have it,
#                 you risk an OOM kill.
repl-diskless-load disabled

# Master send PINGs to its replicas in a predefined interval. It's possible to
# change this interval with the repl_ping_replica_period option. The default
# value is 10 seconds.
#
# repl-ping-replica-period 10

# The following option sets the replication timeout for:
#
# 1) Bulk transfer I/O during SYNC, from the point of view of replica.
# 2) Master timeout from the point of view of replicas (data, pings).
# 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).
#
# It is important to make sure that this value is greater than the value
# specified for repl-ping-replica-period otherwise a timeout will be detected
# every time there is low traffic between the master and the replica. The default
# value is 60 seconds.
#
# repl-timeout 60

# Disable TCP_NODELAY on the replica socket after SYNC?
#
# If you select "yes" Redis will use a smaller number of TCP packets and
# less bandwidth to send data to replicas. But this can add a delay for
# the data to appear on the replica side, up to 40 milliseconds with
# Linux kernels using a default configuration.
#
# If you select "no" the delay for data to appear on the replica side will
# be reduced but more bandwidth will be used for replication.
#
# By default we optimize for low latency, but in very high traffic conditions
# or when the master and replicas are many hops away, turning this to "yes" may
# be a good idea.
repl-disable-tcp-nodelay no

# Set the replication backlog size. The backlog is a buffer that accumulates
# replica data when replicas are disconnected for some time, so that when a
# replica wants to reconnect again, often a full resync is not needed, but a
# partial resync is enough, just passing the portion of data the replica
# missed while disconnected.
#
# The bigger the replication backlog, the longer the replica can endure the
# disconnect and later be able to perform a partial resynchronization.
#
# The backlog is only allocated if there is at least one replica connected.
#
# repl-backlog-size 1mb

# After a master has no connected replicas for some time, the backlog will be
# freed. The following option configures the amount of seconds that need to
# elapse, starting from the time the last replica disconnected, for the backlog
# buffer to be freed.
#
# Note that replicas never free the backlog for timeout, since they may be
# promoted to masters later, and should be able to correctly "partially
# resynchronize" with other replicas: hence they should always accumulate backlog.
#
# A value of 0 means to never release the backlog.
#
# repl-backlog-ttl 3600

# The replica priority is an integer number published by Redis in the INFO
# output. It is used by Redis Sentinel in order to select a replica to promote
# into a master if the master is no longer working correctly.
#
# A replica with a low priority number is considered better for promotion, so
# for instance if there are three replicas with priority 10, 100, 25 Sentinel
# will pick the one with priority 10, that is the lowest.
#
# However a special priority of 0 marks the replica as not able to perform the
# role of master, so a replica with priority of 0 will never be selected by
# Redis Sentinel for promotion.
#
# By default the priority is 100.
replica-priority 100

# The propagation error behavior controls how Redis will behave when it is
# unable to handle a command being processed in the replication stream from a master
# or processed while reading from an AOF file. Errors that occur during propagation
# are unexpected, and can cause data inconsistency. However, there are edge cases
# in earlier versions of Redis where it was possible for the server to replicate or persist
# commands that would fail on future versions. For this reason the default behavior
# is to ignore such errors and continue processing commands.
#
# If an application wants to ensure there is no data divergence, this configuration
# should be set to 'panic' instead. The value can also be set to 'panic-on-replicas'
# to only panic when a replica encounters an error on the replication stream. One of
# these two panic values will become the default value in the future once there are
# sufficient safety mechanisms in place to prevent false positive crashes.
#
# propagation-error-behavior ignore

# Replica ignore disk write errors controls the behavior of a replica when it is
# unable to persist a write command received from its master to disk. By default,
# this configuration is set to 'no' and will crash the replica in this condition.
# It is not recommended to change this default, however in order to be compatible
# with older versions of Redis this config can be toggled to 'yes' which will just
# log a warning and execute the write command it got from the master.
#
# replica-ignore-disk-write-errors no

# -----------------------------------------------------------------------------
# By default, Redis Sentinel includes all replicas in its reports. A replica
# can be excluded from Redis Sentinel's announcements. An unannounced replica
# will be ignored by the 'sentinel replicas <master>' command and won't be
# exposed to Redis Sentinel's clients.
#
# This option does not change the behavior of replica-priority. Even with
# replica-announced set to 'no', the replica can be promoted to master. To
# prevent this behavior, set replica-priority to 0.
#
# replica-announced yes

# It is possible for a master to stop accepting writes if there are less than
# N replicas connected, having a lag less or equal than M seconds.
#
# The N replicas need to be in "online" state.
#
# The lag in seconds, that must be <= the specified value, is calculated from
# the last ping received from the replica, that is usually sent every second.
#
# This option does not GUARANTEE that N replicas will accept the write, but
# will limit the window of exposure for lost writes in case not enough replicas
# are available, to the specified number of seconds.
#
# For example to require at least 3 replicas with a lag <= 10 seconds use:
#
# min-replicas-to-write 3
# min-replicas-max-lag 10
#
# Setting one or the other to 0 disables the feature.
#
# By default min-replicas-to-write is set to 0 (feature disabled) and
# min-replicas-max-lag is set to 10.

# A Redis master is able to list the address and port of the attached
# replicas in different ways. For example the "INFO replication" section
# offers this information, which is used, among other tools, by
# Redis Sentinel in order to discover replica instances.
# Another place where this info is available is in the output of the
# "ROLE" command of a master.
#
# The listed IP address and port normally reported by a replica is
# obtained in the following way:
#
#   IP: The address is auto detected by checking the peer address
#   of the socket used by the replica to connect with the master.
#
#   Port: The port is communicated by the replica during the replication
#   handshake, and is normally the port that the replica is using to
#   listen for connections.
#
# However when port forwarding or Network Address Translation (NAT) is
# used, the replica may actually be reachable via different IP and port
# pairs. The following two options can be used by a replica in order to
# report to its master a specific set of IP and port, so that both INFO
# and ROLE will report those values.
#
# There is no need to use both the options if you need to override just
# the port or the IP address.
#
# replica-announce-ip 5.5.5.5
# replica-announce-port 1234

############################### KEYS TRACKING #################################

# Redis implements server assisted support for client side caching of values.
# This is implemented using an invalidation table that remembers, using
# a radix key indexed by key name, what clients have which keys. In turn
# this is used in order to send invalidation messages to clients. Please
# check this page to understand more about the feature:
#
#   https://redis.io/topics/client-side-caching
#
# When tracking is enabled for a client, all the read only queries are assumed
# to be cached: this will force Redis to store information in the invalidation
# table. When keys are modified, such information is flushed away, and
# invalidation messages are sent to the clients. However if the workload is
# heavily dominated by reads, Redis could use more and more memory in order
# to track the keys fetched by many clients.
#
# For this reason it is possible to configure a maximum fill value for the
# invalidation table. By default it is set to 1M of keys, and once this limit
# is reached, Redis will start to evict keys in the invalidation table
# even if they were not modified, just to reclaim memory: this will in turn
# force the clients to invalidate the cached values. Basically the table
# maximum size is a trade off between the memory you want to spend server
# side to track information about who cached what, and the ability of clients
# to retain cached objects in memory.
#
# If you set the value to 0, it means there are no limits, and Redis will
# retain as many keys as needed in the invalidation table.
# In the "stats" INFO section, you can find information about the number of
# keys in the invalidation table at every given moment.
#
# Note: when key tracking is used in broadcasting mode, no memory is used
# in the server side so this setting is useless.
#
# tracking-table-max-keys 1000000

################################## SECURITY ###################################

# Warning: since Redis is pretty fast, an outside user can try up to
# 1 million passwords per second against a modern box. This means that you
# should use very strong passwords, otherwise they will be very easy to break.
# Note that because the password is really a shared secret between the client
# and the server, and should not be memorized by any human, the password
# can be easily a long string from /dev/urandom or whatever, so by using a
# long and unguessable password no brute force attack will be possible.

# Redis ACL users are defined in the following format:
#
#   user <username> ... acl rules ...
#
# For example:
#
#   user worker +@list +@connection ~jobs:* on >ffa9203c493aa99
#
# The special username "default" is used for new connections. If this user
# has the "nopass" rule, then new connections will be immediately authenticated
# as the "default" user without the need of any password provided via the
# AUTH command. Otherwise if the "default" user is not flagged with "nopass"
# the connections will start in not authenticated state, and will require
# AUTH (or the HELLO command AUTH option) in order to be authenticated and
# start to work.
#
# The ACL rules that describe what a user can do are the following:
#
#  on           Enable the user: it is possible to authenticate as this user.
#  off          Disable the user: it's no longer possible to authenticate
#               with this user, however the already authenticated connections
#               will still work.
#  skip-sanitize-payload    RESTORE dump-payload sanitization is skipped.
#  sanitize-payload         RESTORE dump-payload is sanitized (default).
#  +<command>   Allow the execution of that command.
#               May be used with `|` for allowing subcommands (e.g "+config|get")
#  -<command>   Disallow the execution of that command.
#               May be used with `|` for blocking subcommands (e.g "-config|set")
#  +@<category> Allow the execution of all the commands in such category
#               with valid categories are like @admin, @set, @sortedset, ...
#               and so forth, see the full list in the server.c file where
#               the Redis command table is described and defined.
#               The special category @all means all the commands, but currently
#               present in the server, and that will be loaded in the future
#               via modules.
#  +<command>|first-arg  Allow a specific first argument of an otherwise
#                        disabled command. It is only supported on commands with
#                        no sub-commands, and is not allowed as negative form
#                        like -SELECT|1, only additive starting with "+". This
#                        feature is deprecated and may be removed in the future.
#  allcommands  Alias for +@all. Note that it implies the ability to execute
#               all the future commands loaded via the modules system.
#  nocommands   Alias for -@all.
#  ~<pattern>   Add a pattern of keys that can be mentioned as part of
#               commands. For instance ~* allows all the keys. The pattern
#               is a glob-style pattern like the one of KEYS.
#               It is possible to specify multiple patterns.
# %R~<pattern>  Add key read pattern that specifies which keys can be read 
#               from.
# %W~<pattern>  Add key write pattern that specifies which keys can be
#               written to. 
#  allkeys      Alias for ~*
#  resetkeys    Flush the list of allowed keys patterns.
#  &<pattern>   Add a glob-style pattern of Pub/Sub channels that can be
#               accessed by the user. It is possible to specify multiple channel
#               patterns.
#  allchannels  Alias for &*
#  resetchannels            Flush the list of allowed channel patterns.
#  ><password>  Add this password to the list of valid password for the user.
#               For example >mypass will add "mypass" to the list.
#               This directive clears the "nopass" flag (see later).
#  <<password>  Remove this password from the list of valid passwords.
#  nopass       All the set passwords of the user are removed, and the user
#               is flagged as requiring no password: it means that every
#               password will work against this user. If this directive is
#               used for the default user, every new connection will be
#               immediately authenticated with the default user without
#               any explicit AUTH command required. Note that the "resetpass"
#               directive will clear this condition.
#  resetpass    Flush the list of allowed passwords. Moreover removes the
#               "nopass" status. After "resetpass" the user has no associated
#               passwords and there is no way to authenticate without adding
#               some password (or setting it as "nopass" later).
#  reset        Performs the following actions: resetpass, resetkeys, off,
#               -@all. The user returns to the same state it has immediately
#               after its creation.
# (<options>)   Create a new selector with the options specified within the
#               parentheses and attach it to the user. Each option should be 
#               space separated. The first character must be ( and the last 
#               character must be ).
# clearselectors            Remove all of the currently attached selectors. 
#                           Note this does not change the "root" user permissions,
#                           which are the permissions directly applied onto the
#                           user (outside the parentheses).
#
# ACL rules can be specified in any order: for instance you can start with
# passwords, then flags, or key patterns. However note that the additive
# and subtractive rules will CHANGE MEANING depending on the ordering.
# For instance see the following example:
#
#   user alice on +@all -DEBUG ~* >somepassword
#
# This will allow "alice" to use all the commands with the exception of the
# DEBUG command, since +@all added all the commands to the set of the commands
# alice can use, and later DEBUG was removed. However if we invert the order
# of two ACL rules the result will be different:
#
#   user alice on -DEBUG +@all ~* >somepassword
#
# Now DEBUG was removed when alice had yet no commands in the set of allowed
# commands, later all the commands are added, so the user will be able to
# execute everything.
#
# Basically ACL rules are processed left-to-right.
#
# The following is a list of command categories and their meanings:
# * keyspace - Writing or reading from keys, databases, or their metadata 
#     in a type agnostic way. Includes DEL, RESTORE, DUMP, RENAME, EXISTS, DBSIZE,
#     KEYS, EXPIRE, TTL, FLUSHALL, etc. Commands that may modify the keyspace,
#     key or metadata will also have `write` category. Commands that only read
#     the keyspace, key or metadata will have the `read` category.
# * read - Reading from keys (values or metadata). Note that commands that don't
#     interact with keys, will not have either `read` or `write`.
# * write - Writing to keys (values or metadata)
# * admin - Administrative commands. Normal applications will never need to use
#     these. Includes REPLICAOF, CONFIG, DEBUG, SAVE, MONITOR, ACL, SHUTDOWN, etc.
# * dangerous - Potentially dangerous (each should be considered with care for
#     various reasons). This includes FLUSHALL, MIGRATE, RESTORE, SORT, KEYS,
#     CLIENT, DEBUG, INFO, CONFIG, SAVE, REPLICAOF, etc.
# * connection - Commands affecting the connection or other connections.
#     This includes AUTH, SELECT, COMMAND, CLIENT, ECHO, PING, etc.
# * blocking - Potentially blocking the connection until released by another
#     command.
# * fast - Fast O(1) commands. May loop on the number of arguments, but not the
#     number of elements in the key.
# * slow - All commands that are not Fast.
# * pubsub - PUBLISH / SUBSCRIBE related
# * transaction - WATCH / MULTI / EXEC related commands.
# * scripting - Scripting related.
# * set - Data type: sets related.
# * sortedset - Data type: zsets related.
# * list - Data type: lists related.
# * hash - Data type: hashes related.
# * string - Data type: strings related.
# * bitmap - Data type: bitmaps related.
# * hyperloglog - Data type: hyperloglog related.
# * geo - Data type: geo related.
# * stream - Data type: streams related.
#
# For more information about ACL configuration please refer to
# the Redis web site at https://redis.io/topics/acl

# ACL LOG
#
# The ACL Log tracks failed commands and authentication events associated
# with ACLs. The ACL Log is useful to troubleshoot failed commands blocked
# by ACLs. The ACL Log is stored in memory. You can reclaim memory with
# ACL LOG RESET. Define the maximum entry length of the ACL Log below.
acllog-max-len 128

# Using an external ACL file
#
# Instead of configuring users here in this file, it is possible to use
# a stand-alone file just listing users. The two methods cannot be mixed:
# if you configure users here and at the same time you activate the external
# ACL file, the server will refuse to start.
#
# The format of the external ACL user file is exactly the same as the
# format that is used inside redis.conf to describe users.
#
# aclfile /etc/redis/users.acl

# IMPORTANT NOTE: starting with Redis 6 "requirepass" is just a compatibility
# layer on top of the new ACL system. The option effect will be just setting
# the password for the default user. Clients will still authenticate using
# AUTH <password> as usually, or more explicitly with AUTH default <password>
# if they follow the new protocol: both will work.
#
# The requirepass is not compatible with aclfile option and the ACL LOAD
# command, these will cause requirepass to be ignored.
#
# requirepass foobared

# New users are initialized with restrictive permissions by default, via the
# equivalent of this ACL rule 'off resetkeys -@all'. Starting with Redis 6.2, it
# is possible to manage access to Pub/Sub channels with ACL rules as well. The
# default Pub/Sub channels permission if new users is controlled by the
# acl-pubsub-default configuration directive, which accepts one of these values:
#
# allchannels: grants access to all Pub/Sub channels
# resetchannels: revokes access to all Pub/Sub channels
#
# From Redis 7.0, acl-pubsub-default defaults to 'resetchannels' permission.
#
# acl-pubsub-default resetchannels

# Command renaming (DEPRECATED).
#
# ------------------------------------------------------------------------
# WARNING: avoid using this option if possible. Instead use ACLs to remove
# commands from the default user, and put them only in some admin user you
# create for administrative purposes.
# ------------------------------------------------------------------------
#
# It is possible to change the name of dangerous commands in a shared
# environment. For instance the CONFIG command may be renamed into something
# hard to guess so that it will still be available for internal-use tools
# but not available for general clients.
#
# Example:
#
# rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52
#
# It is also possible to completely kill a command by renaming it into
# an empty string:
#
# rename-command CONFIG ""
#
# Please note that changing the name of commands that are logged into the
# AOF file or transmitted to replicas may cause problems.

################################### CLIENTS ####################################

# Set the max number of connected clients at the same time. By default
# this limit is set to 10000 clients, however if the Redis server is not
# able to configure the process file limit to allow for the specified limit
# the max number of allowed clients is set to the current file limit
# minus 32 (as Redis reserves a few file descriptors for internal uses).
#
# Once the limit is reached Redis will close all the new connections sending
# an error 'max number of clients reached'.
#
# IMPORTANT: When Redis Cluster is used, the max number of connections is also
# shared with the cluster bus: every node in the cluster will use two
# connections, one incoming and another outgoing. It is important to size the
# limit accordingly in case of very large clusters.
#
# maxclients 10000

############################## MEMORY MANAGEMENT ################################

# Set a memory usage limit to the specified amount of bytes.
# When the memory limit is reached Redis will try to remove keys
# according to the eviction policy selected (see maxmemory-policy).
#
# If Redis can't remove keys according to the policy, or if the policy is
# set to 'noeviction', Redis will start to reply with errors to commands
# that would use more memory, like SET, LPUSH, and so on, and will continue
# to reply to read-only commands like GET.
#
# This option is usually useful when using Redis as an LRU or LFU cache, or to
# set a hard memory limit for an instance (using the 'noeviction' policy).
#
# WARNING: If you have replicas attached to an instance with maxmemory on,
# the size of the output buffers needed to feed the replicas are subtracted
# from the used memory count, so that network problems / resyncs will
# not trigger a loop where keys are evicted, and in turn the output
# buffer of replicas is full with DELs of keys evicted triggering the deletion
# of more keys, and so forth until the database is completely emptied.
#
# In short... if you have replicas attached it is suggested that you set a lower
# limit for maxmemory so that there is some free RAM on the system for replica
# output buffers (but this is not needed if the policy is 'noeviction').
#
# maxmemory <bytes>

# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory
# is reached. You can select one from the following behaviors:
#
# volatile-lru -> Evict using approximated LRU, only keys with an expire set.
# allkeys-lru -> Evict any key using approximated LRU.
# volatile-lfu -> Evict using approximated LFU, only keys with an expire set.
# allkeys-lfu -> Evict any key using approximated LFU.
# volatile-random -> Remove a random key having an expire set.
# allkeys-random -> Remove a random key, any key.
# volatile-ttl -> Remove the key with the nearest expire time (minor TTL)
# noeviction -> Don't evict anything, just return an error on write operations.
#
# LRU means Least Recently Used
# LFU means Least Frequently Used
#
# Both LRU, LFU and volatile-ttl are implemented using approximated
# randomized algorithms.
#
# Note: with any of the above policies, when there are no suitable keys for
# eviction, Redis will return an error on write operations that require
# more memory. These are usually commands that create new keys, add data or
# modify existing keys. A few examples are: SET, INCR, HSET, LPUSH, SUNIONSTORE,
# SORT (due to the STORE argument), and EXEC (if the transaction includes any
# command that requires memory).
#
# The default is:
#
# maxmemory-policy noeviction

# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated
# algorithms (in order to save memory), so you can tune it for speed or
# accuracy. By default Redis will check five keys and pick the one that was
# used least recently, you can change the sample size using the following
# configuration directive.
#
# The default of 5 produces good enough results. 10 Approximates very closely
# true LRU but costs more CPU. 3 is faster but not very accurate.
#
# maxmemory-samples 5

# Eviction processing is designed to function well with the default setting.
# If there is an unusually large amount of write traffic, this value may need to
# be increased.  Decreasing this value may reduce latency at the risk of
# eviction processing effectiveness
#   0 = minimum latency, 10 = default, 100 = process without regard to latency
#
# maxmemory-eviction-tenacity 10

# Starting from Redis 5, by default a replica will ignore its maxmemory setting
# (unless it is promoted to master after a failover or manually). It means
# that the eviction of keys will be just handled by the master, sending the
# DEL commands to the replica as keys evict in the master side.
#
# This behavior ensures that masters and replicas stay consistent, and is usually
# what you want, however if your replica is writable, or you want the replica
# to have a different memory setting, and you are sure all the writes performed
# to the replica are idempotent, then you may change this default (but be sure
# to understand what you are doing).
#
# Note that since the replica by default does not evict, it may end using more
# memory than the one set via maxmemory (there are certain buffers that may
# be larger on the replica, or data structures may sometimes take more memory
# and so forth). So make sure you monitor your replicas and make sure they
# have enough memory to never hit a real out-of-memory condition before the
# master hits the configured maxmemory setting.
#
# replica-ignore-maxmemory yes

# Redis reclaims expired keys in two ways: upon access when those keys are
# found to be expired, and also in background, in what is called the
# "active expire key". The key space is slowly and interactively scanned
# looking for expired keys to reclaim, so that it is possible to free memory
# of keys that are expired and will never be accessed again in a short time.
#
# The default effort of the expire cycle will try to avoid having more than
# ten percent of expired keys still in memory, and will try to avoid consuming
# more than 25% of total memory and to add latency to the system. However
# it is possible to increase the expire "effort" that is normally set to
# "1", to a greater value, up to the value "10". At its maximum value the
# system will use more CPU, longer cycles (and technically may introduce
# more latency), and will tolerate less already expired keys still present
# in the system. It's a tradeoff between memory, CPU and latency.
#
# active-expire-effort 1

############################# LAZY FREEING ####################################

# Redis has two primitives to delete keys. One is called DEL and is a blocking
# deletion of the object. It means that the server stops processing new commands
# in order to reclaim all the memory associated with an object in a synchronous
# way. If the key deleted is associated with a small object, the time needed
# in order to execute the DEL command is very small and comparable to most other
# O(1) or O(log_N) commands in Redis. However if the key is associated with an
# aggregated value containing millions of elements, the server can block for
# a long time (even seconds) in order to complete the operation.
#
# For the above reasons Redis also offers non blocking deletion primitives
# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and
# FLUSHDB commands, in order to reclaim memory in background. Those commands
# are executed in constant time. Another thread will incrementally free the
# object in the background as fast as possible.
#
# DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.
# It's up to the design of the application to understand when it is a good
# idea to use one or the other. However the Redis server sometimes has to
# delete keys or flush the whole database as a side effect of other operations.
# Specifically Redis deletes objects independently of a user call in the
# following scenarios:
#
# 1) On eviction, because of the maxmemory and maxmemory policy configurations,
#    in order to make room for new data, without going over the specified
#    memory limit.
# 2) Because of expire: when a key with an associated time to live (see the
#    EXPIRE command) must be deleted from memory.
# 3) Because of a side effect of a command that stores data on a key that may
#    already exist. For example the RENAME command may delete the old key
#    content when it is replaced with another one. Similarly SUNIONSTORE
#    or SORT with STORE option may delete existing keys. The SET command
#    itself removes any old content of the specified key in order to replace
#    it with the specified string.
# 4) During replication, when a replica performs a full resynchronization with
#    its master, the content of the whole database is removed in order to
#    load the RDB file just transferred.
#
# In all the above cases the default is to delete objects in a blocking way,
# like if DEL was called. However you can configure each case specifically
# in order to instead release memory in a non-blocking way like if UNLINK
# was called, using the following configuration directives.

lazyfree-lazy-eviction no
lazyfree-lazy-expire no
lazyfree-lazy-server-del no
replica-lazy-flush no

# It is also possible, for the case when to replace the user code DEL calls
# with UNLINK calls is not easy, to modify the default behavior of the DEL
# command to act exactly like UNLINK, using the following configuration
# directive:

lazyfree-lazy-user-del no

# FLUSHDB, FLUSHALL, SCRIPT FLUSH and FUNCTION FLUSH support both asynchronous and synchronous
# deletion, which can be controlled by passing the [SYNC|ASYNC] flags into the
# commands. When neither flag is passed, this directive will be used to determine
# if the data should be deleted asynchronously.

lazyfree-lazy-user-flush no

################################ THREADED I/O #################################

# Redis is mostly single threaded, however there are certain threaded
# operations such as UNLINK, slow I/O accesses and other things that are
# performed on side threads.
#
# Now it is also possible to handle Redis clients socket reads and writes
# in different I/O threads. Since especially writing is so slow, normally
# Redis users use pipelining in order to speed up the Redis performances per
# core, and spawn multiple instances in order to scale more. Using I/O
# threads it is possible to easily speedup two times Redis without resorting
# to pipelining nor sharding of the instance.
#
# By default threading is disabled, we suggest enabling it only in machines
# that have at least 4 or more cores, leaving at least one spare core.
# Using more than 8 threads is unlikely to help much. We also recommend using
# threaded I/O only if you actually have performance problems, with Redis
# instances being able to use a quite big percentage of CPU time, otherwise
# there is no point in using this feature.
#
# So for instance if you have a four cores boxes, try to use 2 or 3 I/O
# threads, if you have a 8 cores, try to use 6 threads. In order to
# enable I/O threads use the following configuration directive:
#
# io-threads 4
#
# Setting io-threads to 1 will just use the main thread as usual.
# When I/O threads are enabled, we only use threads for writes, that is
# to thread the write(2) syscall and transfer the client buffers to the
# socket. However it is also possible to enable threading of reads and
# protocol parsing using the following configuration directive, by setting
# it to yes:
#
# io-threads-do-reads no
#
# Usually threading reads doesn't help much.
#
# NOTE 1: This configuration directive cannot be changed at runtime via
# CONFIG SET. Also, this feature currently does not work when SSL is
# enabled.
#
# NOTE 2: If you want to test the Redis speedup using redis-benchmark, make
# sure you also run the benchmark itself in threaded mode, using the
# --threads option to match the number of Redis threads, otherwise you'll not
# be able to notice the improvements.

############################ KERNEL OOM CONTROL ##############################

# On Linux, it is possible to hint the kernel OOM killer on what processes
# should be killed first when out of memory.
#
# Enabling this feature makes Redis actively control the oom_score_adj value
# for all its processes, depending on their role. The default scores will
# attempt to have background child processes killed before all others, and
# replicas killed before masters.
#
# Redis supports these options:
#
# no:       Don't make changes to oom-score-adj (default).
# yes:      Alias to "relative" see below.
# absolute: Values in oom-score-adj-values are written as is to the kernel.
# relative: Values are used relative to the initial value of oom_score_adj when
#           the server starts and are then clamped to a range of -1000 to 1000.
#           Because typically the initial value is 0, they will often match the
#           absolute values.
oom-score-adj no

# When oom-score-adj is used, this directive controls the specific values used
# for master, replica and background child processes. Values range -2000 to
# 2000 (higher means more likely to be killed).
#
# Unprivileged processes (not root, and without CAP_SYS_RESOURCE capabilities)
# can freely increase their value, but not decrease it below its initial
# settings. This means that setting oom-score-adj to "relative" and setting the
# oom-score-adj-values to positive values will always succeed.
oom-score-adj-values 0 200 800


#################### KERNEL transparent hugepage CONTROL ######################

# Usually the kernel Transparent Huge Pages control is set to "madvise" or
# or "never" by default (/sys/kernel/mm/transparent_hugepage/enabled), in which
# case this config has no effect. On systems in which it is set to "always",
# redis will attempt to disable it specifically for the redis process in order
# to avoid latency problems specifically with fork(2) and CoW.
# If for some reason you prefer to keep it enabled, you can set this config to
# "no" and the kernel global to "always".

disable-thp yes

############################## APPEND ONLY MODE ###############################

# By default Redis asynchronously dumps the dataset on disk. This mode is
# good enough in many applications, but an issue with the Redis process or
# a power outage may result into a few minutes of writes lost (depending on
# the configured save points).
#
# The Append Only File is an alternative persistence mode that provides
# much better durability. For instance using the default data fsync policy
# (see later in the config file) Redis can lose just one second of writes in a
# dramatic event like a server power outage, or a single write if something
# wrong with the Redis process itself happens, but the operating system is
# still running correctly.
#
# AOF and RDB persistence can be enabled at the same time without problems.
# If the AOF is enabled on startup Redis will load the AOF, that is the file
# with the better durability guarantees.
#
# Please check https://redis.io/topics/persistence for more information.

appendonly no

# The base name of the append only file.
#
# Redis 7 and newer use a set of append-only files to persist the dataset
# and changes applied to it. There are two basic types of files in use:
#
# - Base files, which are a snapshot representing the complete state of the
#   dataset at the time the file was created. Base files can be either in
#   the form of RDB (binary serialized) or AOF (textual commands).
# - Incremental files, which contain additional commands that were applied
#   to the dataset following the previous file.
#
# In addition, manifest files are used to track the files and the order in
# which they were created and should be applied.
#
# Append-only file names are created by Redis following a specific pattern.
# The file name's prefix is based on the 'appendfilename' configuration
# parameter, followed by additional information about the sequence and type.
#
# For example, if appendfilename is set to appendonly.aof, the following file
# names could be derived:
#
# - appendonly.aof.1.base.rdb as a base file.
# - appendonly.aof.1.incr.aof, appendonly.aof.2.incr.aof as incremental files.
# - appendonly.aof.manifest as a manifest file.

appendfilename "appendonly.aof"

# For convenience, Redis stores all persistent append-only files in a dedicated
# directory. The name of the directory is determined by the appenddirname
# configuration parameter.

appenddirname "appendonlydir"

# The fsync() call tells the Operating System to actually write data on disk
# instead of waiting for more data in the output buffer. Some OS will really flush
# data on disk, some other OS will just try to do it ASAP.
#
# Redis supports three different modes:
#
# no: don't fsync, just let the OS flush the data when it wants. Faster.
# always: fsync after every write to the append only log. Slow, Safest.
# everysec: fsync only one time every second. Compromise.
#
# The default is "everysec", as that's usually the right compromise between
# speed and data safety. It's up to you to understand if you can relax this to
# "no" that will let the operating system flush the output buffer when
# it wants, for better performances (but if you can live with the idea of
# some data loss consider the default persistence mode that's snapshotting),
# or on the contrary, use "always" that's very slow but a bit safer than
# everysec.
#
# More details please check the following article:
# http://antirez.com/post/redis-persistence-demystified.html
#
# If unsure, use "everysec".

# appendfsync always
appendfsync everysec
# appendfsync no

# When the AOF fsync policy is set to always or everysec, and a background
# saving process (a background save or AOF log background rewriting) is
# performing a lot of I/O against the disk, in some Linux configurations
# Redis may block too long on the fsync() call. Note that there is no fix for
# this currently, as even performing fsync in a different thread will block
# our synchronous write(2) call.
#
# In order to mitigate this problem it's possible to use the following option
# that will prevent fsync() from being called in the main process while a
# BGSAVE or BGREWRITEAOF is in progress.
#
# This means that while another child is saving, the durability of Redis is
# the same as "appendfsync no". In practical terms, this means that it is
# possible to lose up to 30 seconds of log in the worst scenario (with the
# default Linux settings).
#
# If you have latency problems turn this to "yes". Otherwise leave it as
# "no" that is the safest pick from the point of view of durability.

no-appendfsync-on-rewrite no

# Automatic rewrite of the append only file.
# Redis is able to automatically rewrite the log file implicitly calling
# BGREWRITEAOF when the AOF log size grows by the specified percentage.
#
# This is how it works: Redis remembers the size of the AOF file after the
# latest rewrite (if no rewrite has happened since the restart, the size of
# the AOF at startup is used).
#
# This base size is compared to the current size. If the current size is
# bigger than the specified percentage, the rewrite is triggered. Also
# you need to specify a minimal size for the AOF file to be rewritten, this
# is useful to avoid rewriting the AOF file even if the percentage increase
# is reached but it is still pretty small.
#
# Specify a percentage of zero in order to disable the automatic AOF
# rewrite feature.

auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# An AOF file may be found to be truncated at the end during the Redis
# startup process, when the AOF data gets loaded back into memory.
# This may happen when the system where Redis is running
# crashes, especially when an ext4 filesystem is mounted without the
# data=ordered option (however this can't happen when Redis itself
# crashes or aborts but the operating system still works correctly).
#
# Redis can either exit with an error when this happens, or load as much
# data as possible (the default now) and start if the AOF file is found
# to be truncated at the end. The following option controls this behavior.
#
# If aof-load-truncated is set to yes, a truncated AOF file is loaded and
# the Redis server starts emitting a log to inform the user of the event.
# Otherwise if the option is set to no, the server aborts with an error
# and refuses to start. When the option is set to no, the user requires
# to fix the AOF file using the "redis-check-aof" utility before to restart
# the server.
#
# Note that if the AOF file will be found to be corrupted in the middle
# the server will still exit with an error. This option only applies when
# Redis will try to read more data from the AOF file but not enough bytes
# will be found.
aof-load-truncated yes

# Redis can create append-only base files in either RDB or AOF formats. Using
# the RDB format is always faster and more efficient, and disabling it is only
# supported for backward compatibility purposes.
aof-use-rdb-preamble yes

# Redis supports recording timestamp annotations in the AOF to support restoring
# the data from a specific point-in-time. However, using this capability changes
# the AOF format in a way that may not be compatible with existing AOF parsers.
aof-timestamp-enabled no

################################ SHUTDOWN #####################################

# Maximum time to wait for replicas when shutting down, in seconds.
#
# During shut down, a grace period allows any lagging replicas to catch up with
# the latest replication offset before the master exists. This period can
# prevent data loss, especially for deployments without configured disk backups.
#
# The 'shutdown-timeout' value is the grace period's duration in seconds. It is
# only applicable when the instance has replicas. To disable the feature, set
# the value to 0.
#
# shutdown-timeout 10

# When Redis receives a SIGINT or SIGTERM, shutdown is initiated and by default
# an RDB snapshot is written to disk in a blocking operation if save points are configured.
# The options used on signaled shutdown can include the following values:
# default:  Saves RDB snapshot only if save points are configured.
#           Waits for lagging replicas to catch up.
# save:     Forces a DB saving operation even if no save points are configured.
# nosave:   Prevents DB saving operation even if one or more save points are configured.
# now:      Skips waiting for lagging replicas.
# force:    Ignores any errors that would normally prevent the server from exiting.
#
# Any combination of values is allowed as long as "save" and "nosave" are not set simultaneously.
# Example: "nosave force now"
#
# shutdown-on-sigint default
# shutdown-on-sigterm default

################ NON-DETERMINISTIC LONG BLOCKING COMMANDS #####################

# Maximum time in milliseconds for EVAL scripts, functions and in some cases
# modules' commands before Redis can start processing or rejecting other clients.
#
# If the maximum execution time is reached Redis will start to reply to most
# commands with a BUSY error.
#
# In this state Redis will only allow a handful of commands to be executed.
# For instance, SCRIPT KILL, FUNCTION KILL, SHUTDOWN NOSAVE and possibly some
# module specific 'allow-busy' commands.
#
# SCRIPT KILL and FUNCTION KILL will only be able to stop a script that did not
# yet call any write commands, so SHUTDOWN NOSAVE may be the only way to stop
# the server in the case a write command was already issued by the script when
# the user doesn't want to wait for the natural termination of the script.
#
# The default is 5 seconds. It is possible to set it to 0 or a negative value
# to disable this mechanism (uninterrupted execution). Note that in the past
# this config had a different name, which is now an alias, so both of these do
# the same:
# lua-time-limit 5000
# busy-reply-threshold 5000

################################ REDIS CLUSTER  ###############################

# Normal Redis instances can't be part of a Redis Cluster; only nodes that are
# started as cluster nodes can. In order to start a Redis instance as a
# cluster node enable the cluster support uncommenting the following:
#
# cluster-enabled yes

# Every cluster node has a cluster configuration file. This file is not
# intended to be edited by hand. It is created and updated by Redis nodes.
# Every Redis Cluster node requires a different cluster configuration file.
# Make sure that instances running in the same system do not have
# overlapping cluster configuration file names.
#
# cluster-config-file nodes-6379.conf

# Cluster node timeout is the amount of milliseconds a node must be unreachable
# for it to be considered in failure state.
# Most other internal time limits are a multiple of the node timeout.
#
# cluster-node-timeout 15000

# The cluster port is the port that the cluster bus will listen for inbound connections on. When set 
# to the default value, 0, it will be bound to the command port + 10000. Setting this value requires 
# you to specify the cluster bus port when executing cluster meet.
# cluster-port 0

# A replica of a failing master will avoid to start a failover if its data
# looks too old.
#
# There is no simple way for a replica to actually have an exact measure of
# its "data age", so the following two checks are performed:
#
# 1) If there are multiple replicas able to failover, they exchange messages
#    in order to try to give an advantage to the replica with the best
#    replication offset (more data from the master processed).
#    Replicas will try to get their rank by offset, and apply to the start
#    of the failover a delay proportional to their rank.
#
# 2) Every single replica computes the time of the last interaction with
#    its master. This can be the last ping or command received (if the master
#    is still in the "connected" state), or the time that elapsed since the
#    disconnection with the master (if the replication link is currently down).
#    If the last interaction is too old, the replica will not try to failover
#    at all.
#
# The point "2" can be tuned by user. Specifically a replica will not perform
# the failover if, since the last interaction with the master, the time
# elapsed is greater than:
#
#   (node-timeout * cluster-replica-validity-factor) + repl-ping-replica-period
#
# So for example if node-timeout is 30 seconds, and the cluster-replica-validity-factor
# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the
# replica will not try to failover if it was not able to talk with the master
# for longer than 310 seconds.
#
# A large cluster-replica-validity-factor may allow replicas with too old data to failover
# a master, while a too small value may prevent the cluster from being able to
# elect a replica at all.
#
# For maximum availability, it is possible to set the cluster-replica-validity-factor
# to a value of 0, which means, that replicas will always try to failover the
# master regardless of the last time they interacted with the master.
# (However they'll always try to apply a delay proportional to their
# offset rank).
#
# Zero is the only value able to guarantee that when all the partitions heal
# the cluster will always be able to continue.
#
# cluster-replica-validity-factor 10

# Cluster replicas are able to migrate to orphaned masters, that are masters
# that are left without working replicas. This improves the cluster ability
# to resist to failures as otherwise an orphaned master can't be failed over
# in case of failure if it has no working replicas.
#
# Replicas migrate to orphaned masters only if there are still at least a
# given number of other working replicas for their old master. This number
# is the "migration barrier". A migration barrier of 1 means that a replica
# will migrate only if there is at least 1 other working replica for its master
# and so forth. It usually reflects the number of replicas you want for every
# master in your cluster.
#
# Default is 1 (replicas migrate only if their masters remain with at least
# one replica). To disable migration just set it to a very large value or
# set cluster-allow-replica-migration to 'no'.
# A value of 0 can be set but is useful only for debugging and dangerous
# in production.
#
# cluster-migration-barrier 1

# Turning off this option allows to use less automatic cluster configuration.
# It both disables migration to orphaned masters and migration from masters
# that became empty.
#
# Default is 'yes' (allow automatic migrations).
#
# cluster-allow-replica-migration yes

# By default Redis Cluster nodes stop accepting queries if they detect there
# is at least a hash slot uncovered (no available node is serving it).
# This way if the cluster is partially down (for example a range of hash slots
# are no longer covered) all the cluster becomes, eventually, unavailable.
# It automatically returns available as soon as all the slots are covered again.
#
# However sometimes you want the subset of the cluster which is working,
# to continue to accept queries for the part of the key space that is still
# covered. In order to do so, just set the cluster-require-full-coverage
# option to no.
#
# cluster-require-full-coverage yes

# This option, when set to yes, prevents replicas from trying to failover its
# master during master failures. However the replica can still perform a
# manual failover, if forced to do so.
#
# This is useful in different scenarios, especially in the case of multiple
# data center operations, where we want one side to never be promoted if not
# in the case of a total DC failure.
#
# cluster-replica-no-failover no

# This option, when set to yes, allows nodes to serve read traffic while the
# cluster is in a down state, as long as it believes it owns the slots.
#
# This is useful for two cases.  The first case is for when an application
# doesn't require consistency of data during node failures or network partitions.
# One example of this is a cache, where as long as the node has the data it
# should be able to serve it.
#
# The second use case is for configurations that don't meet the recommended
# three shards but want to enable cluster mode and scale later. A
# master outage in a 1 or 2 shard configuration causes a read/write outage to the
# entire cluster without this option set, with it set there is only a write outage.
# Without a quorum of masters, slot ownership will not change automatically.
#
# cluster-allow-reads-when-down no

# This option, when set to yes, allows nodes to serve pubsub shard traffic while
# the cluster is in a down state, as long as it believes it owns the slots.
#
# This is useful if the application would like to use the pubsub feature even when
# the cluster global stable state is not OK. If the application wants to make sure only
# one shard is serving a given channel, this feature should be kept as yes.
#
# cluster-allow-pubsubshard-when-down yes

# Cluster link send buffer limit is the limit on the memory usage of an individual
# cluster bus link's send buffer in bytes. Cluster links would be freed if they exceed
# this limit. This is to primarily prevent send buffers from growing unbounded on links
# toward slow peers (E.g. PubSub messages being piled up).
# This limit is disabled by default. Enable this limit when 'mem_cluster_links' INFO field
# and/or 'send-buffer-allocated' entries in the 'CLUSTER LINKS` command output continuously increase.
# Minimum limit of 1gb is recommended so that cluster link buffer can fit in at least a single
# PubSub message by default. (client-query-buffer-limit default value is 1gb)
#
# cluster-link-sendbuf-limit 0
 
# Clusters can configure their announced hostname using this config. This is a common use case for 
# applications that need to use TLS Server Name Indication (SNI) or dealing with DNS based
# routing. By default this value is only shown as additional metadata in the CLUSTER SLOTS
# command, but can be changed using 'cluster-preferred-endpoint-type' config. This value is 
# communicated along the clusterbus to all nodes, setting it to an empty string will remove 
# the hostname and also propagate the removal.
#
# cluster-announce-hostname ""

# Clusters can advertise how clients should connect to them using either their IP address,
# a user defined hostname, or by declaring they have no endpoint. Which endpoint is
# shown as the preferred endpoint is set by using the cluster-preferred-endpoint-type
# config with values 'ip', 'hostname', or 'unknown-endpoint'. This value controls how
# the endpoint returned for MOVED/ASKING requests as well as the first field of CLUSTER SLOTS. 
# If the preferred endpoint type is set to hostname, but no announced hostname is set, a '?' 
# will be returned instead.
#
# When a cluster advertises itself as having an unknown endpoint, it's indicating that
# the server doesn't know how clients can reach the cluster. This can happen in certain 
# networking situations where there are multiple possible routes to the node, and the 
# server doesn't know which one the client took. In this case, the server is expecting
# the client to reach out on the same endpoint it used for making the last request, but use
# the port provided in the response.
#
# cluster-preferred-endpoint-type ip

# In order to setup your cluster make sure to read the documentation
# available at https://redis.io web site.

########################## CLUSTER DOCKER/NAT support  ########################

# In certain deployments, Redis Cluster nodes address discovery fails, because
# addresses are NAT-ted or because ports are forwarded (the typical case is
# Docker and other containers).
#
# In order to make Redis Cluster working in such environments, a static
# configuration where each node knows its public address is needed. The
# following four options are used for this scope, and are:
#
# * cluster-announce-ip
# * cluster-announce-port
# * cluster-announce-tls-port
# * cluster-announce-bus-port
#
# Each instructs the node about its address, client ports (for connections
# without and with TLS) and cluster message bus port. The information is then
# published in the header of the bus packets so that other nodes will be able to
# correctly map the address of the node publishing the information.
#
# If cluster-tls is set to yes and cluster-announce-tls-port is omitted or set
# to zero, then cluster-announce-port refers to the TLS port. Note also that
# cluster-announce-tls-port has no effect if cluster-tls is set to no.
#
# If the above options are not used, the normal Redis Cluster auto-detection
# will be used instead.
#
# Note that when remapped, the bus port may not be at the fixed offset of
# clients port + 10000, so you can specify any port and bus-port depending
# on how they get remapped. If the bus-port is not set, a fixed offset of
# 10000 will be used as usual.
#
# Example:
#
# cluster-announce-ip 10.1.1.5
# cluster-announce-tls-port 6379
# cluster-announce-port 0
# cluster-announce-bus-port 6380

################################## SLOW LOG ###################################

# The Redis Slow Log is a system to log queries that exceeded a specified
# execution time. The execution time does not include the I/O operations
# like talking with the client, sending the reply and so forth,
# but just the time needed to actually execute the command (this is the only
# stage of command execution where the thread is blocked and can not serve
# other requests in the meantime).
#
# You can configure the slow log with two parameters: one tells Redis
# what is the execution time, in microseconds, to exceed in order for the
# command to get logged, and the other parameter is the length of the
# slow log. When a new command is logged the oldest one is removed from the
# queue of logged commands.

# The following time is expressed in microseconds, so 1000000 is equivalent
# to one second. Note that a negative number disables the slow log, while
# a value of zero forces the logging of every command.
slowlog-log-slower-than 10000

# There is no limit to this length. Just be aware that it will consume memory.
# You can reclaim memory used by the slow log with SLOWLOG RESET.
slowlog-max-len 128

################################ LATENCY MONITOR ##############################

# The Redis latency monitoring subsystem samples different operations
# at runtime in order to collect data related to possible sources of
# latency of a Redis instance.
#
# Via the LATENCY command this information is available to the user that can
# print graphs and obtain reports.
#
# The system only logs operations that were performed in a time equal or
# greater than the amount of milliseconds specified via the
# latency-monitor-threshold configuration directive. When its value is set
# to zero, the latency monitor is turned off.
#
# By default latency monitoring is disabled since it is mostly not needed
# if you don't have latency issues, and collecting data has a performance
# impact, that while very small, can be measured under big load. Latency
# monitoring can easily be enabled at runtime using the command
# "CONFIG SET latency-monitor-threshold <milliseconds>" if needed.
latency-monitor-threshold 0

################################ LATENCY TRACKING ##############################

# The Redis extended latency monitoring tracks the per command latencies and enables
# exporting the percentile distribution via the INFO latencystats command,
# and cumulative latency distributions (histograms) via the LATENCY command.
#
# By default, the extended latency monitoring is enabled since the overhead
# of keeping track of the command latency is very small.
# latency-tracking yes

# By default the exported latency percentiles via the INFO latencystats command
# are the p50, p99, and p999.
# latency-tracking-info-percentiles 50 99 99.9

############################# EVENT NOTIFICATION ##############################

# Redis can notify Pub/Sub clients about events happening in the key space.
# This feature is documented at https://redis.io/topics/notifications
#
# For instance if keyspace events notification is enabled, and a client
# performs a DEL operation on key "foo" stored in the Database 0, two
# messages will be published via Pub/Sub:
#
# PUBLISH __keyspace@0__:foo del
# PUBLISH __keyevent@0__:del foo
#
# It is possible to select the events that Redis will notify among a set
# of classes. Every class is identified by a single character:
#
#  K     Keyspace events, published with __keyspace@<db>__ prefix.
#  E     Keyevent events, published with __keyevent@<db>__ prefix.
#  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...
#  $     String commands
#  l     List commands
#  s     Set commands
#  h     Hash commands
#  z     Sorted set commands
#  x     Expired events (events generated every time a key expires)
#  e     Evicted events (events generated when a key is evicted for maxmemory)
#  n     New key events (Note: not included in the 'A' class)
#  t     Stream commands
#  d     Module key type events
#  m     Key-miss events (Note: It is not included in the 'A' class)
#  A     Alias for g$lshzxetd, so that the "AKE" string means all the events
#        (Except key-miss events which are excluded from 'A' due to their
#         unique nature).
#
#  The "notify-keyspace-events" takes as argument a string that is composed
#  of zero or multiple characters. The empty string means that notifications
#  are disabled.
#
#  Example: to enable list and generic events, from the point of view of the
#           event name, use:
#
#  notify-keyspace-events Elg
#
#  Example 2: to get the stream of the expired keys subscribing to channel
#             name __keyevent@0__:expired use:
#
#  notify-keyspace-events Ex
#
#  By default all notifications are disabled because most users don't need
#  this feature and the feature has some overhead. Note that if you don't
#  specify at least one of K or E, no events will be delivered.
notify-keyspace-events ""

############################### ADVANCED CONFIG ###############################

# Hashes are encoded using a memory efficient data structure when they have a
# small number of entries, and the biggest entry does not exceed a given
# threshold. These thresholds can be configured using the following directives.
hash-max-listpack-entries 512
hash-max-listpack-value 64

# Lists are also encoded in a special way to save a lot of space.
# The number of entries allowed per internal list node can be specified
# as a fixed maximum size or a maximum number of elements.
# For a fixed maximum size, use -5 through -1, meaning:
# -5: max size: 64 Kb  <-- not recommended for normal workloads
# -4: max size: 32 Kb  <-- not recommended
# -3: max size: 16 Kb  <-- probably not recommended
# -2: max size: 8 Kb   <-- good
# -1: max size: 4 Kb   <-- good
# Positive numbers mean store up to _exactly_ that number of elements
# per list node.
# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),
# but if your use case is unique, adjust the settings as necessary.
list-max-listpack-size -2

# Lists may also be compressed.
# Compress depth is the number of quicklist ziplist nodes from *each* side of
# the list to *exclude* from compression.  The head and tail of the list
# are always uncompressed for fast push/pop operations.  Settings are:
# 0: disable all list compression
# 1: depth 1 means "don't start compressing until after 1 node into the list,
#    going from either the head or tail"
#    So: [head]->node->node->...->node->[tail]
#    [head], [tail] will always be uncompressed; inner nodes will compress.
# 2: [head]->[next]->node->node->...->node->[prev]->[tail]
#    2 here means: don't compress head or head->next or tail->prev or tail,
#    but compress all nodes between them.
# 3: [head]->[next]->[next]->node->node->...->node->[prev]->[prev]->[tail]
# etc.
list-compress-depth 0

# Sets have a special encoding in just one case: when a set is composed
# of just strings that happen to be integers in radix 10 in the range
# of 64 bit signed integers.
# The following configuration setting sets the limit in the size of the
# set in order to use this special memory saving encoding.
set-max-intset-entries 512

# Similarly to hashes and lists, sorted sets are also specially encoded in
# order to save a lot of space. This encoding is only used when the length and
# elements of a sorted set are below the following limits:
zset-max-listpack-entries 128
zset-max-listpack-value 64

# HyperLogLog sparse representation bytes limit. The limit includes the
# 16 bytes header. When an HyperLogLog using the sparse representation crosses
# this limit, it is converted into the dense representation.
#
# A value greater than 16000 is totally useless, since at that point the
# dense representation is more memory efficient.
#
# The suggested value is ~ 3000 in order to have the benefits of
# the space efficient encoding without slowing down too much PFADD,
# which is O(N) with the sparse encoding. The value can be raised to
# ~ 10000 when CPU is not a concern, but space is, and the data set is
# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.
hll-sparse-max-bytes 3000

# Streams macro node max size / items. The stream data structure is a radix
# tree of big nodes that encode multiple items inside. Using this configuration
# it is possible to configure how big a single node can be in bytes, and the
# maximum number of items it may contain before switching to a new node when
# appending new stream entries. If any of the following settings are set to
# zero, the limit is ignored, so for instance it is possible to set just a
# max entries limit by setting max-bytes to 0 and max-entries to the desired
# value.
stream-node-max-bytes 4096
stream-node-max-entries 100

# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in
# order to help rehashing the main Redis hash table (the one mapping top-level
# keys to values). The hash table implementation Redis uses (see dict.c)
# performs a lazy rehashing: the more operation you run into a hash table
# that is rehashing, the more rehashing "steps" are performed, so if the
# server is idle the rehashing is never complete and some more memory is used
# by the hash table.
#
# The default is to use this millisecond 10 times every second in order to
# actively rehash the main dictionaries, freeing memory when possible.
#
# If unsure:
# use "activerehashing no" if you have hard latency requirements and it is
# not a good thing in your environment that Redis can reply from time to time
# to queries with 2 milliseconds delay.
#
# use "activerehashing yes" if you don't have such hard requirements but
# want to free memory asap when possible.
activerehashing yes

# The client output buffer limits can be used to force disconnection of clients
# that are not reading data from the server fast enough for some reason (a
# common reason is that a Pub/Sub client can't consume messages as fast as the
# publisher can produce them).
#
# The limit can be set differently for the three different classes of clients:
#
# normal -> normal clients including MONITOR clients
# replica -> replica clients
# pubsub -> clients subscribed to at least one pubsub channel or pattern
#
# The syntax of every client-output-buffer-limit directive is the following:
#
# client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds>
#
# A client is immediately disconnected once the hard limit is reached, or if
# the soft limit is reached and remains reached for the specified number of
# seconds (continuously).
# So for instance if the hard limit is 32 megabytes and the soft limit is
# 16 megabytes / 10 seconds, the client will get disconnected immediately
# if the size of the output buffers reach 32 megabytes, but will also get
# disconnected if the client reaches 16 megabytes and continuously overcomes
# the limit for 10 seconds.
#
# By default normal clients are not limited because they don't receive data
# without asking (in a push way), but just after a request, so only
# asynchronous clients may create a scenario where data is requested faster
# than it can read.
#
# Instead there is a default limit for pubsub and replica clients, since
# subscribers and replicas receive data in a push fashion.
#
# Note that it doesn't make sense to set the replica clients output buffer
# limit lower than the repl-backlog-size config (partial sync will succeed
# and then replica will get disconnected).
# Such a configuration is ignored (the size of repl-backlog-size will be used).
# This doesn't have memory consumption implications since the replica client
# will share the backlog buffers memory.
#
# Both the hard or the soft limit can be disabled by setting them to zero.
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit replica 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60

# Client query buffers accumulate new commands. They are limited to a fixed
# amount by default in order to avoid that a protocol desynchronization (for
# instance due to a bug in the client) will lead to unbound memory usage in
# the query buffer. However you can configure it here if you have very special
# needs, such us huge multi/exec requests or alike.
#
# client-query-buffer-limit 1gb

# In some scenarios client connections can hog up memory leading to OOM
# errors or data eviction. To avoid this we can cap the accumulated memory
# used by all client connections (all pubsub and normal clients). Once we
# reach that limit connections will be dropped by the server freeing up
# memory. The server will attempt to drop the connections using the most 
# memory first. We call this mechanism "client eviction".
#
# Client eviction is configured using the maxmemory-clients setting as follows:
# 0 - client eviction is disabled (default)
#
# A memory value can be used for the client eviction threshold,
# for example:
# maxmemory-clients 1g
#
# A percentage value (between 1% and 100%) means the client eviction threshold
# is based on a percentage of the maxmemory setting. For example to set client
# eviction at 5% of maxmemory:
# maxmemory-clients 5%

# In the Redis protocol, bulk requests, that are, elements representing single
# strings, are normally limited to 512 mb. However you can change this limit
# here, but must be 1mb or greater
#
# proto-max-bulk-len 512mb

# Redis calls an internal function to perform many background tasks, like
# closing connections of clients in timeout, purging expired keys that are
# never requested, and so forth.
#
# Not all tasks are performed with the same frequency, but Redis checks for
# tasks to perform according to the specified "hz" value.
#
# By default "hz" is set to 10. Raising the value will use more CPU when
# Redis is idle, but at the same time will make Redis more responsive when
# there are many keys expiring at the same time, and timeouts may be
# handled with more precision.
#
# The range is between 1 and 500, however a value over 100 is usually not
# a good idea. Most users should use the default of 10 and raise this up to
# 100 only in environments where very low latency is required.
hz 10

# Normally it is useful to have an HZ value which is proportional to the
# number of clients connected. This is useful in order, for instance, to
# avoid too many clients are processed for each background task invocation
# in order to avoid latency spikes.
#
# Since the default HZ value by default is conservatively set to 10, Redis
# offers, and enables by default, the ability to use an adaptive HZ value
# which will temporarily raise when there are many connected clients.
#
# When dynamic HZ is enabled, the actual configured HZ will be used
# as a baseline, but multiples of the configured HZ value will be actually
# used as needed once more clients are connected. In this way an idle
# instance will use very little CPU time while a busy instance will be
# more responsive.
dynamic-hz yes

# When a child rewrites the AOF file, if the following option is enabled
# the file will be fsync-ed every 4 MB of data generated. This is useful
# in order to commit the file to the disk more incrementally and avoid
# big latency spikes.
aof-rewrite-incremental-fsync yes

# When redis saves RDB file, if the following option is enabled
# the file will be fsync-ed every 4 MB of data generated. This is useful
# in order to commit the file to the disk more incrementally and avoid
# big latency spikes.
rdb-save-incremental-fsync yes

# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good
# idea to start with the default settings and only change them after investigating
# how to improve the performances and how the keys LFU change over time, which
# is possible to inspect via the OBJECT FREQ command.
#
# There are two tunable parameters in the Redis LFU implementation: the
# counter logarithm factor and the counter decay time. It is important to
# understand what the two parameters mean before changing them.
#
# The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis
# uses a probabilistic increment with logarithmic behavior. Given the value
# of the old counter, when a key is accessed, the counter is incremented in
# this way:
#
# 1. A random number R between 0 and 1 is extracted.
# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).
# 3. The counter is incremented only if R < P.
#
# The default lfu-log-factor is 10. This is a table of how the frequency
# counter changes with a different number of accesses with different
# logarithmic factors:
#
# +--------+------------+------------+------------+------------+------------+
# | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   |
# +--------+------------+------------+------------+------------+------------+
# | 0      | 104        | 255        | 255        | 255        | 255        |
# +--------+------------+------------+------------+------------+------------+
# | 1      | 18         | 49         | 255        | 255        | 255        |
# +--------+------------+------------+------------+------------+------------+
# | 10     | 10         | 18         | 142        | 255        | 255        |
# +--------+------------+------------+------------+------------+------------+
# | 100    | 8          | 11         | 49         | 143        | 255        |
# +--------+------------+------------+------------+------------+------------+
#
# NOTE: The above table was obtained by running the following commands:
#
#   redis-benchmark -n 1000000 incr foo
#   redis-cli object freq foo
#
# NOTE 2: The counter initial value is 5 in order to give new objects a chance
# to accumulate hits.
#
# The counter decay time is the time, in minutes, that must elapse in order
# for the key counter to be divided by two (or decremented if it has a value
# less <= 10).
#
# The default value for the lfu-decay-time is 1. A special value of 0 means to
# decay the counter every time it happens to be scanned.
#
# lfu-log-factor 10
# lfu-decay-time 1

########################### ACTIVE DEFRAGMENTATION #######################
#
# What is active defragmentation?
# -------------------------------
#
# Active (online) defragmentation allows a Redis server to compact the
# spaces left between small allocations and deallocations of data in memory,
# thus allowing to reclaim back memory.
#
# Fragmentation is a natural process that happens with every allocator (but
# less so with Jemalloc, fortunately) and certain workloads. Normally a server
# restart is needed in order to lower the fragmentation, or at least to flush
# away all the data and create it again. However thanks to this feature
# implemented by Oran Agra for Redis 4.0 this process can happen at runtime
# in a "hot" way, while the server is running.
#
# Basically when the fragmentation is over a certain level (see the
# configuration options below) Redis will start to create new copies of the
# values in contiguous memory regions by exploiting certain specific Jemalloc
# features (in order to understand if an allocation is causing fragmentation
# and to allocate it in a better place), and at the same time, will release the
# old copies of the data. This process, repeated incrementally for all the keys
# will cause the fragmentation to drop back to normal values.
#
# Important things to understand:
#
# 1. This feature is disabled by default, and only works if you compiled Redis
#    to use the copy of Jemalloc we ship with the source code of Redis.
#    This is the default with Linux builds.
#
# 2. You never need to enable this feature if you don't have fragmentation
#    issues.
#
# 3. Once you experience fragmentation, you can enable this feature when
#    needed with the command "CONFIG SET activedefrag yes".
#
# The configuration parameters are able to fine tune the behavior of the
# defragmentation process. If you are not sure about what they mean it is
# a good idea to leave the defaults untouched.

# Active defragmentation is disabled by default
# activedefrag no

# Minimum amount of fragmentation waste to start active defrag
# active-defrag-ignore-bytes 100mb

# Minimum percentage of fragmentation to start active defrag
# active-defrag-threshold-lower 10

# Maximum percentage of fragmentation at which we use maximum effort
# active-defrag-threshold-upper 100

# Minimal effort for defrag in CPU percentage, to be used when the lower
# threshold is reached
# active-defrag-cycle-min 1

# Maximal effort for defrag in CPU percentage, to be used when the upper
# threshold is reached
# active-defrag-cycle-max 25

# Maximum number of set/hash/zset/list fields that will be processed from
# the main dictionary scan
# active-defrag-max-scan-fields 1000

# Jemalloc background thread for purging will be enabled by default
jemalloc-bg-thread yes

# It is possible to pin different threads and processes of Redis to specific
# CPUs in your system, in order to maximize the performances of the server.
# This is useful both in order to pin different Redis threads in different
# CPUs, but also in order to make sure that multiple Redis instances running
# in the same host will be pinned to different CPUs.
#
# Normally you can do this using the "taskset" command, however it is also
# possible to this via Redis configuration directly, both in Linux and FreeBSD.
#
# You can pin the server/IO threads, bio threads, aof rewrite child process, and
# the bgsave child process. The syntax to specify the cpu list is the same as
# the taskset command:
#
# Set redis server/io threads to cpu affinity 0,2,4,6:
# server_cpulist 0-7:2
#
# Set bio threads to cpu affinity 1,3:
# bio_cpulist 1,3
#
# Set aof rewrite child process to cpu affinity 8,9,10,11:
# aof_rewrite_cpulist 8-11
#
# Set bgsave child process to cpu affinity 1,10,11
# bgsave_cpulist 1,10-11

# In some cases redis will emit warnings and even refuse to start if it detects
# that the system is in bad state, it is possible to suppress these warnings
# by setting the following config which takes a space delimited list of warnings
# to suppress
#
# ignore-warnings ARM64-COW-BUG
```

### `builtin/builtin/redis/pkg.py`

```python
"""
This module provides classes and methods to launch Redis.
Redis cluster is used if the hostfile has many hosts
"""
from jarvis_cd.core.pkg import Application, Color
from jarvis_cd.shell import Exec, LocalExecInfo, PsshExecInfo
from jarvis_cd.shell.process import Kill


class Redis(Application):
    """
    This class provides methods to launch the Ior application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'port',
                'msg': 'The port to use for the cluster',
                'type': int,
                'default': 6379,
                'choices': [],
                'args': [],
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        # Create the redis hostfile
        self.copy_template_file(f'{self.pkg_dir}/config/redis.conf',
                                f'{self.shared_dir}/redis.conf',
                                {
                                    'PORT': self.config['port']
                                })

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        hostfile = self.hostfile
        host_str = [f'{host}:{self.config["port"]}' for host in hostfile.hosts]
        host_str = ' '.join(host_str)
        cluster_config_file = f'{self.private_dir}/nodes.conf'
        # Create redis servers
        self.log('Starting individual servers', color=Color.YELLOW)
        cmd = [
            'redis-server',
            f'{self.shared_dir}/redis.conf',
        ]
        if len(hostfile) > 1:
            cmd += [
                f'--cluster-enabled yes',
                f'--cluster-config-file {cluster_config_file}',
                f'--cluster-node-timeout 5000',
            ]

        cmd = ' '.join(cmd)
        Exec(cmd,
             PsshExecInfo(env=self.mod_env,
                          hostfile=hostfile,
                          exec_async=True)).run()
        self.log(f'Sleeping for {self.config["sleep"]} seconds', color=Color.YELLOW)
        time.sleep(self.config['sleep'])

        # Create redis clients
        if len(hostfile) > 1:
            self.log('Flushing all data and resetting the cluster', color=Color.YELLOW)
            for host in hostfile.hosts:
                Exec(f'redis-cli -p {self.config["port"]} -h {host} flushall',
                     LocalExecInfo(env=self.mod_env,
                                   hostfile=hostfile)).run()
                Exec(f'redis-cli -p {self.config["port"]} -h {host} cluster reset',
                     LocalExecInfo(env=self.mod_env,
                                   hostfile=hostfile)).run()

            self.log('Creating the cluster', color=Color.YELLOW)
            cmd = [
                'redis-cli',
                f'--cluster create {host_str}',
                '--cluster-replicas 0',
                '--cluster-yes'
            ]
            cmd = ' '.join(cmd)
            print(cmd)
            Exec(cmd,
                 LocalExecInfo(env=self.mod_env,
                               hostfile=hostfile)).run()
            self.log(f'Sleeping for {self.config["sleep"]} seconds', color=Color.YELLOW)
            time.sleep(self.config['sleep'])

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        for i in range(3):
            Kill('redis-server',
                 PsshExecInfo(env=self.env,
                              hostfile=self.hostfile)).run()

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        pass
```

### `builtin/builtin/spark_cluster/pkg.py`

```python
"""
This module provides classes and methods to launch the SparkCluster service.
SparkCluster is ....
"""

from jarvis_cd.core.pkg import Service
from jarvis_cd.shell import Exec, PsshExecInfo


class SparkCluster(Service):
    """
    This class provides methods to launch the SparkCluster service.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'port',
                'msg': '',
                'type': int,
                'default': 7077,
            },
            {
                'name': 'num_nodes',
                'msg': '',
                'type': int,
                'default': 1,
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        self.config['SPARK_SCRIPTS'] = self.env['SPARK_SCRIPTS']
        self.env['SPARK_MASTER_HOST'] = self.hostfile.hosts[0]
        self.env['SPARK_MASTER_PORT'] = '7077'
        self.env['SPARK_WORKER_PORT'] = '7078'

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        # Start the master node
        Exec(f'{self.config["SPARK_SCRIPTS"]}/sbin/start-master.sh',
             PsshExecInfo(env=self.env,
                          hosts=self.hostfile.subset(1))).run()
        time.sleep(1)
        # Start the worker nodes
        Exec(f'{self.config["SPARK_SCRIPTS"]}/sbin/start-worker.sh '
             f'{self.env["SPARK_MASTER_HOST"]}:{self.env["SPARK_MASTER_PORT"]}',
             PsshExecInfo(env=self.mod_env,
                          hosts=self.hostfile.subset(self.config['num_nodes']))).run()
        time.sleep(self.config['sleep'])

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        # Start the master node
        Exec(f'{self.config["SPARK_SCRIPTS"]}/sbin/stop-master.sh',
             PsshExecInfo(env=self.env,
                          hosts=self.hostfile.subset(1))).run()
        # Start the worker nodes
        Exec(f'{self.config["SPARK_SCRIPTS"]}/sbin/stop-worker.sh '
             f'{self.env["SPARK_MASTER_HOST"]}',
             PsshExecInfo(env=self.env,
                          hosts=self.hostfile)).run()

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        pass

    def status(self):
        """
        Check whether or not an application is running. E.g., are OrangeFS
        servers running?

        :return: True or false
        """
        return True
```

### `builtin/builtin/test_pkg/package.py`

```python
"""
Test_pkg application package for Jarvis-CD.
This is an application that runs and completes automatically.
"""
from jarvis_cd.core.pkg import Application


class Test_pkg(Application):
    """
    Test_pkg application implementation.
    """
    
    def _init(self):
        """
        Initialize application-specific variables.
        Don't assume that self.config is initialized.
        """
        self.input_file = None
        self.output_file = None
        self.nprocs = None
        
    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        
        :return: List of argument dictionaries
        """
        return [
            {
                'name': 'input_file',
                'msg': 'Path to input file',
                'type': str,
                'default': '/tmp/test_pkg_input.dat'
            },
            {
                'name': 'output_file',
                'msg': 'Path to output file',
                'type': str,
                'default': '/tmp/test_pkg_output.dat'
            },
            {
                'name': 'nprocs',
                'msg': 'Number of processes',
                'type': int,
                'default': 1
            },
            {
                'name': 'ppn',
                'msg': 'Processes per node',
                'type': int,
                'default': 1
            }
        ]
        
    def configure(self, **kwargs):
        """
        Configure the application with given parameters.
        
        :param kwargs: Configuration parameters
        """
        self.update_config(kwargs, rebuild=False)
        
        # Validate configuration
        if self.config['nprocs'] <= 0:
            raise ValueError("Number of processes must be positive")
            
        print(f"Configured test_pkg application:")
        print(f"  Input: {self.config['input_file']}")
        print(f"  Output: {self.config['output_file']}")
        print(f"  Processes: {self.config['nprocs']}")
        
    def start(self):
        """
        Run the test_pkg application.
        """
        print(f"Running test_pkg application")
        
        # Prepare input data if needed
        self._prepare_input()
        
        # Example application execution - replace with actual commands
        # cmd = [
        #     'test_pkg',
        #     '--input', self.config['input_file'],
        #     '--output', self.config['output_file']
        # ]
        
        # Example MPI execution:
        # from jarvis_util import MpiExecInfo, Exec
        # Exec(' '.join(cmd),
        #      MpiExecInfo(env=self.mod_env,
        #                  hostfile=self.hostfile,
        #                  nprocs=self.config['nprocs'],
        #                  ppn=self.config['ppn']))
        
        print(f"test_pkg application completed")
        
    def stop(self):
        """
        Stop the application (usually not needed for apps).
        """
        print(f"Stopping test_pkg application")
        
    def clean(self):
        """
        Clean up application data and temporary files.
        """
        print(f"Cleaning test_pkg application data")
        
        # Remove output files
        import os
        if os.path.exists(self.config['output_file']):
            os.remove(self.config['output_file'])
            
        # Remove any temporary files
        # os.system(f'rm -f {self.config["output_file"]}*')
        
        print(f"test_pkg application cleaned")
        
    def _prepare_input(self):
        """
        Prepare input data for the application.
        """
        import os
        
        input_file = self.config['input_file']
        
        # Create input directory if needed
        os.makedirs(os.path.dirname(input_file), exist_ok=True)
        
        # Generate or copy input data
        if not os.path.exists(input_file):
            print(f"Generating input file: {input_file}")
            with open(input_file, 'w') as f:
                f.write(f"# test_pkg input data\n")
                f.write(f"# Generated by Jarvis-CD\n")
```

### `builtin/builtin/wrf/INSTALL.md`

```markdown
# Installation and usage of WRF

## install with spack
```
spack install wrf
```


## manually installation

# Installation and usage of WRF
A detailed installation and usage can be found here:    
https://github.com/grc-iit/jarvis-cd/wiki/7.5.-WRF
```

### `builtin/builtin/wrf/USE.md`

```markdown
# WRF With Adios2

## 1. Setup Environment

Create the environment variables needed by Hermes + WRF
```bash
export LD_LIBRARY_PATH=/coeus-adapter/build/bin:$LD_LIBRARY_PATH
export PATH=/coeus-adapter/build/bin:$PATH
module load adios2
export DIR=~/Build_WRF/LIBRARIES
export LD_LIBRARY_PATH=$DIR/lib:$LD_LIBRARY_PATH
export PATH=$DIR/bin:$PATH
export LD_LIBRARY_PATH=/adios2/lib:$LD_LIBRARY_PATH
```

## 2. Install Jarvis and set up Jarvis
Please refer this website for more information about Jarvis.  
https://grc.iit.edu/docs/jarvis/jarvis-cd/index

## 3. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by Hermes
and Gray Scott.

```bash
jarvis pipeline create wrf
```

## 3. Save Environment

Store the current environment in the pipeline.
```bash
jarvis pipeline env build
```

## 4. change some parameter in namelist.input file
```
io_form_history = 14
io_form_restart = 14
frames_per_outfile   = 1000000,
```
## 5. Add pkgs to the Pipeline

Create a Jarvis pipeline with Hermes, the Hermes MPI-IO interceptor,
and wrf
```bash
jarvis pipeline append wrf wrf_location=/WRF/test/em_real nprocs=4 ppn=6 engine=bp5

```

## 6. Run the Experiment

Run the experiment
```bash
jarvis pipeline run
```

## 7. Clean Data

To clean data produced by Hermes + Gray-Scott:
```bash
jarvis pipeline clean
```










# WRF With Hermes

## 1. Setup Environment

Create the environment variables needed by Hermes + WRF
```bash
export LD_LIBRARY_PATH=/coeus-adapter/build/bin:$LD_LIBRARY_PATH
export PATH=/coeus-adapter/build/bin:$PATH
module load adios2
spack load hermes@master
export DIR=~/Build_WRF/LIBRARIES
export LD_LIBRARY_PATH=$DIR/lib:$LD_LIBRARY_PATH
export PATH=$DIR/bin:$PATH
export LD_LIBRARY_PATH=/adios2/lib:$LD_LIBRARY_PATH
```

## 2. Install Jarvis and set up Jarvis
Please refer this website for more information about Jarvis.  
https://grc.iit.edu/docs/jarvis/jarvis-cd/index

## 3. Create a Pipeline

The Jarvis pipeline will store all configuration data needed by Hermes
and Gray Scott.

```bash
jarvis pipeline create wrf
```

## 3. Save Environment

Store the current environment in the pipeline.
```bash
jarvis pipeline env build
```

## 4. change some parameter in namelist.input file
```
io_form_history = 14
io_form_restart = 14
frames_per_outfile   = 1000000,
```
## 5. Add pkgs to the Pipeline

Create a Jarvis pipeline with Hermes, the Hermes MPI-IO interceptor,
and wrf
```bash
jarvis pipeline append hermes_run --sleep=10 provider=sockets
jarvis pipeline append wrf wrf_location=/WRF/test/em_real nprocs=4 ppn=6 engine=hermes

```

## 6. Run the Experiment

Run the experiment
```bash
jarvis pipeline run
```

## 7. Clean Data

To clean data produced by Hermes + Gray-Scott:
```bash
jarvis pipeline clean
```
```

### `builtin/builtin/wrf/config/adios2.xml`

```xml
<?xml version="1.0"?>
<adios-config>
  <io name="wrfout_d01_2019-11-26_12:00:00">
    <engine type="BP5">
      <parameter key="RendezvousReaderCount" value="0"/>
      <parameter key="QueueLimit" value="1"/>
      <parameter key="QueueFullPolicy" value="Discard"/>
      <parameter key="OpenTimeoutSecs" value="30.0"/>
    </engine>
  </io>
</adios-config>
```

### `builtin/builtin/wrf/config/hermes.xml`

```xml
<?xml version="1.0"?>
<adios-config>

    <!-- <io name="wrfout_d01_2018-06-17_00:00:00"> -->
    <io name="wrfout_d01_2019-11-26_12:00:00">


        <engine type="Plugin">
            <parameter key="PluginName" value="hermes"/>
            <parameter key="PluginLibrary" value="hermes_engine"/>
            <parameter key="ppn" value="##ppn##"/>
            <parameter key="db_file" value="##db_path##"/>
            <parameter key="execution_order" value="##Order##"/>
        </engine>

    </io>
</adios-config>
```

### `builtin/builtin/wrf/pkg.py`

```python
"""
This module provides classes and methods to launch the Wrf application.
Wrf is ....
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, MpiExecInfo, PsshExecInfo
from jarvis_cd.shell.process import Rm


class Wrf(Application):
    """
        This class provides methods to launch the Wrf application.
        """

    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'nprocs',
                'msg': 'Number of processes',
                'type': int,
                'default': 1,
            },
            {
                'name': 'ppn',
                'msg': 'The number of processes per node',
                'type': int,
                'default': None,
            },
            {
                'name': 'wrf_location',
                'msg': 'The location of wrf.exe',
                'type': str,
                'default': None,
            },
            {
                'name': 'engine',
                'msg': 'Engine to be used',
                'choices': ['bp5', 'hermes'],
                'type': str,
                'default': 'bp5',
            },
            {
                'name': 'Execution_order',
                'msg': 'Path where the bp5 will be stored',
                'type': str,
                'default': None,
            },
            {
                'name': 'db_path',
                'msg': 'Path where the DB will be stored',
                'type': str,
                'default': 'benchmark_metadata.db',
            },


        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        if self.config['engine'].lower() == 'bp5':
            self.copy_template_file(f'{self.pkg_dir}/config/adios2.xml',
                            f'{self.config["wrf_location"]}/adios2.xml')
        elif self.config['engine'].lower() in ['hermes', 'hermes_derived']:
                self.copy_template_file(f'{self.pkg_dir}/config/hermes.xml',
                        f'{self.config["wrf_location"]}/adios2.xml', replacements={
                    'ppn': self.config['ppn'],
                    'db_path': self.config['db_path'],
                    'Order': self.config['Execution_order'],
                    })
        else:
            raise Exception('Engine not defined')



    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        Exec('wrf.exe',
             MpiExecInfo(nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'],
                         hostfile=self.hostfile,
                         env=self.mod_env,
                         cwd=self.config['wrf_location']
                         )).run()

        pass

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        output_file = [self.config['db_path']]
        Rm(output_file, PsshExecInfo(hostfile=self.hostfile)).run()
        pass
```

### `builtin/builtin/ycsbc/pkg.py`

```python
"""
This module provides classes and methods to launch Redis.
Redis cluster is used if the hostfile has many hosts
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo


class Ycsbc(Application):
    """
    This class provides methods to launch the Ior application.
    """
    def _init(self):
        """
        Initialize paths
        """
        pass

    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        For thorough documentation of these parameters, view:
        https://github.com/scs-lab/jarvis-util/wiki/3.-Argument-Parsing

        :return: List(dict)
        """
        return [
            {
                'name': 'db_name',
                'msg': 'The DB to test',
                'type': str,
                'default': 'rocksdb',
                'choices': ['hermes', 'rocksdb', 'leveldb', 'redis'],
                'args': [],
            },
            {
                'name': 'workload',
                'msg': 'The workload to use',
                'type': str,
                'default': 'a',
                'choices': ['a', 'b', 'c', 'd', 'e', 'f'],
                'args': [],
            },
            {
                'name': 'status',
                'msg': 'Whether or not to print statuses every 10 sec',
                'type': bool,
                'default': True,
                'args': [],
            },
        ]

    def _configure(self, **kwargs):
        """
        Converts the Jarvis configuration to application-specific configuration.
        E.g., OrangeFS produces an orangefs.xml file.

        :param kwargs: Configuration parameters for this pkg.
        :return: None
        """
        pass

    def start(self):
        """
        Launch an application. E.g., OrangeFS will launch the servers, clients,
        and metadata services on all necessary pkgs.

        :return: None
        """
        root = self.env['YCSBC_ROOT']
        db_name = self.config["db_name"]
        workload = f'workload{self.config["workload"]}'
        props = f'{root}/{db_name}/{db_name}.properties'
        if not os.path.exists(props):
            props_arg = ''
        else:
            props_arg = f'-P {props}'
        cmd = [
            'ycsb -run',
            f'-db {db_name}',
            f'-P {root}/workloads/{workload}',
            props_arg,
            f'-s' if self.config['status'] else ''
        ]
        cmd = ' '.join(cmd)
        print(cmd)
        self.exec = Exec(cmd,
             LocalExecInfo(env=self.mod_env,
                           hostfile=self.hostfile,
                           collect_output=True)).run()

    def stop(self):
        """
        Stop a running application. E.g., OrangeFS will terminate the servers,
        clients, and metadata services.

        :return: None
        """
        pass

    def clean(self):
        """
        Destroy all data for an application. E.g., OrangeFS will delete all
        metadata and data directories in addition to the orangefs.xml file.

        :return: None
        """
        pass

    def _get_stat(self, stat_dict):
        """
        Get statistics from the application.

        :param stat_dict: A dictionary of statistics.
        :return: None
        """
        output = self.exec.stdout['localhost']
        if 'throughput(ops/sec)' in output:
            throughput = re.search(r'throughput\(ops\/sec\): ([0-9.]+)', output).group(1)
            stat_dict[f'{self.pkg_id}.throughput'] = throughput
        stat_dict[f'{self.pkg_id}.runtime'] = self.start_time
```

### `builtin/config/ares.yaml`

```yaml
CONFIG_DIR: ${HOME}/jarvis-pipelines
CUR_PIPELINE: null
HOSTFILE: null
PRIVATE_DIR: /mnt/ssd/${USER}/jarvis-pipelines
SHARED_DIR: ${HOME}/jarvis-pipelines
```

### `builtin/config/deception.yaml`

```yaml
CONFIG_DIR: ${HOME}/jarvis-pipelines
CUR_PIPELINE: null
HOSTFILE: null
PRIVATE_DIR: /scratch/${USER}/jarvis-pipelines # local SSD
SHARED_DIR: ${HOME}/jarvis-pipelines
```

### `builtin/config/polaris.yaml`

```yaml
CONFIG_DIR: ${HOME}/SCS_lab/jarvis_pipeline
CUR_PIPELINE: null
HOSTFILE: null
PRIVATE_DIR: /tmp/${USER}/jarvis
SHARED_DIR: /lus/grand/projects/VeloC/${USER}/SCS_lab/
```

### `builtin/pipelines/examples/ior_container_test.yaml`

```yaml
# Pipeline: IOR Container Test
# Purpose: Test IOR deployment using Docker/Podman containers
# Requirements: Docker or Podman installed, shared filesystem access
# Expected Runtime: 2-5 minutes

name: ior_container_test

# Auto-build environment from current shell
# No env field - will capture current environment automatically

# Container configuration
container_build: ior_test_container
container_engine: docker
container_base: iowarp/iowarp-build:latest

pkgs:
  # IOR benchmark with container deployment
  - pkg_type: builtin.ior
    pkg_name: ior_container
    # Deployment mode: 'default' for bare metal, 'container' for containerized
    deploy_mode: container
    # IOR benchmark parameters
    nprocs: 2
    ppn: 2
    block: 1G
    xfer: 1M
    api: posix
    out: /tmp/ior_test_file
    log: /tmp/ior_output.log
    write: true
    read: true
    fpp: false
    reps: 1
    direct: false
```

### `builtin/pipelines/examples/ior_podman_test.yaml`

```yaml
# Pipeline: IOR Podman Container Test
# Purpose: Test IOR deployment using Podman containers
# Requirements: Podman installed, shared filesystem access
# Expected Runtime: 2-5 minutes

name: ior_podman_test

# Auto-build environment from current shell
# No env field - will capture current environment automatically

# Container configuration
container_build: ior_test_container_podman
container_engine: podman
container_base: docker.io/iowarp/iowarp-build:latest

pkgs:
  # IOR benchmark with container deployment
  - pkg_type: builtin.ior
    pkg_name: ior_container
    # Deployment mode: 'default' for bare metal, 'container' for containerized
    deploy_mode: container
    # IOR benchmark parameters
    nprocs: 2
    ppn: 2
    block: 1G
    xfer: 1M
    api: posix
    out: /tmp/ior_test_file
    log: /tmp/ior_output.log
    write: true
    read: true
    fpp: false
    reps: 1
    direct: false
```

### `builtin/pipelines/simple_test.yaml`

```yaml
name: simple_test_pipeline
env: chimaera

# Simple test without interceptors first
pkgs:
  - pkg_type: example_app
    pkg_name: simple_test
    message: "Simple test without interceptors"
    output_file: simple_test_output.txt
```

### `builtin/pipelines/test_interceptor.yaml`

```yaml
name: test_interceptor_pipeline 

# Define interceptors at the top level
interceptors:
  - pkg_type: example_interceptor
    pkg_name: example_interceptor
    library_path: /usr/lib/example_interceptor.so
    custom_env_var: test_intercepted_value
    debug_mode: true

# Define the main pipeline packages
pkgs:
  - pkg_type: example_app
    pkg_name: example_app1
    message: "First app without interceptor"
    output_file: app1_output.txt
    
  - pkg_type: example_app
    pkg_name: example_app2
    message: "Second app with interceptor"
    output_file: app2_output.txt
    interceptors:
      - example_interceptor
      
  - pkg_type: example_app
    pkg_name: example_app3
    message: "Third app without interceptor"
    output_file: app3_output.txt
```

### `builtin/pipelines/test_simple.yaml`

```yaml
name: test_simple_from_index
pkgs:
  - pkg_type: builtin.ior
    pkg_name: ior_from_index
    block: 16m
    xfer: 512k
    api: posix
    write: true
    read: false
    fpp: false
    reps: 1
    nprocs: 1
    ppn: 1
    out: /tmp/ior_from_index.bin
    do_dbg: false
    dbg_port: 4000
    interceptors: []
```

### `builtin/pipelines/unit_tests/test_interceptor.yaml`

```yaml
name: test_interceptor_from_index
pkgs:
  - pkg_type: example_app
    pkg_name: example_app 
    interceptors: ["example_interceptor"]
interceptors:
  - pkg_type: example_interceptor
    pkg_name: example_interceptor
    library_path: /usr/lib/example_interceptor.so
    custom_env_var: test_intercepted_value
    debug_mode: true
```

### `builtin/resource_graph/ares.yaml`

```yaml
fs:
- avail: 500GB
  dev_type: ssd
  device: /dev/sdb1
  fs_type: xfs
  model: Samsung SSD 860
  mount: /mnt/ssd/${USER}
  parent: /dev/sdb
  shared: false
  uuid: 45b6abb3-7786-4b68-95d0-a8fac92e0d70
  needs_root: false
- avail: 500GB
  dev_type: hdd
  device: null
  fs_type: xfs
  model: null
  mount: ${HOME}
  parent: null
  shared: true
  uuid: null
  needs_root: false
- avail: 240GB
  dev_type: nvme
  device: /dev/nvme0n1p1
  fs_type: xfs
  model: TOSHIBA-RD400
  mount: /mnt/nvme/${USER}
  parent: /dev/nvme0n1
  shared: false
  uuid: cbc9b18c-fbb0-4ab7-a9fd-8cc719a93f64
  needs_root: false
- avail: 900GB
  dev_type: hdd
  device: /dev/sdc1
  fs_type: xfs
  model: ST1000LM049-2GH1
  mount: /mnt/hdd/${USER}
  parent: /dev/sdc
  shared: false
  uuid: 7857cbad-2e46-40c2-835a-b297bc5ee1d2
  needs_root: false
```

### `builtin/resource_graph/deception.yaml`

```yaml
fs:
- avail: 262200417583104
  dev_type: nvme
  device: nvme0n1
  fs_type: null
  needs_root: false
  model: INTEL SSDPEKKA512G8
  mount: /scratch/${USER}
  parent: null
  shared: false
  uuid: null
- avail: 2147483648000
  dev_type: hdd
  device: null
  fs_type: null
  needs_root: false
  model: null
  mount: /rcfs/projects/chess/${USER}
  parent: null
  shared: true
  uuid: null
```

### `builtin/resource_graph/delta.yaml`

```yaml
fs:
- avail: 800165027840
  dev_type: nvme
  device: nvme0n1p1
  fs_type: null
  model: MZXL5800HBHQ-000H3
  mount: /tmp
  parent: nvme0n1
  shared: false
  uuid: null
  needs_root: false
- avail: 2026619832316723
  dev_type: hdd
  device: harbor-dt.delta.internal.ncsa.edu:/harbor/ncsa/delta/u
  fs_type: null
  model: null
  mount: /u/${USER}
  parent: null
  shared: true
  uuid: null
  needs_root: false
```

### `builtin/resource_graph/g2-standard-4.yaml`

```yaml
fs:
- avail: 107238916608
  dev_type: nvme
  device: nvme0n1p1
  fs_type: null
  model: nvme_card-pd
  mount: ${HOME}
  parent: nvme0n1
  shared: false
  uuid: null
  needs_root: false
```

### `builtin/resource_graph/polaris.yaml`

```yaml
fs:
- avail: 54975581388800
  dev_type: hdd
  device: null
  fs_type: null
  model: null
  mount: /lus/grand/projects/VeloC/${USER}/SCS_lab
  parent: null
  shared: true
  uuid: null
  needs_root: false
```

### `docker/build.Dockerfile`

```dockerfile
FROM iowarp/iowarp-deps:latest
LABEL maintainer="llogan@hawk.iit.edu"
LABEL version="0.0"
LABEL description="IOWarp ppi-jarvis-cd Docker image"

# Copy the workspace
COPY . /workspace
WORKDIR /workspace

# Clean any existing build artifacts and install the package
RUN sudo rm -rf *.egg-info build dist && \
    sudo pip install --break-system-packages .

# Add ppi-jarvis-cd to Spack configuration
RUN echo "  py-ppi-jarvis-cd:" >> ~/.spack/packages.yaml && \
    echo "    externals:" >> ~/.spack/packages.yaml && \
    echo "    - spec: py-ppi-jarvis-cd" >> ~/.spack/packages.yaml && \
    echo "      prefix: /usr/local" >> ~/.spack/packages.yaml && \
    echo "    buildable: false" >> ~/.spack/packages.yaml

# Setup jarvis
RUN jarvis init
```

### `docker/deploy.Dockerfile`

```dockerfile
FROM iowarp/ppi-jarvis-cd-build:latest
LABEL maintainer="llogan@hawk.iit.edu"
LABEL version="0.0"
LABEL description="IOWarp ppi-jarvis-cd deployment image"
```

### `docker/local.sh`

```bash
#!/bin/bash

# Build iowarp-runtime Docker image

# Get the project root directory (parent of docker folder)
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
PROJECT_ROOT="$( cd "${SCRIPT_DIR}/.." && pwd )"

echo $PROJECT_ROOT
echo $SCRIPT_DIR
# Build the Docker image
docker build  --no-cache -t iowarp/ppi-jarvis-cd-build:latest -f "${SCRIPT_DIR}/build.Dockerfile" "${PROJECT_ROOT}"
```

### `docs/argparse.md`

```markdown
# ArgParse Class Documentation

## Overview

The `ArgParse` class is a custom argument parsing library designed to support complex command-line interfaces with menus, commands, and sophisticated argument handling. It provides an alternative to Python's built-in `argparse` module with enhanced support for positional arguments, argument ranking, and remainder handling.

## Location

```python
from jarvis_cd.util.argparse import ArgParse
```

## Class Hierarchy

```
ArgParse (base class)
├── Custom application parsers (inherit from ArgParse)
```

## Core Concepts

### Menu
A grouping of related commands. Menus can be nested and provide organization for command sets.

### Command
A specific action that can be executed, belonging to a menu. Commands have names and can have aliases.

### Arguments
Parameters that commands accept, supporting:
- **Positional arguments**: Ordered by class and rank
- **Keyword arguments**: Named parameters with `--` or `-` prefixes
- **Remainder arguments**: Catch-all for additional parameters

## Constructor

```python
def __init__(self):
    """
    Initialize the argument parser.
    Sets up internal data structures for menus, commands, and arguments.
    """
```

**Parameters**: None

**Attributes**:
- `menus`: Dictionary of defined menus
- `commands`: Dictionary of defined commands
- `command_args`: Dictionary mapping commands to their arguments
- `kwargs`: Parsed keyword arguments (populated after parsing)
- `remainder`: Remaining unparsed arguments (populated after parsing)

## Core Methods

### define_options()

```python
def define_options(self):
    """
    Override this method to define your command structure.
    Called during initialization to set up menus, commands, and arguments.
    """
```

**Purpose**: Must be overridden in subclasses to define the argument structure.

### add_menu()

```python
def add_menu(self, name: str, msg: str = ""):
    """
    Add a menu to the parser.
    
    :param name: Space-separated menu path (e.g., "vpic" or "app subcommand")
    :param msg: Description of the menu
    """
```

**Parameters**:
- `name` (str): Menu identifier. Space-separated for nested menus
- `msg` (str, optional): Human-readable description

**Example**:
```python
self.add_menu('', msg="Main menu")
self.add_menu('vpic', msg="VPIC simulation commands")
```

### add_cmd()

```python
def add_cmd(self, name: str, keep_remainder: bool = False, aliases: Optional[List[str]] = None):
    """
    Add a command to a menu.
    
    :param name: Full command name including menu (e.g., "vpic run")
    :param keep_remainder: Whether to collect unparsed arguments in self.remainder
    :param aliases: Alternative command names
    """
```

**Parameters**:
- `name` (str): Full command path (menu + command name)
- `keep_remainder` (bool): If True, unparsed args go to `self.remainder`
- `aliases` (List[str], optional): Alternative command names

**Example**:
```python
self.add_cmd('vpic run', keep_remainder=False, aliases=['vpic r', 'vpic runner'])
```

### add_args()

```python
def add_args(self, args_list: List[Dict[str, Any]]):
    """
    Add arguments to the most recently added command.
    
    :param args_list: List of argument dictionaries
    """
```

**Parameters**:
- `args_list` (List[Dict]): List of argument specifications

**Argument Dictionary Structure**:
```python
{
    'name': str,           # Argument name
    'msg': str,            # Description
    'type': type,          # Type to cast to (str, int, bool, list)
    'default': Any,        # Default value
    'class': str,          # Grouping class for positional ordering
    'rank': int,           # Order within class
    'required': bool,      # Whether argument is mandatory
    'pos': bool,           # Whether it's a positional argument
    'aliases': List[str],  # Alternative names
    'args': List[Dict],    # For list types: structure of list items
}
```

### parse()

```python
def parse(self, args: List[str]) -> Dict[str, Any]:
    """
    Parse command line arguments.
    
    :param args: List of command line arguments
    :return: Dictionary of parsed arguments
    """
```

**Parameters**:
- `args` (List[str]): Command-line arguments to parse

**Returns**:
- `Dict[str, Any]`: Parsed arguments accessible via `self.kwargs`

**Side Effects**:
- Populates `self.kwargs` with parsed arguments
- Populates `self.remainder` with unparsed arguments
- Calls appropriate command handler method

## Utility Methods

### subset()
```python
def subset(self, count: int, path: Optional[str] = None) -> 'Hostfile'
```

### copy()
```python
def copy(self) -> 'Hostfile'
```

## Argument Types and Parsing

### Positional Arguments
Arguments with `'pos': True` are parsed in order determined by:
1. `'class'` field (alphabetically)
2. `'rank'` field (numerically)
3. Arguments without class come last

### Keyword Arguments
- Long form: `--argument=value` or `--argument value`
- Short form: `-a value`
- Aliases supported for argument names

### List Arguments
Special handling for `'type': list`:

**Set Mode** (with `=`):
```bash
--devices="[(/mnt/home, 5), (/mnt/home2, 6)]"
```

**Append Mode** (without `=`):
```bash
--d "(/mnt/home, 5)" --d "(/mnt/home2, 6)"
```

### Type Casting
Automatic conversion based on `'type'` field:
- `str`: String conversion
- `int`: Integer conversion
- `bool`: Boolean conversion (`'true'`, `'1'`, `'yes'`, `'on'` → True)
- `list`: Special list parsing

## Example Implementation

```python
class MyAppArgParse(ArgParse):
    def define_options(self):
        # Main menu with remainder collection
        self.add_menu('')
        self.add_cmd('', keep_remainder=True)
        self.add_args([
            {
                'name': 'verbose',
                'msg': 'Enable verbose output',
                'type': bool,
                'default': False
            }
        ])

        # VPIC simulation menu
        self.add_menu('vpic', msg="VPIC simulation commands")
        self.add_cmd('vpic run', keep_remainder=False, aliases=['vpic r'])
        self.add_args([
            {
                'name': 'steps',
                'msg': 'Number of simulation steps',
                'type': int,
                'required': True,
                'pos': True,
                'class': 'sim',
                'rank': 0
            },
            {
                'name': 'grid_size',
                'msg': 'Grid size',
                'type': int,
                'default': 256,
                'pos': True,
                'class': 'sim',
                'rank': 1
            },
            {
                'name': 'output_dir',
                'msg': 'Output directory',
                'type': str,
                'default': './output'
            },
            {
                'name': 'nodes',
                'msg': 'Compute nodes',
                'type': list,
                'aliases': ['n'],
                'args': [
                    {
                        'name': 'hostname',
                        'msg': 'Node hostname',
                        'type': str
                    },
                    {
                        'name': 'cores',
                        'msg': 'Number of cores',
                        'type': int
                    }
                ]
            }
        ])

    def main_menu(self):
        """Handler for main menu command"""
        print(f"Main menu called with: {self.kwargs}")
        print(f"Remainder: {self.remainder}")

    def vpic_run(self):
        """Handler for vpic run command"""
        print(f"Running VPIC simulation with {self.kwargs['steps']} steps")
        print(f"Grid size: {self.kwargs['grid_size']}")
        if 'nodes' in self.kwargs:
            print(f"Using nodes: {self.kwargs['nodes']}")

# Usage
parser = MyAppArgParse()
parser.define_options()

# Parse various command formats
result = parser.parse(['vpic', 'run', '1000', '512', '--output_dir=/tmp/sim'])
result = parser.parse(['vpic', 'r', '100'])  # Using alias
result = parser.parse(['--verbose=true', 'extra', 'args'])  # Main menu with remainder
```

## Command Handler Methods

When a command is parsed, the parser automatically calls a method named after the command:
- Command `"vpic run"` → calls `vpic_run()` method
- Command `""` (empty) → calls `main_menu()` method
- Spaces and hyphens in command names become underscores

## Error Handling

The parser raises `ValueError` for:
- Missing required arguments
- Invalid argument types during casting
- Attempting to add arguments without a command

## Advanced Features

### Argument Classes and Ranking
Use `'class'` and `'rank'` to control positional argument order:

```python
{
    'name': 'input_file',
    'pos': True,
    'class': 'files',
    'rank': 0
},
{
    'name': 'output_file', 
    'pos': True,
    'class': 'files',
    'rank': 1
},
{
    'name': 'verbose',
    'pos': True,
    'class': 'options',
    'rank': 0
}
```

Order: `files` arguments first (input_file, output_file), then `options` arguments (verbose).

### Complex List Arguments
For structured list data:

```python
{
    'name': 'servers',
    'type': list,
    'args': [
        {'name': 'hostname', 'type': str},
        {'name': 'port', 'type': int},
        {'name': 'ssl', 'type': bool}
    ]
}
```

Usage: `--servers="[(server1, 8080, true), (server2, 8443, false)]"`

## Testing

Comprehensive unit tests are available at `test/unit/test_argparse.py` covering:
- Command aliases
- Argument ranking and ordering
- List argument parsing (both set and append modes)
- Type casting
- Required argument validation
- Remainder handling
- Edge cases and error conditions

## Best Practices

1. **Define clear argument classes** for logical grouping of positional arguments
2. **Use meaningful rank values** to control argument order within classes
3. **Provide aliases** for frequently used commands and arguments
4. **Set appropriate defaults** for optional arguments
5. **Use descriptive names and messages** for better user experience
6. **Implement command handler methods** with matching names
7. **Test edge cases** thoroughly, especially with complex list arguments
```

### `docs/hostfile.md`

```markdown
# Hostfile Class Documentation

## Overview

The `Hostfile` class provides a powerful and flexible way to manage sets of hostnames for distributed computing and cluster management. It supports parsing hostfile text with advanced pattern expansion, automatic IP resolution, and various host manipulation operations.

## Location

```python
from jarvis_cd.util.hostfile import Hostfile
```

## Core Features

- **Pattern Expansion**: Automatic expansion of bracket notation for host ranges
- **Multiple Input Sources**: Load from files, text, or manual lists  
- **IP Resolution**: Automatic hostname-to-IP mapping with optional disable
- **Host Manipulation**: Subset, copy, enumerate, and string operations
- **Local Detection**: Automatic detection of localhost-only configurations

## Constructor

```python
def __init__(self, path: Optional[str] = None, hosts: Optional[List[str]] = None, 
             hosts_ip: Optional[List[str]] = None, text: Optional[str] = None, 
             find_ips: bool = True, load_path: bool = True):
    """
    Constructor. Parse hostfile or store existing host list.

    :param path: The path to the hostfile
    :param hosts: a list of strings representing all hostnames
    :param hosts_ip: a list of strings representing all host IPs
    :param text: Text of a hostfile
    :param find_ips: Whether to construct host_ip and all_host_ip fields
    :param load_path: whether or not path should exist and be read from on init
    """
```

**Parameters**:
- `path` (str, optional): Path to hostfile on filesystem
- `hosts` (List[str], optional): Manual list of hostnames
- `hosts_ip` (List[str], optional): Manual list of IP addresses
- `text` (str, optional): Raw hostfile text content
- `find_ips` (bool): Enable automatic IP resolution (default: True)
- `load_path` (bool): Whether to read from path during initialization (default: True)

**Behavior**:
- If no parameters provided: Creates localhost-only hostfile
- If `hosts` provided: Uses manual host list
- If `path` provided and `load_path=True`: Loads from filesystem
- If `text` provided: Parses hostfile content
- If `find_ips=True`: Automatically resolves hostnames to IPs

## Attributes

After initialization, the following attributes are available:

```python
hostfile.path          # str: Path to hostfile (if loaded from file)
hostfile.hosts         # List[str]: List of hostnames
hostfile.hosts_ip      # List[str]: List of IP addresses
hostfile.find_ips      # bool: Whether IP resolution is enabled
```

## Pattern Expansion

The Hostfile class supports sophisticated pattern expansion using bracket notation:

### Numeric Ranges
```python
# Input: "ares-comp-[02-04]"
# Output: ["ares-comp-02", "ares-comp-03", "ares-comp-04"]

# Zero-padding preserved
# Input: "node-[001-003]"  
# Output: ["node-001", "node-002", "node-003"]
```

### Alphabetic Ranges
```python
# Lowercase: "server-[a-c]"
# Output: ["server-a", "server-b", "server-c"]

# Uppercase: "server-[A-C]"
# Output: ["server-A", "server-B", "server-C"]
```

### Lists and Complex Patterns
```python
# List notation: "compute-[1,3,5]"
# Output: ["compute-1", "compute-3", "compute-5"]

# Mixed ranges and lists: "ares-comp-[05-09,11,12-14]-40g"
# Output: ["ares-comp-05-40g", "ares-comp-06-40g", ..., "ares-comp-14-40g"]
```

### Multi-line Hostfiles
```
ares-comp-01
ares-comp-[02-04]
ares-comp-[05-09,11,12-14]-40g
```

## Core Methods

### subset()

```python
def subset(self, count: int, path: Optional[str] = None) -> 'Hostfile':
    """
    Return a subset of the first 'count' hosts.
    
    :param count: Number of hosts to include
    :param path: Optional path for the new hostfile
    :return: New Hostfile with subset of hosts
    """
```

**Example**:
```python
hostfile = Hostfile(text="node-[01-10]", find_ips=False)
subset = hostfile.subset(3)  # First 3 hosts
print(subset.hosts)  # ['node-01', 'node-02', 'node-03']
```

### copy()

```python
def copy(self) -> 'Hostfile':
    """
    Return a complete copy of this hostfile.
    
    :return: New Hostfile with same hosts and settings
    """
```

**Example**:
```python
original = Hostfile(hosts=['host1', 'host2'])
copy = original.copy()
# copy.hosts is a separate list with same content
```

### is_local()

```python
def is_local(self) -> bool:
    """
    Whether this file contains only 'localhost'.
    
    :return: True if localhost-only, False otherwise
    """
```

**Example**:
```python
localhost_file = Hostfile()  # Default constructor
print(localhost_file.is_local())  # True

multi_host = Hostfile(hosts=['host1', 'host2'])
print(multi_host.is_local())  # False
```

### save()

```python
def save(self, path: str) -> 'Hostfile':
    """
    Save hostfile to filesystem.
    
    :param path: File path to save to
    :return: Self for method chaining
    """
```

**Example**:
```python
hostfile = Hostfile(text="node-[01-03]")
hostfile.save('/tmp/my_hostfile.txt')
```

### list()

```python
def list(self) -> List['Hostfile']:
    """
    Return a list of single-host Hostfile objects.
    
    :return: List of Hostfile objects, one per host
    """
```

**Example**:
```python
hostfile = Hostfile(hosts=['host1', 'host2'])
host_list = hostfile.list()
print(len(host_list))        # 2
print(host_list[0].hosts)    # ['host1']
print(host_list[1].hosts)    # ['host2']
```

### enumerate()

```python
def enumerate(self):
    """
    Return enumerated list of single-host Hostfile objects.
    
    :return: Generator of (index, Hostfile) tuples
    """
```

**Example**:
```python
hostfile = Hostfile(hosts=['host1', 'host2'])
for i, single_host in hostfile.enumerate():
    print(f"{i}: {single_host.hosts[0]}")
# Output:
# 0: host1
# 1: host2
```

### host_str()

```python
def host_str(self, sep: str = ',') -> str:
    """
    Return hosts as a separated string.
    
    :param sep: Separator string (default: comma)
    :return: Hosts joined by separator
    """
```

**Example**:
```python
hostfile = Hostfile(hosts=['host1', 'host2', 'host3'])
print(hostfile.host_str())      # "host1,host2,host3"
print(hostfile.host_str('|'))   # "host1|host2|host3"
```

### ip_str()

```python
def ip_str(self, sep: str = ',') -> str:
    """
    Return host IPs as a separated string.
    
    :param sep: Separator string (default: comma)
    :return: IPs joined by separator
    """
```

**Example**:
```python
hostfile = Hostfile(hosts=['localhost'])
print(hostfile.ip_str())  # "127.0.0.1" (or similar)
```

## Built-in Methods

### Length and Indexing

```python
len(hostfile)           # Number of hosts
hostfile[0]             # First hostname
hostfile[-1]            # Last hostname
hostfile[1:3]           # Slice of hostnames
```

### Iteration

```python
for hostname in hostfile:
    print(hostname)

# Or get all as list
all_hosts = list(hostfile)
```

### String Representation

```python
str(hostfile)           # "Hostfile(3 hosts: host1,host2,host3)"
repr(hostfile)          # "Hostfile(hosts=['host1', 'host2'], hosts_ip=[...])"
```

## Usage Examples

### Default Localhost

```python
# Create localhost hostfile
hostfile = Hostfile()
print(hostfile.hosts)      # ['localhost']
print(hostfile.is_local()) # True
```

### From File

```python
# Load from filesystem
hostfile = Hostfile(path='/etc/hostfile.txt')
print(f"Loaded {len(hostfile)} hosts")
```

### From Text Pattern

```python
# Parse text with pattern expansion
text = """
compute-[01-05]
gpu-[a-d]
storage-[1,3,5]
"""
hostfile = Hostfile(text=text, find_ips=False)
print(hostfile.hosts)
# ['compute-01', 'compute-02', ..., 'gpu-a', 'gpu-b', ..., 'storage-1', ...]
```

### Manual Host List

```python
# Create from manual list
hosts = ['node1.cluster.edu', 'node2.cluster.edu', 'node3.cluster.edu']
hostfile = Hostfile(hosts=hosts, find_ips=False)
```

### Disable IP Resolution

```python
# For performance when IPs not needed
hostfile = Hostfile(text="node-[001-100]", find_ips=False)
print(hostfile.hosts_ip)  # []
```

### Host Manipulation

```python
# Create large hostfile
hostfile = Hostfile(text="node-[01-20]", find_ips=False)

# Get subset for testing
test_hosts = hostfile.subset(3)
print(test_hosts.hosts)  # ['node-01', 'node-02', 'node-03']

# Create backup copy
backup = hostfile.copy()

# Get comma-separated string for external tools
host_string = hostfile.host_str()
```

### File Operations

```python
# Load, modify, and save
hostfile = Hostfile(path='input_hosts.txt')
subset = hostfile.subset(10)
subset.save('first_10_hosts.txt')
```

### Working with Individual Hosts

```python
hostfile = Hostfile(text="node-[01-03]", find_ips=False)

# Process each host individually
for i, single_host in hostfile.enumerate():
    print(f"Processing host {i}: {single_host.hosts[0]}")
    # single_host is a Hostfile with one host
    
# Or get list of single-host objects
host_objects = hostfile.list()
first_host = host_objects[0]  # Hostfile with just first host
```

## IP Resolution

When `find_ips=True` (default), the class automatically resolves hostnames:

```python
hostfile = Hostfile(hosts=['localhost', 'google.com'])
print(hostfile.hosts)     # ['localhost', 'google.com'] 
print(hostfile.hosts_ip)  # ['127.0.0.1', '142.250.191.14'] (example)
```

**Notes**:
- Resolution happens during initialization
- Failed resolutions use hostname as IP
- Disable with `find_ips=False` for performance
- `localhost` is always resolved correctly

## Error Handling

```python
# File not found
try:
    hostfile = Hostfile(path='/nonexistent/file.txt')
except FileNotFoundError:
    print("Hostfile not found")

# Invalid patterns are treated as literals
hostfile = Hostfile(text="invalid-[pattern", find_ips=False)
print(hostfile.hosts)  # ['invalid-[pattern']
```

## Performance Considerations

1. **IP Resolution**: Disable with `find_ips=False` for large host lists
2. **Pattern Complexity**: Complex patterns with large ranges may take time to expand
3. **File Loading**: Use `load_path=False` when creating derived hostfiles

## Testing

Comprehensive unit tests are available at `test/unit/test_hostfile.py` covering:
- Pattern expansion (numeric, alphabetic, mixed)
- File loading and saving
- IP resolution
- Host manipulation methods
- Edge cases and error conditions
- Zero-padding preservation
- Multi-line hostfiles

## Integration Examples

### With ArgParse

```python
class MyAppArgParse(ArgParse):
    def define_options(self):
        self.add_menu('')
        self.add_cmd('run')
        self.add_args([
            {
                'name': 'hostfile',
                'msg': 'Path to hostfile',
                'type': str,
                'required': True
            },
            {
                'name': 'node_count',
                'msg': 'Number of nodes to use',
                'type': int,
                'default': None
            }
        ])
    
    def run(self):
        # Load hostfile
        hostfile = Hostfile(path=self.kwargs['hostfile'])
        
        # Use subset if specified
        if self.kwargs['node_count']:
            hostfile = hostfile.subset(self.kwargs['node_count'])
            
        print(f"Running on {len(hostfile)} hosts: {hostfile.host_str()}")
```

### Cluster Management

```python
def deploy_to_cluster(app_path, hostfile_path):
    """Deploy application to cluster hosts"""
    hostfile = Hostfile(path=hostfile_path)
    
    if hostfile.is_local():
        print("Running locally")
        run_local(app_path)
    else:
        print(f"Deploying to {len(hostfile)} hosts")
        for i, single_host in hostfile.enumerate():
            hostname = single_host.hosts[0]
            ip = single_host.hosts_ip[0]
            print(f"Deploying to {hostname} ({ip})")
            deploy_to_host(app_path, hostname)
```

## Best Practices

1. **Use `find_ips=False`** for large host lists when IPs aren't needed
2. **Validate hostfile existence** before loading from paths
3. **Use `subset()`** for testing with smaller host counts
4. **Save derived hostfiles** for reuse in complex workflows
5. **Check `is_local()`** to handle single-machine vs. distributed cases
6. **Use pattern expansion** to reduce hostfile maintenance overhead
7. **Test pattern expansion** with small examples before large deployments
```

### `docs/modules.md`

```markdown
# Jarvis-CD Module Management

The `jarvis mod` system provides a complete solution for creating and managing modulefiles for manually-installed packages. This system generates both YAML and TCL modulefiles that can be used for environment management and integration with module systems like Environment Modules.

## Table of Contents

1. [Overview](#overview)
2. [Basic Usage Workflow](#basic-usage-workflow)
3. [Module Directory Structure](#module-directory-structure)
4. [CLI Commands Reference](#cli-commands-reference)
5. [Modulefile Formats](#modulefile-formats)
6. [Environment Profile Building](#environment-profile-building)
7. [Advanced Usage](#advanced-usage)
8. [Integration with Package Managers](#integration-with-package-managers)

## Overview

The Jarvis module system allows you to:

- **Create isolated package environments** with dedicated directory structures
- **Generate TCL modulefiles** compatible with Environment Modules
- **Generate YAML configuration files** for easy scripting and automation
- **Manage environment variables** (PATH, LD_LIBRARY_PATH, etc.) systematically
- **Build environment profiles** in multiple formats for IDE and build system integration
- **Track package dependencies** between modules

### Key Features

- **Automatic TCL generation**: YAML configurations are automatically converted to TCL modulefiles
- **Environment variable management**: Support for both prepend and set operations
- **Dependency tracking**: Module dependencies are tracked and included in TCL files
- **Current module context**: Work with a "current" module to avoid repetitive typing
- **Profile building**: Export current environment for IDEs and build systems

## Basic Usage Workflow

Here's a complete example of manually installing zlib using the jarvis module system:

```bash
# 1. Create a new module
jarvis mod create zlib

# 2. Navigate to source directory and download/build
cd $(jarvis mod src zlib)
wget https://www.zlib.net/zlib-1.3.tar.gz
tar -xzf zlib-1.3.tar.gz
cd zlib-1.3

# 3. Configure with module installation prefix
./configure --prefix=$(jarvis mod root zlib)

# 4. Build and install
make -j8 install

# 5. Configure module environment variables
jarvis mod prepend zlib PATH="$(jarvis mod root zlib)/bin"
jarvis mod prepend zlib LD_LIBRARY_PATH="$(jarvis mod root zlib)/lib"
jarvis mod prepend zlib PKG_CONFIG_PATH="$(jarvis mod root zlib)/lib/pkgconfig"

# 6. Set package-specific environment variables
jarvis mod setenv zlib ZLIB_ROOT="$(jarvis mod root zlib)"
jarvis mod setenv zlib ZLIB_VERSION="1.3"
```

After this workflow, you'll have:
- Package installed in `~/.ppi-jarvis-mods/packages/zlib/`
- YAML configuration at `~/.ppi-jarvis-mods/modules/zlib.yaml`
- TCL modulefile at `~/.ppi-jarvis-mods/modules/zlib`

## Module Directory Structure

The jarvis module system creates a well-organized directory structure:

```
~/.ppi-jarvis-mods/
├── packages/                    # Package installation directories
│   ├── zlib/                   # Example package directory
│   │   ├── bin/               # Installed binaries
│   │   ├── lib/               # Installed libraries
│   │   ├── include/           # Header files
│   │   └── src/               # Source code directory
│   └── openssl/
│       ├── bin/
│       ├── lib/
│       ├── include/
│       └── src/
└── modules/                    # Module configuration files
    ├── zlib.yaml              # YAML configuration
    ├── zlib                   # TCL modulefile
    ├── openssl.yaml
    └── openssl
```

### Directory Purposes

- **`packages/`**: Root directory for all package installations
- **`packages/{module}/`**: Installation prefix for each package (`$(jarvis mod root {module})`)
- **`packages/{module}/src/`**: Source code working directory (`$(jarvis mod src {module})`)
- **`modules/`**: Configuration and modulefile storage
- **`modules/{module}.yaml`**: YAML configuration file (`$(jarvis mod yaml {module})`)
- **`modules/{module}`**: TCL modulefile (`$(jarvis mod tcl {module})`)

## CLI Commands Reference

### Module Creation and Management

#### `jarvis mod create [mod_name]`
Create a new module with directory structure and configuration files.

```bash
# Create module with specific name
jarvis mod create mypackage

# Create module with generated name (if no name provided)
jarvis mod create
# Output: No module name provided, using: module_1640995200
```

**Creates:**
- Package directory: `~/.ppi-jarvis-mods/packages/{mod_name}/`
- Source directory: `~/.ppi-jarvis-mods/packages/{mod_name}/src/`
- YAML configuration: `~/.ppi-jarvis-mods/modules/{mod_name}.yaml`
- TCL modulefile: `~/.ppi-jarvis-mods/modules/{mod_name}`
- Sets the new module as current

#### `jarvis mod cd <mod_name>`
Set the current active module for subsequent operations.

```bash
jarvis mod cd zlib
# Set current module: zlib

# Now you can omit module name in other commands
jarvis mod root        # Uses current module (zlib)
jarvis mod prepend PATH="/opt/zlib/bin"  # Modifies current module
```

#### `jarvis mod list`
List all available modules with current module indicator.

```bash
jarvis mod list
# Available modules:
#   openssl
# * zlib        # * indicates current module
```

#### `jarvis mod destroy [mod_name]`
Completely remove a module and all its files.

```bash
# Destroy specific module
jarvis mod destroy old_package

# Destroy current module
jarvis mod destroy
```

**Removes:**
- Package directory and all contents
- YAML configuration and TCL modulefile
- Clears current module if it was the destroyed one

#### `jarvis mod clear [mod_name]`
Clear the module package directory, preserving only the `src/` directory. Useful for cleaning build artifacts while keeping source code.

```bash
# Clear specific module
jarvis mod clear mypackage

# Clear current module
jarvis mod cd mypackage
jarvis mod clear
```

**Removes:**
- All directories except `src/` (e.g., `bin/`, `lib/`, `include/`, `build/`)
- All files in the package root (e.g., `README.md`, build artifacts)

**Preserves:**
- The `src/` directory and all its contents

**Use case:** After building a package from source, you may want to clean up intermediate build files while keeping the original source code for rebuilding or reference.

### Environment Variable Management

#### `jarvis mod prepend [mod_name] ENV=VAL1;VAL2;VAL3 ...`
Prepend values to environment variables (typically for PATH-like variables).

```bash
# Prepend to specific module
jarvis mod prepend zlib PATH="/opt/zlib/bin" LD_LIBRARY_PATH="/opt/zlib/lib"

# Prepend to current module
jarvis mod prepend PKG_CONFIG_PATH="/opt/zlib/lib/pkgconfig"

# Multiple values using semicolon separator
jarvis mod prepend zlib PATH="/opt/zlib/bin;/opt/zlib/sbin"
```

#### `jarvis mod setenv [mod_name] ENV=VAL ...`
Set environment variables to specific values.

```bash
# Set variables for specific module
jarvis mod setenv zlib ZLIB_ROOT="/opt/zlib" ZLIB_VERSION="1.3"

# Set variables for current module
jarvis mod setenv CC="gcc-9" CXX="g++-9"
```

### Directory Path Commands

#### `jarvis mod root [mod_name]`
Print the root installation directory for a module.

```bash
jarvis mod root zlib
# /home/user/.ppi-jarvis-mods/packages/zlib

# Use in shell commands
cd $(jarvis mod root zlib)
./configure --prefix=$(jarvis mod root zlib)
```

#### `jarvis mod src [mod_name]`
Print the source directory for a module.

```bash
jarvis mod src zlib
# /home/user/.ppi-jarvis-mods/packages/zlib/src

# Use in shell commands
cd $(jarvis mod src zlib)
wget https://example.com/source.tar.gz
```

#### `jarvis mod tcl [mod_name]`
Print the path to the TCL modulefile.

```bash
jarvis mod tcl zlib
# /home/user/.ppi-jarvis-mods/modules/zlib

# Use with module command
module load $(jarvis mod tcl zlib)
```

#### `jarvis mod yaml [mod_name]`
Print the path to the YAML configuration file.

```bash
jarvis mod yaml zlib
# /home/user/.ppi-jarvis-mods/modules/zlib.yaml

# View configuration
cat $(jarvis mod yaml zlib)
```

#### `jarvis mod dir`
Print the global modules directory containing all YAML and TCL modulefiles.

```bash
jarvis mod dir
# /home/user/.ppi-jarvis-mods/modules

# Use to navigate to modules directory
cd $(jarvis mod dir)

# List all modulefiles
ls $(jarvis mod dir)
```

#### `jarvis mod profile [m=method] [path=file]`
Build a snapshot of current environment variables in various formats (same as environment profile building section).

#### `jarvis mod import <mod_name> <command>`
Create a module by automatically detecting environment changes before/after running a command.

```bash
# Import a module from a setup script
jarvis mod import mypackage "source /opt/mypackage/setup.sh"

# Import from an export command
jarvis mod import testlib "export PATH=/opt/testlib/bin:\$PATH"

# Import from a more complex command
jarvis mod import compiler "module load gcc/9.3.0 && export CC=gcc CXX=g++"
```

**Features:**
- Automatically detects changes in PATH-like environment variables
- Stores the command in the YAML file for later updates
- Creates both YAML and TCL modulefiles
- Tracks changes in: PATH, LD_LIBRARY_PATH, LIBRARY_PATH, INCLUDE, CPATH, PKG_CONFIG_PATH, CMAKE_PREFIX_PATH, JAVA_HOME, PYTHONPATH, CFLAGS, LDFLAGS

#### `jarvis mod update [mod_name]`
Update a module by re-running its stored command.

```bash
# Update specific module
jarvis mod update mypackage

# Update current module
jarvis mod cd mypackage
jarvis mod update
```

**Use cases:**
- Refresh module after environment changes
- Update module after software reinstallation
- Synchronize module with updated setup scripts

### Environment Profile Building

#### `jarvis mod profile [m=method] [path=file]`
Build a snapshot of current environment variables in various formats.

```bash
# Print to stdout in default format (dotenv)
jarvis mod profile

# Print in VSCode launch.json format
jarvis mod profile m=vscode

# Print in CLion environment format
jarvis mod profile m=clion

# Save to file in dotenv format
jarvis mod profile path=.env

# Save to file in CMake format
jarvis mod profile m=cmake path=env.cmake
```

#### `jarvis mod build profile [m=method] [path=file]`
Alternative command for building environment profiles (same functionality as `mod profile`).

```bash
# Print to stdout in default format (dotenv)
jarvis mod build profile

# Print in VSCode launch.json format
jarvis mod build profile m=vscode

# Print in CLion environment format
jarvis mod build profile m=clion

# Save to file in dotenv format
jarvis mod build profile path=.env

# Save to file in CMake format
jarvis mod build profile m=cmake path=env.cmake
```

**Supported formats:**
- **`dotenv`**: Standard .env file format (`VAR="value"`)
- **`cmake`**: CMake set commands (`set(ENV{VAR} "value")`)
- **`vscode`**: VSCode launch.json environment block
- **`clion`**: CLion environment configuration

## Modulefile Formats

### YAML Configuration Format

The YAML file contains structured configuration that's easy to read and modify:

```yaml
command: source /opt/mypackage/setup.sh  # Stored command for updates (optional)
deps:
  ppi-jarvis-util: true         # Module dependencies
doc:
  Name: zlib                    # Package documentation
  Version: "1.3"
  doc: "Compression library"
prepends:                       # Variables to prepend to
  CFLAGS: []
  CMAKE_PREFIX_PATH: []
  CPATH: []
  INCLUDE: []
  LDFLAGS: []
  LD_LIBRARY_PATH:
  - /home/user/.ppi-jarvis-mods/packages/zlib/lib
  LIBRARY_PATH: []
  PATH:
  - /home/user/.ppi-jarvis-mods/packages/zlib/bin
  PKG_CONFIG_PATH:
  - /home/user/.ppi-jarvis-mods/packages/zlib/lib/pkgconfig
  PYTHONPATH: []
setenvs:                        # Variables to set
  ZLIB_ROOT: /home/user/.ppi-jarvis-mods/packages/zlib
  ZLIB_VERSION: "1.3"
```

#### Command Storage
When modules are created using `jarvis mod import`, the original command is stored in the `command` field. This allows for:
- **Reproducible updates**: `jarvis mod update` can re-run the exact same command
- **Documentation**: The command serves as documentation for how the environment was set up
- **Version control**: Commands can be tracked and modified as needed

### TCL Modulefile Format

The TCL file is automatically generated from the YAML configuration:

```tcl
#%Module1.0
module-whatis 'Name: zlib'
module-whatis 'Version: 1.3'
module-whatis 'doc: Compression library'
module load ppi-jarvis-util
prepend-path LD_LIBRARY_PATH /home/user/.ppi-jarvis-mods/packages/zlib/lib
prepend-path PATH /home/user/.ppi-jarvis-mods/packages/zlib/bin
prepend-path PKG_CONFIG_PATH /home/user/.ppi-jarvis-mods/packages/zlib/lib/pkgconfig
setenv ZLIB_ROOT /home/user/.ppi-jarvis-mods/packages/zlib
setenv ZLIB_VERSION 1.3
```

### Module Dependencies

Dependencies between modules are tracked in the `deps` section:

```yaml
deps:
  base-compilers: true    # This module depends on base-compilers
  mpi-runtime: true       # This module depends on mpi-runtime
  optional-package: false # This dependency is disabled
```

Dependencies marked as `true` will generate `module load` statements in the TCL file.

## Environment Profile Building

The profile building feature captures important environment variables and exports them in various formats for IDE and build system integration.

### Captured Variables

The system captures these common environment variables:
- **PATH**: Executable search paths
- **LD_LIBRARY_PATH**: Dynamic library search paths
- **LIBRARY_PATH**: Static library search paths
- **INCLUDE, CPATH**: Header file search paths
- **PKG_CONFIG_PATH**: pkg-config search paths
- **CMAKE_PREFIX_PATH**: CMake package search paths
- **JAVA_HOME**: Java installation path
- **PYTHONPATH**: Python module search paths

### Output Formats

#### .env Format (dotenv)
```bash
PATH="/usr/local/bin:/usr/bin:/bin"
LD_LIBRARY_PATH="/usr/local/lib:/usr/lib"
CMAKE_PREFIX_PATH="/usr/local"
```

#### CMake Format
```cmake
set(ENV{PATH} "/usr/local/bin:/usr/bin:/bin")
set(ENV{LD_LIBRARY_PATH} "/usr/local/lib:/usr/lib")
set(ENV{CMAKE_PREFIX_PATH} "/usr/local")
```

#### VSCode Format
```json
"environment": {
  "PATH": "/usr/local/bin:/usr/bin:/bin",
  "LD_LIBRARY_PATH": "/usr/local/lib:/usr/lib",
  "CMAKE_PREFIX_PATH": "/usr/local"
}
```

#### CLion Format
```
PATH=/usr/local/bin:/usr/bin:/bin;LD_LIBRARY_PATH=/usr/local/lib:/usr/lib;CMAKE_PREFIX_PATH=/usr/local
```

## Advanced Usage

### Automatic Module Import Workflow

Use `jarvis mod import` for software that provides setup scripts:

```bash
# Import a module from Spack
jarvis mod import "spack-gcc" "spack load gcc@11.2.0"

# Import from Environment Modules
jarvis mod import "intel-compiler" "module load intel/2021.4"

# Import from custom setup script
jarvis mod import "mylib" "source /opt/mylib/env-setup.sh"

# Import from multiple commands
jarvis mod import "dev-env" "export CC=gcc && export CXX=g++ && export PATH=/opt/tools:\$PATH"

# Update modules when software is updated
jarvis mod update spack-gcc
```

### Working with Dependencies

Create modules that depend on other modules:

```bash
# Create base module
jarvis mod create base-tools
jarvis mod prepend base-tools PATH="/opt/base/bin"

# Create dependent module
jarvis mod create advanced-tools
jarvis mod prepend advanced-tools PATH="/opt/advanced/bin"

# Add dependency using the dep command
jarvis mod dep add base-tools advanced-tools

# Or add to current module
jarvis mod cd advanced-tools
jarvis mod dep add base-tools
```

The generated TCL file will include:
```tcl
module load base-tools
prepend-path PATH /opt/advanced/bin
```

#### Dependency Management Commands

**Add a dependency:**
```bash
jarvis mod dep add <dependency> [module_name]
```
Adds `dependency` as a requirement for `module_name`. If `module_name` is omitted, uses the current module.

**Remove a dependency:**
```bash
jarvis mod dep remove <dependency> [module_name]
```
Removes `dependency` from `module_name`. If `module_name` is omitted, uses the current module.

**Example:**
```bash
# Add multiple dependencies to a module
jarvis mod cd myapp
jarvis mod dep add zlib
jarvis mod dep add openssl
jarvis mod dep add python

# Remove a dependency
jarvis mod dep remove python

# Work with a specific module
jarvis mod dep add gcc myapp
jarvis mod dep remove gcc myapp
```

### Batch Environment Setup

Set up multiple environment variables at once:

```bash
# Configure compiler environment
jarvis mod create gcc-toolchain
jarvis mod setenv gcc-toolchain \
  CC="gcc-9" \
  CXX="g++-9" \
  FC="gfortran-9" \
  CFLAGS="-O3 -march=native" \
  CXXFLAGS="-O3 -march=native"

jarvis mod prepend gcc-toolchain \
  PATH="/opt/gcc-9/bin" \
  LD_LIBRARY_PATH="/opt/gcc-9/lib64" \
  LIBRARY_PATH="/opt/gcc-9/lib64" \
  CMAKE_PREFIX_PATH="/opt/gcc-9"
```

### Integration with Build Systems

#### CMake Integration
```bash
# Build environment profile for CMake
jarvis mod profile m=cmake path=build-env.cmake

# Use in CMakeLists.txt
include(build-env.cmake)
```

#### IDE Integration
```bash
# Generate VSCode environment
jarvis mod profile m=vscode

# Copy output to .vscode/launch.json environment section
```

### Module Templating

Create reusable module templates by copying YAML files:

```bash
# Create template module
jarvis mod create template-package

# Copy and customize
cp $(jarvis mod yaml template-package) $(jarvis mod yaml new-package)
# Edit new-package.yaml as needed

# Regenerate TCL file
jarvis mod setenv new-package DUMMY="trigger_regeneration"
```

## Integration with Package Managers

The jarvis module system integrates well with other package management approaches:

### With Spack
```bash
# Install with Spack, then create module
spack install zlib
jarvis mod create zlib-spack
jarvis mod prepend zlib-spack PATH="$(spack location -i zlib)/bin"
jarvis mod prepend zlib-spack LD_LIBRARY_PATH="$(spack location -i zlib)/lib"
```

### With Conda/Mamba
```bash
# Install with conda, then create module
conda create -n myenv package_name
jarvis mod create conda-myenv
jarvis mod prepend conda-myenv PATH="$CONDA_PREFIX/bin"
jarvis mod setenv conda-myenv CONDA_ENV_NAME="myenv"
```

### With Manual Installation
The primary use case - installing packages manually:

```bash
jarvis mod create custom-package
cd $(jarvis mod src custom-package)

# Download and build
git clone https://github.com/project/repo.git
cd repo
mkdir build && cd build
cmake -DCMAKE_INSTALL_PREFIX=$(jarvis mod root custom-package) ..
make -j8 install

# Configure module
jarvis mod prepend custom-package PATH="$(jarvis mod root custom-package)/bin"
jarvis mod prepend custom-package LD_LIBRARY_PATH="$(jarvis mod root custom-package)/lib"
```

## Best Practices

### Naming Conventions
- Use descriptive module names: `gcc-toolchain`, `openmpi-4.1`, `custom-analysis-tools`
- Include version numbers when managing multiple versions: `zlib-1.3`, `openssl-3.0`
- Use hyphens rather than underscores for consistency

### Directory Organization
- Keep source code in the `src/` directory for easy reference
- Use consistent installation prefixes via `$(jarvis mod root module)`
- Document installation procedures in README files within source directories

### Environment Management
- Prepend to PATH-like variables rather than setting them completely
- Set package-specific variables (like `ZLIB_ROOT`) using `setenv`
- Use dependency tracking for complex package relationships

### Profile Building
- Create environment profiles before starting development sessions
- Use appropriate formats for different tools (VSCode, CLion, CMake)
- Keep profiles up to date as your environment changes

This comprehensive module management system provides a complete solution for managing manually-installed packages with automatic modulefile generation and flexible environment configuration.
```

### `docs/package_dev_guide.md`

```markdown
# Jarvis-CD Package Development Guide

This guide explains how to develop custom packages for Jarvis-CD, including the repository structure, abstract methods, and implementation examples.

## Table of Contents

1. [Repository Structure](#repository-structure)
2. [Pipeline Indexes](#pipeline-indexes)
3. [Package Types](#package-types)
4. [Abstract Methods](#abstract-methods)
5. [Environment Variables](#environment-variables)
6. [Configuration](#configuration)
7. [Package Directory Structure](#package-directory-structure)
8. [Execution System](#execution-system)
9. [Utility Classes](#utility-classes)
10. [Interceptor Development](#interceptor-development)
11. [Implementation Examples](#implementation-examples)
12. [Best Practices](#best-practices)

## Repository Structure

Jarvis-CD packages are organized in repositories with a specific structure that supports both packages and pipeline indexes. **IMPORTANT**: All repositories must include a subdirectory with the same name as the repository to be properly recognized by Jarvis-CD.

### Required Repository Structure

```
my_repo/
├── my_repo/                  # REQUIRED: subdirectory with same name as repo
│   ├── package1/
│   │   ├── __init__.py
│   │   └── pkg.py            # Main package implementation
│   ├── package2/
│   │   ├── __init__.py
│   │   └── pkg.py
│   └── __init__.py
├── pipelines/                # REQUIRED: pipeline index directory
│   ├── basic_workflow.yaml
│   ├── performance_test.yaml
│   ├── examples/
│   │   ├── simple_demo.yaml
│   │   └── advanced_demo.yaml
│   └── io_benchmarks/
│       ├── ior_test.yaml
│       └── fio_test.yaml
└── README.md                 # Optional: repository documentation
```

### Key Requirements

1. **Repository Root**: Contains two main subdirectories: `{repo_name}/` and `pipelines/`
2. **Package Directory**: Must be `{repo_name}/{package_name}/` (e.g., `my_repo/ior/`, `my_repo/redis/`)
3. **Pipeline Index Directory**: Must be `pipelines/` for pipeline script discovery
4. **Main File**: Must be named `pkg.py` (not `package.py`)
5. **Class Name**: Must follow UpperCamelCase naming convention. For single words use capitalized form (e.g., `Ior`, `Redis`). For snake_case package names, convert to UpperCamelCase (e.g., `data_stagein` → `DataStagein`, `redis_benchmark` → `RedisBenchmark`)
6. **Init Files**: Include `__init__.py` files for proper Python module structure

### Package Class Naming Convention

Package class names must follow the UpperCamelCase (PascalCase) naming convention:

| Package Directory | Expected Class Name | Notes |
|-------------------|-------------------|-------|
| `ior` | `Ior` | Single word - capitalize first letter |
| `redis` | `Redis` | Single word - capitalize first letter |
| `data_stagein` | `DataStagein` | Snake_case - convert to UpperCamelCase |
| `redis_benchmark` | `RedisBenchmark` | Snake_case - convert to UpperCamelCase |
| `cosmic_tagger` | `CosmicTagger` | Snake_case - convert to UpperCamelCase |
| `adios2_gray_scott` | `Adios2GrayScott` | Mixed - convert to UpperCamelCase |

**Important**: Package loading will fail with a fatal error if the class name doesn't match this convention, preventing the package from being added to pipelines.

### Adding Repositories

```bash
# Add a repository to Jarvis
jarvis repo add /path/to/my_repo

# List registered repositories
jarvis repo list
```

## Pipeline Indexes

Pipeline indexes allow repositories to provide pre-configured pipeline scripts that users can discover, load, and copy. These scripts demonstrate common workflows, provide testing templates, and serve as examples for package usage.

### Pipeline Index Structure

The `pipelines/` directory in your repository serves as the pipeline index. It can contain:

- **YAML Files**: Pipeline scripts that can be loaded directly
- **Subdirectories**: Organized collections of related pipeline scripts
- **Nested Structure**: Multiple levels of organization

```
pipelines/
├── basic_workflow.yaml           # Simple pipeline script
├── performance_test.yaml         # Performance testing pipeline
├── examples/                     # Example pipelines subdirectory
│   ├── simple_demo.yaml
│   ├── advanced_demo.yaml
│   └── multi_node_example.yaml
├── benchmarks/                   # Benchmark pipelines subdirectory
│   ├── io_tests/
│   │   ├── ior_benchmark.yaml
│   │   └── fio_benchmark.yaml
│   └── compute_tests/
│       ├── hpl_benchmark.yaml
│       └── stream_benchmark.yaml
└── integration_tests/            # Integration test pipelines
    ├── full_stack_test.yaml
    └── component_test.yaml
```

### Pipeline Index Commands

Users can interact with pipeline indexes using the following commands:

#### List Available Pipeline Scripts

```bash
# List all pipeline scripts from all repositories
jarvis ppl index list

# List pipeline scripts from a specific repository
jarvis ppl index list my_repo
```

The output shows both files and directories with color coding:
- **Files**: Default color - these are loadable pipeline scripts
- **Directories**: Cyan color with "(directory)" label - these contain more scripts

#### Load Pipeline Script from Index

```bash
# Load a pipeline script directly into the current workspace
jarvis ppl index load my_repo.examples.simple_demo

# Load from nested directory structure
jarvis ppl index load my_repo.benchmarks.io_tests.ior_benchmark
```

#### Copy Pipeline Script from Index

```bash
# Copy pipeline script to current directory
jarvis ppl index copy my_repo.examples.simple_demo

# Copy to specific location
jarvis ppl index copy my_repo.examples.simple_demo /path/to/output/

# Copy to specific filename
jarvis ppl index copy my_repo.examples.simple_demo ./my_custom_pipeline.yaml
```

### Creating Pipeline Scripts for Your Repository

When developing packages, include example pipeline scripts that demonstrate:

1. **Basic Usage**: Simple pipeline showing package basics
2. **Advanced Configuration**: Pipeline with comprehensive configuration options
3. **Integration Examples**: Pipelines showing how your packages work with others
4. **Performance Testing**: Pipelines for benchmarking and validation
5. **Development/Testing**: Pipelines for package development and debugging

#### Example Pipeline Script

```yaml
# pipelines/examples/basic_usage.yaml
name: basic_usage_example
env:
  # Optional: define environment for this pipeline
  EXAMPLE_VAR: "value"

pkgs:
  - pkg_type: my_repo.my_package
    pkg_name: main_app
    # Package configuration
    input_file: "test_input.dat"
    output_dir: "/tmp/output"
    threads: 4

interceptors:
  # Optional: interceptors for monitoring/profiling
  - pkg_type: builtin.profiler
    pkg_name: perf_monitor
    sampling_rate: 1000
    output_file: "/tmp/profile.out"
```

### Pipeline Index Best Practices

#### 1. Organize by Purpose

```
pipelines/
├── examples/          # Basic usage examples
├── benchmarks/        # Performance testing
├── tutorials/         # Step-by-step learning
├── validation/        # Package validation tests
└── integration/       # Multi-package workflows
```

#### 2. Use Descriptive Names

```
# ✅ Good names
ior_single_node_test.yaml
multi_node_mpi_benchmark.yaml
storage_performance_analysis.yaml

# ❌ Poor names
test.yaml
example.yaml
config.yaml
```

#### 3. Include Documentation Comments

```yaml
# Pipeline: I/O Performance Benchmark
# Purpose: Measures I/O performance using IOR with different block sizes
# Requirements: MPI environment, shared filesystem
# Expected Runtime: 10-15 minutes
name: io_performance_benchmark

# Environment setup for consistent testing
env:
  IOR_HINT: "posix"
  TEST_DIR: "/shared/benchmark"

pkgs:
  - pkg_type: my_repo.ior
    pkg_name: ior_test
    # Test with 1GB files using 4 processes
    nprocs: 4
    block: "1G"
    transfer: "64K"
    test_file: "${TEST_DIR}/ior_test_file"
```

#### 4. Provide Multiple Complexity Levels

```
pipelines/
├── simple_demo.yaml           # Minimal configuration
├── intermediate_demo.yaml     # Common options configured
└── advanced_demo.yaml         # Full configuration showcase
```

#### 5. Include Validation Pipelines

```yaml
# pipelines/validation/package_test.yaml
# Validation pipeline to ensure package works correctly
name: package_validation
pkgs:
  - pkg_type: my_repo.my_package
    pkg_name: validation_test
    # Minimal configuration for basic functionality test
    mode: "validation"
    quick_test: true
    expected_output: "test_passed"
```

### Repository Integration

When users add your repository with `jarvis repo add`, both the packages and pipeline indexes become available:

```bash
# Add repository (exposes both packages and pipeline indexes)
jarvis repo add /path/to/my_repo

# Discover packages
jarvis ppl append my_repo.package_name

# Discover pipeline scripts
jarvis ppl index list my_repo
jarvis ppl index load my_repo.examples.basic_usage
```

This integration provides a complete development ecosystem where users can:
1. **Discover**: Find available packages and example pipelines
2. **Learn**: Use example pipelines to understand package capabilities
3. **Develop**: Copy and modify pipeline scripts for their own use
4. **Validate**: Use provided test pipelines to verify functionality

## Package Types

Jarvis-CD provides several base classes for different types of packages. **The recommended approach is to use multi-implementation packages with RouteApp**, which allows a single package to support multiple deployment modes (e.g., bare metal, containerized).

## Multi-Implementation Packages (RECOMMENDED)

**This is the default and recommended way to create packages.** Multi-implementation packages use the `RouteApp` pattern to support multiple deployment modes from a single package interface.

### Architecture Overview

A multi-implementation package consists of:

1. **Router Class (`pkg.py`)**: Main package class that inherits from `RouteApp` and defines the configuration menu
2. **Implementation Delegates**: Separate files for each deployment mode (e.g., `default.py`, `container.py`)
3. **Deployment Mode Routing**: The router automatically delegates lifecycle methods to the appropriate implementation based on `deploy_mode`

### Directory Structure

```
my_package/
├── __init__.py               # Package initialization
├── pkg.py                    # Router class (inherits from RouteApp)
├── default.py                # Default (bare metal) implementation
├── container.py              # Container implementation (optional)
└── README.md                 # Package documentation
```

### The RouteApp Pattern

`RouteApp` is a base class that provides automatic routing to deployment-specific implementations. It eliminates code duplication and makes packages deployment-agnostic.

#### Router Class Example (`pkg.py`)

```python
"""
IOR benchmark package - supports both bare metal and containerized deployment.
"""
from jarvis_cd.core.route_pkg import RouteApp


class Ior(RouteApp):
    """
    Router class for IOR deployment - delegates to default or container implementation.
    """

    def _configure_menu(self):
        """
        Define configuration parameters shared by all deployment modes.

        :return: List of configuration dictionaries
        """
        # Get base menu from RouteApp (includes deploy_mode parameter)
        base_menu = super()._configure_menu()

        # Override deploy_mode choices to specify available deployment modes for this package
        for item in base_menu:
            if item['name'] == 'deploy_mode':
                item['choices'] = ['default', 'container']
                break

        # Add package-specific parameters
        ior_menu = [
            {
                'name': 'nprocs',
                'msg': 'Number of processes',
                'type': int,
                'default': 1,
            },
            {
                'name': 'block',
                'msg': 'Amount of data to generate per-process',
                'type': str,
                'default': '32m',
            },
            {
                'name': 'xfer',
                'msg': 'The size of data transfer',
                'type': str,
                'default': '1m',
            },
            {
                'name': 'api',
                'msg': 'The I/O api to use',
                'type': str,
                'choices': ['posix', 'mpiio', 'hdf5'],
                'default': 'posix',
            }
        ]

        return base_menu + ior_menu
```

**Key Points:**
- Router class name matches package name (e.g., `Ior` for `builtin.ior`)
- Only implements `_configure_menu()` to define parameters and available deployment modes
- Overrides `deploy_mode` choices to specify which deployment modes are supported (e.g., `['default', 'container']`)
- All lifecycle methods (`start`, `stop`, `clean`, `kill`, `status`) are automatically delegated
- Configuration menu is shared across all deployment modes
- The `deploy_mode` parameter defaults to `'default'` and is automatically included in the configuration menu

#### Default Implementation (`default.py`)

```python
"""
IOR benchmark - bare metal deployment.
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, MpiExecInfo, PsshExecInfo, Mkdir


class IorDefault(Application):
    """
    IOR deployment on bare metal using MPI.
    """

    def _init(self):
        """Initialize paths"""
        pass

    def _configure(self, **kwargs):
        """
        Configure for bare metal deployment.

        :param kwargs: Configuration parameters
        """
        # Call parent configuration
        super()._configure(**kwargs)

        # Create output directory on all nodes
        import os
        import pathlib
        out = os.path.expandvars(self.config['out'])
        parent_dir = str(pathlib.Path(out).parent)
        Mkdir(parent_dir,
              PsshExecInfo(env=self.mod_env,
                           hostfile=self.jarvis.hostfile)).run()

    def start(self):
        """
        Start IOR benchmark.
        """
        cmd = [
            'ior',
            f'-b {self.config["block"]}',
            f'-t {self.config["xfer"]}',
            f'-a {self.config["api"]}',
            f'-o {self.config["out"]}',
        ]

        Exec(' '.join(cmd),
             MpiExecInfo(env=self.mod_env,
                         hostfile=self.jarvis.hostfile,
                         nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'])).run()

    def stop(self):
        """Stop IOR (usually no action needed for benchmarks)"""
        pass

    def clean(self):
        """Clean IOR output files"""
        from jarvis_cd.shell import Rm
        Rm(self.config['out'] + '*',
           PsshExecInfo(env=self.env,
                        hostfile=self.jarvis.hostfile)).run()
```

**Key Points:**
- Class name is `{PackageName}Default` (e.g., `IorDefault`)
- Inherits from `Application` or `Service`
- Implements standard lifecycle methods for bare metal deployment
- Has access to all package configuration via `self.config`

#### Container Implementation (`container.py`)

```python
"""
IOR benchmark - containerized deployment.
"""
from jarvis_cd.core.container_pkg import ContainerApplication


class IorContainer(ContainerApplication):
    """
    IOR deployment using Docker/Podman containers.
    """

    def augment_container(self) -> str:
        """
        Generate Dockerfile commands to install IOR in a container.

        :return: Dockerfile commands as a string
        """
        return """
# Install IOR using spack
RUN . "${SPACK_DIR}/share/spack/setup-env.sh" && \\
    spack install -y ior

# Copy IOR executables to /usr/bin
RUN . "${SPACK_DIR}/share/spack/setup-env.sh" && \\
    spack load ior && \\
    cp -r $(spack location -i ior)/bin/* /usr/bin || true && \\
    cp -r $(spack location -i mpi)/bin/* /usr/bin || true
"""

    def _configure(self, **kwargs):
        """
        Configure container deployment.

        :param kwargs: Configuration parameters
        """
        # Call parent configuration
        super()._configure(**kwargs)

        # Note: For pipeline-level containers, Dockerfile and compose files
        # are generated by the pipeline, not by individual packages.
```

**Key Points:**
- Class name is `{PackageName}Container` (e.g., `IorContainer`)
- Inherits from `ContainerApplication`
- Implements `augment_container()` to add package to container image
- `start()`, `stop()`, `clean()` are handled by pipeline-level container orchestration

### Deploy Mode Routing

The `deploy_mode` parameter determines which implementation delegate is used:

| `deploy_mode` Value | Delegate Class Name | Implementation File |
|---------------------|---------------------|---------------------|
| `default` | `{PackageName}Default` | `default.py` |
| `container` | `{PackageName}Container` | `container.py` |
| `kubernetes` | `{PackageName}Kubernetes` | `kubernetes.py` |
| Custom | `{PackageName}{CustomMode}` | `{custom_mode}.py` |

**Routing Logic:**
1. Router reads `deploy_mode` from `self.config['deploy_mode']` (defaults to `'default'`)
2. Router constructs delegate class name: `f"{PackageName}{DeployMode.title()}"`
3. Router imports and instantiates delegate from appropriate file
4. Router forwards lifecycle method calls to delegate

**Configuration:**
- The `deploy_mode` parameter is automatically included in the package configuration menu
- Subclasses specify available modes by overriding the `choices` field
- Users can see available deployment modes via `jarvis pkg conf --help`

### Deploy Mode Configuration

The `deploy_mode` can be set at two levels:

#### 1. Pipeline-Level (Recommended for Containers)

Set `deploy_mode` at the pipeline level to containerize all packages:

```yaml
name: my_pipeline

# Container configuration - applies to all packages with container support
deploy_mode: container
container_name: my_app_container
container_engine: podman
container_base: docker.io/iowarp/iowarp-build:latest

pkgs:
  - pkg_type: builtin.ior
    pkg_name: ior_benchmark
    # Inherits deploy_mode=container from pipeline
    nprocs: 4
    block: 1G
```

#### 2. Package-Level (Per-Package Control)

Set `deploy_mode` per package for mixed deployments:

```yaml
name: my_pipeline

pkgs:
  # Run IOR in container
  - pkg_type: builtin.ior
    pkg_name: ior_benchmark
    deploy_mode: container  # Package-specific setting
    nprocs: 4

  # Run database on bare metal
  - pkg_type: builtin.redis
    pkg_name: database
    deploy_mode: default  # Bare metal deployment
    port: 6379
```

### Adding Multiple Deployment Modes

To support additional deployment modes:

1. **Add the implementation file**:

```python
# custom_mode.py
from jarvis_cd.core.pkg import Application

class IorCustomMode(Application):
    """Custom deployment mode"""

    def _configure(self, **kwargs):
        super()._configure(**kwargs)
        # Custom configuration logic

    def start(self):
        # Custom start logic
        pass
```

2. **Update the router's configuration menu** to include the new choice:

```python
# pkg.py
def _configure_menu(self):
    base_menu = super()._configure_menu()

    # Add new deployment mode to choices
    for item in base_menu:
        if item['name'] == 'deploy_mode':
            item['choices'] = ['default', 'container', 'custom_mode']
            break

    # ... rest of menu ...
    return base_menu + ior_menu
```

3. **Use it in pipeline YAML**:

```yaml
pkgs:
  - pkg_type: builtin.ior
    deploy_mode: custom_mode  # Routes to IorCustomMode class
```

### Benefits of Multi-Implementation Pattern

1. **Single Package Interface**: Users interact with one package regardless of deployment mode
2. **No Code Duplication**: Configuration menu defined once in router class
3. **Easy Maintenance**: Update deployment logic without changing package interface
4. **Flexible Deployment**: Mix deployment modes within single pipeline
5. **Container Support**: Seamless integration with containerized deployments

### Migration from Single-Implementation Packages

To migrate an existing package to multi-implementation:

1. **Create router class** in `pkg.py`:
   ```python
   from jarvis_cd.core.route_pkg import RouteApp

   class MyPackage(RouteApp):
       def _configure_menu(self):
           base_menu = super()._configure_menu()
           # Move configuration menu here
           return base_menu + my_menu
   ```

2. **Move existing implementation** to `default.py`:
   ```python
   from jarvis_cd.core.pkg import Application

   class MyPackageDefault(Application):
       # Move existing lifecycle methods here
       def _configure(self, **kwargs):
           super()._configure(**kwargs)
           # Existing configuration logic

       def start(self):
           # Existing start logic
           pass
   ```

3. **Add container implementation** (optional) in `container.py`:
   ```python
   from jarvis_cd.core.container_pkg import ContainerApplication

   class MyPackageContainer(ContainerApplication):
       def augment_container(self) -> str:
           return """# Dockerfile commands"""
   ```

4. **Update `__init__.py`**:
   ```python
   from .pkg import MyPackage
   __all__ = ['MyPackage']
   ```

## Traditional Package Types (Legacy)

The following base classes are available for packages that don't need multiple deployment modes. However, **RouteApp is now recommended** even for single-mode packages to support future extensibility.

### 1. SimplePackage (jarvis_cd.basic.pkg.SimplePackage)

**Most common base class** - Use this for packages that need interceptor support. Most builtin packages inherit from this.

```python
from jarvis_cd.core.pkg import SimplePackage

class MyPackage(SimplePackage):
    def _init(self):
        # Initialize variables
        self.my_var = None
    
    def _configure_menu(self):
        # Get base menu from SimplePackage (includes interceptors)
        base_menu = super()._configure_menu()
        
        # Add package-specific menu items
        package_menu = [
            {
                'name': 'input_file',
                'msg': 'Input file path',
                'type': str,
                'default': 'input.dat'
            }
        ]
        
        return base_menu + package_menu
    
    def _configure(self, **kwargs):
        # Configure the package - update_config() called automatically
        
    def start(self):
        # Process interceptors automatically
        self._process_interceptors()
        # Run the package
        pass
```

### 2. Application (jarvis_cd.basic.pkg.Application)

For applications that run and complete automatically (e.g., benchmarks, data processing tools).

```python
from jarvis_cd.core.pkg import Application

class MyApp(Application):
    def _init(self):
        # Initialize variables
        self.output_file = None
        
    def _configure_menu(self):
        return [
            {
                'name': 'output_file',
                'msg': 'Output file path',
                'type': str,
                'default': 'output.dat'
            }
        ]
    
    def _configure(self, **kwargs):
        # Configuration automatically updated
    
    def start(self):
        # Run the application
        pass
    
    def stop(self):
        # Usually not needed for applications
        pass
```

### 3. Service (jarvis_cd.basic.pkg.Service)

For long-running services that need manual stopping (e.g., databases, web servers).

```python
from jarvis_cd.core.pkg import Service

class MyService(Service):
    def _init(self):
        # Initialize variables
        self.daemon_process = None
        
    def _configure_menu(self):
        return [
            {
                'name': 'port',
                'msg': 'Service port',
                'type': int,
                'default': 8080
            }
        ]
    
    def _configure(self, **kwargs):
        # Configuration automatically updated
    
    def start(self):
        # Start the service
        pass
    
    def stop(self):
        # Stop the service
        pass
    
    def status(self) -> str:
        # Return service status
        return "running"
```

### 4. Interceptor (jarvis_cd.core.pkg.Interceptor)

For packages that modify environment variables to intercept system calls (e.g., profiling tools, I/O interceptors). Interceptors work by modifying `LD_PRELOAD` and other environment variables to inject custom libraries into target applications.

**Key Method: `modify_env()`**
Interceptors must implement the `modify_env()` method, which is automatically called by Jarvis to modify the environment before other packages run. This method should use `setenv()` and `prepend_env()` to modify environment variables, particularly `LD_PRELOAD`.

```python
from jarvis_cd.core.pkg import Interceptor
import os

class MyInterceptor(Interceptor):
    def _init(self):
        # Initialize variables
        self.interceptor_lib = None
        
    def _configure_menu(self):
        return [
            {
                'name': 'library_path',
                'msg': 'Path to interceptor library',
                'type': str,
                'default': '/usr/lib/libinterceptor.so'
            },
            {
                'name': 'enable_tracing',
                'msg': 'Enable detailed tracing',
                'type': bool,
                'default': False
            }
        ]
    
    def _configure(self, **kwargs):
        # Configuration automatically updated
        
        # Find the interceptor library using the built-in find_library method
        lib_path = self.find_library('interceptor')
        if not lib_path:
            lib_path = self.config['library_path']
            
        if not os.path.exists(lib_path):
            raise FileNotFoundError(f"Interceptor library not found: {lib_path}")
            
        self.interceptor_lib = lib_path
        self.log(f"Found interceptor library: {lib_path}")
    
    def modify_env(self):
        """
        Modify environment for interception - called automatically by Jarvis.
        
        This method is where interceptors set up LD_PRELOAD and other environment
        variables needed for interception to work.
        """
        # Add interceptor library to LD_PRELOAD
        current_preload = self.mod_env.get('LD_PRELOAD', '')
        if current_preload:
            new_preload = f"{self.interceptor_lib}:{current_preload}"
        else:
            new_preload = self.interceptor_lib
            
        self.setenv('LD_PRELOAD', new_preload)
        
        # Set interceptor configuration environment variables
        if self.config['enable_tracing']:
            self.setenv('INTERCEPTOR_TRACE', '1')
            self.setenv('INTERCEPTOR_TRACE_FILE', f'{self.shared_dir}/trace.log')
        
        self.log(f"Interceptor environment configured with LD_PRELOAD: {new_preload}")
```

**Important Notes:**
- Interceptors use `modify_env()` method, not `start()` 
- `modify_env()` is called automatically during pipeline start (runtime), not configuration
- Use `setenv()` and `prepend_env()` methods to modify environment variables
- Interceptors share the same `mod_env` reference with the target package
- LD_PRELOAD modifications directly affect the package's execution environment

### Pipeline-Level Interceptor Architecture - NEW SYSTEM

#### Pipeline YAML Structure

Interceptors are now defined at the pipeline level in a separate `interceptors` section:

```yaml
name: my_pipeline
pkgs:
  - pkg_type: example_app
    pkg_name: my_app
    interceptors: ["profiler", "tracer"]  # References to pipeline interceptors
interceptors:
  - pkg_type: performance_profiler
    pkg_name: profiler
    sampling_rate: 1000
    output_file: /tmp/profile.out
  - pkg_type: io_tracer  
    pkg_name: tracer
    trace_reads: true
    trace_writes: true
```

#### Key Architecture Changes

1. **Pipeline-Level Definition**: Interceptors are defined once in the `interceptors` section
2. **Package References**: Packages reference interceptors by name in their `interceptors` list
3. **Runtime Application**: Interceptors are applied during `pipeline.start()`, not configuration
4. **Shared Environment**: Interceptors and packages share the exact same `mod_env` object
5. **Unique IDs**: Interceptor IDs must be unique from package IDs within the pipeline

#### Interceptor Lifecycle

```
Pipeline Start → For Each Package → Apply Referenced Interceptors → Run Package
                                  ↓
                           Load Interceptor Instance
                                  ↓
                           Share mod_env Reference
                                  ↓
                           Call interceptor.modify_env()
                                  ↓  
                           Package starts with modified environment
```

## Abstract Methods

All packages inherit from the base `Pkg` class and can override these methods:

### Required Override Methods

#### `_init(self)`
**Purpose**: Initialize package-specific variables  
**Called**: During package instantiation  
**Notes**: Don't assume `self.config` is initialized. Set default values to None.

```python
def _init(self):
    """Initialize package-specific variables"""
    self.my_variable = None
    self.start_time = None
    self.output_file = None
```

#### `_configure_menu(self) -> List[Dict[str, Any]]`
**Purpose**: Define configuration options for the package  
**Called**: When generating CLI help or configuration forms  
**Returns**: List of configuration parameter dictionaries

```python
def _configure_menu(self):
    """Define configuration options"""
    return [
        {
            'name': 'param_name',
            'msg': 'Description of parameter',
            'type': str,
            'default': 'default_value',
            'choices': ['option1', 'option2'],  # Optional
            'args': [],                         # For nested parameters
        }
    ]
```

**Important**: If inheriting from `SimplePackage`, call the parent method:
```python
def _configure_menu(self):
    """Define configuration options"""
    # Get base menu from SimplePackage (includes interceptors)
    base_menu = super()._configure_menu()
    
    # Add package-specific menu items
    package_menu = [
        {
            'name': 'my_param',
            'msg': 'My parameter description',
            'type': str,
            'default': 'default_value'
        }
    ]
    
    return base_menu + package_menu
```

### Lifecycle Methods

#### `_configure(self, **kwargs)`
**Purpose**: Handle package configuration
**Called**: When package is configured via CLI or programmatically
**Use**: Set up environment variables, generate application-specific configuration files, and create directories
**Note**: Override `_configure()`, not `configure()`. The public `configure()` method automatically calls `self.update_config()` before calling `_configure()`.

**IMPORTANT**: `_configure()` is for ALL setup work, including:
- Setting environment variables
- Generating configuration files
- Creating directories (local and remote)
- Validating configuration parameters
- Preparing resources needed for execution

The `start()` method should ONLY execute programs, not perform any setup.

```python
def _configure(self, **kwargs):
    """Configure the package"""
    # No need to call self.update_config() - it's done automatically

    # Set environment variables
    if self.config['custom_path']:
        self.setenv('MY_APP_PATH', self.config['custom_path'])
        self.prepend_env('PATH', self.config['custom_path'] + '/bin')

    # Create directories on all nodes (setup, not execution)
    from jarvis_cd.shell.process import Mkdir
    output_dir = os.path.expandvars(self.config['output_dir'])
    parent_dir = str(pathlib.Path(output_dir).parent)
    Mkdir(parent_dir,
          PsshExecInfo(env=self.mod_env,
                       hostfile=self.jarvis.hostfile)).run()

    # Generate application-specific configuration files
    # This is where you create config files, validate parameters, etc.
```

#### `start(self)`
**Purpose**: Start the package (application, service, or interceptor)
**Called**: During `jarvis ppl run` and `jarvis ppl start`

**IMPORTANT**: `start()` should ONLY execute programs. All setup work (environment variables, directory creation, configuration files) must be done in `_configure()`.

**DO in start():**
- Execute applications
- Run MPI programs
- Start services
- Launch daemons

**DON'T in start():**
- Create directories (do this in `_configure()`)
- Set environment variables (do this in `_configure()`)
- Generate configuration files (do this in `_configure()`)
- Perform validation (do this in `_configure()`)

```python
def start(self):
    """Start the package - ONLY execution, no setup"""
    # ✅ Good - Just execute the program
    cmd = ['my_application', '--config', self.config['config_file']]
    Exec(' '.join(cmd), LocalExecInfo(env=self.mod_env)).run()

    # ❌ Bad - Don't create directories here
    # Mkdir('/output/dir', LocalExecInfo()).run()  # Do this in _configure()!

    # ❌ Bad - Don't set environment variables here
    # self.setenv('VAR', 'value')  # Do this in _configure()!
```

#### `stop(self)`
**Purpose**: Stop the package  
**Called**: During `jarvis ppl run` and `jarvis ppl stop`

```python
def stop(self):
    """Stop the package"""
    # Gracefully shutdown the application/service
    pass
```

#### `kill(self)`
**Purpose**: Forcibly terminate the package  
**Called**: During `jarvis ppl kill`

```python
def kill(self):
    """Forcibly kill the package"""
    from jarvis_cd.shell.process import Kill
    Kill('my_application', PsshExecInfo(hostfile=self.jarvis.hostfile)).run()
```

#### `clean(self)`
**Purpose**: Clean up package data and temporary files  
**Called**: During `jarvis ppl clean`

```python
def clean(self):
    """Clean package data"""
    from jarvis_cd.shell.process import Rm
    Rm(self.config['output_dir'], 
       PsshExecInfo(hostfile=self.jarvis.hostfile)).run()
```

#### `status(self) -> str`
**Purpose**: Return current package status  
**Called**: During `jarvis ppl status`

```python
def status(self) -> str:
    """Return package status"""
    # Check if process is running, files exist, etc.
    return "running" | "stopped" | "error" | "unknown"
```

## Environment Variables

Jarvis-CD manages environment variables through a pipeline-wide system where environment modifications are propagated between packages.

### Environment Loading

When a pipeline is first loaded, the environment is constructed from:
1. **Pipeline Configuration** (`pipeline.yaml` - `env` section)
2. **Environment File** (`env.yaml` in pipeline directory)  
3. **System Environment** (current shell environment)

### Package Environment Dictionaries

Each package has two environment dictionaries:

### `self.env`
- **Purpose**: Shared environment across the pipeline
- **Source**: Loaded from pipeline environment + modifications from previous packages
- **Propagation**: Changes are propagated to subsequent packages in the pipeline
- **Usage**: Use this to set environment variables that should affect later packages

### `self.mod_env`
- **Purpose**: Package-specific environment copy
- **Source**: Deep copy of `self.env` at package load time
- **Scope**: Private to the package, not propagated
- **Usage**: Used in execution commands, modified by interceptors

### Environment Methods

```python
# Set an environment variable
self.setenv('MY_VAR', 'value')

# Prepend to PATH-like variables
self.prepend_env('PATH', '/new/path')
self.prepend_env('LD_LIBRARY_PATH', '/new/lib')

# Track existing environment variables
self.track_env({'EXISTING_VAR': os.environ.get('EXISTING_VAR', '')})
```

### Environment Propagation

Environment changes are automatically propagated between packages:

```python
# Package 1 (e.g., compiler setup)
def configure(self, **kwargs):
    self.setenv('CC', '/usr/bin/gcc-9')
    self.prepend_env('PATH', '/opt/compiler/bin')

# Package 2 (automatically receives Package 1's environment)
def start(self):
    # self.env now contains CC and PATH from Package 1
    # self.mod_env is a deep copy for this package's use
    Exec('make', LocalExecInfo(env=self.mod_env)).run()
```

### Usage in _configure()

**Always use environment methods in the `_configure()` method:**

```python
def _configure(self, **kwargs):
    """Configure package and set environment"""
    # No need to call self.update_config() - it's done automatically
    
    # Set application-specific environment (will be propagated to later packages)
    if self.config['install_path']:
        self.setenv('MY_APP_HOME', self.config['install_path'])
        self.prepend_env('PATH', f"{self.config['install_path']}/bin")
        self.prepend_env('LD_LIBRARY_PATH', f"{self.config['install_path']}/lib")
    
    # Track system environment if needed
    if 'CUDA_HOME' in os.environ:
        self.track_env({'CUDA_HOME': os.environ['CUDA_HOME']})
```

## Configuration

### Configuration Parameters

Each parameter in `_configure_menu()` supports these fields:

- **`name`** (required): Parameter name
- **`msg`** (required): Description for help text
- **`type`** (required): `str`, `int`, `float`, `bool`
- **`default`**: Default value
- **`choices`**: List of valid options
- **`aliases`**: Alternative parameter names
- **`required`**: Whether parameter is mandatory

### Configuration Access

```python
# Access configuration values
def start(self):
    input_file = self.config['input_file']
    num_procs = self.config['nprocs']
    debug_mode = self.config['debug']
```

## Package Directory Structure

Jarvis-CD provides three key directories that packages can use for organizing files, templates, and configuration:

### `self.pkg_dir` - Package Source Directory

The **package directory** contains the package's source code, templates, and static configuration files.

- **Location**: Points to the package's source directory (e.g., `builtin/builtin/my_package/`)
- **Purpose**: Access template files, default configurations, and package resources
- **Usage**: Read-only access to package-specific resources
- **Common subdirectories**: 
  - `config/` - Template configuration files
  - `templates/` - File templates
  - `scripts/` - Helper scripts

```python
def _configure(self, **kwargs):
    # Configuration automatically updated
    
    # Copy template configuration from package source
    template_path = f'{self.pkg_dir}/config/app_config.xml'
    output_path = f'{self.shared_dir}/app_config.xml'
    
    # Copy and customize template file
    self.copy_template_file(template_path, output_path, 
                           replacements={'PORT': self.config['port']})
```

#### Example Package Structure
```
my_package/
├── pkg.py                    # Main package implementation
├── config/                   # Template configurations
│   ├── app.xml              # Application config template
│   ├── logging.conf         # Logging configuration
│   └── defaults.yaml        # Default settings
├── templates/               # File templates
│   ├── Dockerfile.j2        # Container template
│   └── systemd.service      # Service template
└── scripts/                 # Helper scripts
    ├── setup.sh             # Installation script
    └── health_check.py      # Health monitoring
```

### `self.shared_dir` - Runtime Configuration Directory

The **shared directory** is where packages store generated configuration files that are accessible across the pipeline.

- **Location**: Pipeline-specific directory (e.g., `/tmp/jarvis_pipeline_123/shared/`)
- **Purpose**: Store generated configurations, runtime files, and inter-package communication
- **Usage**: Read-write access for generated files
- **Accessibility**: Available to all packages in the pipeline
- **Persistence**: Exists for the duration of the pipeline

```python
def _configure(self, **kwargs):
    # Configuration automatically updated
    
    # Generate runtime configuration files in shared directory
    self.config_file = f'{self.shared_dir}/database.conf'
    self.log_file = f'{self.shared_dir}/app.log'
    
    # Create configuration with runtime values
    config_content = f"""
    database_port={self.config['port']}
    data_directory={self.config['data_dir']}
    log_file={self.log_file}
    """
    
    with open(self.config_file, 'w') as f:
        f.write(config_content)

def start(self):
    # Use configuration file from shared directory
    cmd = ['my_app', '--config', self.config_file]
    Exec(' '.join(cmd), LocalExecInfo(env=self.mod_env)).run()
```

#### Typical Shared Directory Contents
```
shared/
├── adios2.xml              # Generated ADIOS2 configuration
├── database.conf           # Database configuration
├── hostfile                # MPI hostfile
├── pipeline_env.yaml       # Environment variables
└── app_logs/               # Application logs
    ├── app1.log
    └── app2.log
```

### `self.config_dir` - Package Instance Configuration

The **config directory** is a package-specific directory for storing instance-specific configuration files.

- **Location**: Package-specific directory within the pipeline (e.g., `/tmp/jarvis_pipeline_123/packages/my_package/`)
- **Purpose**: Store package-specific runtime configurations and temporary files
- **Usage**: Read-write access for package-specific files
- **Isolation**: Private to each package instance
- **Cleanup**: Can be cleaned when package is stopped or reset

```python
def _configure(self, **kwargs):
    # Configuration automatically updated
    
    # Create package-specific configuration
    param_file = f'{self.config_dir}/simulation.param'
    
    # Generate instance-specific parameter file
    with open(param_file, 'w') as f:
        f.write(f"""
        simulation_steps={self.config['steps']}
        output_frequency={self.config['output_freq']}
        mesh_size={self.config['mesh_size']}
        """)
    
    self.param_file = param_file

def start(self):
    # Use package-specific configuration
    cmd = ['simulator', '--params', self.param_file]
    Exec(' '.join(cmd), MpiExecInfo(
        env=self.mod_env,
        hostfile=self.jarvis.hostfile,
        nprocs=self.config['nprocs']
    )).run()
```

### Best Practices for Directory Usage

#### 1. Template Files in pkg_dir
```python
def _configure(self, **kwargs):
    # Configuration automatically updated
    
    # Use pkg_dir for accessing template files
    template_xml = f'{self.pkg_dir}/config/adios2_template.xml'
    runtime_xml = f'{self.shared_dir}/adios2.xml'
    
    # Copy and customize template
    self.copy_template_file(template_xml, runtime_xml, 
                           replacements={
                               'ENGINE_TYPE': self.config['engine'],
                               'BUFFER_SIZE': str(self.config['buffer_size'])
                           })
```

#### 2. Runtime Files in shared_dir
```python
def _configure(self, **kwargs):
    # Configuration automatically updated
    
    # Store generated files that other packages might need
    self.hostfile_path = f'{self.shared_dir}/mpi_hostfile'
    self.env_file = f'{self.shared_dir}/app_environment.sh'
    
    # Generate hostfile for MPI applications
    with open(self.hostfile_path, 'w') as f:
        for host in self.jarvis.hostfile:
            f.write(f"{host}\n")
```

#### 3. Instance-specific Files in config_dir
```python
def _configure(self, **kwargs):
    # Configuration automatically updated
    
    # Create package-specific working directory
    self.work_dir = f'{self.config_dir}/workfiles'
    os.makedirs(self.work_dir, exist_ok=True)
    
    # Package-specific temporary files
    self.temp_input = f'{self.config_dir}/input.tmp'
    self.temp_output = f'{self.config_dir}/output.tmp'
```

#### 4. File Organization Example
```python
class MySimulation(Application):
    """Scientific simulation package"""
    
    def _configure(self, **kwargs):
        # Configuration automatically updated
        
        # 1. Access template from package source
        input_template = f'{self.pkg_dir}/config/simulation_input.template'
        
        # 2. Generate shared configuration (accessible to other packages)
        self.shared_config = f'{self.shared_dir}/simulation.xml'
        self.copy_template_file(input_template, self.shared_config, 
                               replacements={'TIME_STEPS': str(self.config['steps'])})
        
        # 3. Create package-specific files
        self.work_dir = f'{self.config_dir}/simulation_work'
        os.makedirs(self.work_dir, exist_ok=True)
        
        # 4. Set environment pointing to configurations
        self.setenv('SIMULATION_CONFIG', self.shared_config)
        self.setenv('SIMULATION_WORK_DIR', self.work_dir)
```

#### 5. Cleanup Considerations
```python
def clean(self):
    """Clean package data"""
    # Clean package-specific files
    if os.path.exists(self.config_dir):
        Rm(self.config_dir, LocalExecInfo()).run()
    
    # Clean shared files this package created
    shared_files = [
        f'{self.shared_dir}/my_app_config.xml',
        f'{self.shared_dir}/my_app.log'
    ]
    for file_path in shared_files:
        if os.path.exists(file_path):
            os.remove(file_path)
```

### Directory Lifecycle

1. **Package Load**: Jarvis sets `pkg_dir`, `shared_dir`, and `config_dir`
2. **Configuration**: Package uses these directories in `_configure()`
3. **Execution**: Applications read from generated configuration files
4. **Cleanup**: Package cleans up generated files in `clean()`

This directory structure enables packages to:
- **Separate concerns**: Templates vs. runtime vs. instance-specific files
- **Share configurations**: Between packages through shared_dir
- **Maintain isolation**: Package-specific files in config_dir
- **Enable reusability**: Template files in pkg_dir can be reused

## Execution System

### Available Execution Classes

```python
from jarvis_cd.shell import Exec, LocalExecInfo, MpiExecInfo, PsshExecInfo, SshExecInfo
from jarvis_cd.shell.process import Kill, Rm, Mkdir, Chmod, Which

# Local execution
Exec('command', LocalExecInfo(env=self.mod_env)).run()

# MPI execution
Exec('mpi_command', MpiExecInfo(
    env=self.mod_env,
    hostfile=self.jarvis.hostfile,
    nprocs=self.config['nprocs'],
    ppn=self.config['ppn']
)).run()

# Parallel SSH execution
Exec('command', PsshExecInfo(
    env=self.mod_env,
    hostfile=self.jarvis.hostfile
)).run()

# Process utilities
Kill('process_name', PsshExecInfo(hostfile=self.jarvis.hostfile)).run()
Rm('/path/to/clean', LocalExecInfo()).run()
```

### Hostfile Access

```python
# Access the hostfile for distributed execution
hostfile = self.jarvis.hostfile

# Use in MPI commands
exec_info = MpiExecInfo(
    hostfile=hostfile,
    nprocs=len(hostfile),  # Number of hosts
    ppn=4  # Processes per node
)
```

### Debugging with GdbServer

The `GdbServer` class enables remote debugging by launching applications under gdbserver. This is particularly useful for debugging MPI applications or applications running on remote nodes. The modern pattern uses a multi-command format that allows precise control over process allocation and environment settings.

#### Understanding GdbServer

The `GdbServer` class wraps your command with gdbserver:

```python
from jarvis_cd.shell.process import GdbServer

# Create a GdbServer instance
gdb_server = GdbServer(cmd='./my_app --args', port=2345)

# Get the gdbserver command string
gdbserver_cmd = gdb_server.get_cmd()  # Returns: "gdbserver :2345 ./my_app --args"
```

The `get_cmd()` method returns the complete gdbserver command string that can be used with any execution method.

#### Modern Pattern with LocalExec

For local execution with debugging support, use the multi-command format with conditional debugging:

```python
from jarvis_cd.shell import Exec, LocalExecInfo
from jarvis_cd.shell.process import GdbServer

def start(self):
    # Build your application command
    app_cmd = f'{self.install_dir}/bin/my_app --config {self.config_path}'

    # Create GdbServer wrapper
    gdb_server = GdbServer(app_cmd, self.config.get('dbg_port', 2345))
    gdbserver_cmd = gdb_server.get_cmd()

    if self.config.get('do_dbg', False):
        # Use multi-command format for debugging
        cmd_list = [
            {
                'cmd': gdbserver_cmd,
                'disable_preload': True  # Prevents LD_PRELOAD issues with gdbserver
            },
            {
                'cmd': app_cmd,
                'nprocs': 0  # Don't run the actual command when debugging
            }
        ]
    else:
        # Normal execution without debugging
        cmd_list = app_cmd

    Exec(cmd_list, LocalExecInfo(env=self.mod_env)).run()
```

**Key Points:**
- `disable_preload`: Set to `True` for gdbserver to prevent LD_PRELOAD environment variables from interfering
- When debugging is enabled, set the actual command's `nprocs` to 0 to prevent it from running
- The multi-command format works with LocalExec when you need fine control

#### Modern Pattern with MpiExec

For MPI applications, the pattern allocates one process for gdbserver and the remaining processes for the application:

```python
from jarvis_cd.shell import Exec, MpiExecInfo
from jarvis_cd.shell.process import GdbServer

def start(self):
    # Build your MPI application command
    ior_cmd = f'ior -a {self.config["xfer"]} -t {self.config["tsize"]} -b {self.config["bsize"]}'

    # Create GdbServer wrapper
    gdb_server = GdbServer(ior_cmd, self.config.get('dbg_port', 4000))
    gdbserver_cmd = gdb_server.get_cmd()

    # Use multi-command format with process allocation
    cmd_list = [
        {
            'cmd': gdbserver_cmd,
            'nprocs': 1 if self.config.get('do_dbg', False) else 0,  # 1 process for gdbserver
            'disable_preload': True  # Prevent LD_PRELOAD interference
        },
        {
            'cmd': ior_cmd,
            'nprocs': None  # Remaining processes (automatically calculated)
        }
    ]

    Exec(cmd_list,
         MpiExecInfo(env=self.mod_env,
                     hostfile=self.jarvis.hostfile,
                     nprocs=self.config['nprocs'],
                     ppn=self.config['ppn'])).run()
```

**Process Allocation Explained:**
- When `do_dbg` is `True`: gdbserver gets 1 process, application gets `nprocs - 1`
- When `do_dbg` is `False`: gdbserver gets 0 processes (doesn't run), application gets all `nprocs`
- Setting `nprocs: None` means "use all remaining processes"

#### Complete Working Example

Here's a complete package implementation with debugging support based on the IOR pattern:

```python
class MyApplication(Application):
    def _configure_menu(self):
        return [
            {
                'name': 'do_dbg',
                'msg': 'Enable remote debugging with gdbserver',
                'type': bool,
                'default': False
            },
            {
                'name': 'dbg_port',
                'msg': 'GDB server port for remote debugging',
                'type': int,
                'default': 4000
            },
            {
                'name': 'nprocs',
                'msg': 'Number of MPI processes',
                'type': int,
                'default': 4
            },
            {
                'name': 'ppn',
                'msg': 'Processes per node',
                'type': int,
                'default': 1
            }
        ]

    def start(self):
        # Build the application command
        app_cmd = f'{self.install_dir}/bin/myapp'
        app_cmd += f' --input {self.config["input_file"]}'
        app_cmd += f' --output {self.config["output_file"]}'

        # Create GdbServer wrapper
        gdb_server = GdbServer(app_cmd, self.config.get('dbg_port', 4000))
        gdbserver_cmd = gdb_server.get_cmd()

        # Prepare multi-command list for MPI execution
        cmd_list = [
            {
                'cmd': gdbserver_cmd,
                'nprocs': 1 if self.config.get('do_dbg', False) else 0,
                'disable_preload': True
            },
            {
                'cmd': app_cmd,
                'nprocs': None  # Use remaining processes
            }
        ]

        # Execute with MPI
        Exec(cmd_list,
             MpiExecInfo(env=self.mod_env,
                         hostfile=self.jarvis.hostfile,
                         nprocs=self.config['nprocs'],
                         ppn=self.config['ppn'])).run()

        print(f"Application started with {self.config['nprocs']} processes")
        if self.config.get('do_dbg', False):
            print(f"GDB server listening on port {self.config['dbg_port']}")
            print(f"Connect with: gdb {self.install_dir}/bin/myapp")
            print(f"Then run: target remote hostname:{self.config['dbg_port']}")
```

#### The disable_preload Flag

The `disable_preload` flag is crucial for gdbserver to work correctly:

```python
{
    'cmd': gdbserver_cmd,
    'disable_preload': True  # IMPORTANT: Always set this for gdbserver
}
```

**Why it's needed:**
- Many HPC environments use LD_PRELOAD for performance libraries or MPI wrappers
- These preloaded libraries can interfere with gdbserver's operation
- Setting `disable_preload: True` temporarily clears LD_PRELOAD for the gdbserver command
- The actual application command still gets the full environment with LD_PRELOAD

#### Connecting to GdbServer

Once your application is running under gdbserver, connect from your development machine:

```bash
# First, ensure you have the same binary locally
$ gdb /path/to/local/binary

# Connect to the remote gdbserver
(gdb) target remote hostname:4000

# Load symbols if needed
(gdb) symbol-file /path/to/binary.symbols

# Set breakpoints
(gdb) break main
(gdb) break my_function

# Continue execution
(gdb) continue
```

For MPI applications, you're debugging rank 0 by default. To debug other ranks, you would need to modify the pattern to assign gdbserver to specific ranks.

#### Simplified Pattern for Quick Debugging

If you don't need the multi-command complexity, here's a simpler pattern:

```python
def start(self):
    cmd = f'{self.install_dir}/bin/my_app --config {self.config_path}'

    if self.config.get('do_dbg', False):
        # Simple debugging with GdbServer
        GdbServer(cmd, self.config['dbg_port'],
                  LocalExecInfo(env=self.mod_env)).run()
    else:
        # Normal execution
        Exec(cmd, LocalExecInfo(env=self.mod_env)).run()
```

This simpler pattern works well for non-MPI applications or when you want to debug all processes.

## Utility Classes

Jarvis-CD provides several utility classes to help with common tasks in package development:

### SizeType - Size String Conversion

The `SizeType` class converts size strings (like "1k", "2M", "10G") to integer byte values using binary multipliers (powers of 2).

#### Supported Multipliers

- **k/K**: 1024 (1 << 10) - Kilobytes
- **m/M**: 1048576 (1 << 20) - Megabytes  
- **g/G**: 1073741824 (1 << 30) - Gigabytes
- **t/T**: 1099511627776 (1 << 40) - Terabytes

#### Basic Usage

SizeType works bidirectionally - it can convert from human-readable strings to bytes, or from bytes to human-readable strings:

```python
from jarvis_cd.util import SizeType

# String -> Bytes (parsing user input)
buffer_size = SizeType("1M")        # 1048576 bytes
cache_size = SizeType("512k")       # 524288 bytes
storage_limit = SizeType("10G")     # 10737418240 bytes

# Bytes -> Human Readable (formatting numeric values)
exact_size = SizeType(1048576)      # "1M" when displayed
partial_size = SizeType(1536)       # "1.5K" when displayed  
float_size = SizeType(2048.5)       # "2K" when displayed (rounded)

# Round-trip conversion works perfectly
original = SizeType("1.5M")
bytes_val = original.bytes          # 1572864
reconstructed = SizeType(bytes_val) # Back to "1.5M"
assert str(original) == str(reconstructed)

# Convert to integer bytes (multiple ways)
bytes_value = int(buffer_size)      # 1048576 - using int() conversion
bytes_value = buffer_size.bytes     # 1048576 - using .bytes property  
bytes_value = buffer_size.to_bytes() # 1048576 - using .to_bytes() method

# Use in configuration
def _configure(self, **kwargs):
    # Configuration automatically updated
    
    # Parse buffer size from config
    buffer_size = SizeType(self.config['buffer_size'])
    self.setenv('BUFFER_SIZE', str(buffer_size.bytes))
    
    # Set memory limits  
    mem_limit = SizeType(self.config.get('memory_limit', '1G'))
    if mem_limit.gigabytes > 8:
        print(f"Warning: Large memory limit: {mem_limit.to_human_readable()}")
```

#### Configuration Integration

Use SizeType in `_configure_menu()` for size-based parameters:

```python
def _configure_menu(self):
    return [
        {
            'name': 'buffer_size',
            'msg': 'Buffer size (e.g., 1M, 512K, 2G)',
            'type': str,
            'default': '1M'
        },
        {
            'name': 'cache_size', 
            'msg': 'Cache size (e.g., 100M, 1G)',
            'type': str,
            'default': '100M'
        },
        {
            'name': 'max_file_size',
            'msg': 'Maximum file size (e.g., 10G, 1T)',
            'type': str,
            'default': '10G'
        }
    ]

def _configure(self, **kwargs):
    # Configuration automatically updated
    
    # Convert size strings to bytes for application use
    buffer_bytes = SizeType(self.config['buffer_size']).bytes
    cache_bytes = SizeType(self.config['cache_size']).bytes
    max_file_bytes = SizeType(self.config['max_file_size']).bytes
    
    # Set environment variables as bytes
    self.setenv('BUFFER_SIZE', str(buffer_bytes))
    self.setenv('CACHE_SIZE', str(cache_bytes))
    self.setenv('MAX_FILE_SIZE', str(max_file_bytes))
    
    # Generate configuration file with byte values
    config_content = f"""
    buffer_size={buffer_bytes}
    cache_size={cache_bytes}
    max_file_size={max_file_bytes}
    """
    
    with open(f'{self.shared_dir}/app_config.conf', 'w') as f:
        f.write(config_content)
```

#### Getting Integer Bytes

There are multiple ways to get the size as integer bytes:

```python
size = SizeType("1M")

# Method 1: int() conversion (most common)
bytes_int = int(size)                    # 1048576

# Method 2: .bytes property  
bytes_int = size.bytes                   # 1048576

# Method 3: .to_bytes() method (explicit)
bytes_int = size.to_bytes()              # 1048576

# All return the same integer value
assert int(size) == size.bytes == size.to_bytes()

# Use in environment variables (strings)
self.setenv('BUFFER_SIZE', str(size.bytes))
self.setenv('CACHE_SIZE', str(int(size)))
```

#### Properties and Conversion

```python
size = SizeType("2G")

# Access different units
print(f"Bytes: {size.bytes}")           # 2147483648
print(f"KB: {size.kilobytes}")          # 2097152.0
print(f"MB: {size.megabytes}")          # 2048.0  
print(f"GB: {size.gigabytes}")          # 2.0
print(f"TB: {size.terabytes}")          # 0.001953125

# Human-readable format
print(f"Human: {size.to_human_readable()}")  # "2G"
print(f"String: {str(size)}")                # "2G"
```

#### Arithmetic Operations

```python
# Arithmetic with other SizeType instances
total_memory = SizeType("1G") + SizeType("512M")  # 1.5G
remaining = SizeType("2G") - SizeType("500M")     # 1.5G

# Arithmetic with numbers
doubled = SizeType("1G") * 2                      # 2G
half = SizeType("1G") / 2                         # 512M

# Comparisons
if SizeType("1G") > SizeType("500M"):
    print("1G is larger than 500M")

# Use in sorting
sizes = [SizeType("1M"), SizeType("1G"), SizeType("100K")]
sorted_sizes = sorted(sizes)  # [100K, 1M, 1G]
```

#### Class Methods

```python
# Create from different units
size1 = SizeType.from_bytes(1048576)      # 1M
size2 = SizeType.from_kilobytes(1024)     # 1M
size3 = SizeType.from_megabytes(1)        # 1M
size4 = SizeType.from_gigabytes(1)        # 1G

# Parse method (same as constructor)
size = SizeType.parse("1G")               # Same as SizeType("1G")

# Create from integer bytes and display human-readable
memory_usage = SizeType.from_bytes(67108864)  # 64M
print(f"Memory usage: {memory_usage}")         # "64M"
```

#### Bidirectional Usage Example

```python
class MemoryMonitor(Application):
    def _configure_menu(self):
        return [
            {
                'name': 'max_memory',
                'msg': 'Maximum memory usage (e.g., 1G, 512M)',
                'type': str,
                'default': '1G'
            }
        ]
    
    def _configure(self, **kwargs):
        # Configuration automatically updated
        
        # Parse user-provided limit (string -> bytes)
        self.max_memory = SizeType(self.config['max_memory'])
        self.setenv('MAX_MEMORY_BYTES', str(self.max_memory.bytes))
        
    def monitor_memory(self):
        # Get current memory usage in bytes from system
        current_bytes = self.get_memory_usage()  # Returns integer bytes
        
        # Convert bytes to human-readable for display (bytes -> string)
        current_readable = SizeType(current_bytes)
        
        print(f"Memory usage: {current_readable} / {self.max_memory}")
        
        # Compare with limit
        if current_bytes > self.max_memory.bytes:
            print(f"Warning: Exceeded memory limit!")
            
        return current_readable
```

#### Convenience Functions

```python
from jarvis_cd.util import size_to_bytes, human_readable_size

# Quick conversion to integer bytes (no SizeType object needed)
bytes_val = size_to_bytes("1M")           # 1048576 (integer)
bytes_val = size_to_bytes("512K")         # 524288 (integer)
bytes_val = size_to_bytes("2G")           # 2147483648 (integer)

# Quick human-readable formatting
readable = human_readable_size(1048576)   # "1M"
readable = human_readable_size(2147483648) # "2G"

# Use in configuration parsing
def parse_config_size(config_value):
    return size_to_bytes(config_value)  # Direct integer bytes
```

#### Input Validation and Error Handling

```python
def _configure(self, **kwargs):
    # Configuration automatically updated
    
    try:
        buffer_size = SizeType(self.config['buffer_size'])
        
        # Validate reasonable limits
        if buffer_size.bytes < 1024:  # Less than 1K
            raise ValueError("Buffer size too small (minimum 1K)")
        elif buffer_size.gigabytes > 100:  # More than 100G
            raise ValueError("Buffer size too large (maximum 100G)")
            
        self.buffer_bytes = buffer_size.bytes
        
    except ValueError as e:
        raise ValueError(f"Invalid buffer_size '{self.config['buffer_size']}': {e}")
```

#### Real-World Usage Examples

##### Memory-Intensive Application

```python
class BigDataProcessor(Application):
    def _configure_menu(self):
        return [
            {
                'name': 'chunk_size',
                'msg': 'Data chunk size for processing (e.g., 64M, 1G)',
                'type': str,
                'default': '64M'
            },
            {
                'name': 'memory_limit',
                'msg': 'Maximum memory usage (e.g., 4G, 16G)',
                'type': str,
                'default': '4G'
            }
        ]
    
    def _configure(self, **kwargs):
        # Configuration automatically updated
        
        chunk_size = SizeType(self.config['chunk_size'])
        memory_limit = SizeType(self.config['memory_limit'])
        
        # Calculate number of chunks that fit in memory
        max_chunks = int(memory_limit.bytes / chunk_size.bytes)
        
        # Set application parameters
        self.setenv('CHUNK_SIZE_BYTES', str(chunk_size.bytes))
        self.setenv('MAX_CHUNKS', str(max_chunks))
        self.setenv('MEMORY_LIMIT_BYTES', str(memory_limit.bytes))
        
        print(f"Processing with {chunk_size.to_human_readable()} chunks")
        print(f"Memory limit: {memory_limit.to_human_readable()}")
        print(f"Max concurrent chunks: {max_chunks}")
```

##### Storage Configuration

```python
class DatabaseApp(Service):
    def _configure_menu(self):
        return [
            {
                'name': 'cache_size',
                'msg': 'Database cache size (e.g., 512M, 2G)',
                'type': str,
                'default': '512M'
            },
            {
                'name': 'log_file_size',
                'msg': 'Maximum log file size (e.g., 100M, 1G)',
                'type': str,
                'default': '100M'
            },
            {
                'name': 'data_threshold',
                'msg': 'Archive threshold (e.g., 10G, 100G)',
                'type': str,
                'default': '10G'
            }
        ]
    
    def _configure(self, **kwargs):
        # Configuration automatically updated
        
        cache_size = SizeType(self.config['cache_size'])
        log_file_size = SizeType(self.config['log_file_size'])
        data_threshold = SizeType(self.config['data_threshold'])
        
        # Generate database configuration
        db_config = f"""
        [memory]
        cache_size = {cache_size.bytes}
        
        [logging]
        max_log_file_size = {log_file_size.bytes}
        
        [storage]
        archive_threshold = {data_threshold.bytes}
        """
        
        with open(f'{self.shared_dir}/database.conf', 'w') as f:
            f.write(db_config)
```

#### Best Practices

1. **Always validate sizes** in configuration methods
2. **Use human-readable defaults** in `_configure_menu()` 
3. **Convert to bytes early** in the configuration process
4. **Provide reasonable limits** and error messages
5. **Use properties** for different unit access
6. **Document expected formats** in parameter descriptions

The SizeType class makes it easy to handle size specifications in a user-friendly way while ensuring consistent binary calculations throughout your packages.

### Package Utility Methods

All package classes inherit several utility methods from the base `Pkg` class that provide common functionality for logging, timing, and file processing.

#### log() - Colored Logging

The `log()` method provides colored console output with package context for debugging and status messages.

```python
def log(self, message, color=None):
    """
    Log a message with package context and optional color.
    
    :param message: Message to log
    :param color: Color to use (from jarvis_cd.util.logger.Color enum), defaults to package color
    """
```

##### Usage Examples

```python
from jarvis_cd.util.logger import Color

class MyPackage(Application):
    def start(self):
        # Default package color (light green)
        self.log("Starting application")
        
        # Custom colors for different message types
        self.log("Configuration loaded successfully", Color.GREEN)
        self.log("Warning: Using default settings", Color.YELLOW)
        self.log("Error: Failed to connect", Color.RED)
        self.log("Debug information", Color.LIGHT_BLACK)
        
        # Available colors include:
        # Color.RED, Color.GREEN, Color.YELLOW, Color.BLUE
        # Color.MAGENTA, Color.CYAN, Color.WHITE
        # Color.LIGHT_RED, Color.LIGHT_GREEN, etc.
```

##### Output Format

Messages are automatically formatted with the package class name:
```
[MyPackage] Starting application
[MyPackage] Configuration loaded successfully
```

#### sleep() - Configurable Delays

The `sleep()` method provides configurable delays with logging, useful for testing, synchronization, or rate limiting.

```python
def sleep(self, time_sec=None):
    """
    Sleep for a specified amount of time.
    
    :param time_sec: Time to sleep in seconds. If not provided, uses self.config['sleep']
    """
```

##### Usage Examples

```python
class MyPackage(Application):
    def _configure_menu(self):
        return [
            {
                'name': 'startup_delay',
                'msg': 'Delay before starting (seconds)',
                'type': int,
                'default': 5
            }
        ]
    
    def start(self):
        # Use explicit delay
        self.log("Waiting 3 seconds before startup")
        self.sleep(3)
        
        # Use configured delay (from self.config['sleep'])
        self.sleep()  # Uses default 'sleep' parameter from common menu
        
        # Use custom configuration parameter
        delay = self.config.get('startup_delay', 0)
        if delay > 0:
            self.log(f"Startup delay: {delay} seconds")
            self.sleep(delay)
```

##### Configuration Integration

The `sleep` parameter is automatically available in all package configuration menus:

```bash
# Configure sleep time
jarvis pkg conf mypackage sleep=10

# The package can then use self.sleep() to sleep for 10 seconds
```

#### copy_template_file() - Template Processing

The `copy_template_file()` method copies files while replacing template constants, useful for generating configuration files from templates.

```python
def copy_template_file(self, source_path, dest_path, replacements=None):
    """
    Copy a template file from source to destination, replacing template constants.
    
    Template constants have the format ##CONSTANT_NAME## and are replaced with
    values from the replacements dictionary.
    
    :param source_path: Path to the source template file
    :param dest_path: Path where the processed file should be saved
    :param replacements: Dictionary of replacements {CONSTANT_NAME: value}
    """
```

##### Template Format

Template constants use the format `##CONSTANT_NAME##`:

```xml
<!-- Template file: config/server.xml -->
<server>
    <hostname>##HOSTNAME##</hostname>
    <port>##PORT##</port>
    <threads>##THREAD_COUNT##</threads>
    <memory>##MEMORY_LIMIT##</memory>
</server>
```

##### Usage Examples

```python
class MyPackage(Service):
    def _configure_menu(self):
        return [
            {
                'name': 'hostname',
                'msg': 'Server hostname',
                'type': str,
                'default': 'localhost'
            },
            {
                'name': 'port',
                'msg': 'Server port',
                'type': int,
                'default': 8080
            },
            {
                'name': 'threads',
                'msg': 'Number of worker threads',
                'type': int,
                'default': 4
            }
        ]
    
    def _configure(self, **kwargs):
        # Generate configuration file from template
        config_file = f"{self.shared_dir}/server.xml"
        
        self.copy_template_file(
            source_path=f"{self.pkg_dir}/config/server.xml.template",
            dest_path=config_file,
            replacements={
                'HOSTNAME': self.config['hostname'],
                'PORT': self.config['port'],
                'THREAD_COUNT': self.config['threads'],
                'MEMORY_LIMIT': '2G'
            }
        )
        
        self.log(f"Generated configuration: {config_file}")
```

##### Result

After processing, the template becomes:

```xml
<!-- Generated file: shared_dir/server.xml -->
<server>
    <hostname>localhost</hostname>
    <port>8080</port>
    <threads>4</threads>
    <memory>2G</memory>
</server>
```

##### Advanced Usage

```python
def _configure(self, **kwargs):
    # Use pkg_dir for template source directory
    template_dir = f"{self.pkg_dir}/templates"
    output_dir = self.shared_dir
    
    # Common replacements for multiple files
    common_vars = {
        'USER': os.environ.get('USER', 'unknown'),
        'HOSTNAME': socket.gethostname(),
        'TIMESTAMP': datetime.now().isoformat(),
        'PID': os.getpid()
    }
    
    # Process multiple template files
    templates = [
        ('config.xml.template', 'config.xml'),
        ('startup.sh.template', 'startup.sh'),
        ('logging.conf.template', 'logging.conf')
    ]
    
    for template_name, output_name in templates:
        self.copy_template_file(
            source_path=f"{template_dir}/{template_name}",
            dest_path=f"{output_dir}/{output_name}",
            replacements={
                **common_vars,  # Include common variables
                'SERVICE_NAME': self.config['service_name'],
                'LOG_LEVEL': self.config.get('log_level', 'INFO')
            }
        )
```

##### Error Handling

The method automatically:
- Creates destination directories if they don't exist
- Provides clear error messages for missing template files
- Logs successful operations with replacement counts
- Raises exceptions for template or I/O errors

## Interceptor Development

Interceptors are specialized packages that modify the execution environment to intercept system calls, library calls, or I/O operations. They are commonly used for profiling, monitoring, debugging, and performance analysis.

### Interceptor Architecture

Interceptors work by:
1. **Library Injection**: Adding shared libraries to `LD_PRELOAD`
2. **Environment Modification**: Setting environment variables for interceptor configuration
3. **Call Interception**: Using library preloading to override system/library functions

### The modify_env() Method - Core Interceptor Interface

**All interceptors must implement the `modify_env()` method.** This is the primary interface that Jarvis uses to apply interceptor functionality to other packages in the pipeline.

#### How modify_env() Works - NEW ARCHITECTURE

1. **Called at Runtime**: Jarvis automatically calls `modify_env()` during `pipeline.start()`, just before each package's `start()` method
2. **Shared Environment**: Interceptors and packages share the same `mod_env` reference (same pointer)
3. **LD_PRELOAD Management**: Most interceptors add libraries to `LD_PRELOAD` to inject interception code
4. **Configuration Setup**: The method can set environment variables that configure the interceptor's behavior
5. **Per-Package Application**: Each interceptor is applied only to packages that reference it in their `interceptors` list

#### modify_env() vs start()

- **`modify_env()`**: Used by interceptors to modify the environment. Called during pipeline start, per package.
- **`start()`**: Used by applications and services to start running. Not typically used by interceptors.

```python
class MyInterceptor(Interceptor):
    def modify_env(self):
        """
        Core interceptor method - modifies shared environment for interception.
        Called automatically during pipeline start, just before package starts.
        
        IMPORTANT: self.mod_env is the SAME OBJECT as the target package's mod_env.
        Any changes made here directly affect the package's execution environment.
        """
        # Add interceptor library to LD_PRELOAD (shared with package)
        current_preload = self.mod_env.get('LD_PRELOAD', '')
        if current_preload:
            self.setenv('LD_PRELOAD', f"{self.interceptor_lib}:{current_preload}")
        else:
            self.setenv('LD_PRELOAD', self.interceptor_lib)
        
        # Set interceptor configuration (shared with package)
        self.setenv('INTERCEPTOR_CONFIG_FILE', f'{self.shared_dir}/interceptor.conf')
        
        # Changes are immediately visible to the package since mod_env is shared
```

### The find_library() Method

The `find_library()` method helps locate shared libraries in the system for interceptor use:

```python
def find_library(self, library_name: str) -> Optional[str]:
    """
    Find a shared library by searching LD_LIBRARY_PATH and system paths.
    
    :param library_name: Name of the library to find
    :return: Path to library if found, None otherwise
    """
```

#### Library Search Order

The method searches for libraries in this order:

1. **Package-specific environment** (`self.mod_env` then `self.env`)
2. **System LD_LIBRARY_PATH** 
3. **Standard system paths**:
   - `/usr/lib`
   - `/usr/local/lib`
   - `/usr/lib64`
   - `/usr/local/lib64`
   - `/lib`
   - `/lib64`

#### Library Name Variations

For a library name like `"profiler"`, it searches for:
- `libprofiler.so` (standard shared library)
- `profiler.so` (as-is with .so extension)
- `libprofiler.a` (static library)
- `profiler` (exact name)

#### Usage Examples

```python
# Find a profiling library
profiler_lib = self.find_library('profiler')
if profiler_lib:
    self.setenv('LD_PRELOAD', profiler_lib)
else:
    raise RuntimeError("Profiler library not found")

# Find MPI profiling library
mpi_profiler = self.find_library('mpiP')
if mpi_profiler:
    current_preload = self.mod_env.get('LD_PRELOAD', '')
    if current_preload:
        self.setenv('LD_PRELOAD', f"{mpi_profiler}:{current_preload}")
    else:
        self.setenv('LD_PRELOAD', mpi_profiler)

# Find multiple interceptor libraries
interceptor_libs = []
for lib_name in ['vtune', 'pin', 'callgrind']:
    lib_path = self.find_library(lib_name)
    if lib_path:
        interceptor_libs.append(lib_path)
        
if interceptor_libs:
    self.setenv('LD_PRELOAD', ':'.join(interceptor_libs))
```

### LD_PRELOAD Management

Interceptors commonly need to manage `LD_PRELOAD` to inject multiple libraries:

```python
def add_to_preload(self, library_path: str):
    """Add a library to LD_PRELOAD safely"""
    current_preload = self.mod_env.get('LD_PRELOAD', '')
    
    # Check if library is already in preload
    if library_path in current_preload.split(':'):
        return
        
    if current_preload:
        new_preload = f"{library_path}:{current_preload}"
    else:
        new_preload = library_path
        
    self.setenv('LD_PRELOAD', new_preload)

def remove_from_preload(self, library_path: str):
    """Remove a library from LD_PRELOAD"""
    current_preload = self.mod_env.get('LD_PRELOAD', '')
    if not current_preload:
        return
        
    libs = current_preload.split(':')
    libs = [lib for lib in libs if lib != library_path]
    
    if libs:
        self.setenv('LD_PRELOAD', ':'.join(libs))
    else:
        # Remove LD_PRELOAD entirely if empty
        if 'LD_PRELOAD' in self.mod_env:
            del self.mod_env['LD_PRELOAD']
```

### Complete Interceptor Examples

#### Performance Profiler Interceptor

```python
from jarvis_cd.core.pkg import Interceptor
import os

class PerfProfiler(Interceptor):
    """Performance profiling interceptor using custom profiling library"""
    
    def _configure_menu(self):
        return [
            {
                'name': 'profiler_lib',
                'msg': 'Profiler library name or path',
                'type': str,
                'default': 'libprofiler'
            },
            {
                'name': 'output_file',
                'msg': 'Profiler output file',
                'type': str,
                'default': 'profile.out'
            },
            {
                'name': 'sample_rate',
                'msg': 'Profiling sample rate (Hz)',
                'type': int,
                'default': 1000
            }
        ]
    
    def _configure(self, **kwargs):
        # Configuration automatically updated
        
        # Try to find the profiler library
        profiler_lib = self.find_library(self.config['profiler_lib'])
        if not profiler_lib:
            # Try using the config value as a direct path
            profiler_lib = self.config['profiler_lib']
            if not os.path.exists(profiler_lib):
                raise FileNotFoundError(f"Profiler library not found: {self.config['profiler_lib']}")
        
        self.profiler_path = profiler_lib
        self.log(f"Using profiler library: {self.profiler_path}")
        
        # Set profiler configuration environment
        self.setenv('PROFILER_OUTPUT', self.config['output_file'])
        self.setenv('PROFILER_SAMPLE_RATE', str(self.config['sample_rate']))
    
    def modify_env(self):
        """Modify environment for profiling interception"""
        # Add profiler to LD_PRELOAD
        self.add_to_preload(self.profiler_path)
        self.log(f"Added profiler to LD_PRELOAD: {self.profiler_path}")
    
    def clean(self):
        # Remove profiler output files
        if os.path.exists(self.config['output_file']):
            os.remove(self.config['output_file'])
            
    def add_to_preload(self, library_path: str):
        current_preload = self.mod_env.get('LD_PRELOAD', '')
        if current_preload:
            self.setenv('LD_PRELOAD', f"{library_path}:{current_preload}")
        else:
            self.setenv('LD_PRELOAD', library_path)
```

#### I/O Tracing Interceptor

```python
from jarvis_cd.core.pkg import Interceptor
import os

class IOTracer(Interceptor):
    """I/O operation tracing interceptor"""
    
    def _configure_menu(self):
        return [
            {
                'name': 'trace_reads',
                'msg': 'Trace read operations',
                'type': bool,
                'default': True
            },
            {
                'name': 'trace_writes', 
                'msg': 'Trace write operations',
                'type': bool,
                'default': True
            },
            {
                'name': 'trace_file',
                'msg': 'I/O trace output file',
                'type': str,
                'default': 'io_trace.log'
            },
            {
                'name': 'min_size',
                'msg': 'Minimum I/O size to trace (bytes)',
                'type': int,
                'default': 1024
            }
        ]
    
    def _configure(self, **kwargs):
        # Configuration automatically updated
        
        # Find the I/O tracing library
        io_lib = self.find_library('iotrace')
        if not io_lib:
            raise RuntimeError("I/O tracing library (libiotrace.so) not found")
            
        self.iotrace_lib = io_lib
        
        # Set I/O tracer configuration
        trace_ops = []
        if self.config['trace_reads']:
            trace_ops.append('read')
        if self.config['trace_writes']:
            trace_ops.append('write')
            
        self.setenv('IOTRACE_OPERATIONS', ','.join(trace_ops))
        self.setenv('IOTRACE_OUTPUT', self.config['trace_file'])
        self.setenv('IOTRACE_MIN_SIZE', str(self.config['min_size']))
        
    def modify_env(self):
        """Modify environment for I/O tracing interception"""
        # Add I/O tracer to LD_PRELOAD
        current_preload = self.mod_env.get('LD_PRELOAD', '')
        if current_preload:
            self.setenv('LD_PRELOAD', f"{self.iotrace_lib}:{current_preload}")
        else:
            self.setenv('LD_PRELOAD', self.iotrace_lib)
            
        self.log(f"I/O tracing enabled: {self.config['trace_file']}")
    
    def status(self) -> str:
        if 'LD_PRELOAD' in self.mod_env and self.iotrace_lib in self.mod_env['LD_PRELOAD']:
            return "tracing"
        return "inactive"
        
    def clean(self):
        # Remove trace files
        if os.path.exists(self.config['trace_file']):
            os.remove(self.config['trace_file'])
```

#### Memory Debugging Interceptor

```python
from jarvis_cd.core.pkg import Interceptor

class MemoryDebugger(Interceptor):
    """Memory debugging interceptor using AddressSanitizer or Valgrind"""
    
    def _configure_menu(self):
        return [
            {
                'name': 'tool',
                'msg': 'Memory debugging tool',
                'type': str,
                'choices': ['asan', 'valgrind', 'tcmalloc'],
                'default': 'asan'
            },
            {
                'name': 'output_dir',
                'msg': 'Output directory for debug reports',
                'type': str,
                'default': '/tmp/memdebug'
            },
            {
                'name': 'detect_leaks',
                'msg': 'Enable leak detection',
                'type': bool,
                'default': True
            }
        ]
    
    def _configure(self, **kwargs):
        # Configuration automatically updated
        
        tool = self.config['tool']
        
        if tool == 'asan':
            # Find AddressSanitizer library
            asan_lib = self.find_library('asan')
            if not asan_lib:
                raise RuntimeError("AddressSanitizer library not found")
            self.debug_lib = asan_lib
            
        elif tool == 'valgrind':
            # Valgrind doesn't use LD_PRELOAD, just set options
            self.debug_lib = None
            
        elif tool == 'tcmalloc':
            # Find TCMalloc debug library
            tcmalloc_lib = self.find_library('tcmalloc_debug')
            if not tcmalloc_lib:
                raise RuntimeError("TCMalloc debug library not found")
            self.debug_lib = tcmalloc_lib
            
        # Create output directory
        os.makedirs(self.config['output_dir'], exist_ok=True)
        
    def modify_env(self):
        """Modify environment for memory debugging interception"""
        tool = self.config['tool']
        output_dir = self.config['output_dir']
        
        if tool == 'asan':
            # Configure AddressSanitizer
            asan_options = [
                'abort_on_error=1',
                f'log_path={output_dir}/asan',
                'print_stats=1'
            ]
            
            if self.config['detect_leaks']:
                asan_options.append('detect_leaks=1')
                
            self.setenv('ASAN_OPTIONS', ':'.join(asan_options))
            
            # Add ASAN library to LD_PRELOAD
            current_preload = self.mod_env.get('LD_PRELOAD', '')
            if current_preload:
                self.setenv('LD_PRELOAD', f"{self.debug_lib}:{current_preload}")
            else:
                self.setenv('LD_PRELOAD', self.debug_lib)
                
        elif tool == 'valgrind':
            # Valgrind is handled at execution time, not through LD_PRELOAD
            # Set valgrind options for applications that check for them
            valgrind_options = [
                '--tool=memcheck',
                '--leak-check=full',
                f'--log-file={output_dir}/valgrind.log'
            ]
            self.setenv('VALGRIND_OPTS', ' '.join(valgrind_options))
            
        elif tool == 'tcmalloc':
            # Configure TCMalloc
            self.setenv('TCMALLOC_DEBUG', '1')
            self.setenv('TCMALLOC_DEBUG_LOG', f'{output_dir}/tcmalloc.log')
            
            # Add TCMalloc to LD_PRELOAD
            current_preload = self.mod_env.get('LD_PRELOAD', '')
            if current_preload:
                self.setenv('LD_PRELOAD', f"{self.debug_lib}:{current_preload}")
            else:
                self.setenv('LD_PRELOAD', self.debug_lib)
                
        self.log(f"Memory debugging enabled with {tool}")
```

### Interceptor Best Practices

#### 1. Always Implement modify_env() Method

```python
class MyInterceptor(Interceptor):
    def modify_env(self):
        """
        Required method for all interceptors - called during pipeline start.
        Environment modifications are applied to shared mod_env with target package.
        """
        # Add libraries to LD_PRELOAD (shared environment)
        current_preload = self.mod_env.get('LD_PRELOAD', '')
        if current_preload:
            self.setenv('LD_PRELOAD', f"{self.interceptor_lib}:{current_preload}")
        else:
            self.setenv('LD_PRELOAD', self.interceptor_lib)
        
        # Set interceptor configuration environment variables (shared)
        self.setenv('INTERCEPTOR_CONFIG', self.config['config_file'])
        
        # Log what was configured
        self.log(f"Interceptor applied to package with shared mod_env")
```

#### 2. Always Check Library Availability

```python
def _configure(self, **kwargs):
    # Configuration automatically updated
    
    # Always verify library exists before using
    lib_path = self.find_library('myinterceptor')
    if not lib_path:
        raise RuntimeError(f"Required library 'myinterceptor' not found")
    
    self.interceptor_lib = lib_path
```

#### 2. Provide Fallback Options

```python
def _configure(self, **kwargs):
    # Configuration automatically updated
    
    # Try multiple library names/versions
    for lib_name in ['libprofiler_v2', 'libprofiler', 'profiler']:
        lib_path = self.find_library(lib_name)
        if lib_path:
            self.profiler_lib = lib_path
            break
    else:
        # Fallback to configuration path
        lib_path = self.config.get('library_path')
        if lib_path and os.path.exists(lib_path):
            self.profiler_lib = lib_path
        else:
            raise RuntimeError("No suitable profiler library found")
```

#### 3. Handle Multiple Interceptors

```python
def modify_env(self):
    # Check if other interceptors are already in LD_PRELOAD
    current_preload = self.mod_env.get('LD_PRELOAD', '')
    
    # Don't add if already present
    if self.interceptor_lib not in current_preload.split(':'):
        if current_preload:
            self.setenv('LD_PRELOAD', f"{self.interceptor_lib}:{current_preload}")
        else:
            self.setenv('LD_PRELOAD', self.interceptor_lib)
```

#### 4. Provide Configuration Validation

```python
def _configure(self, **kwargs):
    # Configuration automatically updated
    
    # Validate configuration
    if self.config['sample_rate'] <= 0:
        raise ValueError("Sample rate must be positive")
        
    if not os.path.exists(os.path.dirname(self.config['output_file'])):
        os.makedirs(os.path.dirname(self.config['output_file']), exist_ok=True)
    
    # Find and validate library
    lib_path = self.find_library(self.config['library_name'])
    if not lib_path:
        raise FileNotFoundError(f"Library not found: {self.config['library_name']}")
    
    self.interceptor_lib = lib_path
```

#### 5. Clean Up Properly

```python
def clean(self):
    # Remove output files
    for pattern in ['*.log', '*.trace', '*.prof']:
        for file_path in glob.glob(os.path.join(self.config['output_dir'], pattern)):
            os.remove(file_path)
    
    # Remove output directory if empty
    try:
        os.rmdir(self.config['output_dir'])
    except OSError:
        pass  # Directory not empty
```

## Implementation Examples

### Simple Application Example

```python
"""
Simple benchmark application package.
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo
import os

class SimpleBench(Application):
    """Simple benchmark application"""
    
    def _init(self):
        """Initialize variables"""
        self.output_file = None
    
    def _configure_menu(self):
        """Configuration options"""
        return [
            {
                'name': 'duration',
                'msg': 'Benchmark duration in seconds',
                'type': int,
                'default': 60
            },
            {
                'name': 'output_dir',
                'msg': 'Output directory for results',
                'type': str,
                'default': '/tmp/benchmark'
            }
        ]
    
    def _configure(self, **kwargs):
        """Configure the benchmark"""
        # Configuration automatically updated - no need for self.update_config()
        
        # Set up output directory
        os.makedirs(self.config['output_dir'], exist_ok=True)
        self.output_file = os.path.join(self.config['output_dir'], 'results.txt')
        
        # Set environment variables
        self.setenv('BENCH_OUTPUT_DIR', self.config['output_dir'])
    
    def start(self):
        """Run the benchmark"""
        cmd = [
            'benchmark_tool',
            '--duration', str(self.config['duration']),
            '--output', self.output_file
        ]
        
        Exec(' '.join(cmd), LocalExecInfo(env=self.mod_env)).run()
    
    def clean(self):
        """Clean benchmark output"""
        if self.output_file and os.path.exists(self.output_file):
            os.remove(self.output_file)
```

### MPI Application Example

```python
"""
MPI-based parallel application package.
"""
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo, MpiExecInfo
from jarvis_cd.shell.process import Rm
import os

class ParallelApp(Application):
    """Parallel MPI application"""
    
    def _configure_menu(self):
        """Configuration options"""
        return [
            {
                'name': 'nprocs',
                'msg': 'Number of MPI processes',
                'type': int,
                'default': 4
            },
            {
                'name': 'ppn',
                'msg': 'Processes per node',
                'type': int,
                'default': 2
            },
            {
                'name': 'input_file',
                'msg': 'Input data file',
                'type': str,
                'default': 'input.dat'
            }
        ]
    
    def _configure(self, **kwargs):
        """Configure the application"""
        # Configuration automatically updated
        
        # Set MPI environment
        self.setenv('PARALLEL_APP_INPUT', self.config['input_file'])
    
    def start(self):
        """Run parallel application"""
        # Check for MPI executable
        Exec('which mpiexec', LocalExecInfo(env=self.mod_env)).run()
        
        # Run MPI application
        cmd = ['parallel_app', '--input', self.config['input_file']]
        
        Exec(' '.join(cmd), MpiExecInfo(
            env=self.mod_env,
            hostfile=self.jarvis.hostfile,
            nprocs=self.config['nprocs'],
            ppn=self.config['ppn']
        )).run()
    
    def clean(self):
        """Clean output files"""
        Rm('output_*', LocalExecInfo()).run()
```

### Service Example

```python
"""
Database service package.
"""
from jarvis_cd.core.pkg import Service
from jarvis_cd.shell import Exec, LocalExecInfo
from jarvis_cd.shell.process import Kill, Which
import os
import time

class Database(Service):
    """Database service"""
    
    def _init(self):
        """Initialize service variables"""
        self.pid_file = None
        self.data_dir = None
    
    def _configure_menu(self):
        """Configuration options"""
        return [
            {
                'name': 'port',
                'msg': 'Database port',
                'type': int,
                'default': 5432
            },
            {
                'name': 'data_dir',
                'msg': 'Database data directory',
                'type': str,
                'default': '/var/lib/mydb'
            }
        ]
    
    def _configure(self, **kwargs):
        """Configure database"""
        # Configuration automatically updated
        
        self.data_dir = self.config['data_dir']
        self.pid_file = os.path.join(self.data_dir, 'mydb.pid')
        
        # Set database environment
        self.setenv('MYDB_PORT', str(self.config['port']))
        self.setenv('MYDB_DATA_DIR', self.data_dir)
        
        # Create data directory
        os.makedirs(self.data_dir, exist_ok=True)
    
    def start(self):
        """Start database service"""
        # Check if database is available
        Which('mydb_server', LocalExecInfo()).run()
        
        # Start database
        cmd = [
            'mydb_server',
            '--port', str(self.config['port']),
            '--data-dir', self.data_dir,
            '--pid-file', self.pid_file,
            '--daemonize'
        ]
        
        Exec(' '.join(cmd), LocalExecInfo(env=self.mod_env)).run()
        
        # Wait for service to start
        time.sleep(2)
    
    def stop(self):
        """Stop database service"""
        if os.path.exists(self.pid_file):
            with open(self.pid_file, 'r') as f:
                pid = f.read().strip()
            
            Exec(f'kill {pid}', LocalExecInfo()).run()
    
    def kill(self):
        """Force kill database"""
        Kill('mydb_server', LocalExecInfo()).run()
    
    def status(self) -> str:
        """Check database status"""
        if os.path.exists(self.pid_file):
            return "running"
        return "stopped"
    
    def clean(self):
        """Clean database data"""
        Rm(self.data_dir, LocalExecInfo()).run()
```

### Interceptor Example

```python
"""
Performance profiling interceptor package.
"""
from jarvis_cd.core.pkg import Interceptor
import os

class Profiler(Interceptor):
    """Performance profiling interceptor"""
    
    def _init(self):
        """Initialize profiler variables"""
        self.profiler_lib = None
    
    def _configure_menu(self):
        """Configuration options"""
        return [
            {
                'name': 'profiler_path',
                'msg': 'Path to profiler library',
                'type': str,
                'default': '/usr/lib/libprofiler.so'
            },
            {
                'name': 'output_file',
                'msg': 'Profiler output file',
                'type': str,
                'default': 'profile_output.txt'
            }
        ]
    
    def _configure(self, **kwargs):
        """Configure profiler"""
        # Configuration automatically updated
        
        self.profiler_lib = self.config['profiler_path']
        
        # Set profiler environment
        self.setenv('PROFILER_OUTPUT', self.config['output_file'])
    
    def modify_env(self):
        """Modify environment for profiling interception"""
        # Add profiler library to LD_PRELOAD
        if os.path.exists(self.profiler_lib):
            current_preload = self.mod_env.get('LD_PRELOAD', '')
            if current_preload:
                self.setenv('LD_PRELOAD', f"{self.profiler_lib}:{current_preload}")
            else:
                self.setenv('LD_PRELOAD', self.profiler_lib)
    
    def clean(self):
        """Clean profiler output"""
        output_file = self.config['output_file']
        if os.path.exists(output_file):
            os.remove(output_file)
```

## Best Practices

### 1. CRITICAL: Always Call .run() on Exec Objects

**MOST IMPORTANT**: All Exec objects and process utilities must call `.run()` to actually execute commands. Simply creating an Exec object does not execute anything.

```python
# ✅ Correct - Execute the command
Exec('my_command', LocalExecInfo()).run()

# ❌ Wrong - Command is never executed
Exec('my_command', LocalExecInfo())  # Does nothing!

# ✅ Correct - Store executor and run
executor = Exec('my_command', LocalExecInfo())
executor.run()

# ✅ Correct - Process utilities also need .run()
from jarvis_cd.shell.process import Mkdir, Rm, Which
Mkdir('/output/dir', LocalExecInfo()).run()
Rm('/tmp/files*', LocalExecInfo()).run()
Which('required_tool', LocalExecInfo()).run()

# ❌ Wrong - These commands are never executed
Mkdir('/output/dir', LocalExecInfo())  # Directory not created!
Rm('/tmp/files*', LocalExecInfo())     # Files not removed!
```

This is the most common mistake in package development and will cause your package to appear to work but actually do nothing.

### 2. Separate Configuration from Execution

**CRITICAL**: Maintain strict separation between setup (`_configure()`) and execution (`start()`).

```python
# ✅ Good - All setup in _configure()
def _configure(self, **kwargs):
    # Configuration automatically updated

    # Set environment variables
    self.setenv('MY_APP_HOME', self.config['install_path'])

    # Create directories on all nodes
    output_dir = os.path.expandvars(self.config['output_dir'])
    parent_dir = str(pathlib.Path(output_dir).parent)
    Mkdir(parent_dir,
          PsshExecInfo(env=self.mod_env,
                       hostfile=self.jarvis.hostfile)).run()

    # Generate configuration files
    config_file = f'{self.shared_dir}/app.conf'
    with open(config_file, 'w') as f:
        f.write(f"port={self.config['port']}\n")

# ✅ Good - Only execution in start()
def start(self):
    cmd = ['my_app', '--config', f'{self.shared_dir}/app.conf']
    Exec(' '.join(cmd), LocalExecInfo(env=self.mod_env)).run()

# ❌ Bad - Don't mix setup with execution
def start(self):
    # Don't do this!
    self.setenv('LATE_VAR', 'value')  # Too late - do in _configure()!
    Mkdir('/output/dir', LocalExecInfo()).run()  # Wrong place!

    cmd = ['my_app']
    Exec(' '.join(cmd), LocalExecInfo(env=self.mod_env)).run()
```

**Why this matters:**
- `_configure()` is called once during pipeline configuration
- `start()` may be called multiple times (e.g., after `stop()`)
- Environment variables set in `start()` won't propagate to other packages
- Directory creation in `start()` is wasteful and error-prone

### 2. Handle File Paths Properly

```python
def _configure(self, **kwargs):
    # Configuration automatically updated
    
    # Expand environment variables in paths
    output_dir = os.path.expandvars(self.config['output_dir'])
    
    # Create directories as needed
    os.makedirs(output_dir, exist_ok=True)
    
    # Store absolute paths
    self.output_dir = os.path.abspath(output_dir)
```

### 4. Use Proper Execution Commands

```python
def start(self):
    # ✅ Good - Use .run() method
    Exec('command', LocalExecInfo(env=self.mod_env)).run()
    
    # ✅ Good - Use process utilities
    from jarvis_cd.shell.process import Mkdir
    Mkdir('/path/to/dir', LocalExecInfo()).run()
    
    # ❌ Bad - Don't use subprocess directly
    import subprocess
    subprocess.run(['command'])  # Don't do this
```

### 5. Implement Proper Cleanup

```python
def clean(self):
    """Clean all package data"""
    # Remove output files
    if hasattr(self, 'output_dir') and os.path.exists(self.output_dir):
        Rm(self.output_dir, LocalExecInfo()).run()
    
    # Remove temporary files
    Rm('/tmp/myapp_*', LocalExecInfo()).run()
```

### 6. Error Handling

```python
def start(self):
    """Start with error handling"""
    try:
        # Check prerequisites
        Which('required_command', LocalExecInfo()).run()
        
        # Run main command
        result = Exec('main_command', LocalExecInfo(env=self.mod_env)).run()
        
        # Check result if needed
        if hasattr(result, 'exit_code') and result.exit_code != 0:
            raise RuntimeError("Command failed")
            
    except Exception as e:
        print(f"Error starting {self.__class__.__name__}: {e}")
        raise
```

### 7. Documentation

```python
class MyPackage(Application):
    """
    Brief description of what this package does.
    
    This package provides functionality for...
    """
    
    def _configure_menu(self):
        """
        Define configuration parameters.
        
        For more details on parameter format, see:
        https://docs.jarvis-cd.io/configuration
        """
        return [
            {
                'name': 'param',
                'msg': 'Clear description of parameter purpose',
                'type': str,
                'default': 'sensible_default'
            }
        ]
```

This guide provides the foundation for developing robust Jarvis-CD packages. For more advanced topics, refer to the existing builtin packages in the `builtin/` directory for real-world examples.

## Working with the New Interceptor Architecture

### Pipeline Commands for Interceptors

```bash
# Load a pipeline with interceptors
jarvis ppl load yaml my_pipeline.yaml

# View pipeline configuration (shows both packages and interceptors)
jarvis ppl print

# Start pipeline (interceptors are applied at runtime)
jarvis ppl start

# Check pipeline status
jarvis ppl status
```

### Example Pipeline with Interceptors

```yaml
# my_pipeline.yaml
name: performance_testing
pkgs:
  - pkg_type: builtin.ior
    pkg_name: benchmark
    interceptors: ["profiler", "tracer"]  # Apply both interceptors
    nprocs: 4
    block: "1G"
interceptors:
  - pkg_type: builtin.perf_profiler
    pkg_name: profiler
    sampling_rate: 1000
    output_file: /tmp/perf.out
  - pkg_type: builtin.io_tracer
    pkg_name: tracer
    trace_reads: true
    trace_writes: true
    min_size: 1024
```

### Pipeline Output Example

When you run `jarvis ppl print`, you'll see:

```
Pipeline: performance_testing
Directory: /home/user/.ppi-jarvis/config/pipelines/performance_testing
Packages:
  benchmark:
    Type: builtin.ior
    Global ID: performance_testing.benchmark
    Configuration:
      interceptors: ['profiler', 'tracer']
      nprocs: 4
      block: 1G
Interceptors:
  profiler:
    Type: builtin.perf_profiler
    Global ID: performance_testing.profiler
    Configuration:
      sampling_rate: 1000
      output_file: /tmp/perf.out
  tracer:
    Type: builtin.io_tracer
    Global ID: performance_testing.tracer
    Configuration:
      trace_reads: true
      trace_writes: true
      min_size: 1024
```

### Runtime Execution Flow

1. **Pipeline Start**: `jarvis ppl start` is called
2. **Package Processing**: For each package in the pipeline:
   - Load package instance
   - Check `interceptors` list in package configuration
   - For each referenced interceptor:
     - Load interceptor instance from pipeline interceptors
     - Share the same `mod_env` reference between interceptor and package
     - Call interceptor's `modify_env()` method
   - Start the package with the modified environment

This architecture ensures that interceptors can modify the exact environment that packages will use, providing seamless interception capabilities.

## Pipeline Management Commands

### `jarvis ppl destroy`

Destroys a pipeline by removing its directory and configuration files. Automatically cleans package data before destruction.

**Usage:**
```bash
# Destroy the current pipeline
jarvis ppl destroy

# Destroy a specific pipeline by name
jarvis ppl destroy pipeline_name
```

**Behavior:**
- If no pipeline name is provided, destroys the current pipeline
- Attempts to clean package data before destruction using each package's `clean()` method
- Removes the entire pipeline directory and configuration files
- Clears the current pipeline if the destroyed pipeline was active
- Shows remaining pipelines after successful destruction

**Example:**
```bash
# Create and destroy a test pipeline
jarvis ppl create test_pipeline
jarvis ppl append echo
jarvis ppl destroy  # Destroys current pipeline (test_pipeline)

# Destroy a specific pipeline while working on another
jarvis cd other_pipeline
jarvis ppl destroy test_pipeline  # Destroys test_pipeline, keeps other_pipeline active
```
```

### `docs/pipelines.md`

```markdown
# Jarvis-CD Pipeline Documentation

This comprehensive guide covers pipeline management, YAML configuration, filesystem organization, and CLI commands in Jarvis-CD.

## Table of Contents

1. [Pipeline Overview](#pipeline-overview)
2. [Pipeline Filesystem Structure](#pipeline-filesystem-structure)
3. [Pipeline YAML Format](#pipeline-yaml-format)
4. [Pipeline CLI Commands](#pipeline-cli-commands)
5. [Pipeline Lifecycle](#pipeline-lifecycle)
6. [Environment Management](#environment-management)
7. [Pipeline Indexes](#pipeline-indexes)
8. [Advanced Pipeline Features](#advanced-pipeline-features)
9. [Best Practices](#best-practices)
10. [Troubleshooting](#troubleshooting)

## Pipeline Overview

A **pipeline** in Jarvis-CD is a coordinated sequence of packages (applications, services, and interceptors) that execute together to accomplish a specific computational task. Pipelines provide:

- **Package Coordination**: Sequential or parallel execution of multiple packages
- **Environment Propagation**: Shared environment variables across package executions
- **Interceptor Integration**: Runtime environment modification for profiling, debugging, and monitoring
- **Configuration Management**: Centralized configuration for all pipeline components
- **State Management**: Persistent pipeline state and package configurations

### Key Concepts

- **Package**: Individual computational unit (application, service, or interceptor)
- **Interceptor**: Environment modifier that affects package execution (profiling, I/O tracing, etc.)
- **Environment**: Shared environment variables propagated between packages
- **Configuration**: Package-specific parameters and settings
- **Global ID**: Unique identifier in format `pipeline_name.package_id`

## Pipeline Filesystem Structure

Pipelines are organized in a hierarchical filesystem structure that provides isolation, persistence, and organization.

### User Configuration Directory Structure

```
~/.ppi-jarvis/
├── config/
│   ├── jarvis.yaml              # Global Jarvis configuration
│   ├── packages/                # Standalone package configurations
│   └── pipelines/               # Pipeline storage directory
│       ├── pipeline1/           # Individual pipeline directory
│       │   ├── pipeline.yaml    # Pipeline configuration
│       │   ├── env.yaml        # Environment variables
│       │   └── packages/       # Package-specific directories
│       │       ├── pkg1/       # Package instance directory
│       │       │   ├── config/ # Package configuration files
│       │       │   ├── shared/ # Shared runtime files
│       │       │   └── private/# Private package data
│       │       └── pkg2/
│       └── pipeline2/
├── shared/                     # Global shared data directory
└── private/                    # Global private data directory
```

### Pipeline Directory Structure

Each pipeline gets its own isolated directory:

```
~/.ppi-jarvis/config/pipelines/my_pipeline/
├── pipeline.yaml              # Main pipeline configuration
├── env.yaml                   # Pipeline environment variables
└── packages/                  # Package instance storage
    ├── app1/                  # Package instance directory
    │   ├── config/           # Generated configuration files
    │   │   ├── config.yaml   # Package configuration
    │   │   ├── env.yaml      # Package environment
    │   │   └── mod_env.yaml  # Modified environment (with LD_PRELOAD)
    │   ├── shared/           # Shared files accessible to other packages
    │   └── private/          # Private package data
    ├── database/
    │   ├── config/
    │   ├── shared/
    │   └── private/
    └── profiler/             # Interceptor instance directory
        ├── config/
        ├── shared/
        └── private/
```

### Directory Purposes

#### Pipeline Level
- **`pipeline.yaml`**: Main configuration including packages, interceptors, and metadata
- **`env.yaml`**: Environment variables shared across all packages in the pipeline

#### Package Level  
- **`config/`**: Package-specific configuration files and generated configs
- **`shared/`**: Files that can be accessed by other packages in the pipeline
- **`private/`**: Package-specific private data not shared with other packages

### File Lifecycle

1. **Pipeline Creation**: `pipeline.yaml` and `env.yaml` created
2. **Package Addition**: Package directory structure created under `packages/`
3. **Configuration**: Package `config/` populated with configuration files
4. **Execution**: Runtime files generated in `shared/` and `private/`
5. **Cleanup**: Package directories can be cleaned while preserving configuration

## Pipeline YAML Format

Pipeline YAML files define the complete pipeline configuration including packages, interceptors, environment, and metadata.

### Basic Pipeline Structure

```yaml
# Pipeline name (required)
name: my_pipeline

# Environment configuration (optional)
# Must be a named environment reference or omitted
env: my_custom_environment  # References a named environment

# Main packages (required)
pkgs:
  - pkg_type: repo.package_name
    pkg_name: instance_name
    # Package configuration parameters
    param1: value1
    param2: value2

# Interceptors (optional)
interceptors:
  - pkg_type: repo.interceptor_name
    pkg_name: interceptor_instance
    # Interceptor configuration
    interceptor_param: value
```

### Complete Example Pipeline

```yaml
# Pipeline: High-Performance I/O Benchmark
# Purpose: Measures I/O performance with profiling and tracing
name: io_benchmark_pipeline

# Pipeline-wide environment (named environment reference)
# The 'benchmark_env' should define:
#   BENCHMARK_ROOT: "/tmp/benchmark"
#   MPI_ROOT: "/usr/lib/openmpi"
#   PROFILER_OUTPUT_DIR: "/tmp/profiling"
env: benchmark_env

# Interceptors defined at pipeline level
interceptors:
  # Performance profiler interceptor
  - pkg_type: builtin.perf_profiler
    pkg_name: profiler
    sampling_rate: 1000
    output_file: "${PROFILER_OUTPUT_DIR}/perf.out"
    enable_callgraph: true
    
  # I/O tracing interceptor  
  - pkg_type: builtin.io_tracer
    pkg_name: io_monitor
    trace_reads: true
    trace_writes: true
    trace_file: "${PROFILER_OUTPUT_DIR}/io_trace.log"
    min_size: 4096

# Main pipeline packages
pkgs:
  # Setup shared filesystem
  - pkg_type: builtin.mkfs
    pkg_name: filesystem_setup
    filesystem_type: "ext4"
    mount_point: "${BENCHMARK_ROOT}"
    size: "10G"
    
  # Database service (no interceptors)
  - pkg_type: builtin.redis
    pkg_name: database
    port: 6379
    data_dir: "${BENCHMARK_ROOT}/redis_data"
    memory_limit: "2G"
    
  # I/O benchmark application (with interceptors)
  - pkg_type: builtin.ior
    pkg_name: io_benchmark
    interceptors: ["profiler", "io_monitor"]  # Apply both interceptors
    nprocs: 4
    ppn: 2
    block: "1G"
    transfer: "64K"
    test_file: "${BENCHMARK_ROOT}/ior_test_file"
    
  # Analysis application (with profiler only)
  - pkg_type: builtin.data_analysis
    pkg_name: results_analyzer
    interceptors: ["profiler"]  # Apply profiler only
    input_dir: "${PROFILER_OUTPUT_DIR}"
    output_file: "${BENCHMARK_ROOT}/analysis_results.json"
```

### Environment Types

The `env` field in a pipeline YAML must be either a **named environment reference** (string) or **omitted** (auto-build). Inline environment dictionaries are **not supported**.

#### 1. Named Environment Reference
```yaml
name: my_pipeline
env: production_environment  # References a named environment
```

**Auto-Creation**: If the named environment doesn't exist, Jarvis-CD will automatically create it by capturing the current shell environment and save it with the specified name. This allows you to reference environments that will be built on-demand.

**Creating Named Environments**: To create a named environment with custom variables:
```bash
# Create a named environment from current shell
jarvis ppl env build my_custom_env

# Or build with additional commands (e.g., module loads)
jarvis ppl env build my_custom_env module load gcc/9.3.0 openmpi/4.1.0
```

#### 2. Auto-built Environment (Default)
```yaml
name: my_pipeline
# No env field - automatically captures current environment
```

### Container Configuration

Pipelines can be configured to run all packages in a shared container environment. This is useful for reproducibility, dependency management, and multi-node deployments.

#### Container Pipeline Parameters

```yaml
name: my_containerized_pipeline

# Container configuration (all fields optional)
container_name: my_app_container                    # Container/image name (required for containerization)
container_engine: podman                            # Container engine: docker or podman (default: podman)
container_base: docker.io/iowarp/iowarp-build:latest    # Base image to build FROM
container_ssh_port: 2222                            # SSH port for container communication (default: 2222)

# Optional: Custom extensions to Docker compose file (merged into service config)
container_extensions:
  volumes:
    - /data:/data:ro                                # Mount additional volumes
  environment:
    MY_VAR: value                                   # Add environment variables
  devices:
    - /dev/nvidia0:/dev/nvidia0                     # Add device access

pkgs:
  - pkg_type: builtin.ior
    deploy_mode: container  # Use containerized deployment
    nprocs: 4
```

**Container Configuration Fields:**

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `container_name` | string | `""` (not containerized) | Name for the container image. If set, enables containerization |
| `container_engine` | string | `"podman"` | Container engine to use (`docker` or `podman`) |
| `container_base` | string | `"iowarp/iowarp-build:latest"` | Base Docker image to build from |
| `container_ssh_port` | int | `2222` | SSH port for inter-container and container-host communication |
| `container_extensions` | dict | `{}` | Custom Docker Compose configuration merged into service definition |

#### How Container Pipelines Work

1. **Container Build Phase** (`jarvis ppl load yaml` or `jarvis ppl configure`):
   - Pipeline collects all packages with `deploy_mode=container`
   - Each package's `augment_container()` method adds installation commands to Dockerfile
   - Pipeline generates global Dockerfile in `~/.ppi-jarvis/containers/{container_name}.Dockerfile`
   - Container image is built and tagged as `{container_name}`
   - Manifest file tracks installed packages to avoid unnecessary rebuilds

2. **Container Deployment Phase** (`jarvis ppl start`):
   - Pipeline starts containers on all nodes in hostfile using `pssh`
   - Containers run with `network_mode: host` and `ipc: host` for performance
   - SSH daemon starts inside container for multi-node MPI communication
   - Pipeline YAML is mounted into container at `/root/.ppi-jarvis/shared/pipeline.yaml`
   - Packages execute inside container with full environment and interceptor support

3. **Container Lifecycle**:
   - `jarvis ppl start`: Brings up containers on all nodes, runs pipeline packages
   - `jarvis ppl stop`: Gracefully stops containers
   - `jarvis ppl kill`: Force terminates containers
   - `jarvis ppl clean`: Removes container data

#### Complete Container Pipeline Example

```yaml
# Pipeline: Containerized I/O Benchmark
# Purpose: Run IOR benchmark in reproducible container environment
# Requirements: Podman or Docker installed on all nodes
# Expected Runtime: 5-10 minutes (including container build)

name: ior_container_benchmark

# Auto-build environment from current shell
# No env field - will capture current environment automatically

# Container configuration
container_name: ior_benchmark_container
container_engine: podman
container_base: docker.io/iowarp/iowarp-build:latest
container_ssh_port: 2222

pkgs:
  # IOR benchmark with container deployment
  - pkg_type: builtin.ior
    pkg_name: ior_test
    # Deployment mode: 'default' for bare metal, 'container' for containerized
    deploy_mode: container
    # IOR benchmark parameters
    nprocs: 4
    ppn: 2
    block: 1G
    xfer: 1M
    api: posix
    out: /tmp/ior_test_file
    write: true
    read: true
```

#### Mixed Deployment: Bare Metal + Container

You can run some packages on bare metal and others in containers within the same pipeline:

```yaml
name: mixed_deployment_pipeline

# Container configuration for packages that need it
container_name: app_container
container_engine: docker
container_base: docker.io/iowarp/iowarp-build:latest

pkgs:
  # Run database on bare metal (better performance)
  - pkg_type: builtin.redis
    pkg_name: database
    deploy_mode: default  # Bare metal deployment
    port: 6379
    memory_limit: 8G

  # Run application in container (reproducibility)
  - pkg_type: builtin.my_app
    pkg_name: containerized_app
    deploy_mode: container  # Container deployment
    database_host: localhost
    database_port: 6379
```

#### Container Rebuild Control

Containers are automatically rebuilt when:
- Package manifest changes (packages added/removed)
- Base image changes
- First time loading a pipeline with containers

To force a rebuild:

```bash
# Rebuild specific container
jarvis container update my_container_name

# Rebuild with no cache (clean rebuild)
jarvis container update my_container_name +no_cache

# Specify container engine
jarvis container update my_container_name engine=docker

# Update pipeline and rebuild container
jarvis ppl update +container +no_cache
```

#### Container Architecture

**Global Container Storage:**
- Containers are stored globally in `~/.ppi-jarvis/containers/`
- Multiple pipelines can share the same container image
- Container images are tagged and reused across pipeline instances

**Container File Structure:**
```
~/.ppi-jarvis/containers/
├── my_container.Dockerfile     # Generated Dockerfile
├── my_container.manifest       # Package manifest (JSON)
└── compose_files/              # Per-pipeline compose files
    └── pipeline_name/
        └── docker-compose.yaml
```

**Container Features:**
- **Host Networking**: Containers use `network_mode: host` for direct network access
- **IPC Sharing**: `ipc: host` removes shared memory limits
- **Memory Limits**: Unlimited memlock and 64MB stack via ulimits
- **SSH Support**: Built-in SSH daemon for MPI multi-node communication
- **GPU Access**: Available via host networking mode (no explicit GPU configuration needed)

#### Container Best Practices

1. **Use Specific Base Images**: Specify exact image tags for reproducibility
   ```yaml
   container_base: docker.io/ubuntu:22.04  # ✅ Good - specific version
   container_base: ubuntu:latest           # ❌ Avoid - version can change
   ```

2. **Share Containers Across Pipelines**: Reuse containers with identical package sets
   ```yaml
   # Pipeline 1
   container_name: shared_ior_container

   # Pipeline 2 (reuses same container)
   container_name: shared_ior_container
   ```

3. **Test Locally Before Deployment**: Build and test containers locally first
   ```bash
   jarvis ppl load yaml my_pipeline.yaml  # Builds container
   jarvis ppl start                       # Test locally
   ```

4. **Monitor Container Resources**: Check container disk usage and clean old images
   ```bash
   podman images | grep jar  # List Jarvis containers
   podman system prune       # Clean unused containers
   ```

5. **Use Container Extensions for Custom Configuration**: Extend compose file for specific needs
   ```yaml
   container_extensions:
     # Add GPU support
     deploy:
       resources:
         reservations:
           devices:
             - driver: nvidia
               count: all
               capabilities: [gpu]
   ```

#### Container Extensions

The `container_extensions` parameter allows you to extend the generated Docker Compose service configuration with custom settings. Extensions are deep-merged into the service definition, allowing you to add or override any Docker Compose service parameters.

**Common Use Cases:**

1. **Additional Volume Mounts**:
   ```yaml
   container_extensions:
     volumes:
       - /scratch:/scratch:rw       # Scratch space
       - /datasets:/data:ro         # Read-only datasets
       - /results:/output:rw        # Output directory
   ```

2. **Environment Variables**:
   ```yaml
   container_extensions:
     environment:
       CUDA_VISIBLE_DEVICES: "0,1"
       OMP_NUM_THREADS: "16"
       PYTHONPATH: "/custom/path"
   ```

3. **Device Access** (GPU, InfiniBand, etc.):
   ```yaml
   container_extensions:
     devices:
       - /dev/nvidia0:/dev/nvidia0
       - /dev/nvidiactl:/dev/nvidiactl
       - /dev/infiniband/uverbs0:/dev/infiniband/uverbs0
   ```

4. **Resource Limits**:
   ```yaml
   container_extensions:
     deploy:
       resources:
         limits:
           cpus: '8'
           memory: 16G
         reservations:
           cpus: '4'
           memory: 8G
   ```

5. **Capabilities and Security**:
   ```yaml
   container_extensions:
     cap_add:
       - SYS_PTRACE    # For debugging
       - IPC_LOCK      # For RDMA
     security_opt:
       - seccomp:unconfined
   ```

6. **Complete GPU Configuration Example**:
   ```yaml
   name: gpu_pipeline
   container_name: gpu_app_container
   container_engine: docker
   container_base: docker.io/nvidia/cuda:12.0-base

   container_extensions:
     # Docker GPU configuration (Docker Compose v2.3+ format)
     deploy:
       resources:
         reservations:
           devices:
             - driver: nvidia
               count: all
               capabilities: [gpu, compute, utility]
     # Additional GPU environment variables
     environment:
       NVIDIA_VISIBLE_DEVICES: all
       NVIDIA_DRIVER_CAPABILITIES: compute,utility

   pkgs:
     - pkg_type: builtin.gpu_benchmark
       deploy_mode: container
   ```

**Extension Merging Behavior:**

- **Dictionaries**: Recursively merged (deep merge)
- **Lists**: Extended (items appended)
- **Scalars**: Source value overrides target value

Example merge:
```yaml
# Base configuration (generated by Jarvis)
volumes:
  - /root/.ppi-jarvis/shared:/root/.ppi-jarvis/shared

# Extension
container_extensions:
  volumes:
    - /data:/data

# Result after merge
volumes:
  - /root/.ppi-jarvis/shared:/root/.ppi-jarvis/shared
  - /data:/data
```

### Package Configuration

#### Basic Package Definition
```yaml
pkgs:
  - pkg_type: builtin.ior           # Package type (repo.package)
    pkg_name: benchmark_run         # Instance name (unique in pipeline)
    # Package-specific configuration
    nprocs: 8
    block: "2G"
    transfer: "1M"
```

#### Package with Interceptors
```yaml
pkgs:
  - pkg_type: builtin.application
    pkg_name: app_with_monitoring
    interceptors: ["profiler", "tracer"]  # List of interceptor names
    # Application configuration
    input_file: "data.txt"
    output_dir: "/tmp/results"
```

### Interceptor Configuration

#### Interceptor Definition
```yaml
interceptors:
  - pkg_type: builtin.memory_profiler    # Interceptor package type
    pkg_name: mem_profiler               # Interceptor instance name
    # Interceptor-specific configuration
    sample_interval: 100
    output_format: "json"
    detect_leaks: true
```

#### Multiple Interceptors
```yaml
interceptors:
  - pkg_type: builtin.perf_profiler
    pkg_name: cpu_profiler
    sampling_rate: 1000
    
  - pkg_type: builtin.io_tracer
    pkg_name: io_monitor
    trace_reads: true
    trace_writes: true
    
  - pkg_type: builtin.memory_debugger
    pkg_name: mem_checker
    tool: "asan"
    detect_leaks: true
```

### Advanced YAML Features

#### Environment Variable Substitution

Environment variables from named environments can be referenced in package configurations using `${VAR_NAME}` syntax:

```yaml
# Named environment should define: WORK_DIR, LOG_DIR
env: my_work_environment

pkgs:
  - pkg_type: builtin.app
    pkg_name: worker
    work_directory: "${WORK_DIR}"        # Uses WORK_DIR from named environment
    log_file: "${LOG_DIR}/worker.log"    # Uses LOG_DIR from named environment
    temp_space: "${WORK_DIR}/temp"       # Combines variables
```

#### Complex Configuration
```yaml
pkgs:
  - pkg_type: builtin.mpi_application
    pkg_name: parallel_solver
    # MPI configuration
    nprocs: 16
    ppn: 4
    # Application parameters
    solver_type: "iterative"
    tolerance: 1e-6
    max_iterations: 1000
    # File paths
    input_mesh: "/data/mesh.vtk"
    output_solution: "/results/solution.vtk"
    # Advanced options
    use_gpu: false
    memory_limit: "8G"
    checkpoint_interval: 100
```

## Pipeline CLI Commands

### Pipeline Creation and Management

#### Create New Pipeline
```bash
# Create a new empty pipeline
jarvis ppl create my_pipeline

# Creates directory: ~/.ppi-jarvis/config/pipelines/my_pipeline/
# Sets as current pipeline
```

#### Add Packages to Pipeline
```bash
# Add package to current pipeline
jarvis ppl append builtin.ior

# Add package with alias
jarvis ppl append builtin.ior benchmark_run

# Add package with full repository specification
jarvis ppl append my_repo.custom_app my_app
```

#### Remove Packages from Pipeline
```bash
# Remove package by instance name
jarvis ppl rm benchmark_run

# Remove package (shows available packages if not found)
jarvis ppl rm nonexistent_pkg
```

### Pipeline Loading and Execution

#### Load Pipeline from YAML
```bash
# Load pipeline from YAML file
jarvis ppl load yaml /path/to/pipeline.yaml

# Creates pipeline directory and sets as current
# Overwrites existing pipeline with same name
```

#### Run Pipeline (Complete Lifecycle)
```bash
# Run current pipeline (start → stop)
jarvis ppl run

# Load and run pipeline in one command
jarvis ppl run yaml /path/to/pipeline.yaml
```

#### Pipeline Lifecycle Management
```bash
# Start pipeline (run packages)
jarvis ppl start

# Stop pipeline (graceful shutdown)
jarvis ppl stop

# Force kill all pipeline processes
jarvis ppl kill

# Clean all pipeline data
jarvis ppl clean
```

### Pipeline Information and Status

#### List Available Pipelines
```bash
# List all pipelines with package counts
jarvis ppl list

# Output shows:
#   * current_pipeline (3 packages)  # * indicates current
#     other_pipeline (1 package)
#     broken_pipeline (error reading config)
```

#### Show Pipeline Configuration
```bash
# Print complete pipeline configuration
jarvis ppl print

# Shows:
# - Pipeline name and directory
# - All packages with configuration
# - All interceptors with configuration  
# - Last loaded file (if from YAML)
```

#### Check Pipeline Status
```bash
# Show status of all packages in pipeline
jarvis ppl status

# Shows:
# Pipeline: my_pipeline
# Packages:
#   database: running
#   app1: stopped
#   profiler: no status method
```

### Pipeline Switching and Destruction

#### Switch Current Pipeline
```bash
# Switch to different pipeline
jarvis cd other_pipeline

# Shows basic pipeline information after switching
```

#### Destroy Pipeline
```bash
# Destroy current pipeline
jarvis ppl destroy

# Destroy specific pipeline
jarvis ppl destroy old_pipeline

# Attempts to clean package data before destruction
# Removes entire pipeline directory
```

#### Update Pipeline from File
```bash
# Reload current pipeline from its last loaded YAML file
jarvis ppl update

# Only works if pipeline was loaded from YAML file
# Useful for development and testing
```

### Package Configuration

#### Configure Package in Pipeline
```bash
# Configure package parameters
jarvis pkg conf app_name param1=value1 param2=value2

# Configure package in specific pipeline
jarvis pkg conf pipeline.package_name param=value

# Examples:
jarvis pkg conf ior_benchmark nprocs=8 block=2G
jarvis pkg conf my_pipeline.database port=5432 memory=4G
```

#### Show Package Information
```bash
# Show package README
jarvis pkg readme package_name
jarvis pkg readme repo.package_name
jarvis pkg readme pipeline.package_name

# Show package file paths
jarvis pkg path package_name --conf --shared_dir --pkg_dir
```

### Environment Management

#### Build Pipeline Environment
```bash
# Build environment from current shell
jarvis ppl env build

# Build with additional module loading
jarvis ppl env build module load gcc/9.3.0 openmpi/4.1.0
```

#### Copy Named Environment
```bash
# Copy named environment to current pipeline
jarvis ppl env copy production_env

# Environment must exist in ~/.ppi-jarvis/config/environments/
```

#### Show Pipeline Environment
```bash
# Display current pipeline environment variables
jarvis ppl env show
```

### Pipeline Indexes

#### List Available Pipeline Scripts
```bash
# List all pipeline scripts from all repositories
jarvis ppl index list

# List scripts from specific repository
jarvis ppl index list builtin

# Output shows files and directories with color coding:
#   script.yaml                    # Default color (loadable file)
#   examples/ (directory)          # Cyan color (subdirectory)
```

#### Load Pipeline from Index
```bash
# Load pipeline script directly
jarvis ppl index load builtin.examples.simple_test

# Load from nested directory
jarvis ppl index load my_repo.benchmarks.io_tests.ior_benchmark
```

#### Copy Pipeline Script
```bash
# Copy to current directory
jarvis ppl index copy builtin.examples.simple_test

# Copy to specific location
jarvis ppl index copy builtin.examples.simple_test /tmp/

# Copy with custom filename
jarvis ppl index copy builtin.examples.simple_test ./my_pipeline.yaml
```

## Pipeline Lifecycle

Understanding the pipeline lifecycle helps with debugging and optimization.

### 1. Pipeline Creation/Loading

```bash
jarvis ppl create my_pipeline
# OR
jarvis ppl load yaml pipeline.yaml
```

**Actions Performed:**
- Create pipeline directory: `~/.ppi-jarvis/config/pipelines/my_pipeline/`
- Generate `pipeline.yaml` with pipeline configuration
- Generate `env.yaml` with environment variables
- Set as current pipeline in Jarvis configuration
- Create package subdirectories for each package

### 2. Package Addition

```bash
jarvis ppl append builtin.ior benchmark
```

**Actions Performed:**
- Validate package exists in repositories
- Create package directory: `packages/benchmark/`
- Load package defaults from package's `configure_menu()`
- Generate package configuration entry in `pipeline.yaml`
- Create package subdirectories: `config/`, `shared/`, `private/`

### 3. Package Configuration

```bash
jarvis pkg conf benchmark nprocs=4 block=1G
```

**Actions Performed:**
- Load package instance with current configuration
- Apply type conversion to configuration parameters
- Update package configuration in `pipeline.yaml`
- Call package's `configure()` method
- Generate package-specific configuration files in `config/`

### 4. Pipeline Execution

```bash
jarvis ppl start
```

**Actions Performed:**
- Load pipeline configuration and environment
- For each package in sequence:
  - Load package instance with pipeline environment
  - Apply interceptors (if any):
    - Load interceptor instances
    - Share `mod_env` reference between interceptor and package
    - Call interceptor's `modify_env()` method
  - Call package's `start()` method
  - Propagate environment changes to next packages

### 5. Pipeline Stopping

```bash
jarvis ppl stop
```

**Actions Performed:**
- Load pipeline configuration
- For each package in reverse order:
  - Load package instance
  - Call package's `stop()` method

### 6. Pipeline Cleanup

```bash
jarvis ppl clean
```

**Actions Performed:**
- For each package:
  - Load package instance
  - Call package's `clean()` method
  - Remove temporary files and data

## Environment Management

Environment variables are central to pipeline coordination and package communication.

### Environment Hierarchy

1. **System Environment**: Current shell environment variables
2. **Named Environment**: Predefined environment loaded from `~/.ppi-jarvis/config/environments/`
3. **Pipeline Environment**: Pipeline-specific environment in `env.yaml`
4. **Package Environment**: Package-specific environment modifications

### Environment Propagation

```
System Env → Named/Pipeline Env → Package 1 → Package 2 → Package 3
     ↓              ↓                ↓           ↓           ↓
  PATH=...      CC=gcc-9      PATH+=app1/bin  PATH+=app2  CUSTOM_VAR=val
```

### Environment Variable Types

#### Package Environment (`pkg.env`)
- Contains all environment variables except `LD_PRELOAD`
- Shared across packages in pipeline
- Changes propagated to subsequent packages
- Used for: library paths, application settings, build configuration

#### Modified Environment (`pkg.mod_env`)
- Exact copy of `pkg.env` plus `LD_PRELOAD`
- Private to each package instance
- Modified by interceptors
- Used for: package execution, interceptor injection

### Environment Examples

#### Building Custom Environment
```bash
# Start with clean environment
jarvis ppl env build

# Add development tools
jarvis ppl env build \
  module load gcc/9.3.0 \
  module load openmpi/4.1.0 \
  export CUDA_ROOT=/usr/local/cuda
```

#### Pipeline Environment Configuration

Instead of inline environment dictionaries, use named environments:

```bash
# First, create a named environment with your custom variables
# This can be done by setting environment variables in your shell, then:
export CC="/usr/bin/gcc-9"
export CXX="/usr/bin/g++-9"
export LD_LIBRARY_PATH="/opt/intel/lib:${LD_LIBRARY_PATH}"
export OMP_NUM_THREADS="4"
export CUDA_VISIBLE_DEVICES="0,1"
export BENCHMARK_DATA_DIR="/data/benchmarks"
export RESULTS_OUTPUT_DIR="/tmp/results"

# Build a named environment from your current shell
jarvis ppl env build my_pipeline_env
```

```yaml
# In pipeline YAML, reference the named environment
name: my_pipeline
env: my_pipeline_env  # References the environment created above
```

#### Package Environment Modification
```python
# In package configure() method
def _configure(self, **kwargs):
    # Add application-specific paths
    self.setenv('MY_APP_HOME', '/opt/myapp')
    self.prepend_env('PATH', '/opt/myapp/bin')
    self.prepend_env('LD_LIBRARY_PATH', '/opt/myapp/lib')
    
    # Set application configuration
    self.setenv('MY_APP_CONFIG', f'{self.shared_dir}/app.conf')
    self.setenv('MY_APP_LOG_LEVEL', 'INFO')
```

## Pipeline Indexes

Pipeline indexes provide discoverable, reusable pipeline templates and examples.

### Index Structure in Repositories

```
my_repo/
├── my_repo/                    # Package source code
│   ├── package1/
│   └── package2/
└── pipelines/                  # Pipeline index
    ├── basic_example.yaml
    ├── benchmarks/
    │   ├── io_test.yaml
    │   └── compute_test.yaml
    └── tutorials/
        ├── getting_started.yaml
        └── advanced_features.yaml
```

### Using Pipeline Indexes

#### Discovery
```bash
# List all available pipeline scripts
jarvis ppl index list

# Output:
# Available pipeline scripts:
#   builtin:
#     simple_test.yaml
#     test_interceptor.yaml
#     examples/ (directory)
#       basic_workflow.yaml
#       advanced_demo.yaml
```

#### Loading
```bash
# Load pipeline directly into current workspace
jarvis ppl index load builtin.examples.basic_workflow

# Creates new pipeline from the template
# Sets as current pipeline
```

#### Copying for Customization
```bash
# Copy pipeline script for modification
jarvis ppl index copy builtin.examples.basic_workflow ./my_custom.yaml

# Edit the copied file
vim my_custom.yaml

# Load your customized version
jarvis ppl load yaml ./my_custom.yaml
```

### Creating Pipeline Index Entries

#### Example Index Pipeline
```yaml
# File: pipelines/examples/basic_io_benchmark.yaml
# Pipeline: Basic I/O Performance Test
# Purpose: Simple I/O benchmark with monitoring
# Requirements: MPI environment, named environment 'io_test_env' with TEST_DIR and BLOCK_SIZE
# Expected Runtime: 5-10 minutes

name: basic_io_benchmark
# Named environment should define: TEST_DIR="/tmp/io_test", BLOCK_SIZE="1G"
env: io_test_env

interceptors:
  - pkg_type: builtin.io_tracer
    pkg_name: io_monitor
    trace_reads: true
    trace_writes: true
    output_file: "${TEST_DIR}/io_trace.log"

pkgs:
  - pkg_type: builtin.mkfs
    pkg_name: setup_filesystem
    mount_point: "${TEST_DIR}"
    size: "10G"
    
  - pkg_type: builtin.ior
    pkg_name: io_benchmark
    interceptors: ["io_monitor"]
    nprocs: 4
    block: "${BLOCK_SIZE}"
    test_file: "${TEST_DIR}/test_file"
```

## Advanced Pipeline Features

### Multi-Stage Pipelines

```yaml
name: multi_stage_pipeline

pkgs:
  # Stage 1: Data preparation
  - pkg_type: builtin.data_generator
    pkg_name: data_prep
    data_size: "100G"
    output_dir: "/tmp/data"
    
  # Stage 2: Parallel processing
  - pkg_type: builtin.mpi_processor
    pkg_name: parallel_analysis
    input_dir: "/tmp/data"          # Uses output from stage 1
    nprocs: 16
    
  # Stage 3: Results aggregation
  - pkg_type: builtin.aggregator
    pkg_name: results_summary
    input_pattern: "/tmp/data/results_*"
    output_file: "/tmp/final_results.json"
```

### Conditional Package Execution

```yaml
name: conditional_pipeline

pkgs:
  # Always runs
  - pkg_type: builtin.system_check
    pkg_name: prerequisites
    
  # Runs only if GPU available
  - pkg_type: builtin.gpu_benchmark
    pkg_name: gpu_test
    enable_condition: "has_cuda"
    
  # Fallback for non-GPU systems
  - pkg_type: builtin.cpu_benchmark
    pkg_name: cpu_test
    enable_condition: "no_cuda"
```

### Complex Interceptor Combinations

```yaml
name: comprehensive_monitoring

interceptors:
  # CPU profiling
  - pkg_type: builtin.perf_profiler
    pkg_name: cpu_profiler
    sampling_rate: 1000
    
  # Memory debugging
  - pkg_type: builtin.memory_debugger
    pkg_name: mem_checker
    tool: "asan"
    
  # I/O monitoring
  - pkg_type: builtin.io_tracer
    pkg_name: io_monitor
    trace_all: true
    
  # Network monitoring
  - pkg_type: builtin.network_tracer
    pkg_name: net_monitor
    trace_mpi: true

pkgs:
  # Lightweight application (minimal monitoring)
  - pkg_type: builtin.simple_app
    pkg_name: lightweight
    interceptors: ["io_monitor"]
    
  # Intensive application (full monitoring)
  - pkg_type: builtin.complex_app
    pkg_name: intensive
    interceptors: ["cpu_profiler", "mem_checker", "io_monitor", "net_monitor"]
    
  # Debug version (memory checking only)
  - pkg_type: builtin.debug_app
    pkg_name: debug_version
    interceptors: ["mem_checker"]
```

## Best Practices

### Pipeline Design

#### 1. Use Descriptive Names
```yaml
# ✅ Good
name: hpc_io_performance_benchmark

# ❌ Poor
name: test
```

#### 2. Organize by Logical Stages
```yaml
pkgs:
  # Setup stage
  - pkg_type: builtin.environment_setup
    pkg_name: env_setup
    
  - pkg_type: builtin.data_preparation
    pkg_name: data_prep
    
  # Execution stage
  - pkg_type: builtin.benchmark
    pkg_name: main_benchmark
    
  # Cleanup stage
  - pkg_type: builtin.results_collector
    pkg_name: cleanup
```

#### 3. Use Environment Variables for Paths

Define paths in a named environment, then reference them in package configurations:

```yaml
# Named environment should define: WORK_DIR="/scratch/benchmark", RESULTS_DIR="/home/user/results"
env: benchmark_paths_env

pkgs:
  - pkg_type: builtin.app
    pkg_name: processor
    input_dir: "${WORK_DIR}/input"
    output_dir: "${RESULTS_DIR}/output"
```

### Configuration Management

#### 1. Provide Reasonable Defaults
```yaml
pkgs:
  - pkg_type: builtin.ior
    pkg_name: io_test
    # Provide sensible defaults
    nprocs: 4
    block: "1G"
    transfer: "64K"
    # Allow easy customization
    test_file: "${WORK_DIR}/test_file"
```

#### 2. Document Complex Parameters
```yaml
pkgs:
  - pkg_type: builtin.solver
    pkg_name: numerical_solver
    # Solver configuration
    method: "gmres"              # Options: gmres, bicgstab, cg
    tolerance: 1e-6              # Convergence tolerance
    max_iterations: 1000         # Maximum solver iterations
    preconditioner: "ilu"        # Preconditioning method
```

#### 3. Use Type-Appropriate Values
```yaml
pkgs:
  - pkg_type: builtin.app
    pkg_name: configured_app
    # Integer parameters
    nprocs: 8                    # Not "8"
    port: 5432                   # Not "5432"
    # Boolean parameters  
    enable_debug: true           # Not "true"
    use_gpu: false              # Not "false"
    # String parameters
    log_level: "INFO"           # Quoted for clarity
    output_format: "json"       # Quoted for clarity
```

### Interceptor Usage

#### 1. Apply Interceptors Selectively
```yaml
interceptors:
  - pkg_type: builtin.profiler
    pkg_name: perf_monitor

pkgs:
  # Only monitor performance-critical packages
  - pkg_type: builtin.fast_app
    pkg_name: critical_app
    interceptors: ["perf_monitor"]    # Apply monitoring
    
  # Skip monitoring for simple utilities
  - pkg_type: builtin.file_copy
    pkg_name: file_util
    # No interceptors - lightweight operation
```

#### 2. Group Related Interceptors
```yaml
interceptors:
  # Performance monitoring group
  - pkg_type: builtin.cpu_profiler
    pkg_name: cpu_monitor
  - pkg_type: builtin.memory_profiler  
    pkg_name: mem_monitor
    
  # I/O monitoring group
  - pkg_type: builtin.io_tracer
    pkg_name: io_monitor

pkgs:
  - pkg_type: builtin.compute_app
    pkg_name: cpu_intensive
    interceptors: ["cpu_monitor", "mem_monitor"]
    
  - pkg_type: builtin.io_app
    pkg_name: io_intensive
    interceptors: ["io_monitor"]
```

### Environment Management

#### 1. Minimize Environment Pollution

When creating named environments, be specific and avoid overwriting critical system variables:

```bash
# ✅ Good - specific and necessary
export BENCHMARK_DATA_DIR="/data/benchmark"
export RESULTS_OUTPUT_DIR="/tmp/results"
jarvis ppl env build clean_benchmark_env

# ❌ Poor - overwrites important system variables
export PATH="/my/custom/path"           # Loses system PATH - dangerous!
export LD_LIBRARY_PATH="/my/libs"       # Overwrites system libraries - dangerous!
jarvis ppl env build problematic_env    # Don't do this!
```

#### 2. Use Environment Composition

When building named environments, compose paths properly:

```bash
# Define base directories
export PROJECT_ROOT="/opt/myproject"
export DATA_ROOT="/data"

# Build derived paths
export PROJECT_BIN="${PROJECT_ROOT}/bin"
export PROJECT_LIB="${PROJECT_ROOT}/lib"

# Extend system paths (don't replace them!)
export PATH="${PROJECT_BIN}:${PATH}"
export LD_LIBRARY_PATH="${PROJECT_LIB}:${LD_LIBRARY_PATH}"

# Save as named environment
jarvis ppl env build composed_project_env
```

### Error Handling

#### 1. Validate Prerequisites
```yaml
pkgs:
  # Check system requirements first
  - pkg_type: builtin.system_check
    pkg_name: prerequisites
    required_memory: "8G"
    required_disk: "100G"
    required_commands: ["mpiexec", "gcc"]
    
  # Main application
  - pkg_type: builtin.main_app
    pkg_name: application
```

#### 2. Provide Cleanup Stages
```yaml
pkgs:
  # Setup
  - pkg_type: builtin.setup
    pkg_name: initialization
    
  # Main work
  - pkg_type: builtin.application
    pkg_name: main_work
    
  # Always cleanup (even on failure)
  - pkg_type: builtin.cleanup
    pkg_name: cleanup
    run_on_failure: true
```

## Troubleshooting

### Common Issues and Solutions

#### Pipeline Loading Failures

**Problem**: `jarvis ppl load yaml pipeline.yaml` fails
```
Error: Package not found: my_repo.custom_app
```

**Solutions**:
1. Verify repository is added: `jarvis repo list`
2. Add repository: `jarvis repo add /path/to/my_repo`
3. Check package exists: `jarvis ppl append my_repo.custom_app` (test)

**Problem**: YAML syntax errors
```
Error: yaml.scanner.ScannerError: while parsing a block mapping
```

**Solutions**:
1. Validate YAML syntax: `python -c "import yaml; yaml.safe_load(open('pipeline.yaml'))"`
2. Check indentation (use spaces, not tabs)
3. Quote string values with special characters

#### Package Configuration Issues

**Problem**: Package configuration not applied
```
jarvis pkg conf app param=value
# Parameter not updated in pipeline
```

**Solutions**:
1. Check parameter name: `jarvis pkg conf app --help`
2. Use correct type: `jarvis pkg conf app count=5` (not `count="5"` for integer)
3. Verify package exists in current pipeline: `jarvis ppl print`

#### Environment Problems

**Problem**: Environment variables not propagated
```
Package can't find libraries despite setting LD_LIBRARY_PATH
```

**Solutions**:
1. Set environment in pipeline YAML, not package configuration
2. Use `_configure()` method in package code for environment changes
3. Check environment propagation: `jarvis ppl env show`

#### Interceptor Issues

**Problem**: Interceptor not applied
```
jarvis ppl start
# No profiling output despite interceptor configuration
```

**Solutions**:
1. Verify interceptor exists in pipeline: `jarvis ppl print`
2. Check package references interceptor: look for `interceptors: ["interceptor_name"]`
3. Ensure interceptor package implements `modify_env()` method

### Debugging Commands

#### Check Pipeline State
```bash
# Show complete pipeline configuration
jarvis ppl print

# Check package configuration
jarvis pkg conf package_name --help

# Show package paths
jarvis pkg path package_name --conf_dir --shared_dir
```

#### Environment Debugging
```bash
# Show pipeline environment
jarvis ppl env show

# Check current pipeline
jarvis ppl list

# Show package environment files
jarvis pkg path package_name --env --mod_env
```

#### Process Debugging
```bash
# Check pipeline status
jarvis ppl status

# Show running processes
ps aux | grep jarvis

# Check package logs
tail -f ~/.ppi-jarvis/config/pipelines/my_pipeline/packages/app/shared/*.log
```

### Performance Optimization

#### Pipeline Execution Performance

1. **Minimize Package Count**: Combine related operations into single packages
2. **Optimize Environment**: Reduce environment variable propagation overhead
3. **Selective Interceptors**: Apply interceptors only where needed
4. **Parallel Packages**: Use MPI for parallel execution within packages

#### Storage Optimization

1. **Clean Regularly**: Use `jarvis ppl clean` to remove temporary files
2. **Monitor Disk Usage**: Check pipeline directory sizes
3. **Use Shared Storage**: Place large datasets in shared directories
4. **Archive Old Pipelines**: Remove unused pipeline directories

This comprehensive documentation covers all aspects of pipeline management in Jarvis-CD, from basic concepts to advanced troubleshooting. Use it as a reference for developing, debugging, and optimizing your computational pipelines.
```

### `docs/resource_graph.md`

```markdown
# Jarvis-CD Resource Graph

The Resource Graph is a comprehensive system for discovering, analyzing, and querying storage resources across compute clusters. It collects detailed information about storage devices, filesystems, and performance characteristics from all nodes in the cluster.

## Table of Contents

1. [Overview](#overview)
2. [Building the Resource Graph](#building-the-resource-graph)
3. [Querying Storage Devices](#querying-storage-devices)
4. [Resource Graph CLI Commands](#resource-graph-cli-commands)
5. [Programmatic API](#programmatic-api)
6. [Storage Device Information](#storage-device-information)
7. [Performance Benchmarking](#performance-benchmarking)
8. [Common Use Cases](#common-use-cases)
9. [File Formats](#file-formats)

## Overview

The Resource Graph provides:

- **Automatic Discovery**: Identifies all accessible storage devices across cluster nodes
- **Performance Metrics**: Benchmarks storage performance (4K random write, 1M sequential write)
- **Common Storage Analysis**: Finds storage mount points available across multiple nodes (or all mounts for single-node clusters)
- **Device Classification**: Categorizes storage by type (SSD, HDD, etc.) and filesystem
- **Programmatic Access**: Query storage resources from packages and applications
- **Persistent Storage**: Save and load resource graphs for reuse

### Key Components

- **ResourceGraphManager**: Coordinates resource collection across the cluster
- **ResourceGraph**: Manages storage resource data and provides query methods
- **CLI Commands**: User-friendly commands for building and querying the resource graph

## Building the Resource Graph

### Prerequisites

1. **Set Hostfile**: Configure the cluster nodes to scan
2. **Network Access**: Ensure SSH connectivity to all nodes
3. **Storage Access**: Verify user permissions on target storage devices

### Basic Usage

```bash
# Set the hostfile first
jarvis hostfile set /path/to/hostfile

# Build resource graph with performance benchmarking
jarvis rg build

# Build without benchmarking (faster)
jarvis rg build +no_benchmark

# Build with custom benchmark duration
jarvis rg build duration=60
```

### Hostfile Format

```
node1.cluster.local
node2.cluster.local  
node3.cluster.local
192.168.1.100
localhost
```

### Collection Process

1. **Parallel Collection**: Resource data is collected from all nodes simultaneously
2. **Storage Discovery**: Identifies mounted filesystems accessible to the current user
3. **Device Analysis**: Determines device type, model, and capacity
4. **Performance Benchmarking**: Measures I/O performance (optional)
5. **Common Storage Analysis**: Identifies shared mount points across nodes
6. **Persistent Storage**: Saves results to `~/.ppi-jarvis/resource_graph.yaml`

## Querying Storage Devices

### Get All Storage Devices

```python
from jarvis_cd.core.resource_graph import ResourceGraphManager
from jarvis_cd.core.config import JarvisConfig

# Initialize (automatically loads resource graph if it exists)
jarvis_config = JarvisConfig()
rg_manager = ResourceGraphManager(jarvis_config)

# Or explicitly load resource graph from a specific file
rg_manager.load('/path/to/resource_graph.yaml')

# Get all nodes
nodes = rg_manager.resource_graph.get_all_nodes()
print(f"Cluster nodes: {nodes}")

# Get storage for specific node
storage_devices = rg_manager.resource_graph.get_node_storage('node1')
for device in storage_devices:
    print(f"{device['mount']}: {device['avail']} ({device['dev_type']})")
```

### Filter by Device Type

```python
# Get only SSD devices across all nodes
ssd_devices = rg_manager.resource_graph.filter_by_type('ssd')
for hostname, devices in ssd_devices.items():
    print(f"\n{hostname} SSDs:")
    for device in devices:
        print(f"  {device['mount']}: {device['avail']}")

# Get HDD devices
hdd_devices = rg_manager.resource_graph.filter_by_type('hdd')
```

### Get Common Storage

```python
# Find storage mount points available on multiple nodes
# Note: For single-node clusters, all mount points are considered "common"
common_storage = rg_manager.resource_graph.get_common_storage()
for mount_point, devices in common_storage.items():
    print(f"\nMount {mount_point} available on {len(devices)} nodes:")
    for device in devices:
        print(f"  {device['hostname']}: {device['avail']}")
```

### Filter by Mount Pattern

```python
# Find all /tmp or temporary storage
tmp_storage = rg_manager.resource_graph.filter_by_mount_pattern('/tmp')

# Find home directories  
home_storage = rg_manager.resource_graph.filter_by_mount_pattern('/home')

# Find specific mount points
scratch_storage = rg_manager.resource_graph.filter_by_mount_pattern('/scratch')
```

## Resource Graph CLI Commands

### Build Commands

```bash
# Build complete resource graph
jarvis rg build

# Build without performance benchmarking  
jarvis rg build +no_benchmark

# Build with custom benchmark duration
jarvis rg build duration=30
```

### Query Commands

```bash
# Show resource graph summary
jarvis rg show

# List all nodes in resource graph
jarvis rg nodes

# Show detailed information for specific node
jarvis rg node hostname1

# Filter storage devices by type
jarvis rg filter ssd
jarvis rg filter hdd
jarvis rg filter nvme

# Show resource graph file path (output only the path)
jarvis rg path
```

### Management Commands

```bash
# Load resource graph from custom file
jarvis rg load /path/to/custom_resource_graph.yaml

# Show path to current resource graph file (prints only the path)
jarvis rg path

# Use in shell command substitution
cd $(dirname $(jarvis rg path))  # Navigate to resource graph directory
ls -la $(jarvis rg path)         # List resource graph file details

# Resource graph is automatically saved to ~/.ppi-jarvis/resource_graph.yaml
# after building
```

## Programmatic API

### ResourceGraph Class

```python
from jarvis_cd.util.resource_graph import ResourceGraph

# Create and populate resource graph
rg = ResourceGraph()

# Add node data (typically done during collection)
node_data = {
    'fs': [
        {
            'device': '/dev/sda1',
            'mount': '/home',
            'fs_type': 'ext4', 
            'avail': '100GB',
            'dev_type': 'ssd',
            'model': 'Samsung SSD 970',
            '4k_randwrite_bw': '50MB/s',
            '1m_seqwrite_bw': '500MB/s'
        }
    ]
}
rg.add_node_data('node1', node_data)
```

### Query Methods

```python
# Get all nodes
nodes = rg.get_all_nodes()

# Get storage for specific node
devices = rg.get_node_storage('node1')

# Get summary statistics
summary = rg.get_storage_summary()
print(f"Total devices: {summary['total_devices']}")
print(f"Device types: {summary['device_types']}")

# Filter by type
ssd_devices = rg.filter_by_type('ssd')
hdd_devices = rg.filter_by_type('hdd')

# Filter by mount pattern
tmp_mounts = rg.filter_by_mount_pattern('/tmp')

# Get common storage across nodes
common = rg.get_common_storage()
```

### Device Dictionary Structure

Storage devices are represented as dictionaries with the following fields:

```python
device = devices[0]  # Get first device (dict)

# Basic properties
print(f"Device: {device['device']}")           # /dev/sda1
print(f"Mount: {device['mount']}")             # /home
print(f"Type: {device['dev_type']}")           # ssd
print(f"Filesystem: {device['fs_type']}")      # ext4
print(f"Available: {device['avail']}")         # 100GB
print(f"Hostname: {device['hostname']}")       # node1

# Optional properties (use .get() with defaults)
print(f"Model: {device.get('model', 'unknown')}")        # Samsung SSD 970
print(f"Shared: {device.get('shared', False)}")          # True if on multiple nodes

# Performance metrics (optional, present if benchmarking was performed)
print(f"4K Random Write: {device.get('4k_randwrite_bw', 'unknown')}")  # 50MB/s
print(f"1M Sequential: {device.get('1m_seqwrite_bw', 'unknown')}")     # 500MB/s

# System properties (optional)
print(f"UUID: {device.get('uuid', 'unknown')}")               # Filesystem UUID
print(f"Parent: {device.get('parent', 'unknown')}")           # Parent device
print(f"Needs Root: {device.get('needs_root', False)}")       # Requires root access
```

## Storage Device Information

### Device Detection

The resource graph automatically detects:

- **Physical Devices**: `/dev/sda`, `/dev/nvme0n1`, etc.
- **Mount Points**: `/`, `/home`, `/tmp`, `/scratch`, etc.
- **Filesystem Types**: `ext4`, `xfs`, `btrfs`, `tmpfs`, etc.
- **Device Types**: `ssd`, `hdd`, `nvme`, `unknown`
- **Access Permissions**: Whether current user can write to the mount

### Device Classification

Storage devices are classified by type:

- **`ssd`**: Solid State Drives
- **`hdd`**: Hard Disk Drives  
- **`nvme`**: NVMe devices
- **`unknown`**: Unidentified device types

### Capacity Information

Available space is reported in human-readable format:
- `1.2TB` - Terabytes
- `500GB` - Gigabytes
- `10MB` - Megabytes

## Performance Benchmarking

### Benchmark Types

The resource graph can measure storage performance using:

1. **4K Random Write**: Small random I/O performance
2. **1M Sequential Write**: Large sequential I/O performance

### Benchmark Configuration

```bash
# Enable benchmarking (default)
jarvis rg build

# Disable benchmarking for faster collection
jarvis rg build +no_benchmark

# Custom benchmark duration (default: 25 seconds)
jarvis rg build duration=60
```

### Performance Metrics

```python
# Access performance data
for device in devices:
    if device.get('4k_randwrite_bw', 'unknown') != 'unknown':
        print(f"4K Random Write: {device['4k_randwrite_bw']}")
    if device.get('1m_seqwrite_bw', 'unknown') != 'unknown':
        print(f"1M Sequential: {device['1m_seqwrite_bw']}")
```

### Performance Considerations

- **Benchmarking Time**: Longer durations provide more accurate results
- **I/O Impact**: Benchmarks perform actual write operations
- **Permissions**: Some benchmarks may require specific mount point access
- **Concurrent Access**: Multiple nodes benchmark simultaneously

## Common Use Cases

### 1. Find High-Performance Storage

```python
# Find SSDs with good performance across the cluster
ssd_devices = rg.filter_by_type('ssd')
high_perf_storage = {}

for hostname, devices in ssd_devices.items():
    good_devices = []
    for device in devices:
        # Parse bandwidth (simplified)
        seq_bw = device.get('1m_seqwrite_bw', '')
        if seq_bw and 'GB/s' in seq_bw:
            good_devices.append(device)
    if good_devices:
        high_perf_storage[hostname] = good_devices

print(f"High-performance storage found on {len(high_perf_storage)} nodes")
```

### 2. Find Common Scratch Space

```python
# Get scratch directories available on all nodes
common_storage = rg.get_common_storage()
scratch_spaces = {mount: devices for mount, devices in common_storage.items() 
                  if '/scratch' in mount or '/tmp' in mount}

print("Available scratch spaces:")
for mount, devices in scratch_spaces.items():
    print(f"  {mount}: available on {len(devices)} nodes")
```

### 3. Package Storage Selection

```python
# In a package's _configure method
class MyApp(Application):
    def _configure(self, **kwargs):
        # Configuration automatically updated

        # Get resource graph (automatically loaded on init)
        from jarvis_cd.core.resource_graph import ResourceGraphManager
        rg_manager = ResourceGraphManager(self.jarvis.jarvis_config)

        # Find fast storage for output
        ssd_storage = rg_manager.resource_graph.filter_by_type('ssd')
        if ssd_storage:
            # Use first available SSD
            hostname, devices = next(iter(ssd_storage.items()))
            output_dir = devices[0]['mount'] + '/my_app_output'
            self.setenv('OUTPUT_DIR', output_dir)
            print(f"Using fast storage: {output_dir}")
```

### 4. Storage Capacity Planning

```python
# Calculate total available storage by type
summary = rg.get_storage_summary()
print(f"Cluster storage summary:")
print(f"  Total devices: {summary['total_devices']}")
print(f"  Device types: {summary['device_types']}")

# Find nodes with large storage
large_storage_nodes = {}
for hostname in rg.get_all_nodes():
    devices = rg.get_node_storage(hostname)
    for device in devices:
        # Parse capacity (simplified - actual parsing would be more robust)
        if 'TB' in device['avail']:
            if hostname not in large_storage_nodes:
                large_storage_nodes[hostname] = []
            large_storage_nodes[hostname].append(device)

print(f"Nodes with large storage: {list(large_storage_nodes.keys())}")
```

### 5. Validate Storage Access

```python
# Check that required storage is available
required_mounts = ['/scratch', '/tmp', '/home']
available_mounts = set()

for hostname in rg.get_all_nodes():
    devices = rg.get_node_storage(hostname)
    node_mounts = {device['mount'] for device in devices}
    available_mounts.update(node_mounts)

missing_mounts = set(required_mounts) - available_mounts
if missing_mounts:
    print(f"Warning: Missing required storage: {missing_mounts}")
else:
    print("All required storage mounts available")
```

## File Formats

### YAML Format (Default)

```yaml
nodes:
  node1:
    - device: /dev/sda1
      mount: /home
      fs_type: ext4
      avail: 100GB
      dev_type: ssd
      model: Samsung SSD 970
      shared: true
      4k_randwrite_bw: 50MB/s
      1m_seqwrite_bw: 500MB/s
      
common_mounts:
  /home:
    - device: /dev/sda1
      mount: /home
      # ... device details
      
summary:
  total_nodes: 3
  total_devices: 12
  common_mount_points: 2
  device_types:
    ssd: 8
    hdd: 4
  filesystem_types:
    ext4: 10
    xfs: 2
```

### JSON Format

```python
# Save as JSON
rg.save_to_file(Path('resource_graph.json'), format='json')

# Load JSON
rg.load_from_file(Path('resource_graph.json'))
```

### Loading Custom Resource Graphs

```bash
# Load from custom location
jarvis rg load /shared/cluster_storage.yaml

# Default location (automatically loaded)
# ~/.ppi-jarvis/resource_graph.yaml
```

### Shell Integration

The `jarvis rg path` command outputs only the file path, making it perfect for shell command substitution:

```bash
# Navigate to the resource graph directory
cd $(dirname $(jarvis rg path))

# Edit the resource graph file
vim $(jarvis rg path)

# Copy resource graph to another location
cp $(jarvis rg path) /backup/

# Check file details
ls -la $(jarvis rg path)

# View resource graph contents
cat $(jarvis rg path)

# Backup resource graph with timestamp
cp $(jarvis rg path) $(jarvis rg path).backup.$(date +%Y%m%d)
```

**Error Handling**: If no resource graph exists, the command will exit with status code 1 and print error messages to stderr, ensuring command substitution fails gracefully.

### Integration with Packages

```python
# Access resource graph in package code
class StorageAwareApp(Application):
    def _configure(self, **kwargs):
        # Configuration automatically updated

        # Get resource graph (automatically loaded on init if it exists)
        rg_manager = ResourceGraphManager(self.jarvis.jarvis_config)
        if not rg_manager.resource_graph.get_all_nodes():
            print("No resource graph found. Run 'jarvis rg build' first.")
            return

        # Find optimal storage for this application
        storage_choice = self._select_storage(rg_manager.resource_graph)
        self.setenv('APP_STORAGE_PATH', storage_choice)

    def _select_storage(self, rg):
        """Select optimal storage based on requirements"""
        # Example: prefer SSDs for output
        ssd_storage = rg.filter_by_type('ssd')
        if ssd_storage:
            hostname, devices = next(iter(ssd_storage.items()))
            return devices[0]['mount'] + '/app_output'

        # Fallback to any available storage
        all_nodes = rg.get_all_nodes()
        if all_nodes:
            devices = rg.get_node_storage(all_nodes[0])
            if devices:
                return devices[0]['mount'] + '/app_output'

        return '/tmp/app_output'  # Final fallback
```

The Resource Graph provides a powerful foundation for storage-aware application deployment and performance optimization across heterogeneous clusters.
```

### `docs/shell.md`

```markdown
# Jarvis-CD Shell Execution System

This guide documents the shell execution system in `jarvis_cd.shell`, which provides comprehensive command execution capabilities for local, remote, and parallel execution.

## Table of Contents

1. [Overview](#overview)
2. [Execution Types](#execution-types)
3. [Factory Pattern](#factory-pattern)
4. [Core Classes](#core-classes)
5. [Process Utilities](#process-utilities)
6. [Usage Examples](#usage-examples)
7. [Best Practices](#best-practices)

## Overview

The Jarvis-CD shell system provides a unified interface for executing commands across different environments:

- **Local execution**: Commands run on the local machine
- **SSH execution**: Commands run on remote hosts via SSH
- **MPI execution**: Parallel commands using MPI frameworks
- **File transfer**: Secure copying between hosts
- **Process utilities**: Common system operations

All execution classes follow a consistent interface and provide proper error handling, output collection, and process management.

## Execution Types

### ExecType Enumeration

```python
from jarvis_cd.shell import ExecType

# Available execution types
ExecType.LOCAL      # Local execution
ExecType.SSH        # Single SSH connection
ExecType.PSSH       # Parallel SSH (multiple hosts)
ExecType.MPI        # MPI with auto-detection
ExecType.OPENMPI    # Specific OpenMPI
ExecType.MPICH      # Specific MPICH
ExecType.INTEL_MPI  # Intel MPI
ExecType.CRAY_MPICH # Cray MPICH
ExecType.SCP        # Secure copy
ExecType.PSCP       # Parallel secure copy
```

### ExecInfo Classes

Each execution type has a corresponding `ExecInfo` class that holds execution parameters:

```python
from jarvis_cd.shell import LocalExecInfo, SshExecInfo, MpiExecInfo

# Local execution info
local_info = LocalExecInfo(
    env={'PATH': '/custom/path'},
    cwd='/working/directory',
    timeout=30
)

# SSH execution info
ssh_info = SshExecInfo(
    hostfile=hostfile,
    user='myuser',
    pkey='/path/to/key',
    port=22
)

# MPI execution info
mpi_info = MpiExecInfo(
    hostfile=hostfile,
    nprocs=8,
    ppn=2,
    env={'OMP_NUM_THREADS': '4'}
)
```

## Factory Pattern

### Exec Factory Class

The `Exec` class acts as a factory that automatically selects the appropriate executor based on the `ExecInfo` type:

```python
from jarvis_cd.shell import Exec, LocalExecInfo, MpiExecInfo

# Automatically uses LocalExec
result = Exec('ls -la', LocalExecInfo()).run()

# Automatically uses MpiExec
result = Exec('./parallel_app', MpiExecInfo(nprocs=4)).run()
```

## Core Classes

### LocalExec

Execute commands on the local machine using subprocess.

```python
from jarvis_cd.shell import LocalExec, LocalExecInfo

exec_info = LocalExecInfo(
    env={'MYVAR': 'value'},
    cwd='/tmp',
    collect_output=True,
    hide_output=False
)

executor = LocalExec('python script.py', exec_info)
executor.run()

# Access results
print(f"Exit code: {executor.exit_code['localhost']}")
print(f"Output: {executor.stdout['localhost']}")
print(f"Errors: {executor.stderr['localhost']}")
```

### SshExec

Execute commands on a single remote host via SSH.

```python
from jarvis_cd.shell import SshExec, SshExecInfo
from jarvis_cd.util.hostfile import Hostfile

hostfile = Hostfile(['remote.server.com'])
exec_info = SshExecInfo(
    hostfile=hostfile,
    user='username',
    pkey='/path/to/private/key',
    strict_ssh=False
)

executor = SshExec('hostname && uptime', exec_info)
executor.run()

# Results are keyed by hostname
hostname = hostfile.hosts[0]
print(f"Output from {hostname}: {executor.stdout[hostname]}")
```

### PsshExec

Execute commands on multiple hosts in parallel.

```python
from jarvis_cd.shell import PsshExec, PsshExecInfo
from jarvis_cd.util.hostfile import Hostfile

hostfile = Hostfile(['host1.com', 'host2.com', 'host3.com'])
exec_info = PsshExecInfo(
    hostfile=hostfile,
    user='username',
    pkey='/path/to/key'
)

executor = PsshExec('df -h', exec_info)
executor.run()

# Wait for all hosts to complete
results = executor.wait_all()

# Access results per host
for hostname in hostfile.hosts:
    print(f"Host {hostname}:")
    print(f"  Exit code: {executor.exit_code[hostname]}")
    print(f"  Output: {executor.stdout[hostname]}")
```

### MpiExec

Execute MPI applications with automatic MPI implementation detection.

```python
from jarvis_cd.shell import MpiExec, MpiExecInfo
from jarvis_cd.util.hostfile import Hostfile

hostfile = Hostfile(['node1', 'node2', 'node3'])
exec_info = MpiExecInfo(
    hostfile=hostfile,
    nprocs=12,          # Total processes
    ppn=4,              # Processes per node
    env={
        'OMP_NUM_THREADS': '2',
        'MPI_BUFFER_SIZE': '32M'
    }
)

executor = MpiExec('./my_mpi_app input.dat', exec_info)
executor.run()

# MPI output is typically from rank 0
print(f"MPI output: {executor.stdout['localhost']}")
```

### ScpExec and PscpExec

Transfer files between hosts using rsync over SSH.

```python
from jarvis_cd.shell import ScpExec, ScpExecInfo
from jarvis_cd.util.hostfile import Hostfile

hostfile = Hostfile(['remote.host.com'])
exec_info = ScpExecInfo(
    hostfile=hostfile,
    user='username',
    pkey='/path/to/key'
)

# Single file copy
executor = ScpExec('/local/file.txt', exec_info)
executor.run()

# Multiple files with custom destinations
file_pairs = [
    ('/local/config.yml', '/remote/app/config.yml'),
    ('/local/data/', '/remote/app/data/'),
]
executor = ScpExec(file_pairs, exec_info)
executor.run()

# Check transfer results
for hostname in hostfile.hosts:
    if executor.exit_code[hostname] == 0:
        print(f"Transfer to {hostname} successful")
    else:
        print(f"Transfer to {hostname} failed: {executor.stderr[hostname]}")
```

## Process Utilities

### Kill

Terminate processes by name pattern.

```python
from jarvis_cd.shell.process import Kill
from jarvis_cd.shell import LocalExecInfo, PsshExecInfo

# Kill local processes
Kill('python.*my_script', LocalExecInfo()).run()

# Kill processes on remote hosts
Kill('my_application', PsshExecInfo(hostfile=hostfile)).run()

# Exact name matching (partial=False)
Kill('nginx', LocalExecInfo(), partial=False).run()
```

### KillAll

Kill all processes owned by the current user.

```python
from jarvis_cd.shell.process import KillAll
from jarvis_cd.shell import PsshExecInfo

# Kill all user processes on remote hosts
KillAll(PsshExecInfo(hostfile=hostfile)).run()
```

### Which

Find executable locations.

```python
from jarvis_cd.shell.process import Which
from jarvis_cd.shell import LocalExecInfo

which = Which('mpiexec', LocalExecInfo())
which.run()

if which.exists():
    print(f"mpiexec found at: {which.get_path()}")
else:
    print("mpiexec not found in PATH")
```

### Mkdir

Create directories with proper options.

```python
from jarvis_cd.shell.process import Mkdir
from jarvis_cd.shell import LocalExecInfo, PsshExecInfo

# Create local directories
Mkdir(['/tmp/output', '/tmp/logs'], LocalExecInfo()).run()

# Create directories on remote hosts
Mkdir('/shared/data', PsshExecInfo(hostfile=hostfile), parents=True).run()
```

### Rm

Remove files and directories.

```python
from jarvis_cd.shell.process import Rm
from jarvis_cd.shell import LocalExecInfo, PsshExecInfo

# Remove local files
Rm('/tmp/temp_data*', LocalExecInfo(), recursive=True).run()

# Remove files on remote hosts
Rm(['/tmp/log1.txt', '/tmp/log2.txt'], PsshExecInfo(hostfile=hostfile)).run()
```

### Chmod

Change file permissions.

```python
from jarvis_cd.shell.process import Chmod
from jarvis_cd.shell import LocalExecInfo

# Make scripts executable
Chmod('/path/to/script.sh', '+x', LocalExecInfo()).run()

# Set specific permissions recursively
Chmod('/data/directory', '755', LocalExecInfo(), recursive=True).run()
```

### Sleep

Pause execution for a specified duration.

```python
from jarvis_cd.shell.process import Sleep
from jarvis_cd.shell import LocalExecInfo

# Sleep for 5 seconds
Sleep(5, LocalExecInfo()).run()

# Sleep for 1.5 seconds
Sleep(1.5, LocalExecInfo()).run()
```

### Echo

Print text to stdout.

```python
from jarvis_cd.shell.process import Echo
from jarvis_cd.shell import LocalExecInfo

Echo("Processing complete", LocalExecInfo()).run()
```

## Usage Examples

### Basic Package Implementation

```python
from jarvis_cd.core.pkg import Application
from jarvis_cd.shell import Exec, LocalExecInfo, MpiExecInfo
from jarvis_cd.shell.process import Which, Mkdir, Rm

class MyBenchmark(Application):
    def configure(self, **kwargs):
        self.update_config(kwargs, rebuild=False)
        
        # Set up environment
        self.setenv('BENCHMARK_HOME', self.config['install_path'])
        self.prepend_env('PATH', f"{self.config['install_path']}/bin")
        
    def start(self):
        # Check prerequisites
        which = Which('benchmark_tool', LocalExecInfo(env=self.mod_env))
        which.run()
        if not which.exists():
            raise RuntimeError("benchmark_tool not found in PATH")
            
        # Create output directory
        Mkdir(self.config['output_dir'], LocalExecInfo()).run()
        
        # Run benchmark
        if self.config.get('use_mpi', False):
            exec_info = MpiExecInfo(
                env=self.mod_env,
                hostfile=self.jarvis.hostfile,
                nprocs=self.config['nprocs'],
                ppn=self.config['ppn']
            )
        else:
            exec_info = LocalExecInfo(env=self.mod_env)
            
        cmd = [
            'benchmark_tool',
            '--input', self.config['input_file'],
            '--output', self.config['output_dir'],
            '--iterations', str(self.config['iterations'])
        ]
        
        Exec(' '.join(cmd), exec_info).run()
        
    def clean(self):
        # Remove output files
        Rm(self.config['output_dir'], LocalExecInfo(), recursive=True).run()
```

### Interceptor Implementation

```python
from jarvis_cd.core.pkg import Interceptor
from jarvis_cd.shell.process import Which

class ProfilingInterceptor(Interceptor):
    def configure(self, **kwargs):
        self.update_config(kwargs, rebuild=False)
        
        # Find profiling library
        profiler_lib = self.find_library('profiler')
        if not profiler_lib:
            raise RuntimeError("Profiling library not found")
            
        self.profiler_path = profiler_lib
        
    def modify_env(self):
        # Add profiler to LD_PRELOAD
        current_preload = self.mod_env.get('LD_PRELOAD', '')
        if current_preload:
            preload_value = f"{self.profiler_path}:{current_preload}"
        else:
            preload_value = self.profiler_path
            
        self.setenv('LD_PRELOAD', preload_value)
        self.setenv('PROFILER_OUTPUT', self.config['output_file'])
```

### Complex Multi-Host Deployment

```python
from jarvis_cd.shell import PsshExec, ScpExec, PsshExecInfo, ScpExecInfo
from jarvis_cd.shell.process import Kill, Mkdir
from jarvis_cd.util.hostfile import Hostfile

def deploy_application(hostfile, config):
    """Deploy application to multiple hosts"""
    
    # Create execution info
    exec_info = PsshExecInfo(
        hostfile=hostfile,
        user=config['user'],
        pkey=config['private_key']
    )
    
    # 1. Stop any existing instances
    Kill('my_application', exec_info).run()
    
    # 2. Create necessary directories
    Mkdir(['/app/data', '/app/logs'], exec_info, parents=True).run()
    
    # 3. Copy application files
    files_to_copy = [
        ('/local/app/binary', '/app/binary'),
        ('/local/app/config/', '/app/config/'),
        ('/local/app/data/', '/app/data/')
    ]
    
    scp_info = ScpExecInfo(
        hostfile=hostfile,
        user=config['user'],
        pkey=config['private_key']
    )
    
    ScpExec(files_to_copy, scp_info).run()
    
    # 4. Set permissions
    Chmod('/app/binary', '+x', exec_info).run()
    
    # 5. Start application
    start_cmd = '/app/binary --config /app/config/app.conf --daemon'
    PsshExec(start_cmd, exec_info).run()
    
    # 6. Verify startup
    Sleep(2).run()
    PsshExec('pgrep -f my_application', exec_info).run()
```

## Best Practices

### 1. CRITICAL: Always Call .run() to Execute Commands

**MOST IMPORTANT**: All Exec objects and process utilities must call `.run()` to actually execute commands. Simply creating an Exec object does not execute anything.

```python
# ✅ Correct - Execute the command
from jarvis_cd.shell import Exec, LocalExecInfo

Exec('ls -la', LocalExecInfo()).run()

# ❌ Wrong - Command is never executed
Exec('ls -la', LocalExecInfo())  # Does nothing!

# ✅ Correct - Store executor and run
executor = Exec('./my_app', LocalExecInfo())
executor.run()

# ✅ Correct - Process utilities also need .run()
from jarvis_cd.shell.process import Mkdir, Rm, Which

Which('required_tool', LocalExecInfo()).run()
Mkdir('/output/dir', LocalExecInfo()).run()
Rm('/tmp/files*', LocalExecInfo()).run()

# ❌ Wrong - These commands are never executed
Which('required_tool', LocalExecInfo())  # Check not performed!
Mkdir('/output/dir', LocalExecInfo())    # Directory not created!
Rm('/tmp/files*', LocalExecInfo())       # Files not removed!
```

This is the most common mistake when using the shell system and will cause commands to appear to work but actually do nothing.

### 2. Always Use ExecInfo Classes

```python
# ✅ Good - Use proper ExecInfo
from jarvis_cd.shell import Exec, LocalExecInfo

Exec('command', LocalExecInfo(env=self.mod_env)).run()

# ❌ Bad - Don't create ExecInfo manually
from jarvis_cd.shell.exec_info import ExecInfo, ExecType

exec_info = ExecInfo(exec_type=ExecType.LOCAL)  # Don't do this
```

### 3. Handle Errors Properly

```python
executor = Exec('risky_command', LocalExecInfo())
executor.run()

# Check exit codes
if executor.exit_code['localhost'] != 0:
    error_msg = executor.stderr['localhost']
    raise RuntimeError(f"Command failed: {error_msg}")
```

### 4. Use Environment Variables Correctly

```python
# ✅ Good - Use package environment
def start(self):
    exec_info = LocalExecInfo(env=self.mod_env)
    Exec('my_command', exec_info).run()

# ❌ Bad - Don't use system environment
def start(self):
    exec_info = LocalExecInfo()  # Uses system env
    Exec('my_command', exec_info).run()
```

### 5. Clean Up Resources

```python
def clean(self):
    # Kill processes
    Kill('my_application', PsshExecInfo(hostfile=self.jarvis.hostfile)).run()
    
    # Remove files
    Rm('/tmp/my_app_*', LocalExecInfo(), recursive=True).run()
    
    # Wait a moment for cleanup
    Sleep(1).run()
```

### 6. Use Process Utilities

```python
# ✅ Good - Use utility classes
from jarvis_cd.shell.process import Which, Mkdir, Rm

Which('required_tool', LocalExecInfo()).run()
Mkdir('/output/dir', LocalExecInfo()).run()

# ❌ Bad - Don't construct commands manually
Exec('which required_tool', LocalExecInfo()).run()
Exec('mkdir -p /output/dir', LocalExecInfo()).run()
```

### 7. Handle Timeouts

```python
# Set reasonable timeouts for long-running commands
exec_info = LocalExecInfo(timeout=300)  # 5 minutes
Exec('long_running_command', exec_info).run()
```

### 8. Use Proper File Transfer

```python
# ✅ Good - Use ScpExec for file transfers
from jarvis_cd.shell import ScpExec, ScpExecInfo

ScpExec('/local/file', ScpExecInfo(hostfile=hostfile)).run()

# ❌ Bad - Don't use SSH for file copying
from jarvis_cd.shell import SshExec

SshExec('cp /local/file /remote/file', ssh_info).run()  # Won't work
```

The Jarvis-CD shell system provides a robust foundation for command execution across diverse computing environments. By following these patterns and best practices, you can create reliable, maintainable packages that work consistently across different deployment scenarios.
```

### `jarvis_cd/__init__.py`

```python

```

### `jarvis_cd/core/__init__.py`

```python
"""
Core Jarvis-CD classes and utilities.
"""

from .pkg import Application, Service
from .container_pkg import ContainerApplication, ContainerService

__all__ = [
    'Application',
    'Service',
    'ContainerApplication',
    'ContainerService',
]
```

### `jarvis_cd/core/cli.py`

```python
import sys
import os
from pathlib import Path
from jarvis_cd.util.argparse import ArgParse
from jarvis_cd.core.config import Jarvis
from jarvis_cd.core.pipeline import Pipeline
from jarvis_cd.core.pipeline_index import PipelineIndexManager
from jarvis_cd.core.repository import RepositoryManager
from jarvis_cd.core.pkg import Pkg
from jarvis_cd.core.environment import EnvironmentManager
from jarvis_cd.core.resource_graph import ResourceGraphManager


class JarvisCLI(ArgParse):
    """
    Main Jarvis CLI using the custom ArgParse class.
    Provides commands for initialization, pipeline management, repository management, etc.
    """
    
    def __init__(self):
        super().__init__()
        self.jarvis = None
        self.jarvis_config = None
        self.current_pipeline = None
        self.pipeline_index_manager = None
        self.repo_manager = None
        self.env_manager = None
        self.rg_manager = None
        self.module_manager = None
        
    def define_options(self):
        """Define the complete Jarvis CLI command structure"""
        
        # Main menu (empty command for global options)
        self.add_menu('')
        self.add_cmd('', keep_remainder=True)
        self.add_args([
            {
                'name': 'help',
                'msg': 'Show help information',
                'type': bool,
                'default': False,
                'aliases': ['h']
            }
        ])
        
        # Init command
        self.add_menu('init', msg="Initialize Jarvis configuration")
        self.add_cmd('init', msg="Initialize Jarvis configuration directories", keep_remainder=False)
        self.add_args([
            {
                'name': 'config_dir',
                'msg': 'Configuration directory',
                'type': str,
                'pos': True,
                'default': '~/.ppi-jarvis/config',
                'class': 'dirs',
                'rank': 0
            },
            {
                'name': 'private_dir',
                'msg': 'Private data directory',
                'type': str,
                'pos': True,
                'default': '~/.ppi-jarvis/private',
                'class': 'dirs',
                'rank': 1
            },
            {
                'name': 'shared_dir',
                'msg': 'Shared data directory',
                'type': str,
                'pos': True,
                'default': '~/.ppi-jarvis/shared',
                'class': 'dirs',
                'rank': 2
            },
            {
                'name': 'force',
                'msg': 'Force override of existing repos and resource_graph',
                'type': bool,
                'default': False,
            }
        ])
        
        # Pipeline commands
        self.add_menu('ppl', msg="Pipeline management commands")
        
        self.add_cmd('ppl create', msg="Create a new pipeline", aliases=['ppl c'])
        self.add_args([
            {
                'name': 'pipeline_name',
                'msg': 'Name of the pipeline to create',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('ppl append', msg="Add a package to current pipeline", aliases=['ppl a'])
        self.add_args([
            {
                'name': 'package_spec',
                'msg': 'Package specification (repo.pkg or just pkg)',
                'type': str,
                'required': True,
                'pos': True,
                'class': 'pkg',
                'rank': 0
            },
            {
                'name': 'package_alias',
                'msg': 'Alias for the package in pipeline',
                'type': str,
                'pos': True,
                'class': 'pkg', 
                'rank': 1
            }
        ])
        
        self.add_cmd('ppl run', msg="Run a pipeline", aliases=['ppl r'])
        self.add_args([
            {
                'name': 'load_type',
                'msg': 'Type of pipeline to load (yaml) or current',
                'type': str,
                'pos': True,
                'default': 'current'
            },
            {
                'name': 'pipeline_file',
                'msg': 'Pipeline YAML file to run (required if load_type is yaml)',
                'type': str,
                'pos': True
            }
        ])
        
        self.add_cmd('ppl start', msg="Start current pipeline")
        self.add_args([])
        
        self.add_cmd('ppl stop', msg="Stop current pipeline")
        self.add_args([])
        
        self.add_cmd('ppl kill', msg="Force kill current pipeline")
        self.add_args([])
        
        self.add_cmd('ppl clean', msg="Clean current pipeline data")
        self.add_args([])
        
        self.add_cmd('ppl status', msg="Show current pipeline status")
        self.add_args([])
        
        self.add_cmd('ppl load', msg="Load a pipeline from file")
        self.add_args([
            {
                'name': 'load_type',
                'msg': 'Type of pipeline to load (yaml)',
                'type': str,
                'required': True,
                'pos': True
            },
            {
                'name': 'pipeline_file',
                'msg': 'Pipeline file to load',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('ppl update', msg="Update current pipeline")
        self.add_args([
            {
                'name': 'update_type',
                'msg': 'Type of update (yaml)',
                'type': str,
                'pos': True,
                'default': 'yaml'
            },
            {
                'name': 'container',
                'msg': 'Rebuild container image',
                'type': bool,
                'default': False
            },
            {
                'name': 'no_cache',
                'msg': 'Disable cache during container rebuild',
                'type': bool,
                'default': False
            }
        ])

        self.add_cmd('ppl conf', msg="Configure pipeline parameters")
        self.add_args([
            {
                'name': 'hostfile',
                'msg': 'Path to pipeline-specific hostfile',
                'type': str,
                'default': None,
            },
            {
                'name': 'container_build',
                'msg': 'Container build name (for building custom image)',
                'type': str,
                'default': None,
            },
            {
                'name': 'container_image',
                'msg': 'Pre-built container image to use',
                'type': str,
                'default': None,
            },
            {
                'name': 'container_engine',
                'msg': 'Container engine (docker/podman)',
                'type': str,
                'default': None,
            },
            {
                'name': 'container_base',
                'msg': 'Base container image',
                'type': str,
                'default': None,
            },
            {
                'name': 'container_ssh_port',
                'msg': 'SSH port for containers',
                'type': int,
                'default': None,
            }
        ])

        self.add_cmd('ppl list', msg="List all pipelines", aliases=['ppl ls'])
        self.add_args([])
        
        self.add_cmd('ppl print', msg="Print current pipeline configuration")
        self.add_args([])

        self.add_cmd('ppl path', msg="Print pipeline directory paths")
        self.add_args([
            {
                'name': 'shared',
                'msg': 'Print only shared directory',
                'type': bool,
                'default': False,
                'prefix': '+'
            },
            {
                'name': 'private',
                'msg': 'Print only private directory',
                'type': bool,
                'default': False,
                'prefix': '+'
            },
            {
                'name': 'config',
                'msg': 'Print only config directory',
                'type': bool,
                'default': False,
                'prefix': '+'
            }
        ])

        self.add_cmd('ppl rm', msg="Remove a package from current pipeline", aliases=['ppl remove'])
        self.add_args([
            {
                'name': 'package_spec',
                'msg': 'Package to remove (pkg_id or pipeline.pkg_id)',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('ppl destroy', msg="Destroy a pipeline")
        self.add_args([
            {
                'name': 'pipeline_name',
                'msg': 'Name of pipeline to destroy (optional, defaults to current)',
                'type': str,
                'pos': True
            }
        ])
        
        # Pipeline environment commands - need to add menu first
        self.add_menu('ppl env', msg="Pipeline environment management")
        
        self.add_cmd('ppl env build', msg="Build environment for current pipeline", keep_remainder=True)
        self.add_args([])
        
        self.add_cmd('ppl env copy', msg="Copy named environment to current pipeline")
        self.add_args([
            {
                'name': 'env_name',
                'msg': 'Name of environment to copy',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('ppl env show', msg="Show current pipeline environment")
        self.add_args([])
        
        # Pipeline index commands
        self.add_menu('ppl index', msg="Pipeline index management")
        
        self.add_cmd('ppl index load', msg="Load a pipeline script from an index")
        self.add_args([
            {
                'name': 'index_query',
                'msg': 'Index query (e.g., repo.subdir.script)',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('ppl index copy', msg="Copy a pipeline script from an index")
        self.add_args([
            {
                'name': 'index_query',
                'msg': 'Index query (e.g., repo.subdir.script)',
                'type': str,
                'required': True,
                'pos': True
            },
            {
                'name': 'output',
                'msg': 'Output directory or file (optional)',
                'type': str,
                'required': False,
                'pos': True
            }
        ])
        
        self.add_cmd('ppl index list', msg="List available pipeline scripts in indexes", aliases=['ppl index ls'])
        self.add_args([
            {
                'name': 'repo_name',
                'msg': 'Repository name to list (optional)',
                'type': str,
                'required': False,
                'pos': True
            }
        ])
        
        # Change directory (switch current pipeline)
        self.add_cmd('cd', msg="Change current pipeline")
        self.add_args([
            {
                'name': 'pipeline_name',
                'msg': 'Name of pipeline to switch to',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        # Repository commands
        self.add_menu('repo', msg="Repository management commands")
        
        self.add_cmd('repo add', msg="Add a repository to Jarvis")
        self.add_args([
            {
                'name': 'repo_path',
                'msg': 'Path to repository directory',
                'type': str,
                'required': True,
                'pos': True
            },
            {
                'name': 'force',
                'msg': 'Force overwrite if repository already exists',
                'type': bool,
                'default': False,
                'aliases': ['f']
            }
        ])
        
        self.add_cmd('repo remove', msg="Remove a repository from Jarvis", aliases=['repo rm'])
        self.add_args([
            {
                'name': 'repo_name',
                'msg': 'Name of repository to remove (not full path)',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('repo list', msg="List all registered repositories", aliases=['repo ls'])
        self.add_args([])
        
        self.add_cmd('repo create', msg="Create a new package in repository")
        self.add_args([
            {
                'name': 'package_name',
                'msg': 'Name of package to create',
                'type': str,
                'required': True,
                'pos': True,
                'class': 'pkg',
                'rank': 0
            },
            {
                'name': 'package_type',
                'msg': 'Type of package (service, app, interceptor)',
                'type': str,
                'required': True,
                'pos': True,
                'choices': ['service', 'app', 'interceptor'],
                'class': 'pkg',
                'rank': 1
            }
        ])

        # Container commands
        self.add_menu('container', msg="Container image management commands")

        self.add_cmd('container list', msg="List all container images", aliases=['container ls'])
        self.add_args([])

        self.add_cmd('container remove', msg="Remove a container image", aliases=['container rm'])
        self.add_args([
            {
                'name': 'container_name',
                'msg': 'Name of container to remove',
                'type': str,
                'required': True,
                'pos': True
            }
        ])

        self.add_cmd('container update', msg="Force rebuild a container image")
        self.add_args([
            {
                'name': 'container_name',
                'msg': 'Name of container to rebuild',
                'type': str,
                'required': True,
                'pos': True
            },
            {
                'name': 'no_cache',
                'msg': 'Disable cache during rebuild',
                'type': bool,
                'default': False
            },
            {
                'name': 'engine',
                'msg': 'Container engine to use (docker or podman)',
                'type': str,
                'default': None,
                'choices': ['docker', 'podman']
            }
        ])

        # Package commands
        self.add_menu('pkg', msg="Package management commands")
        
        self.add_cmd('pkg configure', msg="Configure a package", aliases=['pkg conf'], keep_remainder=True)
        self.add_args([
            {
                'name': 'package_spec',
                'msg': 'Package to configure (pkg or pipeline.pkg)',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('pkg readme', msg="Show package README")
        self.add_args([
            {
                'name': 'package_spec',
                'msg': 'Package to show README for (pkg or repo.pkg)',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('pkg path', msg="Show package directory paths")
        self.add_args([
            {
                'name': 'package_spec',
                'msg': 'Package to show paths for (pkg or repo.pkg)',
                'type': str,
                'required': True,
                'pos': True
            },
            {
                'name': 'shared',
                'msg': 'Print only shared directory',
                'type': bool,
                'default': False,
                'prefix': '+'
            },
            {
                'name': 'private',
                'msg': 'Print only private directory',
                'type': bool,
                'default': False,
                'prefix': '+'
            },
            {
                'name': 'config',
                'msg': 'Print only config directory',
                'type': bool,
                'default': False,
                'prefix': '+'
            }
        ])

        self.add_cmd('pkg help', msg="Show package configuration help")
        self.add_args([
            {
                'name': 'package_spec',
                'msg': 'Package to show help for (pkg or repo.pkg)',
                'type': str,
                'required': True,
                'pos': True
            }
        ])

        # Environment commands
        self.add_menu('env', msg="Named environment management")
        
        self.add_cmd('env build', msg="Build a named environment", keep_remainder=True)
        self.add_args([
            {
                'name': 'env_name',
                'msg': 'Name of environment to create',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('env list', msg="List all named environments", aliases=['env ls'])
        self.add_args([])
        
        self.add_cmd('env show', msg="Show a named environment")
        self.add_args([
            {
                'name': 'env_name',
                'msg': 'Name of environment to show',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        # Set hostfile command
        self.add_menu('hostfile', msg="Hostfile management")
        self.add_cmd('hostfile set', msg="Set the hostfile for deployments")
        self.add_args([
            {
                'name': 'hostfile_path',
                'msg': 'Path to hostfile',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        # Resource graph commands
        self.add_menu('rg', msg="Resource graph management")
        
        self.add_cmd('rg build', msg="Build resource graph from hostfile")
        self.add_args([
            {
                'name': 'no_benchmark',
                'msg': 'Skip performance benchmarking',
                'type': bool,
                'default': False
            },
            {
                'name': 'duration',
                'msg': 'Benchmark duration in seconds',
                'type': int,
                'default': 25
            }
        ])
        
        self.add_cmd('rg show', msg="Show resource graph summary")
        self.add_args([])
        
        self.add_cmd('rg nodes', msg="List nodes in resource graph")
        self.add_args([])
        
        self.add_cmd('rg node', msg="Show detailed node information")
        self.add_args([
            {
                'name': 'hostname',
                'msg': 'Hostname to show details for',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('rg filter', msg="Filter storage by device type")
        self.add_args([
            {
                'name': 'dev_type',
                'msg': 'Device type to filter by (ssd, hdd, etc.)',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('rg load', msg="Load resource graph from file")
        self.add_args([
            {
                'name': 'file_path',
                'msg': 'Path to resource graph file',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('rg path', msg="Show path to current resource graph file")
        self.add_args([])
        
        # Build commands
        self.add_menu('build', msg="Build environment profiles and configurations")
        
        self.add_cmd('build profile', msg="Build environment profile")
        self.add_args([
            {
                'name': 'm',
                'msg': 'Output method (dotenv, cmake, clion, vscode)',
                'type': str,
                'default': 'dotenv'
            },
            {
                'name': 'path',
                'msg': 'Output file path (prints to console if not specified)',
                'type': str,
                'required': False
            }
        ])
        
        # Module management commands
        self.add_menu('mod', msg="Module management commands")
        
        self.add_cmd('mod create', msg="Create a new module")
        self.add_args([
            {
                'name': 'mod_name',
                'msg': 'Module name',
                'type': str,
                'required': False,
                'pos': True
            }
        ])
        
        self.add_cmd('mod cd', msg="Set current module")
        self.add_args([
            {
                'name': 'mod_name',
                'msg': 'Module name',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('mod prepend', msg="Prepend environment variables to module", keep_remainder=True)
        self.add_args([
            {
                'name': 'mod_name',
                'msg': 'Module name (optional, uses current)',
                'type': str,
                'required': False,
                'pos': True
            }
        ])
        
        self.add_cmd('mod setenv', msg="Set environment variables in module", keep_remainder=True)
        self.add_args([
            {
                'name': 'mod_name',
                'msg': 'Module name (optional, uses current)',
                'type': str,
                'required': False,
                'pos': True
            }
        ])
        
        self.add_cmd('mod destroy', msg="Destroy a module")
        self.add_args([
            {
                'name': 'mod_name',
                'msg': 'Module name (optional, uses current)',
                'type': str,
                'required': False,
                'pos': True
            }
        ])

        self.add_cmd('mod clear', msg="Clear module directory except src/")
        self.add_args([
            {
                'name': 'mod_name',
                'msg': 'Module name (optional, uses current)',
                'type': str,
                'required': False,
                'pos': True
            }
        ])

        self.add_cmd('mod src', msg="Show module source directory")
        self.add_args([
            {
                'name': 'mod_name',
                'msg': 'Module name (optional, uses current)',
                'type': str,
                'required': False,
                'pos': True
            }
        ])
        
        self.add_cmd('mod root', msg="Show module root directory")
        self.add_args([
            {
                'name': 'mod_name',
                'msg': 'Module name (optional, uses current)',
                'type': str,
                'required': False,
                'pos': True
            }
        ])
        
        self.add_cmd('mod tcl', msg="Show module TCL file path")
        self.add_args([
            {
                'name': 'mod_name',
                'msg': 'Module name (optional, uses current)',
                'type': str,
                'required': False,
                'pos': True
            }
        ])
        
        self.add_cmd('mod yaml', msg="Show module YAML file path")
        self.add_args([
            {
                'name': 'mod_name',
                'msg': 'Module name (optional, uses current)',
                'type': str,
                'required': False,
                'pos': True
            }
        ])
        
        self.add_cmd('mod dir', msg="Show modules directory")
        self.add_args([])
        
        self.add_cmd('mod list', msg="List all modules")
        self.add_args([])
        
        self.add_cmd('mod profile', msg="Build environment profile", keep_remainder=True)
        self.add_args([])
        
        self.add_cmd('mod import', msg="Import module from command", keep_remainder=True)
        self.add_args([
            {
                'name': 'mod_name',
                'msg': 'Module name',
                'type': str,
                'required': True,
                'pos': True
            }
        ])
        
        self.add_cmd('mod update', msg="Update module using stored command")
        self.add_args([
            {
                'name': 'mod_name',
                'msg': 'Module name (optional, uses current)',
                'type': str,
                'required': False,
                'pos': True
            }
        ])
        
        self.add_cmd('mod build profile', msg="Build environment profile")
        self.add_args([
            {
                'name': 'm',
                'msg': 'Output method (dotenv, cmake, clion, vscode)',
                'type': str,
                'default': 'dotenv',
                'aliases': ['method']
            },
            {
                'name': 'path',
                'msg': 'Output file path (optional)',
                'type': str,
                'required': False
            }
        ])

        # Module dependency commands
        self.add_menu('mod dep', msg="Module dependency management")

        self.add_cmd('mod dep add', msg="Add a module dependency")
        self.add_args([
            {
                'name': 'dep_name',
                'msg': 'Dependency module name',
                'type': str,
                'required': True,
                'pos': True
            },
            {
                'name': 'mod_name',
                'msg': 'Module name (optional, uses current)',
                'type': str,
                'required': False,
                'pos': True
            }
        ])

        self.add_cmd('mod dep remove', msg="Remove a module dependency")
        self.add_args([
            {
                'name': 'dep_name',
                'msg': 'Dependency module name',
                'type': str,
                'required': True,
                'pos': True
            },
            {
                'name': 'mod_name',
                'msg': 'Module name (optional, uses current)',
                'type': str,
                'required': False,
                'pos': True
            }
        ])
        
    def _ensure_config_loaded(self):
        """Ensure Jarvis is loaded (doesn't require full initialization)"""
        if self.jarvis_config is None:
            self.jarvis_config = Jarvis.get_instance()

        # Initialize managers that don't require full Jarvis initialization
        if self.repo_manager is None:
            self.repo_manager = RepositoryManager(self.jarvis_config)

    def _ensure_initialized(self):
        """Ensure Jarvis is initialized before running commands"""
        if self.jarvis_config is None:
            self.jarvis_config = Jarvis.get_instance()

        if not self.jarvis_config.is_initialized():
            print("Error: Jarvis not initialized. Run 'jarvis init' first.")
            sys.exit(1)

        # Get Jarvis singleton instance (same as jarvis_config now)
        if self.jarvis is None:
            self.jarvis = self.jarvis_config
            
        # Initialize managers
        if self.repo_manager is None:
            self.repo_manager = RepositoryManager(self.jarvis_config)
        if self.env_manager is None:
            self.env_manager = EnvironmentManager(self.jarvis_config)
        if self.rg_manager is None:
            self.rg_manager = ResourceGraphManager()
        if self.pipeline_index_manager is None:
            self.pipeline_index_manager = PipelineIndexManager(self.jarvis_config)
        if self.module_manager is None:
            from jarvis_cd.core.module_manager import ModuleManager
            self.module_manager = ModuleManager(self.jarvis_config)
        
        # Load current pipeline if one exists
        current_pipeline_name = self.jarvis_config.get_current_pipeline()
        if current_pipeline_name:
            try:
                self.current_pipeline = Pipeline(current_pipeline_name)
            except Exception:
                # Pipeline may not exist or be corrupted, continue without it
                self.current_pipeline = None
    
    def main_menu(self):
        """Handle main menu / help"""
        if self.kwargs.get('help', False) or not self.remainder:
            self._show_help()
        else:
            print(f"Unknown arguments: {' '.join(self.remainder)}")
            self._show_help()
            
    def _show_help(self):
        """Show help information"""
        print("Jarvis-CD: Unified platform for deploying applications and benchmarks")
        print()
        self.print_general_help()
    
    # Command handlers
    def init(self):
        """Initialize Jarvis configuration"""
        config_dir = os.path.expanduser(self.kwargs['config_dir'])
        private_dir = os.path.expanduser(self.kwargs['private_dir'])
        shared_dir = os.path.expanduser(self.kwargs['shared_dir'])
        force = self.kwargs.get('force', False)

        jarvis = Jarvis.get_instance()
        jarvis.initialize(config_dir, private_dir, shared_dir, force=force)

        # Save jarvis instance to self so subsequent commands use the same instance
        self.jarvis_config = jarvis
        self.jarvis = jarvis

        print(f"Jarvis initialized successfully!")
        print(f"Config dir: {config_dir}")
        print(f"Private dir: {private_dir}")
        print(f"Shared dir: {shared_dir}")
        
    def ppl_create(self):
        """Create a new pipeline"""
        self._ensure_initialized()
        pipeline_name = self.kwargs['pipeline_name']
        
        # Create new pipeline
        pipeline = Pipeline()
        pipeline.create(pipeline_name)
        self.current_pipeline = pipeline
        
    def ppl_append(self):
        """Append package to current pipeline"""
        self._ensure_initialized()
        package_spec = self.kwargs['package_spec']
        package_alias = self.kwargs.get('package_alias')
        
        if not self.current_pipeline:
            # Try to load current pipeline
            current_name = self.jarvis_config.get_current_pipeline()
            if current_name:
                self.current_pipeline = Pipeline(current_name)
            else:
                raise ValueError("No current pipeline. Create one with 'jarvis ppl create <name>'")
        
        self.current_pipeline.append(package_spec, package_alias)
        
    def ppl_run(self):
        """Run pipeline"""
        self._ensure_initialized()
        load_type = self.kwargs.get('load_type', 'current')
        pipeline_file = self.kwargs.get('pipeline_file')
        
        if load_type == 'yaml':
            if not pipeline_file:
                raise ValueError("Pipeline file is required when load_type is 'yaml'")
            # Load and run pipeline file in one command
            pipeline = Pipeline()
            pipeline.run(load_type, pipeline_file)
            self.current_pipeline = pipeline
        else:
            # Run current pipeline
            if not self.current_pipeline:
                current_name = self.jarvis_config.get_current_pipeline()
                if current_name:
                    self.current_pipeline = Pipeline(current_name)
                else:
                    raise ValueError("No current pipeline to run")
            
            self.current_pipeline.run()
        
    def ppl_start(self):
        """Start current pipeline"""
        self._ensure_initialized()
        if not self.current_pipeline:
            current_name = self.jarvis_config.get_current_pipeline()
            if current_name:
                self.current_pipeline = Pipeline(current_name)
            else:
                raise ValueError("No current pipeline to start")
        
        self.current_pipeline.start()
        
    def ppl_stop(self):
        """Stop current pipeline"""
        self._ensure_initialized()
        if not self.current_pipeline:
            current_name = self.jarvis_config.get_current_pipeline()
            if current_name:
                self.current_pipeline = Pipeline(current_name)
            else:
                raise ValueError("No current pipeline to stop")
        
        self.current_pipeline.stop()
        
    def ppl_kill(self):
        """Kill current pipeline"""
        self._ensure_initialized()
        if not self.current_pipeline:
            current_name = self.jarvis_config.get_current_pipeline()
            if current_name:
                self.current_pipeline = Pipeline(current_name)
            else:
                raise ValueError("No current pipeline to kill")
        
        self.current_pipeline.kill()
        
    def ppl_clean(self):
        """Clean current pipeline"""
        self._ensure_initialized()
        if not self.current_pipeline:
            current_name = self.jarvis_config.get_current_pipeline()
            if current_name:
                self.current_pipeline = Pipeline(current_name)
            else:
                raise ValueError("No current pipeline to clean")
        
        self.current_pipeline.clean()
        
    def ppl_status(self):
        """Show pipeline status"""
        self._ensure_initialized()
        if not self.current_pipeline:
            current_name = self.jarvis_config.get_current_pipeline()
            if current_name:
                self.current_pipeline = Pipeline(current_name)
            else:
                print("No current pipeline")
                return
        
        status = self.current_pipeline.status()
        print(status)
        
    def ppl_load(self):
        """Load pipeline from file"""
        self._ensure_initialized()
        load_type = self.kwargs['load_type']
        pipeline_file = self.kwargs['pipeline_file']

        pipeline = Pipeline()
        pipeline.load(load_type, pipeline_file)
        pipeline.build_container_if_needed()
        pipeline.configure_all_packages()

        self.current_pipeline = pipeline
        
    def ppl_update(self):
        """Update pipeline from last loaded file"""
        self._ensure_initialized()

        if not self.current_pipeline:
            current_name = self.jarvis_config.get_current_pipeline()
            if current_name:
                self.current_pipeline = Pipeline(current_name)
            else:
                raise ValueError("No current pipeline to update")

        # Use Pipeline.update() method with container rebuild flags
        rebuild_container = self.kwargs.get('container', False)
        no_cache = self.kwargs.get('no_cache', False)
        self.current_pipeline.update(rebuild_container=rebuild_container, no_cache=no_cache)

    def ppl_conf(self):
        """Configure pipeline parameters"""
        self._ensure_initialized()

        if not self.current_pipeline:
            current_name = self.jarvis_config.get_current_pipeline()
            if current_name:
                self.current_pipeline = Pipeline(current_name)
            else:
                raise ValueError("No current pipeline to configure")

        # Check if any parameters were provided
        params_provided = False
        needs_rebuild = False

        # Update hostfile
        if self.kwargs.get('hostfile') is not None:
            from jarvis_cd.util.hostfile import Hostfile
            hostfile_path = self.kwargs['hostfile']
            self.current_pipeline.hostfile = Hostfile(path=hostfile_path)
            print(f"Set pipeline hostfile: {hostfile_path}")
            params_provided = True
            needs_rebuild = True

        # Update container_build
        if self.kwargs.get('container_build') is not None:
            self.current_pipeline.container_build = self.kwargs['container_build']
            print(f"Set container_build: {self.kwargs['container_build']}")
            params_provided = True
            needs_rebuild = True

        # Update container_image
        if self.kwargs.get('container_image') is not None:
            self.current_pipeline.container_image = self.kwargs['container_image']
            print(f"Set container_image: {self.kwargs['container_image']}")
            params_provided = True
            needs_rebuild = False  # Using pre-built image, no rebuild needed

        # Update container_engine
        if self.kwargs.get('container_engine') is not None:
            self.current_pipeline.container_engine = self.kwargs['container_engine']
            print(f"Set container_engine: {self.kwargs['container_engine']}")
            params_provided = True

        # Update container_base
        if self.kwargs.get('container_base') is not None:
            self.current_pipeline.container_base = self.kwargs['container_base']
            print(f"Set container_base: {self.kwargs['container_base']}")
            params_provided = True
            needs_rebuild = True

        # Update container_ssh_port
        if self.kwargs.get('container_ssh_port') is not None:
            self.current_pipeline.container_ssh_port = self.kwargs['container_ssh_port']
            print(f"Set container_ssh_port: {self.kwargs['container_ssh_port']}")
            params_provided = True

        if not params_provided:
            print("No parameters provided. Use -h to see available options.")
            return

        # Save pipeline
        self.current_pipeline.save()

        # Rebuild container if needed (only if container_build is set)
        if needs_rebuild and self.current_pipeline.container_build:
            print("\nRebuilding container with updated configuration...")
            self.current_pipeline.update(rebuild_container=True, no_cache=False)

    def ppl_list(self):
        """List all pipelines"""
        self._ensure_initialized()
        
        pipelines_dir = self.jarvis_config.get_pipelines_dir()
        
        if not pipelines_dir.exists():
            print("No pipelines directory found. Create a pipeline first with 'jarvis ppl create'.")
            return
            
        pipeline_dirs = [d for d in pipelines_dir.iterdir() if d.is_dir()]
        
        if not pipeline_dirs:
            print("No pipelines found. Create a pipeline first with 'jarvis ppl create'.")
            return
            
        current_pipeline_name = self.jarvis_config.get_current_pipeline()
        
        print("Available pipelines:")
        for pipeline_dir in sorted(pipeline_dirs):
            pipeline_name = pipeline_dir.name
            config_file = pipeline_dir / 'pipeline.yaml'
            
            if config_file.exists():
                try:
                    import yaml
                    with open(config_file, 'r') as f:
                        pipeline_config = yaml.safe_load(f) or {}
                    
                    num_packages = len(pipeline_config.get('packages', []))
                    marker = "* " if pipeline_name == current_pipeline_name else "  "
                    print(f"{marker}{pipeline_name} ({num_packages} packages)")
                    
                except Exception as e:
                    marker = "* " if pipeline_name == current_pipeline_name else "  "
                    print(f"{marker}{pipeline_name} (error reading config: {e})")
            else:
                marker = "* " if pipeline_name == current_pipeline_name else "  "
                print(f"{marker}{pipeline_name} (no config file)")
                
        if current_pipeline_name:
            print(f"\nCurrent pipeline: {current_pipeline_name}")
        else:
            print("\nNo current pipeline set. Use 'jarvis cd <pipeline>' to switch.")
        
    def ppl_print(self):
        """Print current pipeline configuration"""
        self._ensure_initialized()
        
        current_pipeline_name = self.jarvis_config.get_current_pipeline()
        
        if not current_pipeline_name:
            print("No current pipeline set. Use 'jarvis cd <pipeline>' to switch.")
            return
            
        if not self.current_pipeline:
            try:
                self.current_pipeline = Pipeline(current_pipeline_name)
            except Exception as e:
                print(f"Error loading current pipeline: {e}")
                return
        
        print(f"Pipeline: {self.current_pipeline.name}")
        print(f"Directory: {self.jarvis_config.get_pipeline_dir(current_pipeline_name)}")

        # Show hostfile configuration
        if hasattr(self.current_pipeline, 'hostfile') and self.current_pipeline.hostfile:
            print(f"Hostfile: {self.current_pipeline.hostfile.path or '(in-memory)'}")
            print(f"  Hosts: {', '.join(self.current_pipeline.hostfile.hosts)}")
        else:
            # Show that it falls back to jarvis global hostfile
            jarvis_hostfile = self.jarvis.hostfile
            print(f"Hostfile: (using global jarvis hostfile)")
            print(f"  Hosts: {', '.join(jarvis_hostfile.hosts)}")

        # Show container configuration if set
        if hasattr(self.current_pipeline, 'is_containerized') and self.current_pipeline.is_containerized():
            print(f"Container Configuration:")
            if self.current_pipeline.container_build:
                print(f"  Build Name: {self.current_pipeline.container_build}")
                print(f"  Base: {self.current_pipeline.container_base}")
            if self.current_pipeline.container_image:
                print(f"  Image: {self.current_pipeline.container_image}")
            print(f"  Engine: {self.current_pipeline.container_engine}")
            print(f"  SSH Port: {self.current_pipeline.container_ssh_port}")

        if self.current_pipeline.packages:
            print("Packages:")
            for pkg_def in self.current_pipeline.packages:
                pkg_id = pkg_def.get('pkg_id', 'unknown')
                pkg_type = pkg_def.get('pkg_type', 'unknown')
                global_id = pkg_def.get('global_id', pkg_id)
                config = pkg_def.get('config', {})
                
                print(f"  {pkg_id}:")
                print(f"    Type: {pkg_type}")
                print(f"    Global ID: {global_id}")
                
                if config:
                    print("    Configuration:")
                    for key, value in config.items():
                        print(f"      {key}: {value}")
                else:
                    print("    Configuration: None")
        else:
            print("No packages in pipeline")
        
        # Print interceptors if they exist
        if hasattr(self.current_pipeline, 'interceptors') and self.current_pipeline.interceptors:
            print("Interceptors:")
            for interceptor_name, interceptor_def in self.current_pipeline.interceptors.items():
                interceptor_type = interceptor_def.get('pkg_type', 'unknown')
                global_id = interceptor_def.get('global_id', interceptor_name)
                config = interceptor_def.get('config', {})
                
                print(f"  {interceptor_name}:")
                print(f"    Type: {interceptor_type}")
                print(f"    Global ID: {global_id}")
                
                if config:
                    print("    Configuration:")
                    for key, value in config.items():
                        print(f"      {key}: {value}")
                else:
                    print("    Configuration: None")
        else:
            print("No interceptors in pipeline")

        if hasattr(self.current_pipeline, 'last_loaded_file') and self.current_pipeline.last_loaded_file:
            print(f"Last loaded from: {self.current_pipeline.last_loaded_file}")

    def ppl_path(self):
        """Print pipeline directory paths"""
        self._ensure_initialized()

        current_pipeline_name = self.jarvis_config.get_current_pipeline()

        if not current_pipeline_name:
            print("No current pipeline set. Use 'jarvis cd <pipeline>' to switch.", file=sys.stderr)
            sys.exit(1)

        # Get directories
        config_dir = self.jarvis_config.get_pipeline_dir(current_pipeline_name)
        shared_dir = self.jarvis_config.get_pipeline_shared_dir(current_pipeline_name)
        private_dir = self.jarvis_config.get_pipeline_private_dir(current_pipeline_name)

        # Check which flags are set
        show_shared = self.kwargs.get('shared', False)
        show_private = self.kwargs.get('private', False)
        show_config = self.kwargs.get('config', False)

        # If no flags set, show all directories
        if not (show_shared or show_private or show_config):
            print(f"config:  {config_dir}")
            print(f"shared:  {shared_dir}")
            print(f"private: {private_dir}")
        else:
            # Show only requested directory (single line, no label)
            if show_config:
                print(config_dir)
            elif show_shared:
                print(shared_dir)
            elif show_private:
                print(private_dir)

    def ppl_rm(self):
        """Remove package from current pipeline"""
        self._ensure_initialized()
        package_spec = self.kwargs['package_spec']
        
        if not self.current_pipeline:
            current_name = self.jarvis_config.get_current_pipeline()
            if current_name:
                self.current_pipeline = Pipeline(current_name)
            else:
                raise ValueError("No current pipeline")
        
        self.current_pipeline.rm(package_spec)
        
    def ppl_destroy(self):
        """Destroy a pipeline"""
        self._ensure_initialized()
        pipeline_name = self.kwargs.get('pipeline_name')
        
        if pipeline_name:
            # Destroy specific pipeline
            pipeline = Pipeline()
            pipeline.destroy(pipeline_name)
        else:
            # Destroy current pipeline
            if not self.current_pipeline:
                current_name = self.jarvis_config.get_current_pipeline()
                if current_name:
                    self.current_pipeline = Pipeline(current_name)
                else:
                    print("No current pipeline to destroy. Specify a pipeline name.")
                    return
            
            self.current_pipeline.destroy()
            self.current_pipeline = None
        
    def cd(self):
        """Change current pipeline"""
        self._ensure_initialized()
        pipeline_name = self.kwargs['pipeline_name']
        
        pipeline_dir = self.jarvis_config.get_pipeline_dir(pipeline_name)
        
        if not pipeline_dir.exists():
            print(f"Pipeline '{pipeline_name}' not found.")
            self.ppl_list()
            return
            
        config_file = pipeline_dir / 'pipeline.yaml'
        if not config_file.exists():
            print(f"Pipeline '{pipeline_name}' exists but has no configuration file.")
            print("You may need to recreate this pipeline.")
            return
            
        # Set current pipeline in configuration
        self.jarvis_config.set_current_pipeline(pipeline_name)
        
        # Load the new current pipeline
        self.current_pipeline = Pipeline(pipeline_name)
        
        print(f"Switched to pipeline: {pipeline_name}")
        
        # Show basic info about the pipeline
        try:
            num_packages = len(self.current_pipeline.packages)
            print(f"Pipeline has {num_packages} packages")
        except Exception as e:
            print(f"Warning: Could not read pipeline configuration: {e}")
        
    def repo_add(self):
        """Add repository"""
        self._ensure_config_loaded()
        repo_path = self.kwargs['repo_path']
        force = self.kwargs.get('force', False)
        self.repo_manager.add_repository(repo_path, force=force)

    def repo_remove(self):
        """Remove repository by name"""
        self._ensure_config_loaded()
        repo_name = self.kwargs['repo_name']
        self.repo_manager.remove_repository_by_name(repo_name)

    def repo_list(self):
        """List repositories"""
        self._ensure_config_loaded()
        self.repo_manager.list_repositories()

    def repo_create(self):
        """Create new package in repository"""
        self._ensure_config_loaded()
        package_name = self.kwargs['package_name']
        package_type = self.kwargs['package_type']
        self.repo_manager.create_package(package_name, package_type)

    def container_list(self):
        """List all container images"""
        self._ensure_initialized()
        from jarvis_cd.core.container import ContainerManager
        container_manager = ContainerManager()
        container_manager.list_containers()

    def container_remove(self):
        """Remove a container image"""
        self._ensure_initialized()
        container_name = self.kwargs['container_name']
        from jarvis_cd.core.container import ContainerManager
        container_manager = ContainerManager()
        container_manager.remove_container(container_name)

    def container_update(self):
        """Force rebuild a container image"""
        self._ensure_initialized()
        container_name = self.kwargs['container_name']
        no_cache = self.kwargs.get('no_cache', False)
        engine = self.kwargs.get('engine')
        from pathlib import Path

        containers_dir = Path.home() / '.ppi-jarvis' / 'containers'
        dockerfile_path = containers_dir / f'{container_name}.Dockerfile'

        if not dockerfile_path.exists():
            print(f"Error: Container '{container_name}' not found")
            print(f"Expected Dockerfile at: {dockerfile_path}")
            sys.exit(1)

        # Determine container engine
        from jarvis_cd.shell import Exec, LocalExecInfo
        import shutil

        if engine:
            # Use specified engine
            use_engine = engine
        elif shutil.which('podman'):
            use_engine = 'podman'
        else:
            use_engine = 'docker'

        # Build command with optional --no-cache flag
        no_cache_flag = " --no-cache" if no_cache else ""
        build_cmd = f"{use_engine} build{no_cache_flag} -t {container_name} -f {dockerfile_path} {containers_dir}"

        cache_msg = " (no cache)" if no_cache else " (with cache)"
        print(f"Rebuilding container image: {container_name}{cache_msg} using {use_engine}")
        Exec(build_cmd, LocalExecInfo()).run()
        print(f"Container image rebuilt: {container_name}")

    def pkg_configure(self):
        """Configure package"""
        self._ensure_initialized()
        package_spec = self.kwargs['package_spec']

        # Parse package specification
        if '.' in package_spec:
            # pipeline.pkg format
            pipeline_name, pkg_id = package_spec.split('.', 1)
            pipeline = Pipeline(pipeline_name)
            pipeline.configure_package(pkg_id, self.remainder)
        else:
            # Just package name - assume current pipeline
            if not self.current_pipeline:
                current_name = self.jarvis_config.get_current_pipeline()
                if current_name:
                    self.current_pipeline = Pipeline(current_name)
                else:
                    raise ValueError("No current pipeline. Specify as pipeline.pkg or create a pipeline first.")

            self.current_pipeline.configure_package(package_spec, self.remainder)
        
    def pkg_readme(self):
        """Show package README"""
        self._ensure_initialized()
        package_spec = self.kwargs['package_spec']
        
        # Parse package specification
        if '.' in package_spec:
            # Check if it's a pipeline.pkg or repo.pkg format
            parts = package_spec.split('.')
            if len(parts) == 2:
                # Could be either pipeline.pkg or repo.pkg
                # Try to determine based on whether it's an existing pipeline
                potential_pipeline = parts[0]
                pipeline_dir = self.jarvis_config.get_pipeline_dir(potential_pipeline)
                
                if pipeline_dir.exists():
                    # It's a pipeline.pkg format
                    pipeline_name, pkg_id = parts
                    pipeline = Pipeline(pipeline_name)
                    pipeline.show_package_readme(pkg_id)
                else:
                    # It's a repo.pkg format - load standalone
                    from jarvis_cd.core.pkg import Pkg
                    pkg_instance = Pkg.load_standalone(package_spec)
                    pkg_instance.show_readme()
            else:
                # It's a repo.pkg format - load standalone
                from jarvis_cd.core.pkg import Pkg
                pkg_instance = Pkg.load_standalone(package_spec)
                pkg_instance.show_readme()
        else:
            # Just package name - could be in current pipeline or standalone
            if self.current_pipeline or self.jarvis_config.get_current_pipeline():
                # Try pipeline first
                if not self.current_pipeline:
                    current_name = self.jarvis_config.get_current_pipeline()
                    self.current_pipeline = Pipeline(current_name)
                
                try:
                    self.current_pipeline.show_package_readme(package_spec)
                except ValueError:
                    # Package not in pipeline, try standalone
                    from jarvis_cd.core.pkg import Pkg
                    pkg_instance = Pkg.load_standalone(package_spec)
                    pkg_instance.show_readme()
            else:
                # No pipeline, load standalone
                from jarvis_cd.core.pkg import Pkg
                pkg_instance = Pkg.load_standalone(package_spec)
                pkg_instance.show_readme()
        
    def pkg_path(self):
        """Show package directory paths"""
        self._ensure_initialized()
        package_spec = self.kwargs['package_spec']

        # Get the requested paths
        path_flags = {
            'shared': self.kwargs.get('shared', False),
            'private': self.kwargs.get('private', False),
            'config': self.kwargs.get('config', False)
        }

        # Parse package specification
        if '.' in package_spec:
            # Check if it's a pipeline.pkg or repo.pkg format
            parts = package_spec.split('.')
            if len(parts) == 2:
                # Could be either pipeline.pkg or repo.pkg
                # Try to determine based on whether it's an existing pipeline
                potential_pipeline = parts[0]
                pipeline_dir = self.jarvis_config.get_pipeline_dir(potential_pipeline)

                if pipeline_dir.exists():
                    # It's a pipeline.pkg format
                    pipeline_name, pkg_id = parts
                    pipeline = Pipeline(pipeline_name)
                    pipeline.show_package_paths(pkg_id, path_flags)
                else:
                    # It's a repo.pkg format - load standalone
                    from jarvis_cd.core.pkg import Pkg
                    pkg_instance = Pkg.load_standalone(package_spec)
                    pkg_instance.show_paths(path_flags)
            else:
                # It's a repo.pkg format - load standalone
                from jarvis_cd.core.pkg import Pkg
                pkg_instance = Pkg.load_standalone(package_spec)
                pkg_instance.show_paths(path_flags)
        else:
            # Just package name - could be in current pipeline or standalone
            if self.current_pipeline or self.jarvis_config.get_current_pipeline():
                # Try pipeline first
                if not self.current_pipeline:
                    current_name = self.jarvis_config.get_current_pipeline()
                    self.current_pipeline = Pipeline(current_name)

                try:
                    self.current_pipeline.show_package_paths(package_spec, path_flags)
                except ValueError:
                    # Package not in pipeline, try standalone
                    from jarvis_cd.core.pkg import Pkg
                    pkg_instance = Pkg.load_standalone(package_spec)
                    pkg_instance.show_paths(path_flags)
            else:
                # No pipeline, load standalone
                from jarvis_cd.core.pkg import Pkg
                pkg_instance = Pkg.load_standalone(package_spec)
                pkg_instance.show_paths(path_flags)

    def pkg_help(self):
        """Show package configuration help"""
        self._ensure_initialized()
        package_spec = self.kwargs['package_spec']

        # Load the package standalone (repo.pkg format like builtin.ior)
        from jarvis_cd.core.pkg import Pkg
        pkg_instance = Pkg.load_standalone(package_spec)

        # Get the argparse instance and print help
        argparse = pkg_instance.get_argparse()
        argparse.print_help()

    def ppl_env_build(self):
        """Build environment for current pipeline and reconfigure packages"""
        self._ensure_initialized()
        self.env_manager.build_pipeline_environment(self.remainder)

        # Reconfigure packages with new environment
        if not self.current_pipeline:
            current_name = self.jarvis_config.get_current_pipeline()
            if current_name:
                self.current_pipeline = Pipeline(current_name)

        if self.current_pipeline:
            # Reload environment from env.yaml (don't reload full pipeline to avoid inline dict error)
            from pathlib import Path
            import yaml
            pipeline_dir = self.jarvis_config.get_pipeline_dir(self.current_pipeline.name)
            env_file = pipeline_dir / 'env.yaml'
            if env_file.exists():
                with open(env_file, 'r') as f:
                    self.current_pipeline.env = yaml.safe_load(f)

            # Reconfigure all packages with the new environment
            self.current_pipeline.build_container_if_needed()
            self.current_pipeline.configure_all_packages()
            print("Pipeline reconfigured with new environment")
        
    def ppl_env_copy(self):
        """Copy named environment to current pipeline"""
        self._ensure_initialized()
        env_name = self.kwargs['env_name']
        self.env_manager.copy_named_environment(env_name)
        
    def ppl_env_show(self):
        """Show current pipeline environment"""
        self._ensure_initialized()
        self.env_manager.show_pipeline_environment()
        
    def env_build(self):
        """Build a named environment"""
        self._ensure_initialized()
        env_name = self.kwargs['env_name']
        self.env_manager.build_named_environment(env_name, self.remainder)
        
    def env_list(self):
        """List all named environments"""
        self._ensure_initialized()
        envs = self.env_manager.list_named_environments()
        if envs:
            print("Available named environments:")
            for env_name in sorted(envs):
                print(f"  {env_name}")
        else:
            print("No named environments found. Create one with 'jarvis env build <name>'")
            
    def env_show(self):
        """Show a named environment"""
        self._ensure_initialized()
        env_name = self.kwargs['env_name']
        self.env_manager.show_named_environment(env_name)
        
    def hostfile_set(self):
        """Set hostfile"""
        self._ensure_initialized()
        hostfile_path = self.kwargs['hostfile_path']
        self.jarvis_config.set_hostfile(hostfile_path)
        
    def rg_build(self):
        """Build resource graph"""
        self._ensure_initialized()
        benchmark = not self.kwargs.get('no_benchmark', False)
        duration = self.kwargs.get('duration', 25)
        self.rg_manager.build(benchmark=benchmark, duration=duration)
        
    def build_profile(self):
        """Build environment profile"""
        self._ensure_initialized()
        method = self.kwargs.get('m', 'dotenv')
        path = self.kwargs.get('path')
        self.module_manager.build_profile(path, method)
        
    def rg_show(self):
        """Show resource graph summary"""
        self._ensure_initialized()
        self.rg_manager.show()
        
    def rg_nodes(self):
        """List nodes in resource graph"""
        self._ensure_initialized()
        self.rg_manager.list_nodes()
        
    def rg_node(self):
        """Show detailed node information"""
        self._ensure_initialized()
        hostname = self.kwargs['hostname']
        self.rg_manager.show_node_details(hostname)
        
    def rg_filter(self):
        """Filter storage by device type"""
        self._ensure_initialized()
        dev_type = self.kwargs['dev_type']
        self.rg_manager.filter_by_type(dev_type)
        
    def rg_load(self):
        """Load resource graph from file"""
        self._ensure_initialized()
        file_path = Path(self.kwargs['file_path'])
        self.rg_manager.load(file_path)
        
    def rg_path(self):
        """Show path to current resource graph file"""
        self._ensure_initialized()
        self.rg_manager.show_path()
        
    # Module management commands
    def mod_create(self):
        """Create a new module"""
        self._ensure_initialized()
        mod_name = self.kwargs.get('mod_name')
        if not mod_name:
            # Generate a unique module name or prompt user
            import time
            mod_name = f"module_{int(time.time())}"
            print(f"No module name provided, using: {mod_name}")
        self.module_manager.create_module(mod_name)
        
    def mod_cd(self):
        """Set current module"""
        self._ensure_initialized()
        mod_name = self.kwargs['mod_name']
        self.module_manager.set_current_module(mod_name)
        
    def mod_prepend(self):
        """Prepend environment variables to module"""
        self._ensure_initialized()
        mod_name = self.kwargs.get('mod_name')
        self.module_manager.prepend_env_vars(mod_name, self.remainder)
        
    def mod_setenv(self):
        """Set environment variables in module"""
        self._ensure_initialized()
        mod_name = self.kwargs.get('mod_name')
        self.module_manager.set_env_vars(mod_name, self.remainder)
        
    def mod_destroy(self):
        """Destroy a module"""
        self._ensure_initialized()
        mod_name = self.kwargs.get('mod_name')
        self.module_manager.destroy_module(mod_name)

    def mod_clear(self):
        """Clear module directory except src/"""
        self._ensure_initialized()
        mod_name = self.kwargs.get('mod_name')
        self.module_manager.clear_module(mod_name)

    def mod_src(self):
        """Show module source directory"""
        self._ensure_initialized()
        mod_name = self.kwargs.get('mod_name')
        print(self.module_manager.get_module_src_dir(mod_name))
        
    def mod_root(self):
        """Show module root directory"""
        self._ensure_initialized()
        mod_name = self.kwargs.get('mod_name')
        print(self.module_manager.get_module_root_dir(mod_name))
        
    def mod_tcl(self):
        """Show module TCL file path"""
        self._ensure_initialized()
        mod_name = self.kwargs.get('mod_name')
        print(self.module_manager.get_module_tcl_path(mod_name))
        
    def mod_yaml(self):
        """Show module YAML file path"""
        self._ensure_initialized()
        mod_name = self.kwargs.get('mod_name')
        print(self.module_manager.get_module_yaml_path(mod_name))
        
    def mod_dir(self):
        """Show modules directory"""
        self._ensure_initialized()
        print(self.module_manager.modules_dir)
        
    def mod_list(self):
        """List all modules"""
        self._ensure_initialized()
        self.module_manager.list_modules()
        
    def mod_profile(self):
        """Build environment profile"""
        self._ensure_initialized()
        # Parse remainder arguments for m= and path= format
        method = 'dotenv'  # default
        path = None
        
        if hasattr(self, 'remainder') and self.remainder:
            for arg in self.remainder:
                if arg.startswith('m='):
                    method = arg[2:]
                elif arg.startswith('path='):
                    path = arg[5:]
        
        self.module_manager.build_profile_new(path, method)
        
    def mod_import(self):
        """Import module from command"""
        self._ensure_initialized()
        mod_name = self.kwargs.get('mod_name')
        
        if not hasattr(self, 'remainder') or not self.remainder:
            raise ValueError("No command provided. Usage: jarvis mod import <mod_name> <command>")
        
        # Join remainder as the command to execute
        command = ' '.join(self.remainder)
        self.module_manager.import_module(mod_name, command)
        
    def mod_update(self):
        """Update module using stored command"""
        self._ensure_initialized()
        mod_name = self.kwargs.get('mod_name')
        self.module_manager.update_module(mod_name)
        
    def mod_build_profile(self):
        """Build environment profile"""
        self._ensure_initialized()
        method = self.kwargs.get('m', 'dotenv')
        path = self.kwargs.get('path')
        self.module_manager.build_profile(path, method)

    def mod_dep_add(self):
        """Add a module dependency"""
        self._ensure_initialized()
        dep_name = self.kwargs['dep_name']
        mod_name = self.kwargs.get('mod_name')
        self.module_manager.add_dependency(mod_name, dep_name)

    def mod_dep_remove(self):
        """Remove a module dependency"""
        self._ensure_initialized()
        dep_name = self.kwargs['dep_name']
        mod_name = self.kwargs.get('mod_name')
        self.module_manager.remove_dependency(mod_name, dep_name)

    def ppl_index_load(self):
        """Load a pipeline script from an index"""
        self._ensure_initialized()
        index_query = self.kwargs['index_query']
        self.pipeline_index_manager.load_pipeline_from_index(index_query)
        
    def ppl_index_copy(self):
        """Copy a pipeline script from an index"""
        self._ensure_initialized()
        index_query = self.kwargs['index_query']
        output = self.kwargs.get('output')
        self.pipeline_index_manager.copy_pipeline_from_index(index_query, output)
        
    def ppl_index_list(self):
        """List available pipeline scripts in indexes"""
        from jarvis_cd.util.logger import logger, Color
        
        self._ensure_initialized()
        repo_name = self.kwargs.get('repo_name')
        available_scripts = self.pipeline_index_manager.list_available_scripts(repo_name)
        
        if not available_scripts:
            print("No pipeline indexes found in any repositories.")
            return
            
        if repo_name:
            print(f"Available pipeline scripts in {repo_name}:")
        else:
            print("Available pipeline scripts:")
            
        for repo, entries in available_scripts.items():
            if repo_name and repo != repo_name:
                continue
            if not repo_name:
                print(f"  {repo}:")
            for entry in entries:
                indent = "  " if repo_name else "    "
                if entry['type'] == 'file':
                    # Print files in default color
                    print(f"{indent}{entry['name']}")
                elif entry['type'] == 'directory':
                    # Print directories in cyan color with (directory) label
                    logger.print(Color.CYAN, f"{indent}{entry['name']} (directory)")


def main():
    """Main entry point for jarvis CLI"""
    try:
        cli = JarvisCLI()
        cli.define_options()
        result = cli.parse(sys.argv[1:])
    except KeyboardInterrupt:
        print("\nOperation cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()
```

### `jarvis_cd/core/config.py`

```python
import os
import yaml
from pathlib import Path
from typing import Dict, Any, Optional, List
from jarvis_cd.util.hostfile import Hostfile


def load_class(import_str: str, path: str, class_name: str):
    """
    Loads a class from a python file.

    :param import_str: A python import string. E.g., for "myrepo.dir1.pkg"
    :param path: The absolute path to the directory which contains the
    beginning of the import statement.
    :param class_name: The name of the class in the file
    :return: The class data type
    """
    import sys
    
    fullpath = os.path.join(path, import_str.replace('.', '/') + '.py')
    
    # If the exact path doesn't exist, try replacing the last component
    if not os.path.exists(fullpath):
        # Handle legacy naming: if looking for "package.py", try "pkg.py"
        if import_str.endswith('.package'):
            legacy_import_str = import_str[:-8] + '.pkg'  # Replace .package with .pkg
            fullpath = os.path.join(path, legacy_import_str.replace('.', '/') + '.py')
            if os.path.exists(fullpath):
                import_str = legacy_import_str
            else:
                return None
        else:
            return None
    
    sys.path.insert(0, path)
    try:
        module = __import__(import_str, fromlist=[class_name])
        cls = getattr(module, class_name, None)
        if cls is None:
            raise AttributeError(f"Class '{class_name}' not found in module '{import_str}'")
        return cls
    except ImportError as e:
        # Re-raise ImportError with more context instead of silently returning None
        raise ImportError(f"Failed to import module '{import_str}' from path '{path}': {e}") from e
    except AttributeError as e:
        # Re-raise AttributeError with more context instead of silently returning None
        raise AttributeError(f"Failed to get class '{class_name}' from module '{import_str}': {e}") from e
    finally:
        sys.path.pop(0)


class Jarvis:
    """
    Singleton class that manages Jarvis configuration and provides global access.
    Combines configuration management with singleton pattern.
    """
    _instance = None

    def __new__(cls, jarvis_root: Optional[str] = None):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self, jarvis_root: Optional[str] = None):
        """Initialize singleton (only happens once)"""
        if self._initialized:
            return

        # Set up jarvis root directory
        if jarvis_root is None:
            self.jarvis_root = Path.home() / '.ppi-jarvis'
        else:
            self.jarvis_root = Path(jarvis_root)

        self.config_file = self.jarvis_root / 'jarvis_config.yaml'
        self.repos_file = self.jarvis_root / 'repos.yaml'
        self.resource_graph_file = self.jarvis_root / 'resource_graph.yaml'

        # Lazy-loaded properties
        self._config = None
        self._repos = None
        self._resource_graph = None
        self._hostfile = None

        # Directory paths
        self.config_dir = None
        self.private_dir = None
        self.shared_dir = None

        # Try to automatically load configuration if it exists
        if self.is_initialized():
            config = self.config
            self.config_dir = config.get('config_dir', str(self.jarvis_root))
            self.private_dir = config.get('private_dir', str(self.jarvis_root / 'private'))
            self.shared_dir = config.get('shared_dir', str(self.jarvis_root / 'shared'))

        self._initialized = True

    @classmethod
    def get_instance(cls, jarvis_root: Optional[str] = None):
        """Get the singleton instance"""
        if cls._instance is None:
            cls._instance = cls(jarvis_root)
        return cls._instance

    def initialize(self, config_dir: str, private_dir: str, shared_dir: str, force: bool = False):
        """
        Initialize Jarvis configuration directories and files.

        :param config_dir: Directory for jarvis metadata
        :param private_dir: Machine-local data directory
        :param shared_dir: Shared data directory across all machines
        :param force: Force override of existing repos and resource_graph files
        """
        from jarvis_cd.util import logger

        # Create jarvis root directory
        self.jarvis_root.mkdir(parents=True, exist_ok=True)

        # Create required directories
        Path(config_dir).mkdir(parents=True, exist_ok=True)
        Path(private_dir).mkdir(parents=True, exist_ok=True)
        Path(shared_dir).mkdir(parents=True, exist_ok=True)

        # Initialize default configuration
        default_config = {
            'config_dir': str(Path(config_dir).absolute()),
            'private_dir': str(Path(private_dir).absolute()),
            'shared_dir': str(Path(shared_dir).absolute()),
            'current_pipeline': None,
            'hostfile': None
        }

        # Save configuration
        self.save_config(default_config)

        # Update instance attributes
        self.config_dir = default_config['config_dir']
        self.private_dir = default_config['private_dir']
        self.shared_dir = default_config['shared_dir']

        # Handle repos.yaml
        repos_exists = self.repos_file.exists()
        if repos_exists and not force:
            logger.warning(f"Existing repos.yaml detected - preserving (use +force to override)")
        elif repos_exists and force:
            logger.warning(f"Existing repos.yaml detected - overriding due to +force")
            builtin_repo_path = self.get_builtin_repo_path()
            if builtin_repo_path.exists():
                builtin_repo_path_str = str(builtin_repo_path.absolute())
            else:
                builtin_repo_path_str = str((self.jarvis_root / 'builtin').absolute())
            default_repos = {'repos': [builtin_repo_path_str]}
            self.save_repos(default_repos)
        else:
            # File doesn't exist, create it
            builtin_repo_path = self.get_builtin_repo_path()
            if builtin_repo_path.exists():
                builtin_repo_path_str = str(builtin_repo_path.absolute())
            else:
                builtin_repo_path_str = str((self.jarvis_root / 'builtin').absolute())
            default_repos = {'repos': [builtin_repo_path_str]}
            self.save_repos(default_repos)

        # Handle resource_graph.yaml
        resource_graph_exists = self.resource_graph_file.exists()
        if resource_graph_exists and not force:
            logger.warning(f"Existing resource_graph.yaml detected - preserving (use +force to override)")
        elif resource_graph_exists and force:
            logger.warning(f"Existing resource_graph.yaml detected - overriding due to +force")
            default_resource_graph = {'storage': {}, 'network': {}}
            self.save_resource_graph(default_resource_graph)
        else:
            # File doesn't exist, create it
            default_resource_graph = {'storage': {}, 'network': {}}
            self.save_resource_graph(default_resource_graph)

        print(f"Jarvis initialized at {self.jarvis_root}")
        print(f"Config directory: {config_dir}")
        print(f"Private directory: {private_dir}")
        print(f"Shared directory: {shared_dir}")

    @property
    def config(self) -> Dict[str, Any]:
        """Get jarvis configuration, loading if necessary"""
        if self._config is None:
            self._config = self.load_config()
        return self._config

    @property
    def repos(self) -> Dict[str, Any]:
        """Get repos configuration, loading if necessary"""
        if self._repos is None:
            self._repos = self.load_repos()
        return self._repos

    @property
    def resource_graph(self) -> Dict[str, Any]:
        """Get resource graph, loading if necessary"""
        if self._resource_graph is None:
            self._resource_graph = self.load_resource_graph()
        return self._resource_graph

    @property
    def hostfile(self) -> Hostfile:
        """Get current hostfile"""
        if self._hostfile is None:
            hostfile_path = self.config.get('hostfile')
            if hostfile_path and os.path.exists(hostfile_path):
                self._hostfile = Hostfile(path=hostfile_path)
            else:
                # Default to localhost
                self._hostfile = Hostfile()
        return self._hostfile

    def load_config(self) -> Dict[str, Any]:
        """Load jarvis configuration from file"""
        if not self.config_file.exists():
            raise FileNotFoundError(f"Jarvis not initialized. Run 'jarvis init' first.")

        with open(self.config_file, 'r') as f:
            return yaml.safe_load(f) or {}

    def load_repos(self) -> Dict[str, Any]:
        """Load repos configuration from file"""
        if not self.repos_file.exists():
            return {'repos': []}

        with open(self.repos_file, 'r') as f:
            return yaml.safe_load(f) or {'repos': []}

    def load_resource_graph(self) -> Dict[str, Any]:
        """Load resource graph from file"""
        if not self.resource_graph_file.exists():
            return {'storage': {}, 'network': {}}

        with open(self.resource_graph_file, 'r') as f:
            return yaml.safe_load(f) or {'storage': {}, 'network': {}}

    def save_config(self, config: Dict[str, Any]):
        """Save jarvis configuration to file"""
        self.jarvis_root.mkdir(parents=True, exist_ok=True)
        with open(self.config_file, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        self._config = config

    def save_repos(self, repos: Dict[str, Any]):
        """Save repos configuration to file"""
        self.jarvis_root.mkdir(parents=True, exist_ok=True)
        with open(self.repos_file, 'w') as f:
            yaml.dump(repos, f, default_flow_style=False)
        self._repos = repos

    def save_resource_graph(self, resource_graph: Dict[str, Any]):
        """Save resource graph to file"""
        self.jarvis_root.mkdir(parents=True, exist_ok=True)
        with open(self.resource_graph_file, 'w') as f:
            yaml.dump(resource_graph, f, default_flow_style=False)
        self._resource_graph = resource_graph

    def add_repo(self, repo_path: str, force: bool = False):
        """Add a repository to the repos configuration"""
        repo_path = str(Path(repo_path).absolute())
        repos = self.repos.copy()

        # Check for existing repository with same name (not just same path)
        repo_name = Path(repo_path).name
        existing_repos_with_same_name = [
            existing_path for existing_path in repos['repos']
            if Path(existing_path).name == repo_name
        ]

        if repo_path in repos['repos']:
            if force:
                # Repository path already exists - remove and re-add to update order
                repos['repos'].remove(repo_path)
                repos['repos'].insert(0, repo_path)
                self.save_repos(repos)
                print(f"Repository already exists - updated position: {repo_path}")
            else:
                print(f"Repository already exists: {repo_path}")
                print("Use --force to override existing repository")
        elif existing_repos_with_same_name:
            if force:
                # Remove existing repositories with same name
                for existing_path in existing_repos_with_same_name:
                    repos['repos'].remove(existing_path)
                    print(f"Removed existing repository: {existing_path}")
                # Add new repository
                repos['repos'].insert(0, repo_path)
                self.save_repos(repos)
                print(f"Added repository (replacing existing): {repo_path}")
            else:
                print(f"Repository with name '{repo_name}' already exists:")
                for existing_path in existing_repos_with_same_name:
                    print(f"  {existing_path}")
                print("Use --force to replace existing repository")
        else:
            repos['repos'].insert(0, repo_path)  # Add to front for priority
            self.save_repos(repos)
            print(f"Added repository: {repo_path}")

    def remove_repo(self, repo_path: str):
        """Remove a repository from the repos configuration"""
        repo_path = str(Path(repo_path).absolute())
        repos = self.repos.copy()

        if repo_path in repos['repos']:
            repos['repos'].remove(repo_path)
            self.save_repos(repos)
            print(f"Removed repository: {repo_path}")
        else:
            print(f"Repository not found: {repo_path}")

    def remove_repo_by_name(self, repo_name: str):
        """
        Remove all repositories with the given name from the repos configuration.

        :param repo_name: Name of repository to remove
        :return: Number of repositories removed
        """
        repos = self.repos.copy()
        initial_count = len(repos['repos'])
        removed_repos = []

        # Find all repositories with matching name
        remaining_repos = []
        for repo_path in repos['repos']:
            if Path(repo_path).name == repo_name:
                removed_repos.append(repo_path)
            else:
                remaining_repos.append(repo_path)

        if removed_repos:
            repos['repos'] = remaining_repos
            self.save_repos(repos)

            print(f"Removed {len(removed_repos)} repository(ies) named '{repo_name}':")
            for repo_path in removed_repos:
                print(f"  - {repo_path}")
        else:
            print(f"No repositories found with name '{repo_name}'")

        return len(removed_repos)

    def cleanup_nonexistent_repos(self):
        """Remove any repositories from configuration that no longer exist on disk"""
        repos = self.repos.copy()
        initial_count = len(repos['repos'])
        removed_repos = []

        # Filter out non-existent repositories
        existing_repos = []
        for repo_path in repos['repos']:
            if Path(repo_path).exists():
                existing_repos.append(repo_path)
            else:
                removed_repos.append(repo_path)

        # Update repos if any were removed
        if removed_repos:
            repos['repos'] = existing_repos
            self.save_repos(repos)

            print(f"Automatically removed {len(removed_repos)} non-existent repositories:")
            for repo_path in removed_repos:
                print(f"  - {repo_path}")

        return len(removed_repos)

    def set_hostfile(self, hostfile_path: str):
        """Set the hostfile path in configuration"""
        hostfile_path = str(Path(hostfile_path).absolute())
        if not os.path.exists(hostfile_path):
            raise FileNotFoundError(f"Hostfile not found: {hostfile_path}")

        config = self.config.copy()
        config['hostfile'] = hostfile_path
        self.save_config(config)
        self._hostfile = None  # Reset cached hostfile
        print(f"Set hostfile: {hostfile_path}")

    def get_pipeline_dir(self, pipeline_name: str) -> Path:
        """Get the config directory for a specific pipeline"""
        return Path(self.config_dir) / 'pipelines' / pipeline_name

    def get_pipeline_shared_dir(self, pipeline_name: str) -> Path:
        """Get the shared directory for a specific pipeline"""
        return Path(self.shared_dir) / pipeline_name

    def get_pipeline_private_dir(self, pipeline_name: str) -> Path:
        """Get the private directory for a specific pipeline"""
        return Path(self.private_dir) / pipeline_name

    def get_current_pipeline_dir(self) -> Optional[Path]:
        """Get the config directory for the current pipeline"""
        current_pipeline = self.config.get('current_pipeline')
        if current_pipeline:
            return self.get_pipeline_dir(current_pipeline)
        return None

    def get_current_pipeline_shared_dir(self) -> Optional[Path]:
        """Get the shared directory for the current pipeline"""
        current_pipeline = self.config.get('current_pipeline')
        if current_pipeline:
            return self.get_pipeline_shared_dir(current_pipeline)
        return None

    def get_current_pipeline_private_dir(self) -> Optional[Path]:
        """Get the private directory for the current pipeline"""
        current_pipeline = self.config.get('current_pipeline')
        if current_pipeline:
            return self.get_pipeline_private_dir(current_pipeline)
        return None

    def set_current_pipeline(self, pipeline_name: str):
        """Set the current active pipeline"""
        config = self.config.copy()
        config['current_pipeline'] = pipeline_name
        self.save_config(config)

    def get_current_pipeline(self) -> Optional[str]:
        """Get the name of the current active pipeline"""
        return self.config.get('current_pipeline')

    def set_current_module(self, module_name: Optional[str]):
        """Set the current active module"""
        config = self.config.copy()
        config['current_module'] = module_name
        self.save_config(config)

    def get_current_module(self) -> Optional[str]:
        """Get the name of the current active module"""
        return self.config.get('current_module')

    def get_pipelines_dir(self) -> Path:
        """Get the directory where all pipelines are stored"""
        config_dir = Path(self.config['config_dir'])
        return config_dir / 'pipelines'

    def get_builtin_repo_path(self) -> Path:
        """Get path to builtin repository"""
        # First check if builtin repo is registered in repos
        for repo_path_str in self.repos['repos']:
            repo_path = Path(repo_path_str)
            if repo_path.name == 'builtin' and repo_path.exists():
                return repo_path

        # Fall back to builtin repo installed to ~/.ppi-jarvis/builtin
        user_builtin = self.jarvis_root / 'builtin'
        if user_builtin.exists():
            return user_builtin

        # Fall back to builtin repo in the same directory as this file (development)
        dev_builtin = Path(__file__).parent.parent.parent / 'builtin'
        if dev_builtin.exists():
            return dev_builtin

        # Fall back to installed package location
        try:
            import importlib.util
            import importlib.metadata
            import site

            # Method 1: Try to find builtin package directly
            try:
                import builtin
                builtin_module_path = Path(builtin.__file__).parent
                if builtin_module_path.exists():
                    return builtin_module_path
            except ImportError:
                pass

            # Method 2: Look for builtin in installed package files
            try:
                dist = importlib.metadata.distribution('jarvis_cd')
                if hasattr(dist, 'files') and dist.files:
                    for file in dist.files:
                        if 'builtin' in str(file) and str(file).endswith('builtin/__init__.py'):
                            # Get the actual installation path
                            for path in site.getsitepackages() + [site.getusersitepackages()]:
                                if path:
                                    candidate = Path(path) / file.parent
                                    if candidate.exists():
                                        return candidate
            except Exception:
                pass

            # Method 3: Search in site-packages
            for path in site.getsitepackages() + [site.getusersitepackages()]:
                if path:
                    builtin_path = Path(path) / 'builtin'
                    if builtin_path.exists() and (builtin_path / '__init__.py').exists():
                        return builtin_path

        except Exception:
            pass

        # Default fallback
        return user_builtin

    def find_package(self, pkg_name: str) -> Optional[str]:
        """
        Find a package in registered repositories.
        Returns the full import path if found.
        Searches repositories in order, respecting priority.
        """
        # Check all registered repos in order (builtin is already in the list)
        for repo_path in self.repos['repos']:
            repo_name = Path(repo_path).name
            if self._check_package_exists(repo_path, repo_name, pkg_name):
                return f'{repo_name}.{pkg_name}'

        return None

    def _check_package_exists(self, repo_path: str, repo_name: str, pkg_name: str) -> bool:
        """Check if a package exists in a repository"""
        # Try both package.py and pkg.py (legacy naming)
        package_file = Path(repo_path) / repo_name / pkg_name / 'package.py'
        if package_file.exists():
            return True

        # Check for legacy pkg.py naming
        legacy_package_file = Path(repo_path) / repo_name / pkg_name / 'pkg.py'
        return legacy_package_file.exists()

    def is_initialized(self) -> bool:
        """Check if Jarvis has been initialized"""
        return self.config_file.exists()
```

### `jarvis_cd/core/container.py`

```python
"""
Container image management for Jarvis-CD.
Handles listing and removing container images.
"""

import yaml
from pathlib import Path
from jarvis_cd.core.config import Jarvis


class ContainerManager:
    """
    Manages container images for Jarvis-CD.
    Provides methods to list and remove container images.
    """

    def __init__(self):
        """Initialize ContainerManager"""
        self.jarvis = Jarvis.get_instance()
        self.containers_dir = Path.home() / '.ppi-jarvis' / 'containers'
        self.containers_dir.mkdir(parents=True, exist_ok=True)

    def list_containers(self):
        """
        List all container images.
        Shows container name, number of packages, and status.
        """
        if not self.containers_dir.exists():
            print("No containers found")
            return

        # Find all .yaml manifest files
        manifest_files = list(self.containers_dir.glob('*.yaml'))

        if not manifest_files:
            print("No containers found")
            return

        print("Container images:")
        for manifest_file in sorted(manifest_files):
            container_name = manifest_file.stem

            # Load manifest to count packages
            try:
                with open(manifest_file, 'r') as f:
                    manifest = yaml.safe_load(f) or {}
                num_packages = len(manifest)
            except:
                num_packages = 0

            # Check if Dockerfile exists
            dockerfile_path = self.containers_dir / f"{container_name}.Dockerfile"
            has_dockerfile = dockerfile_path.exists()

            status = "✓" if has_dockerfile else "✗"
            print(f"  {status} {container_name} ({num_packages} packages)")

    def remove_container(self, container_name: str):
        """
        Remove a container image and its files.
        Removes Dockerfile, manifest, and container image.

        :param container_name: Name of container to remove
        """
        # Remove Dockerfile
        dockerfile_path = self.containers_dir / f"{container_name}.Dockerfile"
        if dockerfile_path.exists():
            dockerfile_path.unlink()
            print(f"Removed Dockerfile: {dockerfile_path}")

        # Remove manifest
        manifest_path = self.containers_dir / f"{container_name}.yaml"
        if manifest_path.exists():
            manifest_path.unlink()
            print(f"Removed manifest: {manifest_path}")

        # Remove container image using container engine
        from jarvis_cd.shell import Exec, LocalExecInfo

        # Try podman first, then docker
        for engine in ['podman', 'docker']:
            try:
                remove_cmd = f"{engine} rmi {container_name}"
                print(f"Removing container image with: {remove_cmd}")
                Exec(remove_cmd, LocalExecInfo()).run()
                print(f"Container image removed: {container_name}")
                break
            except:
                # Try next engine
                continue

        print(f"Container '{container_name}' removed")
```

### `jarvis_cd/core/container_pkg.py`

```python
"""
Base classes for containerized application deployment.
"""
from .pkg import Application
from ..shell import Exec, LocalExecInfo
from ..shell.container_compose_exec import ContainerComposeExec, ContainerBuildExec
from ..shell.container_exec import ContainerExec
import os
import yaml
from pathlib import Path


class ContainerApplication(Application):
    """
    Base class for containerized application deployment using Docker/Podman.

    This class provides common functionality for deploying applications in containers,
    including pipeline YAML generation, Dockerfile creation, and container lifecycle management.
    """

    def _init(self):
        """
        Initialize paths
        """
        pass

    def _generate_container_ppl_yaml(self):
        """
        Generate pipeline YAML file containing just this package.
        This is used to load the pipeline configuration inside the container.
        """
        # Get this package's configuration and interceptors
        pkg_config = self.config.copy()

        # Build package entry - use the actual package type
        pkg_entry = {'pkg_type': self.pkg_type}

        # Add all config parameters except 'deploy'
        for key, value in pkg_config.items():
            if key != 'deploy':
                pkg_entry[key] = value

        # Create pipeline structure
        pipeline_config = {
            'name': f'{self.pipeline.name}_container',
            'pkgs': [pkg_entry]
        }

        # Add interceptors if any
        interceptors_list = pkg_config.get('interceptors', [])
        if interceptors_list:
            pipeline_config['interceptors'] = []
            for interceptor_name in interceptors_list:
                if interceptor_name in self.pipeline.interceptors:
                    interceptor_def = self.pipeline.interceptors[interceptor_name]
                    interceptor_entry = {
                        'pkg_type': interceptor_def['pkg_type']
                    }
                    # Add config parameters
                    for key, value in interceptor_def.get('config', {}).items():
                        interceptor_entry[key] = value
                    pipeline_config['interceptors'].append(interceptor_entry)

        # Write pipeline file to shared directory
        pipeline_file = Path(self.shared_dir) / 'pkg.yaml'
        with open(pipeline_file, 'w') as f:
            yaml.dump(pipeline_config, f, default_flow_style=False)

        print(f"Generated pipeline YAML: {pipeline_file}")

    def _generate_dockerfile(self):
        """
        Generate Dockerfile for the container.
        Subclasses should override this method to provide application-specific Dockerfile content.

        :return: None
        """
        raise NotImplementedError("Subclasses must implement _generate_dockerfile()")

    def _get_container_command(self):
        """
        Get the command to run in the container.
        Subclasses can override this to provide application-specific startup commands.

        Note: This method is deprecated for pipeline-level containers.
        Use pipeline.container_ssh_port instead.

        :return: List representing the container command
        """
        # Use pipeline-level SSH port if available, otherwise fallback to default
        ssh_port = getattr(self.pipeline, 'container_ssh_port', 2222)

        # Default command: setup SSH, start sshd, run pipeline, and keep container running
        return [
            f'cp -r /root/.ssh_host /root/.ssh && '
            f'chmod 700 /root/.ssh && '
            f'chmod 600 /root/.ssh/* 2>/dev/null || true && '
            f'cat /root/.ssh/*.pub > /root/.ssh/authorized_keys 2>/dev/null && '
            f'chmod 600 /root/.ssh/authorized_keys 2>/dev/null || true && '
            f'echo "Host *" > /root/.ssh/config && '
            f'echo "    Port {ssh_port}" >> /root/.ssh/config && '
            f'echo "    StrictHostKeyChecking no" >> /root/.ssh/config && '
            f'chmod 600 /root/.ssh/config && '
            f'sed -i "s/^#*Port .*/Port {ssh_port}/" /etc/ssh/sshd_config && '
            f'/usr/sbin/sshd && '
            f'jarvis ppl run yaml /root/.ppi-jarvis/shared/pkg.yaml && '
            f'tail -f /dev/null'
        ]

    def _get_service_name(self):
        """
        Get the service name for the compose file.
        Subclasses can override this to provide a custom service name.

        :return: Service name string
        """
        # Use package name from pkg_type (e.g., 'builtin.ior' -> 'ior')
        if self.pkg_type and '.' in self.pkg_type:
            return self.pkg_type.split('.')[-1]
        return 'app'

    def _generate_compose_file(self):
        """
        Generate docker/podman compose file.
        Generates a standard compose configuration that works for most containerized applications.

        :return: None
        """
        container_name = f"{self.pipeline.name}_{self.pkg_id}"
        service_name = self._get_service_name()
        shm_size = self.config.get('shm_size', 0)

        # Mount host directories to Jarvis default paths in container
        ssh_dir = os.path.expanduser('~/.ssh')

        compose_config = {
            'services': {
                service_name: {
                    'container_name': container_name,
                    'entrypoint': ['/bin/bash', '-c'],
                    'command': self._get_container_command(),
                    'volumes': [
                        f"{self.private_dir}:/root/.ppi-jarvis/private",
                        f"{self.shared_dir}:/root/.ppi-jarvis/shared",
                        f"{ssh_dir}:/root/.ssh_host:ro"  # Mount SSH keys
                    ]
                }
            }
        }

        # Use global container image if pipeline has container_build or container_image, otherwise build from local Dockerfile
        if hasattr(self.pipeline, 'get_container_image') and self.pipeline.get_container_image():
            compose_config['services'][service_name]['image'] = self.pipeline.get_container_image()
        else:
            compose_config['services'][service_name]['build'] = str(self.shared_dir)

        # Always use host network mode for multi-node MPI support
        compose_config['services'][service_name]['network_mode'] = 'host'

        # Handle shared memory configuration
        if shm_size > 0:
            # This container creates a new shared memory segment
            compose_config['services'][service_name]['shm_size'] = f'{shm_size}m'
            # Set this container as the shm provider for the pipeline
            self.pipeline.shm_container = container_name
            print(f"Created shared memory segment: {shm_size}MB in container {container_name}")
        elif hasattr(self.pipeline, 'shm_container') and self.pipeline.shm_container:
            # This container connects to an existing shared memory segment
            compose_config['services'][service_name]['ipc'] = f"container:{self.pipeline.shm_container}"
            print(f"Connecting to shared memory container: {self.pipeline.shm_container}")

        # Write compose file to shared directory
        compose_file = Path(self.shared_dir) / 'compose.yaml'
        with open(compose_file, 'w') as f:
            yaml.dump(compose_config, f, default_flow_style=False)

        print(f"Generated compose file: {compose_file}")

    def _build_image(self):
        """
        Build the container image using compose build.
        Note: This is no longer used for per-package builds. Container building happens at pipeline level.

        :return: None
        """
        pass

    def start(self):
        """
        Start is handled at the pipeline level for containerized applications.
        Individual packages do not start containers themselves.

        :return: None
        """
        pass

    def stop(self):
        """
        Stop is handled at the pipeline level for containerized applications.
        Individual packages do not stop containers themselves.

        :return: None
        """
        pass

    def clean(self):
        """
        Clean is handled at the pipeline level for containerized applications.
        Individual packages do not clean containers themselves.

        :return: None
        """
        pass

    def kill(self):
        """
        Kill is handled at the pipeline level for containerized applications.
        Individual packages do not kill containers themselves.

        :return: None
        """
        pass


class ContainerService(ContainerApplication):
    """
    Alias for ContainerApplication following service naming conventions.

    This class is identical to ContainerApplication but follows the naming convention
    where long-running containerized applications are called "services".
    """
    pass
```

### `jarvis_cd/core/environment.py`

```python
import os
import yaml
from pathlib import Path
from typing import Dict, Any, List, Optional
from jarvis_cd.core.config import Jarvis


class EnvironmentManager:
    """
    Manages Jarvis environments - both pipeline-specific and named environments.
    """
    
    # Common environment variables that should be captured
    COMMON_ENV_VARS = [
        # Build system variables
        'CMAKE_MODULE_PATH',
        'CMAKE_PREFIX_PATH',
        'PKG_CONFIG_PATH',
        
        # C/C++ include paths
        'CPATH',
        'C_INCLUDE_PATH', 
        'CPLUS_INCLUDE_PATH',
        'INCLUDE_PATH',
        
        # Library paths
        'LD_LIBRARY_PATH',
        'LIBRARY_PATH',
        'DYLD_LIBRARY_PATH',  # macOS
        'LD_PRELOAD',
        
        # Runtime paths
        'PATH',
        'MANPATH',
        
        # Language-specific paths
        'PYTHONPATH',
        'PERL5LIB',
        'CLASSPATH',
        'GOPATH',
        'CARGO_HOME',
        'TCLLIBPATH',
        
        # Development tools
        'JAVA_HOME',
        
        # Compilers
        'CC',
        'CXX', 
        'FC',
        'F77',
        'F90',
        
        # MPI compilers
        'MPICC',
        'MPICXX',
        'MPIFC',
        'MPIF77',
        'MPIF90',
        
        # Other common build variables
        'CFLAGS',
        'CXXFLAGS',
        'FFLAGS',
        'LDFLAGS',
        'LIBS',
    ]
    
    def __init__(self, jarvis_config: Jarvis):
        """
        Initialize environment manager.

        :param jarvis_config: Jarvis configuration singleton
        """
        self.jarvis_config = jarvis_config
        
    def build_pipeline_environment(self, env_args: List[str]):
        """
        Build environment for the current pipeline by capturing current environment
        and adding user-specified variables.
        
        :param env_args: List of environment arguments in VAR=value format
        """
        current_pipeline_dir = self.jarvis_config.get_current_pipeline_dir()
        if not current_pipeline_dir:
            raise ValueError("No current pipeline. Create one with 'jarvis ppl create <name>'")
            
        # Capture current environment
        captured_env = self._capture_current_environment()
        
        # Parse user-provided environment variables
        user_env = self._parse_env_args(env_args)
        
        # Merge environments (user overrides captured)
        final_env = {**captured_env, **user_env}
        
        # Save to pipeline's env.yaml
        env_file = current_pipeline_dir / 'env.yaml'
        with open(env_file, 'w') as f:
            yaml.dump(final_env, f, default_flow_style=False)
            
        print(f"Built environment for current pipeline with {len(final_env)} variables")
        print(f"Captured {len(captured_env)} environment variables")
        print(f"User specified {len(user_env)} additional variables")
        
    def build_named_environment(self, env_name: str, env_args: List[str]):
        """
        Build a named environment that can be reused across pipelines.
        
        :param env_name: Name of the environment
        :param env_args: List of environment arguments in VAR=value format
        """
        # Create env directory if it doesn't exist
        env_dir = self.jarvis_config.jarvis_root / 'env'
        env_dir.mkdir(exist_ok=True)
        
        # Capture current environment
        captured_env = self._capture_current_environment()
        
        # Parse user-provided environment variables
        user_env = self._parse_env_args(env_args)
        
        # Merge environments (user overrides captured)
        final_env = {**captured_env, **user_env}
        
        # Save named environment
        env_file = env_dir / f'{env_name}.yaml'
        with open(env_file, 'w') as f:
            yaml.dump(final_env, f, default_flow_style=False)
            
        print(f"Created named environment '{env_name}' with {len(final_env)} variables")
        print(f"Captured {len(captured_env)} environment variables")
        print(f"User specified {len(user_env)} additional variables")
        print(f"Saved to: {env_file}")
        
    def copy_named_environment(self, env_name: str):
        """
        Copy a named environment to the current pipeline.
        
        :param env_name: Name of the environment to copy
        """
        current_pipeline_dir = self.jarvis_config.get_current_pipeline_dir()
        if not current_pipeline_dir:
            raise ValueError("No current pipeline. Create one with 'jarvis ppl create <name>'")
            
        # Find named environment file
        env_dir = self.jarvis_config.jarvis_root / 'env'
        env_file = env_dir / f'{env_name}.yaml'
        
        if not env_file.exists():
            # List available environments to help user
            available_envs = self.list_named_environments()
            if available_envs:
                print(f"Named environment '{env_name}' not found.")
                print(f"Available environments: {', '.join(available_envs)}")
            else:
                print("No named environments found. Create one with 'jarvis env build <name>'")
            return
            
        # Load named environment
        with open(env_file, 'r') as f:
            named_env = yaml.safe_load(f) or {}
            
        # Copy to pipeline's env.yaml
        pipeline_env_file = current_pipeline_dir / 'env.yaml'
        with open(pipeline_env_file, 'w') as f:
            yaml.dump(named_env, f, default_flow_style=False)
            
        # Get current pipeline name for display
        config_file = current_pipeline_dir / 'pipeline.yaml'
        with open(config_file, 'r') as f:
            pipeline_config = yaml.safe_load(f)
            pipeline_name = pipeline_config['name']
            
        print(f"Copied named environment '{env_name}' to pipeline '{pipeline_name}'")
        print(f"Environment contains {len(named_env)} variables")
        
    def list_named_environments(self) -> List[str]:
        """
        List all available named environments.

        :return: List of environment names
        """
        env_dir = self.jarvis_config.jarvis_root / 'env'
        if not env_dir.exists():
            return []

        env_files = list(env_dir.glob('*.yaml'))
        return [f.stem for f in env_files]

    @staticmethod
    def show_environment(env_file_path: Path, context_name: str):
        """
        Display environment variables from a YAML file.

        This is a unified function used by both pipeline and named environment display.

        :param env_file_path: Path to the environment YAML file
        :param context_name: Name to display in the output (e.g., pipeline name or environment name)
        """
        if not env_file_path.exists():
            print(f"No environment configured for '{context_name}'")
            return

        with open(env_file_path, 'r') as f:
            env_vars = yaml.safe_load(f) or {}

        print(f"Environment for '{context_name}':")
        print(f"Total variables: {len(env_vars)}")
        print()

        if env_vars:
            # Sort by variable name for consistent display
            for var_name in sorted(env_vars.keys()):
                value = env_vars[var_name]
                # Truncate very long values for readability
                if isinstance(value, str) and len(value) > 100:
                    display_value = value[:97] + "..."
                else:
                    display_value = value
                print(f"  {var_name}: {display_value}")
        else:
            print("  No environment variables set")
        
    def show_pipeline_environment(self):
        """
        Show the environment variables for the current pipeline.
        """
        current_pipeline_dir = self.jarvis_config.get_current_pipeline_dir()
        if not current_pipeline_dir:
            print("No current pipeline set")
            return

        # Get current pipeline name for display context
        config_file = current_pipeline_dir / 'pipeline.yaml'
        with open(config_file, 'r') as f:
            pipeline_config = yaml.safe_load(f)
            pipeline_name = pipeline_config['name']

        # Use unified function to display environment
        env_file = current_pipeline_dir / 'env.yaml'
        self.show_environment(env_file, f"pipeline '{pipeline_name}'")
            
    def show_named_environment(self, env_name: str):
        """
        Show the variables in a named environment.

        :param env_name: Name of the environment to show
        """
        env_dir = self.jarvis_config.jarvis_root / 'env'
        env_file = env_dir / f'{env_name}.yaml'

        # Check if environment exists and provide helpful error message
        if not env_file.exists():
            available_envs = self.list_named_environments()
            if available_envs:
                print(f"Named environment '{env_name}' not found.")
                print(f"Available environments: {', '.join(available_envs)}")
            else:
                print("No named environments found")
            return

        # Use unified function to display environment
        self.show_environment(env_file, f"named environment '{env_name}'")
            
    def load_named_environment(self, env_name: str) -> Dict[str, str]:
        """
        Load a named environment and return its variables.
        
        :param env_name: Name of the environment to load
        :return: Dictionary of environment variables
        :raises FileNotFoundError: If the named environment doesn't exist
        """
        env_dir = self.jarvis_config.jarvis_root / 'env'
        env_file = env_dir / f'{env_name}.yaml'
        
        if not env_file.exists():
            available_envs = self.list_named_environments()
            if available_envs:
                error_msg = f"Named environment '{env_name}' not found. Available: {', '.join(available_envs)}"
            else:
                error_msg = f"Named environment '{env_name}' not found. No named environments exist."
            raise FileNotFoundError(error_msg)
            
        with open(env_file, 'r') as f:
            env_vars = yaml.safe_load(f) or {}
            
        return env_vars
            
    def _capture_current_environment(self) -> Dict[str, str]:
        """
        Capture current environment variables that are commonly used in builds.
        
        :return: Dictionary of environment variables
        """
        captured = {}
        
        for var_name in self.COMMON_ENV_VARS:
            if var_name in os.environ:
                captured[var_name] = os.environ[var_name]
                
        return captured
        
    def _parse_env_args(self, env_args: List[str]) -> Dict[str, str]:
        """
        Parse environment arguments in VAR=value format.
        
        :param env_args: List of environment arguments
        :return: Dictionary of parsed environment variables
        """
        parsed = {}
        
        for arg in env_args:
            if '=' in arg:
                key, value = arg.split('=', 1)
                parsed[key] = value
            else:
                print(f"Warning: Ignoring malformed environment argument: {arg}")
                print("Expected format: VAR=value")
                
        return parsed
```

### `jarvis_cd/core/module_manager.py`

```python
"""
Module management system for Jarvis-CD.
Provides functionality for creating and managing modulefiles for manually-installed packages.
"""
import os
import yaml
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional
from jarvis_cd.core.config import Jarvis
from jarvis_cd.util.logger import logger
from jarvis_cd.shell.exec_factory import Exec
from jarvis_cd.shell.exec_info import LocalExecInfo


class ModuleManager:
    """
    Manages modulefiles for manually-installed packages.
    Provides creation, configuration, and generation of TCL and YAML modulefiles.
    """

    def __init__(self, jarvis_config: Jarvis):
        """
        Initialize module manager.

        :param jarvis_config: Jarvis configuration singleton
        """
        self.jarvis_config = jarvis_config
        self.jarvis = jarvis_config  # They are the same now
        
        # Module directory structure
        self.modules_root = Path.home() / '.ppi-jarvis-mods'
        self.packages_dir = self.modules_root / 'packages'
        self.modules_dir = self.modules_root / 'modules'
        
        # Ensure directories exist
        self.packages_dir.mkdir(parents=True, exist_ok=True)
        self.modules_dir.mkdir(parents=True, exist_ok=True)
        
    def create_module(self, mod_name: str):
        """
        Create a new module with directory structure and files.
        
        :param mod_name: Name of the module to create
        """
        # Create package directory
        package_dir = self.packages_dir / mod_name
        package_dir.mkdir(exist_ok=True)
        
        # Create src subdirectory
        src_dir = package_dir / 'src'
        src_dir.mkdir(exist_ok=True)
        
        # Create initial YAML file with default paths
        yaml_file = self.modules_dir / f'{mod_name}.yaml'
        
        # Get the package root directory path
        package_root = str(package_dir)
        
        initial_yaml = {
            'deps': {},
            'doc': {
                'Name': mod_name,
                'Version': 'None',
                'doc': 'None'
            },
            'prepends': {
                'CFLAGS': [],
                'CMAKE_PREFIX_PATH': [
                    f'{package_root}/cmake'
                ],
                'CPATH': [],
                'INCLUDE': [],
                'LDFLAGS': [],
                'LD_LIBRARY_PATH': [
                    f'{package_root}/lib',
                    f'{package_root}/lib64'
                ],
                'LIBRARY_PATH': [
                    f'{package_root}/lib',
                    f'{package_root}/lib64'
                ],
                'PATH': [
                    f'{package_root}/bin',
                    f'{package_root}/sbin'
                ],
                'PKG_CONFIG_PATH': [
                    f'{package_root}/lib/pkgconfig',
                    f'{package_root}/lib64/pkgconfig'
                ],
                'PYTHONPATH': [
                    f'{package_root}/bin',
                    f'{package_root}/lib',
                    f'{package_root}/lib64'
                ]
            },
            'setenvs': {}
        }
        
        with open(yaml_file, 'w') as f:
            yaml.dump(initial_yaml, f, default_flow_style=False)
        
        # Generate initial TCL file
        self._generate_tcl_file(mod_name)
        
        # Set as current module
        self.jarvis_config.set_current_module(mod_name)
        
        print(f"Created module: {mod_name}")
        print(f"Package directory: {package_dir}")
        print(f"YAML file: {yaml_file}")
        print(f"TCL file: {self.modules_dir / mod_name}")
        
    def set_current_module(self, mod_name: str):
        """
        Set the current module in jarvis config.
        
        :param mod_name: Name of the module to set as current
        """
        if not self._module_exists(mod_name):
            raise ValueError(f"Module '{mod_name}' does not exist")
            
        self.jarvis_config.set_current_module(mod_name)
        print(f"Set current module: {mod_name}")
        
    def prepend_env_vars(self, mod_name: Optional[str], env_args: List[str]):
        """
        Prepend environment variables to module configuration.
        
        :param mod_name: Module name (optional, uses current if None)
        :param env_args: Environment arguments in ENV=VAL1;VAL2;VAL3 format
        """
        # Check if mod_name looks like an environment argument (contains =)
        if mod_name and '=' in mod_name:
            # First argument is actually an env var, prepend it to env_args
            env_args = [mod_name] + env_args
            mod_name = None
            
        if mod_name is None:
            mod_name = self.jarvis_config.get_current_module()
            if not mod_name:
                raise ValueError("No current module set. Use 'jarvis mod cd <module>' or specify module name")
        
        if not self._module_exists(mod_name):
            raise ValueError(f"Module '{mod_name}' does not exist")
        
        # Load current YAML configuration
        yaml_file = self.modules_dir / f'{mod_name}.yaml'
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)
        
        # Parse environment arguments
        for arg in env_args:
            if '=' not in arg:
                print(f"Warning: Ignoring malformed argument: {arg}")
                continue
                
            env_var, values_str = arg.split('=', 1)
            values = [v.strip() for v in values_str.split(';') if v.strip()]
            
            # Ensure prepends section exists
            if 'prepends' not in config:
                config['prepends'] = {}
            
            # Initialize environment variable list if not exists
            if env_var not in config['prepends']:
                config['prepends'][env_var] = []
            
            # Prepend new values (reverse order to maintain precedence)
            for value in reversed(values):
                if value not in config['prepends'][env_var]:
                    config['prepends'][env_var].insert(0, value)
        
        # Save updated configuration
        with open(yaml_file, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        
        # Regenerate TCL file
        self._generate_tcl_file(mod_name)
        
        print(f"Updated prepend environment variables for module: {mod_name}")
        
    def set_env_vars(self, mod_name: Optional[str], env_args: List[str]):
        """
        Set environment variables in module configuration.
        
        :param mod_name: Module name (optional, uses current if None)
        :param env_args: Environment arguments in ENV=VAL format
        """
        # Check if mod_name looks like an environment argument (contains =)
        if mod_name and '=' in mod_name:
            # First argument is actually an env var, prepend it to env_args
            env_args = [mod_name] + env_args
            mod_name = None
            
        if mod_name is None:
            mod_name = self.jarvis_config.get_current_module()
            if not mod_name:
                raise ValueError("No current module set. Use 'jarvis mod cd <module>' or specify module name")
        
        if not self._module_exists(mod_name):
            raise ValueError(f"Module '{mod_name}' does not exist")
        
        # Load current YAML configuration
        yaml_file = self.modules_dir / f'{mod_name}.yaml'
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)
        
        # Parse environment arguments
        for arg in env_args:
            if '=' not in arg:
                print(f"Warning: Ignoring malformed argument: {arg}")
                continue
                
            env_var, value = arg.split('=', 1)
            
            # Ensure setenvs section exists
            if 'setenvs' not in config:
                config['setenvs'] = {}
            
            # Set environment variable
            config['setenvs'][env_var] = value
        
        # Save updated configuration
        with open(yaml_file, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        
        # Regenerate TCL file
        self._generate_tcl_file(mod_name)
        
        print(f"Updated set environment variables for module: {mod_name}")
        
    def destroy_module(self, mod_name: Optional[str]):
        """
        Destroy a module by removing its directory and configuration files.
        
        :param mod_name: Module name (optional, uses current if None)
        """
        if mod_name is None:
            mod_name = self.jarvis_config.get_current_module()
            if not mod_name:
                raise ValueError("No current module set. Use 'jarvis mod cd <module>' or specify module name")
        
        if not self._module_exists(mod_name):
            raise ValueError(f"Module '{mod_name}' does not exist")
        
        # Remove package directory
        package_dir = self.packages_dir / mod_name
        if package_dir.exists():
            shutil.rmtree(package_dir)
        
        # Remove module files
        yaml_file = self.modules_dir / f'{mod_name}.yaml'
        tcl_file = self.modules_dir / mod_name
        
        if yaml_file.exists():
            yaml_file.unlink()
        if tcl_file.exists():
            tcl_file.unlink()
        
        # Clear current module if it was the destroyed one
        current_module = self.jarvis_config.get_current_module()
        if current_module == mod_name:
            self.jarvis_config.set_current_module(None)

        print(f"Destroyed module: {mod_name}")

    def clear_module(self, mod_name: Optional[str]):
        """
        Clear module directory contents except for the src/ directory.
        Useful for cleaning up build artifacts while preserving source code.

        :param mod_name: Module name (optional, uses current if None)
        """
        if mod_name is None:
            mod_name = self.jarvis_config.get_current_module()
            if not mod_name:
                raise ValueError("No current module set. Use 'jarvis mod cd <module>' or specify module name")

        if not self._module_exists(mod_name):
            raise ValueError(f"Module '{mod_name}' does not exist")

        # Get package directory
        package_dir = self.packages_dir / mod_name
        if not package_dir.exists():
            logger.warning(f"Package directory does not exist: {package_dir}")
            return

        # Get src directory
        src_dir = package_dir / 'src'

        # Remove all contents except src/
        items_removed = 0
        for item in package_dir.iterdir():
            if item.name == 'src':
                continue  # Skip src directory

            try:
                if item.is_dir():
                    shutil.rmtree(item)
                    logger.info(f"Removed directory: {item.name}/")
                else:
                    item.unlink()
                    logger.info(f"Removed file: {item.name}")
                items_removed += 1
            except Exception as e:
                logger.error(f"Failed to remove {item.name}: {e}")

        if items_removed > 0:
            logger.success(f"Cleared module '{mod_name}': removed {items_removed} items (preserved src/)")
        else:
            logger.info(f"Module '{mod_name}' is already clean (only src/ exists)")

    def get_module_src_dir(self, mod_name: Optional[str]) -> str:
        """
        Get the source directory path for a module.
        
        :param mod_name: Module name (optional, uses current if None)
        :return: Source directory path
        """
        if mod_name is None:
            mod_name = self.jarvis_config.get_current_module()
            if not mod_name:
                raise ValueError("No current module set. Use 'jarvis mod cd <module>' or specify module name")
        
        if not self._module_exists(mod_name):
            raise ValueError(f"Module '{mod_name}' does not exist")
        
        return str(self.packages_dir / mod_name / 'src')
        
    def get_module_root_dir(self, mod_name: Optional[str]) -> str:
        """
        Get the root directory path for a module.
        
        :param mod_name: Module name (optional, uses current if None)
        :return: Root directory path
        """
        if mod_name is None:
            mod_name = self.jarvis_config.get_current_module()
            if not mod_name:
                raise ValueError("No current module set. Use 'jarvis mod cd <module>' or specify module name")
        
        if not self._module_exists(mod_name):
            raise ValueError(f"Module '{mod_name}' does not exist")
        
        return str(self.packages_dir / mod_name)
        
    def get_module_tcl_path(self, mod_name: Optional[str]) -> str:
        """
        Get the TCL file path for a module.
        
        :param mod_name: Module name (optional, uses current if None)
        :return: TCL file path
        """
        if mod_name is None:
            mod_name = self.jarvis_config.get_current_module()
            if not mod_name:
                raise ValueError("No current module set. Use 'jarvis mod cd <module>' or specify module name")
        
        return str(self.modules_dir / mod_name)
        
    def get_module_yaml_path(self, mod_name: Optional[str]) -> str:
        """
        Get the YAML file path for a module.
        
        :param mod_name: Module name (optional, uses current if None)
        :return: YAML file path
        """
        if mod_name is None:
            mod_name = self.jarvis_config.get_current_module()
            if not mod_name:
                raise ValueError("No current module set. Use 'jarvis mod cd <module>' or specify module name")
        
        return str(self.modules_dir / f'{mod_name}.yaml')
        
    def build_profile(self, path: Optional[str] = None, method: str = 'dotenv'):
        """
        Create a snapshot of important currently-loaded environment variables.
        
        :param path: Output file path (if None, print to stdout)
        :param method: Output format (dotenv, cmake, clion, vscode)
        :return: Environment profile dictionary
        """
        env_vars = ['PATH', 'LD_LIBRARY_PATH', 'LIBRARY_PATH',
                    'INCLUDE', 'CPATH', 'PKG_CONFIG_PATH', 'CMAKE_PREFIX_PATH',
                    'JAVA_HOME', 'PYTHONPATH']
        
        profile = {}
        for env_var in env_vars:
            env_data = self._get_env(env_var)
            if len(env_data) == 0:
                profile[env_var] = []
            else:
                profile[env_var] = env_data.split(':')
        
        self._output_profile(profile, path, method)
        return profile
        
    def build_profile_new(self, path=None, method=None):
        """
        Create a snapshot of important currently-loaded environment variables.

        :param path: Output file path (optional)
        :param method: Output format (dotenv, cmake, clion, vscode)
        :return: Environment profile dictionary
        """
        env_vars = ['PATH', 'LD_LIBRARY_PATH', 'LIBRARY_PATH',
                    'INCLUDE', 'CPATH', 'PKG_CONFIG_PATH', 'CMAKE_PREFIX_PATH',
                    'JAVA_HOME', 'PYTHONPATH']
        profile = {}
        for env_var in env_vars:
            env_data = self._get_env(env_var)
            if len(env_data) == 0:
                profile[env_var] = []
            else:
                profile[env_var] = env_data.split(':')
        self.env_profile(profile, path, method)
        return profile

    def env_profile(self, profile, path=None, method='dotenv'):
        """Output environment profile in specified format."""
        # None-path profiles (print to stdout)
        if method == 'clion':
            prof_list = [f'{env_var}={":".join(env_data)}'
                        for env_var, env_data in profile.items()]
            print(';'.join(prof_list))
        elif method == 'vscode':
            vars_list = [f'  "{env_var}": "{":".join(env_data)}"' for env_var, env_data in profile.items()]
            print('"environment": {')
            print(',\n'.join(vars_list))
            print('}')
        elif method == 'dotenv' and path is None:
            # Print dotenv format to stdout if no path specified
            for env_var, env_data in profile.items():
                print(f'{env_var}="{":".join(env_data)}"')
        
        if path is None:
            return
        
        # Path-based profiles
        with open(path, 'w') as f:
            if method == 'dotenv':
                for env_var, env_data in profile.items():
                    f.write(f'{env_var}="{":".join(env_data)}"\n')
            elif method == 'cmake':
                for env_var, env_data in profile.items():
                    f.write(f'set(ENV{{{env_var}}} "{":".join(env_data)}")\n')

    def import_module(self, mod_name: str, command: str):
        """
        Create a module by detecting environment changes before/after running a command.
        
        :param mod_name: Name of the module to create
        :param command: Command to execute and analyze
        """
        print(f"Importing module '{mod_name}' from command: {command}")
        
        # Environment variables to track
        env_vars_to_track = ['PATH', 'LD_LIBRARY_PATH', 'LIBRARY_PATH',
                            'INCLUDE', 'CPATH', 'PKG_CONFIG_PATH', 'CMAKE_PREFIX_PATH',
                            'JAVA_HOME', 'PYTHONPATH', 'CFLAGS', 'LDFLAGS']
        
        # Capture environment before command execution
        env_before = {}
        for env_var in env_vars_to_track:
            env_before[env_var] = os.environ.get(env_var, '')
        
        # Execute the command in a shell that will preserve the environment changes
        # We need to source the command and then print the environment
        shell_script = f"""#!/bin/bash
# Source the original environment
{command}
# Print environment variables we care about
echo "=== ENV_START ==="
"""
        
        for env_var in env_vars_to_track:
            shell_script += f'echo "{env_var}=${{{env_var}}}"\n'
        
        shell_script += 'echo "=== ENV_END ==="'
        
        # Execute the shell script with interactive shell to preserve functions
        exec_info = LocalExecInfo(collect_output=True)
        shell = os.environ.get('SHELL', '/bin/bash')
        executor = Exec(f'{shell} -i -c \'{shell_script}\'', exec_info)
        executor.run()
        
        # Check exit code (it's a dict with hostname keys)
        exit_code = executor.exit_code.get('localhost', 0) if isinstance(executor.exit_code, dict) else executor.exit_code
        if exit_code != 0:
            print(f"Warning: Command exited with non-zero code {exit_code}")
            if executor.stderr:
                stderr_text = ""
                if isinstance(executor.stderr, dict):
                    stderr_text = executor.stderr.get('localhost', '') or ""
                else:
                    stderr_text = executor.stderr or ""
                print(f"STDERR: {stderr_text}")
        
        # Handle different stdout formats (string or dict)
        stdout_text = ""
        if isinstance(executor.stdout, dict):
            stdout_text = executor.stdout.get('localhost', '') or ""
        else:
            stdout_text = executor.stdout or ""
        
        # Parse the environment variables from between the markers
        env_after = {}
        for env_var in env_vars_to_track:
            env_after[env_var] = ''
        
        lines = stdout_text.split('\n')
        in_env_section = False
        
        for line in lines:
            line = line.strip()
            if line == "=== ENV_START ===":
                in_env_section = True
                continue
            elif line == "=== ENV_END ===":
                in_env_section = False
                continue
            elif in_env_section and '=' in line:
                try:
                    var_name, var_value = line.split('=', 1)
                    if var_name in env_vars_to_track:
                        env_after[var_name] = var_value
                except ValueError:
                    continue
        
        # Calculate differences
        env_changes = {}
        for env_var in env_vars_to_track:
            before = env_before[env_var]
            after = env_after[env_var]
            
            if before != after:
                # Split paths and find new additions
                before_paths = set(before.split(':') if before else [])
                after_paths = after.split(':') if after else []
                
                # Remove empty strings
                before_paths.discard('')
                after_paths = [p for p in after_paths if p]
                
                # Find new paths that were added
                new_paths = []
                for path in after_paths:
                    if path not in before_paths:
                        new_paths.append(path)
                
                if new_paths:
                    env_changes[env_var] = new_paths
        
        if not env_changes:
            print("No environment changes detected - creating empty module")
        else:
            print(f"Detected {len(env_changes)} environment variable changes")
        
        # Create the module
        self.create_module(mod_name)
        
        # Update the module configuration with detected changes
        yaml_file = self.modules_dir / f'{mod_name}.yaml'
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)
        
        # Add the command to the config
        config['command'] = command
        
        # Update prepends with detected changes
        for env_var, new_paths in env_changes.items():
            if 'prepends' not in config:
                config['prepends'] = {}
            if env_var not in config['prepends']:
                config['prepends'][env_var] = []
            
            # Prepend the new paths (reverse order to maintain precedence)
            for path in reversed(new_paths):
                if path not in config['prepends'][env_var]:
                    config['prepends'][env_var].insert(0, path)
        
        # Save the updated configuration
        with open(yaml_file, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        
        # Regenerate the TCL file
        self._generate_tcl_file(mod_name)
        
        print(f"Module '{mod_name}' imported successfully")
        
    def update_module(self, mod_name: Optional[str] = None):
        """
        Update a module by re-running its stored command.
        
        :param mod_name: Module name (optional, uses current if None)
        """
        if mod_name is None:
            mod_name = self.jarvis_config.get_current_module()
            if not mod_name:
                raise ValueError("No current module set. Use 'jarvis mod cd <module>' or specify module name")
        
        if not self._module_exists(mod_name):
            raise ValueError(f"Module '{mod_name}' does not exist")
        
        # Load the module configuration to get the stored command
        yaml_file = self.modules_dir / f'{mod_name}.yaml'
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)
        
        stored_command = config.get('command')
        if not stored_command:
            raise ValueError(f"Module '{mod_name}' has no stored command - cannot update")
        
        print(f"Updating module '{mod_name}' with stored command: {stored_command}")
        
        # Remove the existing module (except the package directory)
        yaml_file = self.modules_dir / f'{mod_name}.yaml'
        tcl_file = self.modules_dir / mod_name
        
        if tcl_file.exists():
            tcl_file.unlink()
        
        # Re-import the module with the stored command
        self.import_module(mod_name, stored_command)
        
    def list_modules(self):
        """List all available modules."""
        if not self.modules_dir.exists():
            print("No modules found")
            return
            
        yaml_files = list(self.modules_dir.glob('*.yaml'))
        if not yaml_files:
            print("No modules found")
            return
        
        current_module = self.jarvis_config.get_current_module()
        
        print("Available modules:")
        for yaml_file in sorted(yaml_files):
            mod_name = yaml_file.stem
            marker = " *" if mod_name == current_module else "  "
            print(f"{marker} {mod_name}")

    def add_dependency(self, mod_name: Optional[str], dep_name: str):
        """
        Add a module dependency.

        :param mod_name: Module name (None for current module)
        :param dep_name: Dependency module name to add
        """
        # Use current module if not specified
        if mod_name is None:
            mod_name = self.jarvis_config.get_current_module()
            if not mod_name:
                logger.error("No current module set. Please specify a module name or use 'jarvis mod cd <mod_name>' first.")
                return

        # Check if module exists
        if not self._module_exists(mod_name):
            logger.error(f"Module '{mod_name}' does not exist")
            return

        # Load YAML configuration
        yaml_file = self.modules_dir / f'{mod_name}.yaml'
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)

        # Ensure deps section exists
        if 'deps' not in config:
            config['deps'] = {}

        # Add dependency
        config['deps'][dep_name] = True

        # Save updated configuration
        with open(yaml_file, 'w') as f:
            yaml.dump(config, f, default_flow_style=False, sort_keys=False)

        # Regenerate TCL file
        self._generate_tcl_file(mod_name)

        logger.success(f"Added dependency '{dep_name}' to module '{mod_name}'")

    def remove_dependency(self, mod_name: Optional[str], dep_name: str):
        """
        Remove a module dependency.

        :param mod_name: Module name (None for current module)
        :param dep_name: Dependency module name to remove
        """
        # Use current module if not specified
        if mod_name is None:
            mod_name = self.jarvis_config.get_current_module()
            if not mod_name:
                logger.error("No current module set. Please specify a module name or use 'jarvis mod cd <mod_name>' first.")
                return

        # Check if module exists
        if not self._module_exists(mod_name):
            logger.error(f"Module '{mod_name}' does not exist")
            return

        # Load YAML configuration
        yaml_file = self.modules_dir / f'{mod_name}.yaml'
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)

        # Check if dependency exists
        if 'deps' not in config or dep_name not in config['deps']:
            logger.warning(f"Dependency '{dep_name}' not found in module '{mod_name}'")
            return

        # Remove dependency
        del config['deps'][dep_name]

        # Save updated configuration
        with open(yaml_file, 'w') as f:
            yaml.dump(config, f, default_flow_style=False, sort_keys=False)

        # Regenerate TCL file
        self._generate_tcl_file(mod_name)

        logger.success(f"Removed dependency '{dep_name}' from module '{mod_name}'")

    def _module_exists(self, mod_name: str) -> bool:
        """Check if a module exists."""
        yaml_file = self.modules_dir / f'{mod_name}.yaml'
        return yaml_file.exists()
        
    def _generate_tcl_file(self, mod_name: str):
        """Generate TCL modulefile from YAML configuration."""
        yaml_file = self.modules_dir / f'{mod_name}.yaml'
        tcl_file = self.modules_dir / mod_name
        
        # Load YAML configuration
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)
        
        # Generate TCL content
        tcl_content = ['#%Module1.0']
        
        # Add documentation
        doc = config.get('doc', {})
        if 'Name' in doc:
            tcl_content.append(f"module-whatis 'Name: {doc['Name']}'")
        if 'Version' in doc:
            tcl_content.append(f"module-whatis 'Version: {doc['Version']}'")
        if 'doc' in doc:
            tcl_content.append(f"module-whatis 'doc: {doc['doc']}'")
        
        # Add dependencies
        deps = config.get('deps', {})
        for dep_name, enabled in deps.items():
            if enabled:
                tcl_content.append(f"module load {dep_name}")
        
        # Add prepend paths
        prepends = config.get('prepends', {})
        for env_var, paths in prepends.items():
            for path in paths:
                tcl_content.append(f"prepend-path {env_var} {path}")
        
        # Add set environment variables
        setenvs = config.get('setenvs', {})
        for env_var, value in setenvs.items():
            tcl_content.append(f"setenv {env_var} {value}")
        
        # Write TCL file
        with open(tcl_file, 'w') as f:
            f.write('\n'.join(tcl_content) + '\n')
            
    def _get_env(self, env_var: str) -> str:
        """Get environment variable value."""
        return os.environ.get(env_var, '')
        
    def _output_profile(self, profile: Dict[str, List[str]], path: Optional[str], method: str):
        """Output environment profile in specified format."""
        if method == 'clion':
            # CLion format - semicolon separated list
            prof_list = [f'{env_var}={":".join(env_data)}'
                        for env_var, env_data in profile.items()]
            print(';'.join(prof_list))
        elif method == 'vscode':
            # VSCode format - JSON environment block
            vars_list = [f'  "{env_var}": "{":".join(env_data)}"' 
                        for env_var, env_data in profile.items()]
            print('"environment": {')
            print(',\n'.join(vars_list))
            print('}')
        
        if path is None:
            return
        
        # Path-based profiles
        with open(path, 'w') as f:
            if method == 'dotenv':
                # .env format
                for env_var, env_data in profile.items():
                    f.write(f'{env_var}="{":".join(env_data)}"\n')
            elif method == 'cmake':
                # CMake format
                for env_var, env_data in profile.items():
                    f.write(f'set(ENV{{{env_var}}} "{":".join(env_data)}")\n')
```

### `jarvis_cd/core/pipeline.py`

```python
"""
Pipeline management for Jarvis-CD.
Provides the consolidated Pipeline class that combines pipeline creation, loading, and execution.
"""

import os
import yaml
import copy
from pathlib import Path
from typing import Dict, Any, List, Optional
from jarvis_cd.core.config import load_class, Jarvis
from jarvis_cd.util.logger import logger
from jarvis_cd.util.hostfile import Hostfile


class Pipeline:
    """
    Consolidated pipeline management class.
    Handles pipeline creation, loading, running, and lifecycle management.
    """
    
    def __init__(self, name: str = None):
        """
        Initialize pipeline instance.

        :param name: Pipeline name (optional for new pipelines)
        """
        self.jarvis = Jarvis.get_instance()
        self.name = name
        self.packages = []
        self.interceptors = {}  # Store pipeline-level interceptors by name
        self.env = {}
        self.created_at = None
        self.last_loaded_file = None

        # Container parameters
        self.container_build = ""  # Empty string means use pre-built image (no augment_container)
        self.container_image = ""  # Image to use when container_build is empty
        self.container_engine = "podman"  # Default container engine
        self.container_base = "iowarp/iowarp-build:latest"  # Base image (only used when container_build is set)
        self.container_ssh_port = 2222  # Default SSH port for containers
        self.container_extensions = {}  # Custom extensions to Docker compose file

        # Hostfile parameter (None means use global jarvis hostfile)
        self.hostfile = None

        # Load existing pipeline if name is provided
        if name:
            self.load()

    def get_hostfile(self) -> Hostfile:
        """
        Get the effective hostfile for this pipeline.
        Falls back to global jarvis hostfile if pipeline hostfile is not set.

        :return: Hostfile object
        """
        if self.hostfile:
            return self.hostfile
        return self.jarvis.hostfile

    def is_containerized(self) -> bool:
        """
        Check if this pipeline uses containers.

        :return: True if pipeline uses containers (either build or pre-built image)
        """
        return bool(self.container_build or self.container_image)

    def get_container_image(self) -> str:
        """
        Get the container image name to use.

        :return: Image name (container_image if set, otherwise container_build)
        """
        return self.container_image if self.container_image else self.container_build

    def create(self, pipeline_name: str):
        """
        Create a new pipeline.

        :param pipeline_name: Name of the pipeline to create
        """
        self.name = pipeline_name

        # Create all three directories for the pipeline
        pipeline_config_dir = self.jarvis.get_pipeline_dir(pipeline_name)
        pipeline_shared_dir = self.jarvis.get_pipeline_shared_dir(pipeline_name)
        pipeline_private_dir = self.jarvis.get_pipeline_private_dir(pipeline_name)

        pipeline_config_dir.mkdir(parents=True, exist_ok=True)
        pipeline_shared_dir.mkdir(parents=True, exist_ok=True)
        pipeline_private_dir.mkdir(parents=True, exist_ok=True)

        # Initialize pipeline state
        self.packages = []
        self.interceptors = {}
        self.env = {}
        self.created_at = str(Path().cwd())
        self.last_loaded_file = None

        # Save pipeline configuration and environment
        self.save()

        # Set as current pipeline
        self.jarvis.set_current_pipeline(pipeline_name)

        print(f"Created pipeline: {pipeline_name}")
        print(f"Config directory: {pipeline_config_dir}")
        print(f"Shared directory: {pipeline_shared_dir}")
        print(f"Private directory: {pipeline_private_dir}")
        
    def load(self, load_type: str = None, pipeline_file: str = None):
        """
        Load pipeline from file or current configuration.
        
        :param load_type: Type of pipeline file (e.g., 'yaml')
        :param pipeline_file: Path to pipeline file
        """
        if load_type and pipeline_file:
            self._load_from_file(load_type, pipeline_file)
        elif self.name:
            self._load_from_config()
        else:
            raise ValueError("No pipeline name or file specified")
    
    def save(self):
        """
        Save pipeline configuration and environment to separate files.

        Creates two YAML files:
        - pipeline.yaml: Contains package/interceptor configuration in script format
        - environment.yaml: Contains environment variables only
        """
        if not self.name:
            raise ValueError("Pipeline name not set")

        pipeline_dir = self.jarvis.get_pipeline_dir(self.name)
        pipeline_dir.mkdir(parents=True, exist_ok=True)

        # Create pipeline configuration in the SAME format as pipeline scripts
        # This allows code reuse by using the same parsing logic
        pipeline_config = {
            'name': self.name,
            'pkgs': [],
            'interceptors': []
        }

        # Add metadata fields
        if self.created_at:
            pipeline_config['created_at'] = self.created_at
        if self.last_loaded_file:
            pipeline_config['last_loaded_file'] = self.last_loaded_file
        # Add container parameters (always save, even if empty/default)
        pipeline_config['container_build'] = self.container_build
        pipeline_config['container_image'] = self.container_image
        pipeline_config['container_engine'] = self.container_engine
        pipeline_config['container_base'] = self.container_base
        pipeline_config['container_ssh_port'] = self.container_ssh_port
        if self.container_extensions:
            pipeline_config['container_extensions'] = self.container_extensions

        # Add hostfile parameter (save path if set, None means use global jarvis hostfile)
        # For containerized pipelines, use the container-mounted path
        if self.hostfile:
            if self.container_build or self.container_image:
                # In container, hostfile will be mounted at /root/.ppi-jarvis/hostfile
                pipeline_config['hostfile'] = "/root/.ppi-jarvis/hostfile"
            else:
                # On host, use actual path
                pipeline_config['hostfile'] = self.hostfile.path if self.hostfile.path else ""
        else:
            pipeline_config['hostfile'] = None

        # Convert packages to script format (pkg_type + config parameters)
        for pkg in self.packages:
            pkg_entry = {
                'pkg_type': pkg['pkg_type']
            }
            # Add pkg_name if different from pkg_type
            if pkg['pkg_id'] != pkg['pkg_name']:
                pkg_entry['pkg_name'] = pkg['pkg_id']

            # Add all config parameters except internal ones
            config = pkg.get('config', {})
            for key, value in config.items():
                pkg_entry[key] = value

            pipeline_config['pkgs'].append(pkg_entry)

        # Convert interceptors to script format
        for interceptor_id, interceptor_def in self.interceptors.items():
            interceptor_entry = {
                'pkg_type': interceptor_def['pkg_type']
            }
            # Add pkg_name if different from pkg_type
            if interceptor_id != interceptor_def['pkg_name']:
                interceptor_entry['pkg_name'] = interceptor_id

            # Add all config parameters
            config = interceptor_def.get('config', {})
            for key, value in config.items():
                interceptor_entry[key] = value

            pipeline_config['interceptors'].append(interceptor_entry)

        # Save pipeline configuration (same format as pipeline scripts)
        config_file = pipeline_dir / 'pipeline.yaml'
        with open(config_file, 'w') as f:
            yaml.dump(pipeline_config, f, default_flow_style=False)

        # Save environment to separate file
        env_file = pipeline_dir / 'environment.yaml'
        with open(env_file, 'w') as f:
            yaml.dump(self.env, f, default_flow_style=False)
    
    def destroy(self, pipeline_name: str = None):
        """
        Destroy a pipeline by removing its directory and configuration.
        If no pipeline name is provided, destroy the current pipeline.
        
        :param pipeline_name: Name of pipeline to destroy (optional)
        """
        # Determine which pipeline to destroy
        if pipeline_name is None:
            if not self.name:
                current_pipeline = self.jarvis.get_current_pipeline()
                if not current_pipeline:
                    print("No current pipeline to destroy. Specify a pipeline name or create/switch to one first.")
                    return
                pipeline_name = current_pipeline
            else:
                pipeline_name = self.name
                
        target_pipeline_dir = self.jarvis.get_pipeline_dir(pipeline_name)
        current_pipeline = self.jarvis.get_current_pipeline()
        is_current = (pipeline_name == current_pipeline)
        
        # Check if pipeline exists
        if not target_pipeline_dir.exists():
            print(f"Pipeline '{pipeline_name}' not found.")
            return
        
        # Try to clean packages first if pipeline is loadable
        config_file = target_pipeline_dir / 'pipeline.yaml'
        if config_file.exists():
            try:
                # Load and clean pipeline
                temp_pipeline = Pipeline(pipeline_name)
                print("Attempting to clean package data before destruction...")
                temp_pipeline.clean()
            except Exception as e:
                print(f"Warning: Could not clean packages before destruction: {e}")
        
        # Remove pipeline directory
        import shutil
        try:
            shutil.rmtree(target_pipeline_dir)
            print(f"Destroyed pipeline: {pipeline_name}")
            
            # Clear current pipeline if we destroyed it
            if is_current:
                config = self.jarvis.config.copy()
                config['current_pipeline'] = None
                self.jarvis.save_config(config)
                print("Cleared current pipeline (destroyed pipeline was active)")
                
        except Exception as e:
            print(f"Error destroying pipeline directory: {e}")
    
    def start(self):
        """Start all packages in the pipeline"""
        from jarvis_cd.util.logger import logger

        logger.pipeline(f"Starting pipeline: {self.name}")

        # Check if pipeline is configured for containerized deployment
        if self.is_containerized():
            # Container deployment mode - deploy to all nodes in hostfile
            self._start_containerized_pipeline()
        else:
            # Standard deployment mode - start each package individually
            for pkg_def in self.packages:
                try:
                    # Print BEGIN message
                    logger.success(f"[{pkg_def['pkg_type']}] [START] BEGIN")

                    pkg_instance = self._load_package_instance(pkg_def, self.env)

                    # Apply interceptors to this package before starting
                    self._apply_interceptors_to_package(pkg_instance, pkg_def)

                    if hasattr(pkg_instance, 'start'):
                        pkg_instance.start()
                    else:
                        logger.warning(f"Package {pkg_def['pkg_id']} has no start method")

                    # Propagate environment changes to next packages
                    self.env.update(pkg_instance.env)

                    # Print END message
                    logger.success(f"[{pkg_def['pkg_type']}] [START] END")

                except Exception as e:
                    logger.error(f"Error starting package {pkg_def['pkg_id']}: {e}")
                    raise RuntimeError(f"Pipeline startup failed at package '{pkg_def['pkg_id']}': {e}") from e
    
    def stop(self):
        """Stop all packages in the pipeline"""
        from jarvis_cd.util.logger import logger

        logger.pipeline(f"Stopping pipeline: {self.name}")

        # Check if pipeline is configured for containerized deployment
        if self.is_containerized():
            # Container deployment mode - stop containers on all nodes
            self._stop_containerized_pipeline()
        else:
            # Standard deployment mode - stop each package individually
            for pkg_def in reversed(self.packages):
                try:
                    # Print BEGIN message
                    logger.success(f"[{pkg_def['pkg_type']}] [STOP] BEGIN")

                    pkg_instance = self._load_package_instance(pkg_def, self.env)

                    if hasattr(pkg_instance, 'stop'):
                        pkg_instance.stop()
                    else:
                        logger.warning(f"Package {pkg_def['pkg_id']} has no stop method")

                    # Print END message
                    logger.success(f"[{pkg_def['pkg_type']}] [STOP] END")

                except Exception as e:
                    logger.error(f"Error stopping package {pkg_def['pkg_id']}: {e}")
    
    def kill(self):
        """Force kill all packages in the pipeline"""
        from jarvis_cd.util.logger import logger

        logger.pipeline(f"Killing pipeline: {self.name}")

        # Check if pipeline is configured for containerized deployment
        if self.is_containerized():
            # Container deployment mode - kill containers on all nodes
            self._kill_containerized_pipeline()
        else:
            # Standard deployment mode - kill each package individually
            for pkg_def in self.packages:
                try:
                    # Print BEGIN message
                    logger.success(f"[{pkg_def['pkg_type']}] [KILL] BEGIN")

                    pkg_instance = self._load_package_instance(pkg_def, self.env)

                    if hasattr(pkg_instance, 'kill'):
                        pkg_instance.kill()
                    else:
                        logger.warning(f"Package {pkg_def['pkg_id']} has no kill method")

                    # Print END message
                    logger.success(f"[{pkg_def['pkg_type']}] [KILL] END")

                except Exception as e:
                    logger.error(f"Error killing package {pkg_def['pkg_id']}: {e}")
    
    def status(self) -> str:
        """Get status of the pipeline and its packages"""
        from jarvis_cd.util.logger import logger, Color

        if not self.name:
            return "No pipeline loaded"

        status_info = [f"Pipeline: {self.name}"]
        status_info.append("Packages:")

        # Show status for all packages
        for pkg_def in self.packages:
            try:
                # Print BEGIN message
                logger.success(f"[{pkg_def['pkg_type']}] [STATUS] BEGIN")

                pkg_instance = self._load_package_instance(pkg_def, self.env)

                if pkg_instance and hasattr(pkg_instance, 'status'):
                    pkg_status = pkg_instance.status()
                    status_info.append(f"  {pkg_def['pkg_id']}: {pkg_status}")
                else:
                    status_info.append(f"  {pkg_def['pkg_id']}: no status method")

                # Print END message
                logger.success(f"[{pkg_def['pkg_type']}] [STATUS] END")

            except Exception as e:
                status_info.append(f"  {pkg_def['pkg_id']}: error ({e})")

        return "\n".join(status_info)
    
    def run(self, load_type: Optional[str] = None, pipeline_file: Optional[str] = None):
        """
        Run the pipeline (start all packages, then stop them).
        Optionally load a pipeline file first.

        :param load_type: Type of pipeline file to load (e.g., 'yaml')
        :param pipeline_file: Path to pipeline file to load and run
        """
        try:
            # Load pipeline file if specified
            if load_type and pipeline_file:
                self.load(load_type, pipeline_file)

            self.start()
            logger.pipeline("Pipeline started successfully. Stopping packages...")
            self.stop()
        except Exception as e:
            logger.error(f"Error during pipeline run: {e}")
            logger.info("Attempting to stop packages...")
            try:
                self.stop()
            except Exception as stop_error:
                logger.error(f"Error during cleanup: {stop_error}")
            # Re-raise the original error after cleanup
            raise

    def build_container_if_needed(self):
        """
        Build container image if pipeline has container configuration.
        This must be called BEFORE configure_all_packages() because package configuration
        may need the container image to already exist.

        Only builds if container_build is set. If container_image is set (pre-built image),
        skips the build process entirely.

        Returns True if container was modified and rebuilt, False otherwise.
        """
        # Skip if not using containers at all
        if not self.is_containerized():
            return False

        # Skip if using pre-built image (container_image is set but not container_build)
        if self.container_image and not self.container_build:
            print(f"Using pre-built container image: {self.container_image}")
            return False

        print(f"Building container image: {self.container_build}")

        # Track whether any packages were added
        self._container_modified = False

        # Build container incrementally by adding each package
        for pkg_def in self.packages:
            deploy_mode = pkg_def['config'].get('deploy_mode', 'default')
            if deploy_mode == 'container':
                self._add_package_to_container_build(pkg_def)

        # Also handle interceptors
        for interceptor_id, interceptor_def in self.interceptors.items():
            deploy_mode = interceptor_def.get('config', {}).get('deploy_mode', 'default')
            if deploy_mode == 'container':
                self._add_package_to_container_build(interceptor_def)

        # Build the final container image only if modified
        container_was_modified = self._container_modified
        if container_was_modified and self._get_container_dockerfile_path().exists():
            self._build_container_image()
            print(f"Container image built: {self.get_container_image()}")

        return container_was_modified

    def _add_package_to_container_build(self, pkg_def: Dict[str, Any]):
        """
        Add a package to the container build by calling augment_container().
        This is separate from configuration - it only builds the container.

        :param pkg_def: Package definition dictionary
        """
        pkg_type = pkg_def['pkg_type']
        deploy_mode = pkg_def['config'].get('deploy_mode', 'default')

        # Check if package is already in container
        is_installed, has_conflict = self._check_package_in_container(pkg_type, deploy_mode)

        if has_conflict:
            manifest = self._load_container_manifest()
            installed_mode = manifest[pkg_type]
            raise ValueError(
                f"Package '{pkg_type}' is already installed in container '{self.get_container_image()}' "
                f"with deploy_mode='{installed_mode}', but pipeline requires deploy_mode='{deploy_mode}'. "
                f"Different deploy modes for the same package in one container are not allowed."
            )

        if is_installed:
            print(f"Package {pkg_type} already in container (deploy_mode={deploy_mode})")
            return

        # Load package instance to call augment_container
        try:
            pkg_instance = self._load_package_instance(pkg_def, self.env)

            if hasattr(pkg_instance, 'augment_container'):
                dockerfile_commands = pkg_instance.augment_container()

                if dockerfile_commands:
                    self._add_package_to_container(pkg_type, deploy_mode, dockerfile_commands)
                    self._container_modified = True  # Mark that container was modified
                    print(f"Added {pkg_type} to container")
                else:
                    print(f"Warning: {pkg_type}.augment_container() returned empty commands")
            else:
                print(f"Warning: {pkg_type} does not have augment_container() method")

        except Exception as e:
            print(f"Error adding {pkg_type} to container: {e}")
            raise

    def configure_all_packages(self):
        """
        Configure all packages and interceptors in the pipeline.
        This method loads each package/interceptor instance and calls its configure() method,
        then updates the pipeline environment and saves the configuration.
        """
        print("Configuring interceptors and packages...")

        # Configure interceptors
        for interceptor_id, interceptor_def in self.interceptors.items():
            self._configure_package_instance(interceptor_def, "interceptor")

        # Configure packages
        for pkg_def in self.packages:
            self._configure_package_instance(pkg_def, "package")

        # Save pipeline after configuration
        self.save()
        print("Pipeline configuration saved")

    def update(self, rebuild_container: bool = False, no_cache: bool = False):
        """
        Update pipeline by reconfiguring all packages with their existing configurations.
        This is useful when parts of the pipeline get corrupted or the environment changes.

        :param rebuild_container: Force rebuild of container if containerized
        :param no_cache: Use --no-cache flag when rebuilding container
        """
        # Reconfigure all packages with their existing configurations
        print("Reconfiguring pipeline packages with existing configurations...")
        container_was_modified = self.build_container_if_needed()
        self.configure_all_packages()

        # Handle forced container rebuild if explicitly requested
        if rebuild_container and self.is_containerized():
            from jarvis_cd.shell.exec_factory import Exec
            from jarvis_cd.shell.exec_info import LocalExecInfo

            containers_dir = Path.home() / '.ppi-jarvis' / 'containers'
            dockerfile_path = containers_dir / f'{self.get_container_image()}.Dockerfile'

            if not dockerfile_path.exists():
                raise FileNotFoundError(f"Dockerfile not found at {dockerfile_path}")

            # Use the pipeline's container engine
            use_engine = self.container_engine.lower()

            no_cache_flag = " --no-cache" if no_cache else ""
            build_cmd = f"{use_engine} build{no_cache_flag} -t {self.get_container_image()} -f {dockerfile_path} {containers_dir}"

            print(f"Force rebuilding container '{self.get_container_image()}' using {use_engine}...")
            Exec(build_cmd, LocalExecInfo()).run()
            print(f"Container '{self.get_container_image()}' rebuilt successfully")
        elif container_was_modified:
            print(f"Container '{self.get_container_image()}' was automatically rebuilt due to manifest changes")

    def _configure_package_instance(self, pkg_def: Dict[str, Any], pkg_type_label: str):
        """
        Configure a single package or interceptor instance.

        :param pkg_def: Package definition dictionary
        :param pkg_type_label: Label for logging ("package" or "interceptor")
        """
        from jarvis_cd.util.logger import logger, Color

        try:
            # Print BEGIN message
            logger.success(f"[{pkg_def['pkg_type']}] [CONFIGURE] BEGIN")

            pkg_instance = self._load_package_instance(pkg_def, self.env)
            if hasattr(pkg_instance, 'configure'):
                # Configure the package with its config
                updated_config = pkg_instance.configure(**pkg_instance.config)

                # Update pkg_def with the final config from the package
                if updated_config:
                    pkg_def['config'] = updated_config
                else:
                    pkg_def['config'] = pkg_instance.config.copy()

                # Update the package environment in the pipeline's env
                self.env.update(pkg_instance.env)

            # Print END message
            logger.success(f"[{pkg_def['pkg_type']}] [CONFIGURE] END")

        except Exception as e:
            import traceback
            logger.error(f"Error configuring {pkg_type_label} {pkg_def['pkg_id']}: {e}")
            logger.error("Full traceback:")
            traceback.print_exc()
            raise
    
    def append(self, package_spec: str, package_alias: Optional[str] = None):
        """
        Append a package to the pipeline.
        
        :param package_spec: Package specification (repo.pkg or just pkg)
        :param package_alias: Optional alias for the package
        """
        if not self.name:
            raise ValueError("No pipeline loaded. Create one with create() first")
            
        # Parse package specification
        if '.' in package_spec:
            repo_name, pkg_name = package_spec.split('.', 1)
        else:
            # Try to find package in available repos
            pkg_name = package_spec
            full_spec = self.jarvis.find_package(pkg_name)
            if not full_spec:
                raise ValueError(f"Package not found: {pkg_name}")
            package_spec = full_spec
            
        # Determine package ID
        if package_alias:
            pkg_id = package_alias
        else:
            pkg_id = pkg_name
            
        # Check for duplicate package IDs
        existing_ids = [pkg['pkg_id'] for pkg in self.packages]
        if pkg_id in existing_ids:
            raise ValueError(f"Package ID already exists in pipeline: {pkg_id}")
            
        # Get default configuration from package
        default_config = self._get_package_default_config(package_spec)
        
        # Validate that all required parameters have values
        self._validate_required_config(package_spec, default_config)
        
        # Add package to pipeline
        package_entry = {
            'pkg_type': package_spec,
            'pkg_id': pkg_id,
            'pkg_name': pkg_name,
            'global_id': f"{self.name}.{pkg_id}",
            'config': default_config
        }
        
        self.packages.append(package_entry)
        
        # Save updated configuration
        self.save()
            
        print(f"Added package {package_spec} as {pkg_id} to pipeline")
    
    def rm(self, package_spec: str):
        """
        Remove a package from the pipeline.
        
        :param package_spec: Package specification to remove (pkg_id)
        """
        # Find and remove the package
        package_found = False
        
        for i, pkg_def in enumerate(self.packages):
            if pkg_def['pkg_id'] == package_spec:
                removed_package = self.packages.pop(i)
                package_found = True
                break
                
        if not package_found:
            # List available packages to help the user
            available_ids = [pkg['pkg_id'] for pkg in self.packages]
            if available_ids:
                print(f"Package '{package_spec}' not found in pipeline.")
                print(f"Available packages: {', '.join(available_ids)}")
            else:
                print("No packages in pipeline.")
            return
            
        # Save updated configuration
        self.save()
            
        print(f"Removed package '{removed_package['pkg_id']}' ({removed_package['pkg_type']}) from pipeline '{self.name}'")
    
    def clean(self):
        """Clean all data for packages in the pipeline"""
        from jarvis_cd.util.logger import logger, Color

        logger.pipeline(f"Cleaning pipeline: {self.name}")

        # Clean each package
        for pkg_def in self.packages:
            try:
                # Print BEGIN message
                logger.success(f"[{pkg_def['pkg_type']}] [CLEAN] BEGIN")

                pkg_instance = self._load_package_instance(pkg_def, self.env)

                if hasattr(pkg_instance, 'clean'):
                    pkg_instance.clean()
                else:
                    logger.warning(f"Package {pkg_def['pkg_id']} has no clean method")

                # Print END message
                logger.success(f"[{pkg_def['pkg_type']}] [CLEAN] END")

            except Exception as e:
                logger.error(f"Error cleaning package {pkg_def['pkg_id']}: {e}")
    
    def configure_package(self, pkg_id: str, config_args: List[str]):
        """
        Configure a specific package in the pipeline.

        :param pkg_id: Package ID to configure
        :param config_args: Configuration arguments as command-line args (e.g., ['--nprocs', '4', 'block=32m'])
        """
        # Find package in pipeline
        pkg_def = None
        for pkg in self.packages:
            if pkg['pkg_id'] == pkg_id:
                pkg_def = pkg
                break

        if not pkg_def:
            raise ValueError(f"Package not found: {pkg_id}")

        # Load package instance
        pkg_instance = self._load_package_instance(pkg_def, self.env)

        try:
            # Use PkgArgParse to parse and convert arguments
            argparse = pkg_instance.get_argparse()
            # Parse arguments - prepend 'configure' command
            parsed_args = argparse.parse(['configure'] + config_args)
            converted_args = argparse.kwargs

            # Update package configuration with converted values
            pkg_def['config'].update(converted_args)

            # Configure the package instance
            if hasattr(pkg_instance, 'configure'):
                pkg_instance.configure(**converted_args)
                print(f"Configured package {pkg_id} successfully")
            else:
                print(f"Package {pkg_id} has no configure method")

            # Build container if pipeline is containerized
            if self.is_containerized():
                self._add_package_to_container_image(pkg_instance, pkg_def)

            # Save updated pipeline
            self.save()
            print(f"Saved configuration for {pkg_id}")

        except Exception as e:
            print(f"Error configuring package {pkg_id}: {e}")
            # Show available configuration options
            argparse = pkg_instance.get_argparse()
            argparse.print_help('configure')
    
    def show_package_readme(self, pkg_id: str):
        """
        Show README for a specific package in the pipeline.
        
        :param pkg_id: Package ID to show README for
        """
        # Find package in pipeline
        pkg_def = None
        for pkg in self.packages:
            if pkg['pkg_id'] == pkg_id:
                pkg_def = pkg
                break
                
        if not pkg_def:
            raise ValueError(f"Package not found: {pkg_id}")
        
        # Load package instance and delegate to it
        try:
            pkg_instance = self._load_package_instance(pkg_def, self.env)
            pkg_instance.show_readme()
        except Exception as e:
            print(f"Error showing README for package {pkg_id}: {e}")
    
    def show_package_paths(self, pkg_id: str, path_flags: Dict[str, bool]):
        """
        Show directory paths for a specific package in the pipeline.
        
        :param pkg_id: Package ID to show paths for
        :param path_flags: Dictionary of path flags to show
        """
        # Find package in pipeline
        pkg_def = None
        for pkg in self.packages:
            if pkg['pkg_id'] == pkg_id:
                pkg_def = pkg
                break
                
        if not pkg_def:
            raise ValueError(f"Package not found: {pkg_id}")
        
        # Load package instance and delegate to it
        try:
            pkg_instance = self._load_package_instance(pkg_def, self.env)
            pkg_instance.show_paths(path_flags)
        except Exception as e:
            print(f"Error showing paths for package {pkg_id}: {e}")
    
    def _load_from_config(self):
        """
        Load pipeline from its configuration files.

        Loads from two separate files:
        - pipeline.yaml: Contains package/interceptor configuration in script format
        - environment.yaml: Contains environment variables only
        """
        pipeline_dir = self.jarvis.get_pipeline_dir(self.name)
        config_file = pipeline_dir / 'pipeline.yaml'

        if not config_file.exists():
            raise FileNotFoundError(f"Pipeline configuration not found: {config_file}")

        # Load pipeline configuration (in script format)
        with open(config_file, 'r') as f:
            pipeline_config = yaml.safe_load(f)

        # Extract metadata
        self.created_at = pipeline_config.get('created_at')
        self.last_loaded_file = pipeline_config.get('last_loaded_file')

        # Load container parameters
        self.container_build = pipeline_config.get('container_build', pipeline_config.get('container_name', ''))  # Backwards compat
        self.container_image = pipeline_config.get('container_image', '')
        self.container_engine = pipeline_config.get('container_engine', 'podman')
        self.container_base = pipeline_config.get('container_base', 'iowarp/iowarp-build:latest')
        self.container_ssh_port = pipeline_config.get('container_ssh_port', 2222)
        self.container_extensions = pipeline_config.get('container_extensions', {})

        # Load hostfile parameter (None means use global jarvis hostfile)
        hostfile_path = pipeline_config.get('hostfile')
        if hostfile_path:
            self.hostfile = Hostfile(path=hostfile_path)
        else:
            self.hostfile = None

        # Initialize packages and interceptors
        self.packages = []
        self.interceptors = {}

        # Process interceptors from script format
        interceptors_list = pipeline_config.get('interceptors', [])
        for interceptor_def in interceptors_list:
            interceptor_id = interceptor_def.get('pkg_name', interceptor_def['pkg_type'].split('.')[-1])
            interceptor_entry = self._process_package_definition(interceptor_def, interceptor_id)
            self.interceptors[interceptor_id] = interceptor_entry

        # Process packages from script format
        for pkg_def in pipeline_config.get('pkgs', []):
            pkg_id = pkg_def.get('pkg_name', pkg_def['pkg_type'].split('.')[-1])
            package_entry = self._process_package_definition(pkg_def, pkg_id)
            self.packages.append(package_entry)

        # Load environment from separate file
        env_file = pipeline_dir / 'environment.yaml'
        if env_file.exists():
            with open(env_file, 'r') as f:
                env_config = yaml.safe_load(f)
                if env_config:
                    self.env = env_config
                else:
                    self.env = {}
        else:
            self.env = {}
    
    def _load_from_file(self, load_type: str, pipeline_file: str):
        """Load pipeline from a file"""
        if load_type != 'yaml':
            raise ValueError(f"Unsupported pipeline file type: {load_type}")
            
        pipeline_file = Path(pipeline_file)
        if not pipeline_file.exists():
            raise FileNotFoundError(f"Pipeline file not found: {pipeline_file}")
            
        # Load pipeline definition
        with open(pipeline_file, 'r') as f:
            pipeline_def = yaml.safe_load(f)
            
        self.name = pipeline_def.get('name', pipeline_file.stem)
        
        # Handle environment - can be a string (named env) or missing (auto-build)
        # Inline dictionaries are NOT supported
        env_field = pipeline_def.get('env')

        if env_field is None:
            # No env field defined - automatically build environment
            try:
                from jarvis_cd.core.environment import EnvironmentManager
                env_manager = EnvironmentManager(self.jarvis)
                self.env = env_manager._capture_current_environment()
                print(f"Auto-built environment with {len(self.env)} variables (no 'env' field in pipeline)")
            except Exception as e:
                print(f"Warning: Could not auto-build environment: {e}")
                self.env = {}
        elif isinstance(env_field, str):
            # Reference to named environment
            env_name = env_field
            try:
                from jarvis_cd.core.environment import EnvironmentManager
                env_manager = EnvironmentManager(self.jarvis)
                self.env = env_manager.load_named_environment(env_name)
            except Exception as e:
                # Named environment doesn't exist - build it automatically
                print(f"Named environment '{env_name}' does not exist. Building it now...")
                try:
                    from jarvis_cd.core.environment import EnvironmentManager
                    env_manager = EnvironmentManager(self.jarvis)
                    # Build the named environment from current environment (no additional args)
                    env_manager.build_named_environment(env_name, [])
                    # Now load the newly created environment
                    self.env = env_manager.load_named_environment(env_name)
                    print(f"Built named environment '{env_name}' with {len(self.env)} variables")
                except Exception as build_error:
                    print(f"Warning: Could not build named environment '{env_name}': {build_error}")
                    self.env = {}
        elif isinstance(env_field, dict):
            # Inline environment dictionaries are not allowed
            raise ValueError(
                "Inline environment dictionaries are not supported in pipeline YAML files.\n"
                "The 'env' field must be either:\n"
                "  1. A string referencing a named environment (e.g., env: production_environment)\n"
                "  2. Omitted to auto-build from the current shell environment\n\n"
                "To use custom environment variables:\n"
                "  Option 1 - Create a named environment (reusable across pipelines):\n"
                "    1. Create: jarvis env build <env_name> <commands...>\n"
                "    2. Reference in pipeline YAML: env: <env_name>\n\n"
                "  Option 2 - Build environment for current pipeline (pipeline-specific):\n"
                "    1. Load pipeline: jarvis ppl load yaml <pipeline.yaml>\n"
                "    2. Build environment: jarvis ppl env build <commands...>\n"
                "    3. Pipeline will use the built environment automatically"
            )
        else:
            raise ValueError(
                f"Invalid 'env' field type: {type(env_field).__name__}. "
                "The 'env' field must be either a string (named environment) or omitted (auto-build)."
            )
        
        # Initialize other attributes
        self.created_at = str(Path().cwd())
        self.last_loaded_file = str(pipeline_file.absolute())
        self.packages = []
        self.interceptors = {}  # Store pipeline-level interceptors by name

        # Load container parameters
        self.container_build = pipeline_def.get('container_build', pipeline_def.get('container_name', ''))  # Backwards compat
        self.container_image = pipeline_def.get('container_image', '')
        self.container_engine = pipeline_def.get('container_engine', 'podman')
        self.container_base = pipeline_def.get('container_base', 'iowarp/iowarp-build:latest')
        self.container_ssh_port = pipeline_def.get('container_ssh_port', 2222)
        self.container_extensions = pipeline_def.get('container_extensions', {})

        # Load hostfile parameter (None means use global jarvis hostfile)
        hostfile_path = pipeline_def.get('hostfile')
        if hostfile_path:
            self.hostfile = Hostfile(path=hostfile_path)
        else:
            self.hostfile = None

        # Debug output
        print(f"DEBUG: Loaded container_name='{self.get_container_image()}'")
        print(f"DEBUG: Loaded container_base='{self.container_base}'")
        
        # Process interceptors
        interceptors_list = pipeline_def.get('interceptors', [])
        for interceptor_def in interceptors_list:
            interceptor_id = interceptor_def.get('pkg_name', interceptor_def['pkg_type'].split('.')[-1])
            interceptor_entry = self._process_package_definition(interceptor_def, interceptor_id)
            self.interceptors[interceptor_id] = interceptor_entry

        # Process packages
        for pkg_def in pipeline_def.get('pkgs', []):
            pkg_id = pkg_def.get('pkg_name', pkg_def['pkg_type'])
            package_entry = self._process_package_definition(pkg_def, pkg_id)
            self.packages.append(package_entry)
        
        # Validate that interceptor and package IDs are unique
        self._validate_unique_ids()

        # Save pipeline configuration and environment
        self.save()

        # Generate container files if this is a containerized pipeline
        if self.is_containerized():
            print(f"Generating container configuration files...")
            self._generate_pipeline_container_yaml()

            # Check if container needs rebuilding
            needs_rebuild = self._check_container_needs_rebuild()
            self._generate_pipeline_dockerfile()  # Updates manifest

            if needs_rebuild:
                print(f"Container manifest changed, rebuilding...")
                self._build_global_container_image()
            else:
                print(f"Container manifest unchanged, skipping rebuild (use 'jarvis container update {self.get_container_image()}' to force)")

            self._generate_pipeline_compose_file()

        # Set as current pipeline
        self.jarvis.set_current_pipeline(self.name)

        print(f"Loaded pipeline: {self.name}")
        print(f"Packages: {[pkg['pkg_id'] for pkg in self.packages]}")
    
    def _load_package_instance(self, pkg_def: Dict[str, Any], pipeline_env: Optional[Dict[str, str]] = None):
        """
        Load a package instance from package definition.
        
        :param pkg_def: Package definition dictionary
        :param pipeline_env: Pipeline environment variables
        :return: Package instance
        """
        from jarvis_cd.core.pkg import Pkg
        
        pkg_type = pkg_def['pkg_type']
        
        # Find package class
        if '.' in pkg_type:
            # Full specification like "builtin.ior"
            import_parts = pkg_type.split('.')
            repo_name = import_parts[0]
            pkg_name = import_parts[1]
        else:
            # Just package name, search in repos
            full_spec = self.jarvis.find_package(pkg_type)
            if not full_spec:
                raise ValueError(f"Package not found: {pkg_type}")
            import_parts = full_spec.split('.')
            repo_name = import_parts[0]
            pkg_name = import_parts[1]
            
        # Determine class name (convert snake_case to PascalCase)
        class_name = ''.join(word.capitalize() for word in pkg_name.split('_'))
        
        # Load class
        if repo_name == 'builtin':
            repo_path = str(self.jarvis.get_builtin_repo_path())
        else:
            # Find repo path in registered repos
            repo_path = None
            for registered_repo in self.jarvis.repos['repos']:
                if Path(registered_repo).name == repo_name:
                    repo_path = registered_repo
                    break
                    
            if not repo_path:
                raise ValueError(f"Repository not found: {repo_name}")
                
        import_str = f"{repo_name}.{pkg_name}.pkg"
        try:
            pkg_class = load_class(import_str, repo_path, class_name)
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            raise ValueError(
                f"Failed to load package '{pkg_type}':\n"
                f"  Repository: {repo_name}\n"
                f"  Package: {pkg_name}\n"
                f"  Repo path: {repo_path}\n"
                f"  Import string: {import_str}\n"
                f"  Class name: {class_name}\n"
                f"  Error: {e}\n"
                f"  Traceback:\n{error_details}"
            )

        if not pkg_class:
            raise ValueError(f"Package class not found: {class_name} in {import_str}")

        # Create instance with pipeline context
        try:
            pkg_instance = pkg_class(pipeline=self)
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            raise ValueError(
                f"Failed to instantiate package '{pkg_type}':\n"
                f"  Class: {class_name}\n"
                f"  Error during __init__: {e}\n"
                f"  Traceback:\n{error_details}"
            )

        # Set basic attributes
        pkg_instance.pkg_type = pkg_def['pkg_type']
        pkg_instance.pkg_id = pkg_def['pkg_id']
        pkg_instance.global_id = pkg_def['global_id']

        # Initialize directories now that pkg_id is set
        pkg_instance._ensure_directories()

        # Set configuration
        base_config = pkg_def.get('config', {})
        base_config.setdefault('do_dbg', False)
        base_config.setdefault('dbg_port', 50000)
        pkg_instance.config = base_config
            
        # Set up environment variables - mod_env is exact replica of env plus LD_PRELOAD
        if pipeline_env is None:
            pipeline_env = {}

        # env contains everything except LD_PRELOAD
        pkg_instance.env = {k: v for k, v in pipeline_env.items() if k != 'LD_PRELOAD'}

        # mod_env is exact replica of env plus LD_PRELOAD (if it exists)
        pkg_instance.mod_env = pkg_instance.env.copy()
        if 'LD_PRELOAD' in pipeline_env:
            pkg_instance.mod_env['LD_PRELOAD'] = pipeline_env['LD_PRELOAD']

        return pkg_instance
    
    def _process_package_definition(self, pkg_def: Dict[str, Any], pkg_id: str) -> Dict[str, Any]:
        """
        Process a package definition from YAML, merging YAML config with defaults.

        :param pkg_def: Package definition from YAML
        :param pkg_id: Package ID to use
        :return: Complete package entry with merged configuration
        """
        pkg_type = pkg_def['pkg_type']

        # Resolve pkg_type to full specification (repo.package) if not already specified
        if '.' not in pkg_type:
            resolved_type = self.jarvis.find_package(pkg_type)
            if resolved_type:
                pkg_type = resolved_type
            # If not found, keep original (will fail later during loading)

        # Get default configuration from package
        default_config = self._get_package_default_config(pkg_type)

        # Extract config from YAML
        yaml_config = {k: v for k, v in pkg_def.items()
                      if k not in ['pkg_type', 'pkg_name']}

        # Merge YAML config on top of defaults
        merged_config = default_config.copy()
        merged_config.update(yaml_config)

        return {
            'pkg_type': pkg_type,
            'pkg_id': pkg_id,
            'pkg_name': pkg_type.split('.')[-1],
            'global_id': f"{self.name}.{pkg_id}",
            'config': merged_config
        }

    def _get_package_default_config(self, package_spec: str) -> Dict[str, Any]:
        """
        Get default configuration values for a package by parsing with PkgArgParse.
        Equivalent to calling 'configure' with no parameters.
        """
        try:
            # Create a temporary package definition to load the package
            temp_pkg_def = {
                'pkg_type': package_spec,
                'pkg_id': 'temp',
                'pkg_name': package_spec.split('.')[-1],
                'global_id': 'temp.temp',
                'config': {}
            }

            # Load package instance
            pkg_instance = self._load_package_instance(temp_pkg_def)

            # Use PkgArgParse to get defaults by parsing 'configure' with no args
            argparse = pkg_instance.get_argparse()
            argparse.parse(['configure'])
            default_config = argparse.kwargs

            return default_config

        except Exception as e:
            # Package loading failure should be fatal - cannot add package to pipeline
            raise ValueError(f"Failed to load package '{package_spec}': {e}")

        return {}
    
    def _validate_unique_ids(self):
        """
        Validate that interceptor IDs and package IDs are unique within the pipeline.
        """
        # Get all package IDs
        package_ids = {pkg['pkg_id'] for pkg in self.packages}
        
        # Get all interceptor IDs
        interceptor_ids = set(self.interceptors.keys())
        
        # Check for conflicts
        conflicts = package_ids & interceptor_ids
        if conflicts:
            conflict_list = ', '.join(conflicts)
            raise ValueError(f"ID conflicts between packages and interceptors: {conflict_list}. "
                           f"Package and interceptor IDs must be unique within the pipeline.")
    
    def _validate_required_config(self, package_spec: str, config: Dict[str, Any]):
        """
        Validate that all required configuration parameters have values.
        
        :param package_spec: Package specification 
        :param config: Configuration dictionary
        :raises ValueError: If required parameters are missing or None
        """
        try:
            # Create a temporary package definition to load the package
            temp_pkg_def = {
                'pkg_type': package_spec,
                'pkg_id': 'temp',
                'pkg_name': package_spec.split('.')[-1],
                'global_id': 'temp.temp',
                'config': {}
            }
            
            # Load package instance
            pkg_instance = self._load_package_instance(temp_pkg_def)
            
            # Get configuration menu to check for required parameters
            if hasattr(pkg_instance, 'configure_menu'):
                config_menu = pkg_instance.configure_menu()
                if config_menu:
                    missing_required = []
                    for menu_item in config_menu:
                        name = menu_item.get('name')
                        default_value = menu_item.get('default')
                        
                        # If parameter has no default and is not provided in config, it's required
                        if name and default_value is None and (name not in config or config[name] is None):
                            missing_required.append(name)
                    
                    if missing_required:
                        raise ValueError(f"Missing required configuration parameters for {package_spec}: {', '.join(missing_required)}")
                        
        except Exception as e:
            if "Missing required configuration parameters" in str(e):
                raise  # Re-raise our validation error
            # Other errors during validation are not fatal
            print(f"Warning: Could not validate configuration for {package_spec}: {e}")
    
    def _apply_interceptors_to_package(self, pkg_instance, pkg_def):
        """
        Apply interceptors to a package instance during pipeline start.

        :param pkg_instance: The package instance to apply interceptors to
        :param pkg_def: The package definition from pipeline configuration
        """
        from jarvis_cd.util.logger import logger, Color

        # Get interceptors list from package configuration
        interceptors_list = pkg_def.get('config', {}).get('interceptors', [])

        if not interceptors_list:
            return

        logger.warning(f"Applying {len(interceptors_list)} interceptors to {pkg_def['pkg_id']}")

        for interceptor_name in interceptors_list:
            try:
                # Find interceptor in pipeline-level interceptors
                if interceptor_name not in self.interceptors:
                    logger.error(f"Warning: Interceptor '{interceptor_name}' not found in pipeline interceptors")
                    continue

                interceptor_def = self.interceptors[interceptor_name]

                # Print BEGIN message with full interceptor type
                logger.success(f"[{interceptor_def['pkg_type']}] [MODIFY_ENV] BEGIN")

                # Load interceptor instance
                interceptor_instance = self._load_package_instance(interceptor_def, self.env)

                # Verify it's an interceptor and has modify_env method
                if not hasattr(interceptor_instance, 'modify_env'):
                    logger.error(f"Warning: Package '{interceptor_name}' does not have modify_env() method")
                    continue

                # Share the same mod_env reference between interceptor and package
                interceptor_instance.mod_env = pkg_instance.mod_env
                interceptor_instance.env = pkg_instance.env

                # Call modify_env on the interceptor to modify the shared environment
                interceptor_instance.modify_env()

                # The mod_env is shared, so changes are automatically applied to the package

                # Print END message
                logger.success(f"[{interceptor_def['pkg_type']}] [MODIFY_ENV] END")

            except Exception as e:
                logger.error(f"Error applying interceptor '{interceptor_name}': {e}")

    def _get_container_manifest_path(self) -> Path:
        """Get the path to the container manifest file."""
        if not self.is_containerized():
            raise ValueError("Container name not set")
        containers_dir = Path.home() / '.ppi-jarvis' / 'containers'
        containers_dir.mkdir(parents=True, exist_ok=True)
        return containers_dir / f"{self.get_container_image()}.yaml"

    def _get_container_dockerfile_path(self) -> Path:
        """Get the path to the container Dockerfile."""
        if not self.is_containerized():
            raise ValueError("Container name not set")
        containers_dir = Path.home() / '.ppi-jarvis' / 'containers'
        containers_dir.mkdir(parents=True, exist_ok=True)
        return containers_dir / f"{self.get_container_image()}.Dockerfile"

    def _load_container_manifest(self) -> Dict[str, str]:
        """
        Load the container manifest.
        Returns dict mapping pkg_type -> deploy_mode.
        """
        manifest_path = self._get_container_manifest_path()
        if not manifest_path.exists():
            return {}

        with open(manifest_path, 'r') as f:
            manifest = yaml.safe_load(f) or {}
        return manifest

    def _save_container_manifest(self, manifest: Dict[str, str]):
        """
        Save the container manifest.
        manifest is a dict mapping pkg_type -> deploy_mode.
        """
        manifest_path = self._get_container_manifest_path()
        with open(manifest_path, 'w') as f:
            yaml.dump(manifest, f, default_flow_style=False)

    def _check_package_in_container(self, pkg_type: str, deploy_mode: str) -> tuple:
        """
        Check if a package is already installed in the container.

        :param pkg_type: Package type (e.g., 'builtin.ior')
        :param deploy_mode: Deploy mode for this package
        :return: (is_installed, needs_error) tuple
        """
        manifest = self._load_container_manifest()

        if pkg_type not in manifest:
            return (False, False)  # Not installed

        installed_mode = manifest[pkg_type]
        if installed_mode == deploy_mode:
            return (True, False)  # Already installed with same mode

        # Installed with different mode - this is an error
        return (True, True)

    def _add_package_to_container(self, pkg_type: str, deploy_mode: str, dockerfile_commands: str):
        """
        Add a package to the container image.

        :param pkg_type: Package type (e.g., 'builtin.ior')
        :param deploy_mode: Deploy mode for this package
        :param dockerfile_commands: Dockerfile commands to append
        """
        # Update manifest
        manifest = self._load_container_manifest()
        manifest[pkg_type] = deploy_mode
        self._save_container_manifest(manifest)

        # Append to Dockerfile
        dockerfile_path = self._get_container_dockerfile_path()

        # Create Dockerfile with base image if it doesn't exist
        if not dockerfile_path.exists():
            with open(dockerfile_path, 'w') as f:
                f.write(f"FROM {self.container_base}\n\n")
                f.write("# Disable prompt during packages installation\n")
                f.write("ARG DEBIAN_FRONTEND=noninteractive\n\n")

        # Append package installation commands
        with open(dockerfile_path, 'a') as f:
            f.write(f"# Package: {pkg_type} (deploy_mode: {deploy_mode})\n")
            f.write(dockerfile_commands)
            f.write("\n")

        # Add CMD instruction to run pipeline using the shared pkg.yaml
        # This will be overwritten if more packages are added, but the final package will set the correct CMD
        with open(dockerfile_path, 'a') as f:
            f.write("\n# Run pipeline from shared directory\n")
            f.write('CMD ["jarvis", "ppl", "run", "yaml", "/root/.ppi-jarvis/shared/pkg.yaml"]\n')

    def _add_package_to_container_image(self, pkg_instance, pkg_def: Dict[str, Any]):
        """
        Add a configured package to the container image.
        Calls augment_container() on the package instance and appends to Dockerfile.

        :param pkg_instance: Configured package instance
        :param pkg_def: Package definition dictionary
        """
        pkg_type = pkg_def['pkg_type']
        deploy_mode = pkg_def['config'].get('deploy_mode', 'default')

        # Check if package is already in container
        is_installed, has_conflict = self._check_package_in_container(pkg_type, deploy_mode)

        if has_conflict:
            manifest = self._load_container_manifest()
            installed_mode = manifest[pkg_type]
            raise ValueError(
                f"Package '{pkg_type}' is already installed in container '{self.get_container_image()}' "
                f"with deploy_mode='{installed_mode}', but pipeline requires deploy_mode='{deploy_mode}'. "
                f"Different deploy modes for the same package in one container are not allowed."
            )

        if is_installed:
            print(f"Package {pkg_type} already in container (deploy_mode={deploy_mode})")
            return

        # Call augment_container on the instance
        print(f"Adding {pkg_type} to container (deploy_mode={deploy_mode})")

        try:
            if hasattr(pkg_instance, 'augment_container'):
                dockerfile_commands = pkg_instance.augment_container()

                if dockerfile_commands:
                    self._add_package_to_container(pkg_type, deploy_mode, dockerfile_commands)
                    print(f"Added {pkg_type} to container")

                    # Rebuild container image
                    self._build_container_image()
                else:
                    print(f"Warning: {pkg_type}.augment_container() returned empty commands")
            else:
                print(f"Warning: {pkg_type} does not have augment_container() method")

        except Exception as e:
            print(f"Error calling augment_container() for {pkg_type}: {e}")
            raise

    def _build_container_image(self):
        """
        Build the container image using ContainerBuildExec.
        Creates a temporary compose file and uses ContainerBuildExec for the build.
        """
        from jarvis_cd.shell.container_compose_exec import ContainerBuildExec
        from jarvis_cd.shell import LocalExecInfo

        dockerfile_path = self._get_container_dockerfile_path()

        # Create a minimal compose file for building
        compose_content = f"""version: '3.8'

services:
  {self.get_container_image()}:
    build:
      context: {dockerfile_path.parent}
      dockerfile: {dockerfile_path.name}
    image: {self.get_container_image()}
"""

        # Write temporary compose file
        compose_path = dockerfile_path.parent / f"{self.get_container_image()}.compose.yaml"
        with open(compose_path, 'w') as f:
            f.write(compose_content)

        # Use ContainerBuildExec to build
        print(f"Building container image: {self.get_container_image()}")
        prefer_podman = self.container_engine.lower() == 'podman'
        build_exec = ContainerBuildExec(str(compose_path), LocalExecInfo(), prefer_podman=prefer_podman)
        build_exec.run()
        print(f"Container image built: {self.get_container_image()}")

        # Clean up temporary compose file
        compose_path.unlink()

    def _generate_pipeline_container_yaml(self):
        """
        Generate pipeline-wide YAML configuration file for use inside containers.
        This includes all packages and interceptors in the pipeline.

        :return: Path to generated YAML file
        """
        import yaml
        from pathlib import Path

        # Create pipeline configuration with all packages
        pipeline_config = {
            'name': f'{self.name}_container',
            'pkgs': []
        }

        # Add all packages (excluding 'deploy' config)
        for pkg_def in self.packages:
            pkg_entry = {'pkg_type': pkg_def['pkg_type']}
            for key, value in pkg_def['config'].items():
                if key not in ['deploy', 'deploy_mode', 'deploy_ssh_port']:
                    pkg_entry[key] = value
            pipeline_config['pkgs'].append(pkg_entry)

        # Add interceptors if any
        if self.interceptors:
            pipeline_config['interceptors'] = []
            for interceptor_name, interceptor_def in self.interceptors.items():
                interceptor_entry = {'pkg_type': interceptor_def['pkg_type']}
                for key, value in interceptor_def.get('config', {}).items():
                    if key not in ['deploy', 'deploy_mode', 'deploy_ssh_port']:
                        interceptor_entry[key] = value
                pipeline_config['interceptors'].append(interceptor_entry)

        # Write to shared directory
        shared_dir = self.jarvis.get_pipeline_shared_dir(self.name)
        yaml_path = shared_dir / 'pipeline.yaml'
        with open(yaml_path, 'w') as f:
            yaml.dump(pipeline_config, f, default_flow_style=False)

        print(f"Generated pipeline YAML: {yaml_path}")
        return yaml_path

    def _check_container_needs_rebuild(self):
        """
        Check if the container needs to be rebuilt by comparing the current package list
        with the saved manifest.

        :return: True if rebuild is needed, False otherwise
        """
        from pathlib import Path
        import json

        containers_dir = Path.home() / '.ppi-jarvis' / 'containers'
        manifest_path = containers_dir / f'{self.get_container_image()}.manifest'

        # If no manifest exists, need to build
        if not manifest_path.exists():
            return True

        # Load existing manifest
        with open(manifest_path, 'r') as f:
            old_manifest = json.load(f)

        # Create current manifest
        pkg_types = sorted([pkg_def['pkg_type'] for pkg_def in self.packages])
        interceptor_types = sorted([idef['pkg_type'] for idef in self.interceptors.values()])
        current_manifest = {
            'packages': pkg_types,
            'interceptors': interceptor_types,
            'container_base': self.container_base
        }

        # Compare manifests
        return old_manifest != current_manifest

    def _generate_pipeline_dockerfile(self):
        """
        Generate global container Dockerfile in ~/.ppi-jarvis/containers/ by calling
        augment_container() on all packages. This container is built once and reused
        across pipelines with the same container_build name.

        :return: Path to generated Dockerfile
        """
        from pathlib import Path

        # Use global containers directory
        containers_dir = Path.home() / '.ppi-jarvis' / 'containers'
        containers_dir.mkdir(parents=True, exist_ok=True)
        dockerfile_path = containers_dir / f'{self.get_container_image()}.Dockerfile'

        # Start with base image
        print(f"Using base image: {self.container_base}")
        with open(dockerfile_path, 'w') as f:
            f.write(f"FROM {self.container_base}\n\n")
            f.write("# Disable prompt during packages installation\n")
            f.write("ARG DEBIAN_FRONTEND=noninteractive\n\n")

        # Add each package's container augmentation
        for pkg_def in self.packages:
            try:
                pkg_instance = self._load_package_instance(pkg_def, self.env)
                if hasattr(pkg_instance, 'augment_container'):
                    dockerfile_commands = pkg_instance.augment_container()
                    if dockerfile_commands:
                        with open(dockerfile_path, 'a') as f:
                            f.write(f"# Package: {pkg_def['pkg_type']}\n")
                            f.write(dockerfile_commands)
                            f.write("\n")
            except Exception as e:
                print(f"Warning: Could not augment container for {pkg_def['pkg_type']}: {e}")

        # Add interceptors
        for interceptor_name, interceptor_def in self.interceptors.items():
            try:
                pkg_instance = self._load_package_instance(interceptor_def, self.env)
                if hasattr(pkg_instance, 'augment_container'):
                    dockerfile_commands = pkg_instance.augment_container()
                    if dockerfile_commands:
                        with open(dockerfile_path, 'a') as f:
                            f.write(f"# Interceptor: {interceptor_def['pkg_type']}\n")
                            f.write(dockerfile_commands)
                            f.write("\n")
            except Exception as e:
                print(f"Warning: Could not augment container for interceptor {interceptor_def['pkg_type']}: {e}")

        # Note: CMD is not added to global Dockerfile - it will be specified in docker-compose

        # Save container manifest (list of package types for rebuild detection)
        manifest_path = containers_dir / f'{self.get_container_image()}.manifest'
        pkg_types = [pkg_def['pkg_type'] for pkg_def in self.packages]
        interceptor_types = [idef['pkg_type'] for idef in self.interceptors.values()]
        manifest = {
            'packages': sorted(pkg_types),
            'interceptors': sorted(interceptor_types),
            'container_base': self.container_base
        }
        import json
        with open(manifest_path, 'w') as f:
            json.dump(manifest, f, indent=2)

        print(f"Generated global Dockerfile: {dockerfile_path}")
        return dockerfile_path

    def _build_global_container_image(self):
        """
        Build the global container image from the Dockerfile in ~/.ppi-jarvis/containers/.
        This image is tagged with container_build name and can be reused across pipelines.
        """
        from pathlib import Path
        from jarvis_cd.shell import LocalExecInfo, Exec

        containers_dir = Path.home() / '.ppi-jarvis' / 'containers'
        dockerfile_path = containers_dir / f'{self.get_container_image()}.Dockerfile'

        if not dockerfile_path.exists():
            raise FileNotFoundError(f"Dockerfile not found: {dockerfile_path}")

        print(f"Building global container image: {self.get_container_image()}")

        # Determine build command based on container engine
        if self.container_engine.lower() == 'podman':
            build_cmd = f"podman build -t {self.get_container_image()} -f {dockerfile_path} {containers_dir}"
        else:
            build_cmd = f"docker build -t {self.get_container_image()} -f {dockerfile_path} {containers_dir}"

        # Build the image
        Exec(build_cmd, LocalExecInfo()).run()
        print(f"Container image built: {self.get_container_image()}")

    def _generate_pipeline_compose_file(self):
        """
        Generate pipeline-specific docker-compose file that uses the global container image.
        This compose file is stored in the pipeline's shared directory.

        :return: Path to generated compose file
        """
        import yaml
        import os

        shared_dir = self.jarvis.get_pipeline_shared_dir(self.name)
        compose_path = shared_dir / 'docker-compose.yaml'

        container_name = f"{self.name}_container"

        # Use pipeline-level SSH port configuration
        ssh_port = self.container_ssh_port

        # Build container command - start SSH and run pipeline, then stop SSH
        ssh_dir = os.path.expanduser('~/.ssh')
        container_cmd = (
            f'set -e && '
            f'cp -r /root/.ssh_host /root/.ssh && '
            f'chmod 700 /root/.ssh && '
            f'chmod 600 /root/.ssh/* 2>/dev/null || true && '
            f'cat /root/.ssh/*.pub > /root/.ssh/authorized_keys 2>/dev/null && '
            f'chmod 600 /root/.ssh/authorized_keys 2>/dev/null || true && '
            f'echo "Host *" > /root/.ssh/config && '
            f'echo "    Port {ssh_port}" >> /root/.ssh/config && '
            f'echo "    StrictHostKeyChecking no" >> /root/.ssh/config && '
            f'chmod 600 /root/.ssh/config && '
            f'sed -i "s/^#*Port .*/Port {ssh_port}/" /etc/ssh/sshd_config && '
            f'/usr/sbin/sshd && '
            f'jarvis ppl run yaml /root/.ppi-jarvis/shared/pipeline.yaml; '
            f'EXIT_CODE=$?; '
            f'pkill sshd; '
            f'exit $EXIT_CODE'
        )

        # Create compose configuration using the global container image
        private_dir = self.jarvis.get_pipeline_private_dir(self.name)

        # Prepare volume mounts
        volumes = [
            f"{private_dir}:/root/.ppi-jarvis/private",
            f"{shared_dir}:/root/.ppi-jarvis/shared",
            f"{ssh_dir}:/root/.ssh_host:ro"
        ]

        # Add hostfile volume mount if hostfile is set
        hostfile = self.get_hostfile()
        if hostfile and hostfile.path:
            volumes.append(f"{hostfile.path}:/root/.ppi-jarvis/hostfile:ro")

        service_config = {
            'container_name': container_name,
            'image': self.get_container_image(),  # Use pre-built global image
            'entrypoint': ['/bin/bash', '-c'],
            'command': [container_cmd],
            'network_mode': 'host',
            'ipc': 'host',  # Share IPC namespace with host (removes shm limits)
            'volumes': volumes
        }

        # Note: GPU configuration is not included by default
        # If GPU access is needed, users should add it to their pipeline configuration
        # or use host network mode which provides direct device access

        # Apply container extensions from pipeline configuration
        if self.container_extensions:
            # Deep merge container_extensions into service_config
            self._merge_dict(service_config, self.container_extensions)

        compose_config = {
            'services': {
                self.name: service_config
            }
        }

        # Write compose file
        with open(compose_path, 'w') as f:
            yaml.dump(compose_config, f, default_flow_style=False)

        print(f"Generated docker-compose file: {compose_path}")
        return compose_path

    def _merge_dict(self, target: dict, source: dict):
        """
        Deep merge source dictionary into target dictionary.
        Lists are extended, dictionaries are recursively merged.

        :param target: Target dictionary to merge into
        :param source: Source dictionary to merge from
        """
        for key, value in source.items():
            if key in target:
                if isinstance(target[key], dict) and isinstance(value, dict):
                    # Recursively merge nested dictionaries
                    self._merge_dict(target[key], value)
                elif isinstance(target[key], list) and isinstance(value, list):
                    # Extend lists
                    target[key].extend(value)
                else:
                    # Override value
                    target[key] = value
            else:
                # Add new key
                target[key] = value

    def _start_containerized_pipeline(self):
        """
        Start containerized pipeline by deploying containers to all nodes in hostfile using pssh.
        Uses the pre-built global container image.
        """
        from jarvis_cd.util.logger import logger
        from jarvis_cd.shell import LocalExecInfo, PsshExecInfo
        from jarvis_cd.shell.container_compose_exec import ContainerComposeExec

        logger.info("Starting containerized pipeline deployment")

        # Get compose file path (already generated during load)
        shared_dir = self.jarvis.get_pipeline_shared_dir(self.name)
        compose_path = shared_dir / 'docker-compose.yaml'

        if not compose_path.exists():
            raise FileNotFoundError(f"Compose file not found: {compose_path}. Did you load the pipeline?")

        # Determine container runtime preference
        prefer_podman = self.container_engine.lower() == 'podman'

        # Check if we have a hostfile
        hostfile = self.get_hostfile()
        if not hostfile or len(hostfile) == 0:
            logger.warning("No hostfile found, deploying to localhost only")
            exec_info = LocalExecInfo()
        else:
            logger.info(f"Deploying containers to all nodes in hostfile")
            exec_info = PsshExecInfo(hostfile=hostfile)

        # Start containers (uses pre-built image)
        ContainerComposeExec(str(compose_path), exec_info, action='up', prefer_podman=prefer_podman).run()

        logger.success(f"Containers started")

    def _stop_containerized_pipeline(self):
        """
        Stop containerized pipeline by stopping containers on all nodes in hostfile using pssh.
        """
        from jarvis_cd.util.logger import logger
        from jarvis_cd.shell import LocalExecInfo, PsshExecInfo
        from jarvis_cd.shell.container_compose_exec import ContainerComposeExec

        logger.info("Stopping containerized pipeline")

        # Determine container runtime preference
        prefer_podman = self.container_engine.lower() == 'podman'

        # Get compose file path
        shared_dir = self.jarvis.get_pipeline_shared_dir(self.name)
        compose_path = shared_dir / 'docker-compose.yaml'

        # Check if we have a hostfile
        hostfile = self.get_hostfile()
        if not hostfile or len(hostfile) == 0:
            logger.warning("No hostfile found, stopping on localhost only")
            exec_info = LocalExecInfo()
        else:
            logger.info(f"Stopping containers on all nodes in hostfile")
            exec_info = PsshExecInfo(hostfile=hostfile)

        # Stop containers
        ContainerComposeExec(str(compose_path), exec_info, action='down', prefer_podman=prefer_podman).run()

        logger.success(f"Containers stopped")

    def _kill_containerized_pipeline(self):
        """
        Kill containerized pipeline by force-stopping containers on all nodes in hostfile using pssh.
        """
        from jarvis_cd.util.logger import logger
        from jarvis_cd.shell import LocalExecInfo, PsshExecInfo
        from jarvis_cd.shell.container_compose_exec import ContainerComposeExec

        logger.info("Force-killing containerized pipeline")

        # Determine container runtime preference
        prefer_podman = self.container_engine.lower() == 'podman'

        # Get compose file path
        shared_dir = self.jarvis.get_pipeline_shared_dir(self.name)
        compose_path = shared_dir / 'docker-compose.yaml'

        # Check if we have a hostfile
        hostfile = self.get_hostfile()
        if not hostfile or len(hostfile) == 0:
            logger.warning("No hostfile found, force-killing on localhost only")
            exec_info = LocalExecInfo()
        else:
            logger.info(f"Force-killing containers on all nodes in hostfile")
            exec_info = PsshExecInfo(hostfile=hostfile)

        # Kill and then remove containers
        ContainerComposeExec(str(compose_path), exec_info, action='kill', prefer_podman=prefer_podman).run()
        ContainerComposeExec(str(compose_path), exec_info, action='down', prefer_podman=prefer_podman).run()

        logger.success(f"Containers force-killed")
```

### `jarvis_cd/core/pipeline_index.py`

```python
"""
Pipeline Index Manager for Jarvis-CD.
Manages pipeline indexes stored in repo 'pipelines' directories.
"""

import os
import shutil
from pathlib import Path
from typing import List, Optional, Tuple, Dict
from jarvis_cd.core.config import Jarvis


class PipelineIndexManager:
    """
    Manages pipeline indexes - collections of pipeline scripts stored in repo 'pipelines' directories.
    """

    def __init__(self, jarvis_config: Jarvis):
        """
        Initialize pipeline index manager.

        :param jarvis_config: Jarvis configuration singleton
        """
        self.jarvis_config = jarvis_config
        
    def parse_index_query(self, index_query: str) -> Tuple[str, List[str], str]:
        """
        Parse an index query into repo name, subdirectories, and script name.
        
        :param index_query: Dotted string like 'repo.subdir1.subdir2.script'
        :return: Tuple of (repo_name, subdirs_list, script_name)
        """
        if not index_query or '.' not in index_query:
            raise ValueError(f"Invalid index query: '{index_query}'. Expected format: repo.path.to.script")
            
        parts = index_query.split('.')
        if len(parts) < 2:
            raise ValueError(f"Invalid index query: '{index_query}'. Must have at least repo.script")
            
        repo_name = parts[0]
        script_name = parts[-1]
        subdirs = parts[1:-1]  # Everything between repo and script
        
        return repo_name, subdirs, script_name
        
    def find_repo_path(self, repo_name: str) -> Optional[Path]:
        """
        Find the path to a repository by name.
        
        :param repo_name: Name of the repository
        :return: Path to repository or None if not found
        """
        # Check if it's the builtin repo
        if repo_name == 'builtin':
            return self.jarvis_config.get_builtin_repo_path()
            
        # Search in registered repos
        for repo_path_str in self.jarvis_config.repos['repos']:
            repo_path = Path(repo_path_str)
            if repo_path.name == repo_name and repo_path.exists():
                return repo_path
                
        return None
        
    def find_pipeline_script(self, index_query: str) -> Optional[Path]:
        """
        Find a pipeline script from an index query.
        
        :param index_query: Dotted string like 'repo.subdir1.subdir2.script'
        :return: Path to the script file or None if not found
        """
        repo_name, subdirs, script_name = self.parse_index_query(index_query)
        
        # Find the repository
        repo_path = self.find_repo_path(repo_name)
        if not repo_path:
            return None
            
        # Build path to pipeline index
        pipelines_dir = repo_path / 'pipelines'
        if not pipelines_dir.exists():
            return None
            
        # Build path through subdirectories
        script_dir = pipelines_dir
        for subdir in subdirs:
            script_dir = script_dir / subdir
            if not script_dir.exists():
                return None
                
        # Look for script with .yaml extension
        script_path = script_dir / f'{script_name}.yaml'
        if script_path.exists():
            return script_path
            
        return None
        
    def list_available_scripts(self, repo_name: Optional[str] = None) -> Dict[str, List[Dict[str, str]]]:
        """
        List all available pipeline scripts in indexes.
        
        :param repo_name: Optional specific repo to list, or None for all repos
        :return: Dictionary mapping repo names to lists of entry dictionaries with 'name' and 'type' keys
        """
        available_scripts = {}
        
        repos_to_check = []
        if repo_name:
            # Check specific repo
            repo_path = self.find_repo_path(repo_name)
            if repo_path:
                repos_to_check.append((repo_name, repo_path))
        else:
            # Check all repos
            # Builtin repo
            builtin_path = self.jarvis_config.get_builtin_repo_path()
            if builtin_path and builtin_path.exists():
                repos_to_check.append(('builtin', builtin_path))
                
            # Registered repos
            for repo_path_str in self.jarvis_config.repos['repos']:
                repo_path = Path(repo_path_str)
                if repo_path.exists():
                    repos_to_check.append((repo_path.name, repo_path))
                    
        # Scan each repo for pipeline scripts
        for repo_name, repo_path in repos_to_check:
            pipelines_dir = repo_path / 'pipelines'
            if not pipelines_dir.exists():
                continue
                
            entries = []
            self._scan_pipeline_directory(pipelines_dir, entries, repo_name)
            
            if entries:
                # Sort by name
                available_scripts[repo_name] = sorted(entries, key=lambda x: x['name'])
                
        return available_scripts
        
    def _scan_pipeline_directory(self, directory: Path, entries: List[Dict[str, str]], repo_name: str, current_path: str = ""):
        """
        Recursively scan a pipeline directory for .yaml files and directories.
        
        :param directory: Directory to scan
        :param entries: List to append found entries to
        :param repo_name: Name of the repository
        :param current_path: Current path within the pipelines directory
        """
        try:
            for item in directory.iterdir():
                if item.is_file() and item.suffix == '.yaml':
                    # Build the index query for this script
                    script_name = item.stem  # Remove .yaml extension
                    if current_path:
                        index_query = f"{repo_name}.{current_path}.{script_name}"
                    else:
                        index_query = f"{repo_name}.{script_name}"
                    entries.append({'name': index_query, 'type': 'file'})
                elif item.is_dir():
                    # Add directory entry
                    if current_path:
                        dir_query = f"{repo_name}.{current_path}.{item.name}"
                    else:
                        dir_query = f"{repo_name}.{item.name}"
                    entries.append({'name': dir_query, 'type': 'directory'})
                    
                    # Recursively scan subdirectory
                    if current_path:
                        new_path = f"{current_path}.{item.name}"
                    else:
                        new_path = item.name
                    self._scan_pipeline_directory(item, entries, repo_name, new_path)
        except (OSError, PermissionError):
            # Skip directories we can't read
            pass
            
    def load_pipeline_from_index(self, index_query: str):
        """
        Load a pipeline script from an index directly into the current pipeline.
        
        :param index_query: Dotted string like 'repo.subdir1.subdir2.script'
        """
        script_path = self.find_pipeline_script(index_query)
        if not script_path:
            # List available scripts to help user
            print(f"Pipeline script not found: {index_query}")
            self._print_available_scripts()
            return
            
        # Use Pipeline class to load the script
        from jarvis_cd.core.pipeline import Pipeline
        
        try:
            pipeline = Pipeline()
            pipeline.load('yaml', str(script_path))
            print(f"Loaded pipeline from index: {index_query}")
        except Exception as e:
            print(f"Error loading pipeline from index '{index_query}': {e}")
            
    def copy_pipeline_from_index(self, index_query: str, output_path: Optional[str] = None):
        """
        Copy a pipeline script from an index to a local directory.
        
        :param index_query: Dotted string like 'repo.subdir1.subdir2.script'
        :param output_path: Optional output directory or file path. Defaults to current directory.
        """
        script_path = self.find_pipeline_script(index_query)
        if not script_path:
            # List available scripts to help user
            print(f"Pipeline script not found: {index_query}")
            self._print_available_scripts()
            return
            
        # Determine output path
        if output_path is None:
            # Copy to current directory with same filename
            output_file = Path.cwd() / script_path.name
        else:
            output_path = Path(output_path)
            if output_path.is_dir() or output_path.suffix == '':
                # Output is a directory, use original filename
                output_file = output_path / script_path.name
            else:
                # Output is a specific file
                output_file = output_path
                
        # Create output directory if needed
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            shutil.copy2(script_path, output_file)
            print(f"Copied pipeline script from '{index_query}' to '{output_file}'")
        except Exception as e:
            print(f"Error copying pipeline script: {e}")
            
    def _print_available_scripts(self):
        """
        Print available pipeline scripts to help user with valid index queries.
        """
        from jarvis_cd.util.logger import logger, Color
        
        available_scripts = self.list_available_scripts()
        
        if not available_scripts:
            print("No pipeline indexes found in any repositories.")
            return
            
        print("Available pipeline scripts:")
        for repo_name, entries in available_scripts.items():
            print(f"  {repo_name}:")
            for entry in entries:
                if entry['type'] == 'file':
                    # Print files in default color
                    print(f"    {entry['name']}")
                elif entry['type'] == 'directory':
                    # Print directories in cyan color
                    logger.print(Color.CYAN, f"    {entry['name']} (directory)")
```

### `jarvis_cd/core/pkg.py`

```python
"""
Base package classes for Jarvis-CD.
Provides the consolidated Pkg class and its subclasses for Services, Applications, and Interceptors.
"""

import os
import yaml
import time
import inspect
from pathlib import Path
from typing import Dict, Any, List, Optional
from jarvis_cd.core.config import Jarvis, load_class
from jarvis_cd.util.hostfile import Hostfile


class Pkg:
    """
    Consolidated base class for all Jarvis packages.
    Provides common functionality and interface for services, applications, and interceptors.
    """
    
    @classmethod
    def load_standalone(cls, package_spec: str):
        """
        Load a package instance for standalone operations (not in a pipeline context).
        Creates a minimal standalone pipeline context for the package.

        :param package_spec: Package specification (repo.pkg or just pkg)
        :return: Package instance
        """
        from jarvis_cd.core.config import load_class, Jarvis

        jarvis = Jarvis.get_instance()

        # Parse package specification
        if '.' in package_spec:
            # Full specification like "builtin.ior"
            import_parts = package_spec.split('.')
            repo_name = import_parts[0]
            pkg_name = import_parts[1]
        else:
            # Just package name, search in repos
            full_spec = jarvis.find_package(package_spec)
            if not full_spec:
                raise ValueError(f"Package not found: {package_spec}")
            import_parts = full_spec.split('.')
            repo_name = import_parts[0]
            pkg_name = import_parts[1]

        # Determine class name (convert snake_case to PascalCase)
        class_name = ''.join(word.capitalize() for word in pkg_name.split('_'))

        # Load class
        if repo_name == 'builtin':
            repo_path = str(jarvis.get_builtin_repo_path())
        else:
            # Find repo path in registered repos
            repo_path = None
            for registered_repo in jarvis.repos['repos']:
                if Path(registered_repo).name == repo_name:
                    repo_path = registered_repo
                    break

            if not repo_path:
                raise ValueError(f"Repository not found: {repo_name}")

        import_str = f"{repo_name}.{pkg_name}.pkg"
        try:
            pkg_class = load_class(import_str, repo_path, class_name)
        except Exception as e:
            raise ValueError(f"Failed to load package '{package_spec}': Error loading class {class_name} from {import_str}: {e}")

        if not pkg_class:
            raise ValueError(f"Package class not found: {class_name} in {import_str}")

        # Create a minimal standalone pipeline object
        class StandalonePipeline:
            def __init__(self):
                self.name = "standalone"

        standalone_pipeline = StandalonePipeline()

        # Create instance with standalone pipeline
        pkg_instance = pkg_class(pipeline=standalone_pipeline)

        # Set basic attributes for standalone use
        pkg_instance.pkg_id = pkg_name
        pkg_instance.global_id = f"standalone.{pkg_name}"

        # Initialize directories now that pkg_id is set
        pkg_instance._ensure_directories()

        return pkg_instance
    
    def __init__(self, pipeline):
        """
        Initialize package with default values.

        :param pipeline: Parent pipeline instance (REQUIRED)
        """
        self.jarvis = Jarvis.get_instance()
        self.pipeline = pipeline
        self.pkg_dir = None          # Directory containing the package source (pkg.py file)
        self.config_dir = None       # Directory for saving package configuration files
        self.shared_dir = None
        self.private_dir = None
        self.env = {}                # Base environment (everything except LD_PRELOAD)
        self.mod_env = {}           # Modified environment (exact replica of env + LD_PRELOAD)
        self.config = {'interceptors': {}}
        self.pkg_type = None
        self.global_id = None
        self.pkg_id = None

        # Note: Directories will be initialized by Pipeline._load_package_instance
        # after pkg_id is set, or by user code for standalone packages

        # Call user-defined initialization
        self._init()

        # Set pkg_dir to the directory containing this package's source
        self._detect_pkg_dir()

    @property
    def hostfile(self) -> Hostfile:
        """
        Get the effective hostfile for this package.
        Property wrapper around get_hostfile() for convenience.

        :return: Hostfile object
        """
        return self.get_hostfile()

    def get_hostfile(self) -> Hostfile:
        """
        Get the effective hostfile for this package.
        Falls back to pipeline hostfile if package hostfile is not set.

        :return: Hostfile object
        """
        # Check if package has a hostfile configured
        hostfile_path = self.config.get('hostfile', '')
        if hostfile_path:
            return Hostfile(path=hostfile_path)

        # Fall back to pipeline's hostfile
        if hasattr(self.pipeline, 'get_hostfile'):
            return self.pipeline.get_hostfile()

        # Fall back to global jarvis hostfile
        return self.jarvis.hostfile

    def _init(self):
        """
        Override this method to initialize package-specific variables.
        Don't assume that self.config is initialized.
        This provides an overview of the parameters of this class.
        Default values should almost always be None.
        """
        pass
        
    def _configure_menu(self) -> List[Dict[str, Any]]:
        """
        Override this method to define configuration options.
        
        :return: List of configuration option dictionaries
        """
        return []
        
    def _configure(self, **kwargs):
        """
        Override this method to handle package configuration.
        Takes as input a dictionary with keys determined from _configure_menu.
        Updates self.config and generates application-specific configuration files.
        
        :param kwargs: Configuration parameters
        """
        self.update_config(kwargs, rebuild=False)
        
    def configure_menu(self):
        """
        Get the complete configuration menu including common parameters.
        Returns the menu in argument dictionary format so parameters can be set from command line.
        
        :return: List of configuration option dictionaries
        """
        # Get package-specific menu
        package_menu = self._configure_menu()
        
        # Add common parameters that all packages should have
        common_menu = [
            {
                'name': 'deploy_mode',
                'msg': 'Deployment mode',
                'type': str,
                'choices': ['default', 'container'],
                'default': 'default',
            },
            {
                'name': 'interceptors',
                'msg': 'List of interceptor package names to apply',
                'type': list,
                'default': [],
                'args': [
                    {
                        'name': 'interceptor_name',
                        'msg': 'Name of an interceptor package',
                        'type': str,
                    }
                ]
            },
            {
                'name': 'sleep',
                'msg': 'Sleep time in seconds',
                'type': int,
                'default': 0,
            },
            {
                'name': 'do_dbg',
                'msg': 'Enable debug mode',
                'type': bool,
                'default': False,
            },
            {
                'name': 'dbg_port',
                'msg': 'Debug port number',
                'type': int,
                'default': 1234,
            },
            {
                'name': 'timeout',
                'msg': 'Operation timeout in seconds',
                'type': int,
                'default': 300,
            },
            {
                'name': 'retry_count',
                'msg': 'Number of retry attempts',
                'type': int,
                'default': 3,
            },
            {
                'name': 'hide_output',
                'msg': 'Hide command output',
                'type': bool,
                'default': False,
            },
            {
                'name': 'hostfile',
                'msg': 'Path to hostfile (empty string means use pipeline hostfile)',
                'type': str,
                'default': '',
            }
        ]

        # Combine package-specific and common menus
        return package_menu + common_menu

    def get_argparse(self):
        """
        Get PkgArgParse instance for this package.
        Used to display configuration help.

        :return: PkgArgParse instance
        """
        from jarvis_cd.util import PkgArgParse
        pkg_name = getattr(self, 'pkg_id', None) or self.__class__.__name__
        return PkgArgParse(pkg_name, self.configure_menu())

    def configure(self, **kwargs):
        """
        Public configuration method that calls internal _configure.

        :param kwargs: Configuration parameters
        :return: Configuration dictionary
        """
        # Ensure package directories are set
        self._ensure_directories()

        # Apply menu defaults first
        self._apply_menu_defaults()

        # Update configuration with provided parameters
        self.update_config(kwargs, rebuild=False)

        # Print hostfile being used
        hostfile = self.get_hostfile()
        if hostfile and hostfile.path:
            print(f"Package {self.pkg_id} using hostfile: {hostfile.path}")
        else:
            print(f"Package {self.pkg_id} using default hostfile (no path set)")

        # Call the internal configuration method
        self._configure(**kwargs)

        return self.config.copy()

    def _get_delegate(self, deploy_mode: str):
        """
        Get or create the delegate implementation based on deploy mode.

        This method provides a unified delegation pattern for packages that have
        multiple implementations (e.g., default vs containerized).

        The method attempts to import a module relative to the package and instantiate
        a class named {BaseClassName}{DeployMode}. For example:
        - Ior class with deploy_mode='container' -> from .container import IorContainer
        - Ior class with deploy_mode='default' -> from .default import IorDefault

        :param deploy_mode: Deployment mode (e.g., 'default', 'container', 'docker')
        :return: Delegate instance
        """
        # Check if we already have a delegate for this mode
        delegate_key = f'_delegate_{deploy_mode}'
        if hasattr(self, delegate_key) and getattr(self, delegate_key) is not None:
            return getattr(self, delegate_key)

        # Get base class name (e.g., 'Ior' from 'Ior' class)
        base_class_name = self.__class__.__name__

        # Build delegate class name: {BaseClassName}{DeployModeCapitalized}
        # e.g., 'Ior' + 'Container' = 'IorContainer'
        deploy_mode_capitalized = ''.join(word.capitalize() for word in deploy_mode.split('_'))
        delegate_class_name = f"{base_class_name}{deploy_mode_capitalized}"

        # Import the module dynamically relative to current package
        # e.g., if we're in builtin.ior.pkg, import builtin.ior.{deploy_mode}
        import importlib
        current_module = self.__class__.__module__  # e.g., 'builtin.ior.pkg'
        package_path = current_module.rsplit('.', 1)[0]  # e.g., 'builtin.ior'
        module_path = f"{package_path}.{deploy_mode}"

        try:
            module = importlib.import_module(module_path)
        except ImportError as e:
            raise ImportError(
                f"Failed to import deployment module '{module_path}' for deploy mode '{deploy_mode}'. "
                f"Expected file: {deploy_mode}.py in the same directory as {self.__class__.__name__}. "
                f"Error: {e}"
            )

        # Get the delegate class from the module
        try:
            delegate_class = getattr(module, delegate_class_name)
        except AttributeError:
            raise AttributeError(
                f"Module '{module_path}' does not contain class '{delegate_class_name}'. "
                f"Expected class name: {delegate_class_name}"
            )

        # Create delegate instance
        delegate = delegate_class.__new__(delegate_class)

        # Initialize the delegate with base class
        Pkg.__init__(delegate, pipeline=self.pipeline)

        # Copy our state to the delegate
        delegate.pkg_type = self.pkg_type
        delegate.pkg_id = self.pkg_id
        delegate.global_id = self.global_id
        delegate.config = self.config
        delegate.env = self.env
        delegate.mod_env = self.mod_env
        delegate._ensure_directories()

        # Cache the delegate
        setattr(self, delegate_key, delegate)

        return delegate

    def _ensure_directories(self):
        """
        Ensure package directories are set based on pipeline context.
        This method is called during __init__ so directories are always available.

        Directory structure:
        - config_dir: pipelines/pipeline_name/packages/pkg_id
        - shared_dir: pipeline_name/pkg_id
        - private_dir: pipeline_name/pkg_id
        """
        if not self.config_dir or not self.shared_dir or not self.private_dir:
            pkg_id = getattr(self, 'pkg_id', None) or self.__class__.__name__.lower()

            # Get directories from pipeline
            pipeline_config_dir = self.jarvis.get_pipeline_dir(self.pipeline.name)
            pipeline_shared_dir = self.jarvis.get_pipeline_shared_dir(self.pipeline.name)
            pipeline_private_dir = self.jarvis.get_pipeline_private_dir(self.pipeline.name)

            if not self.config_dir:
                self.config_dir = str(pipeline_config_dir / 'packages' / pkg_id)
            if not self.shared_dir:
                self.shared_dir = str(pipeline_shared_dir / pkg_id)
            if not self.private_dir:
                self.private_dir = str(pipeline_private_dir / pkg_id)

            # Create directories if they don't exist
            for dir_path in [self.config_dir, self.shared_dir, self.private_dir]:
                if dir_path:
                    Path(dir_path).mkdir(parents=True, exist_ok=True)
                    
    def _detect_pkg_dir(self):
        """
        Detect the directory containing this package's source code (where pkg.py is located).
        """
        try:
            # Get the file path of the class definition
            class_file = inspect.getfile(self.__class__)
            # Get the directory containing the package file
            self.pkg_dir = str(Path(class_file).parent)
        except Exception as e:
            # Fallback: leave pkg_dir as None if detection fails
            pass
            
    def _apply_menu_defaults(self):
        """
        Apply default values from the configuration menu to ensure all parameters have values.
        """
        menu = self.configure_menu()
        for item in menu:
            param_name = item.get('name')
            default_value = item.get('default')
            if param_name and param_name not in self.config and default_value is not None:
                self.config[param_name] = default_value
        
    def update_config(self, new_config: Dict[str, Any], rebuild: bool = True):
        """
        Update package configuration.
        
        :param new_config: New configuration values
        :param rebuild: Whether to rebuild configuration files
        """
        self.config.update(new_config)
        
        if rebuild and hasattr(self, '_configure'):
            self._configure(**self.config)
            
    def start(self):
        """
        Start the package.
        Called during pipeline run and start operations.
        Override this method in package implementations.
        """
        pass
        
    def stop(self):
        """
        Stop the package.
        Called during pipeline stop operations.
        Override this method in package implementations.
        """
        pass
        
    def kill(self):
        """
        Kill the package.
        Called during pipeline kill operations.
        Override this method in package implementations.
        """
        pass
        
    def clean(self):
        """
        Clean package data.
        Called during pipeline clean operations.
        Destroys all data for the package.
        Override this method in package implementations.
        """
        pass
        
    def status(self) -> str:
        """
        Override this method to return package status.
        Called during pipeline status operations.
        
        :return: Status string
        """
        return "unknown"
        
    def track_env(self, env_track_dict: Dict[str, str]):
        """
        Track environment variables.
        
        :param env_track_dict: Dictionary of environment variables to track
        """
        # Add to env (but not LD_PRELOAD)
        for key, value in env_track_dict.items():
            if key != 'LD_PRELOAD':
                self.env[key] = value
        
        # mod_env is exact replica of env plus LD_PRELOAD
        self.mod_env = self.env.copy()
        if 'LD_PRELOAD' in env_track_dict:
            self.mod_env['LD_PRELOAD'] = env_track_dict['LD_PRELOAD']
        
    def prepend_env(self, env_name: str, val: str):
        """
        Prepend a value to an environment variable.
        
        :param env_name: Environment variable name
        :param val: Value to prepend
        """
        # For LD_PRELOAD, only update mod_env
        if env_name == 'LD_PRELOAD':
            current_val = self.mod_env.get(env_name, '')
            if current_val:
                self.mod_env[env_name] = f"{val}:{current_val}"
            else:
                self.mod_env[env_name] = val
        else:
            # For other variables, update env
            current_val = self.env.get(env_name, '')
            if current_val:
                self.env[env_name] = f"{val}:{current_val}"
            else:
                self.env[env_name] = val
            
            # Keep mod_env in sync (exact replica of env + LD_PRELOAD)
            self.mod_env[env_name] = self.env[env_name]
            
    def setenv(self, env_name: str, val: str):
        """
        Set an environment variable.
        
        :param env_name: Environment variable name
        :param val: Value to set
        """
        # For LD_PRELOAD, only update mod_env
        if env_name == 'LD_PRELOAD':
            self.mod_env[env_name] = val
        else:
            # For other variables, update env
            self.env[env_name] = val
            
            # Keep mod_env in sync (exact replica of env + LD_PRELOAD)
            self.mod_env[env_name] = val

    def augment_container(self) -> str:
        """
        Generate Dockerfile commands to install this package in a container.
        This is an instance method called after package configuration.
        The package can access its configuration via self.config.

        Override this method in package implementations to provide
        package-specific installation commands.

        :return: Dockerfile commands as a string
        """
        # Default: no installation needed (packages should override this)
        return ""

    def find_library(self, library_name: str) -> Optional[str]:
        """
        Find a shared library by searching LD_LIBRARY_PATH and system paths.
        
        :param library_name: Name of the library to find
        :return: Path to library if found, None otherwise
        """
        import shutil
        
        # Generate possible library filenames
        lib_filenames = [
            f"lib{library_name}.so",     # Standard shared library
            f"{library_name}.so",        # Library name as-is with .so
            f"lib{library_name}.a",      # Static library
            library_name                 # Exact name as provided
        ]
        
        # Collect all library search paths in priority order
        search_paths = []
        
        # 1. Package-specific environment (mod_env takes precedence over env)
        mod_ld_path = self.mod_env.get('LD_LIBRARY_PATH')
        if mod_ld_path:
            search_paths.extend(mod_ld_path.split(':'))
        
        env_ld_path = self.env.get('LD_LIBRARY_PATH')
        if env_ld_path:
            search_paths.extend(env_ld_path.split(':'))
            
        # 2. System LD_LIBRARY_PATH
        system_ld_path = os.environ.get('LD_LIBRARY_PATH')
        if system_ld_path:
            search_paths.extend(system_ld_path.split(':'))
        
        # 3. Common system library directories
        search_paths.extend([
            "/usr/lib",
            "/usr/local/lib", 
            "/usr/lib64",
            "/usr/local/lib64",
            "/lib",
            "/lib64"
        ])
        
        # Search for the library in all paths
        for search_path in search_paths:
            if not search_path:  # Skip empty paths
                continue
                
            search_dir = Path(search_path)
            if not search_dir.exists():
                continue
                
            for lib_filename in lib_filenames:
                lib_path = search_dir / lib_filename
                print(lib_path)
                if lib_path.exists():
                    return str(lib_path)
        
        # Fallback: try using shutil.which for executable-style lookup
        for lib_filename in lib_filenames:
            lib_path = shutil.which(lib_filename)
            if lib_path:
                return lib_path
                
        return None
    
    def log(self, message, color=None):
        """
        Log a message with package context and optional color.

        :param message: Message to log
        :param color: Color to use (from jarvis_cd.util.logger.Color enum), defaults to YELLOW for info messages
        """
        from jarvis_cd.util.logger import logger, Color

        formatted_message = f"[{self.__class__.__name__}] {message}"

        if color is not None:
            logger.print(color, formatted_message)
        else:
            # Default to yellow for info messages
            logger.warning(formatted_message)
        
    def sleep(self, time_sec=None):
        """
        Sleep for a specified amount of time.
        
        :param time_sec: Time to sleep in seconds. If not provided, uses self.config['sleep']
        """
        if time_sec is None:
            time_sec = self.config.get('sleep', 0)
            
        self.log(f"Sleeping for {time_sec} seconds")
        if time_sec > 0:
            time.sleep(time_sec)
            
    def copy_template_file(self, source_path, dest_path, replacements=None):
        """
        Copy a template file from source to destination, replacing template constants.
        
        Template constants have the format ##CONSTANT_NAME## and are replaced with
        values from the replacements dictionary.
        
        :param source_path: Path to the source template file
        :param dest_path: Path where the processed file should be saved
        :param replacements: Dictionary of replacements {CONSTANT_NAME: value}
        
        Example:
            self.copy_template_file(f'{self.pkg_dir}/config/hermes.xml',
                                   self.adios2_xml_path,
                                   replacements={'PPN': 1})
        """
        try:
            if replacements is None:
                replacements = {}
                
            # Read the template file
            with open(source_path, 'r') as f:
                content = f.read()
            
            # Replace template constants
            for key, value in replacements.items():
                template_token = f"##{key}##"
                content = content.replace(template_token, str(value))
            
            # Ensure destination directory exists
            dest_dir = Path(dest_path).parent
            dest_dir.mkdir(parents=True, exist_ok=True)
            
            # Write the processed content to destination
            with open(dest_path, 'w') as f:
                f.write(content)
                
            self.log(f"Copied template file {source_path} -> {dest_path} with {len(replacements)} replacements")
            
        except FileNotFoundError:
            self.log(f"Error: Template file not found: {source_path}")
            raise
        except Exception as e:
            self.log(f"Error copying template file {source_path} -> {dest_path}: {e}")
            raise
            
        
    
    def show_readme(self):
        """
        Show README.md for this package.
        """
        if not self.pkg_dir:
            print("Package directory not set - cannot locate README")
            return
            
        readme_path = Path(self.pkg_dir) / 'README.md'
        
        if readme_path.exists():
            print(f"=== README for {self.__class__.__name__} ===")
            print(f"Location: {readme_path}")
            print()
            try:
                with open(readme_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                print(content)
            except Exception as e:
                print(f"Error reading README: {e}")
        else:
            print(f"No README found for package {self.__class__.__name__}")
            print(f"Expected location: {readme_path}")
    
    def show_paths(self, path_flags: Dict[str, bool]):
        """
        Show directory paths based on flags.
        
        :param path_flags: Dictionary of path flags to show
        """
        try:
            # Ensure directories are set
            self._ensure_directories()
            
            paths_to_show = []
            
            # Check each flag and add corresponding paths
            if path_flags.get('conf'):
                if self.config_dir:
                    paths_to_show.append(f"{self.config_dir}/config.yaml")
                    
            if path_flags.get('env'):
                if self.config_dir:
                    paths_to_show.append(f"{self.config_dir}/env.yaml")
                    
            if path_flags.get('mod_env'):
                if self.config_dir:
                    paths_to_show.append(f"{self.config_dir}/mod_env.yaml")
                    
            if path_flags.get('conf_dir'):
                if self.config_dir:
                    paths_to_show.append(self.config_dir)
                    
            if path_flags.get('shared_dir'):
                if self.shared_dir:
                    paths_to_show.append(self.shared_dir)
                    
            if path_flags.get('priv_dir'):
                if self.private_dir:
                    paths_to_show.append(self.private_dir)
                    
            if path_flags.get('pkg_dir'):
                if self.pkg_dir:
                    paths_to_show.append(self.pkg_dir)
            
            # Print only the paths, one per line (for shell usage)
            for path in paths_to_show:
                if path:  # Only print non-None paths
                    print(path)
                    
        except Exception as e:
            print(f"Error getting package paths: {e}", file=sys.stderr)


class Service(Pkg):
    """
    Base class for long-running services.
    Services typically need to be manually stopped.
    """

    def __init__(self, pipeline):
        super().__init__(pipeline=pipeline)
        
    def _init(self):
        """
        Initialize service-specific variables.
        Override in subclasses.
        """
        pass


class Application(Pkg):
    """
    Base class for applications that run and complete automatically.
    Applications typically don't need manual stopping.
    """

    def __init__(self, pipeline):
        super().__init__(pipeline=pipeline)
        
    def _init(self):
        """
        Initialize application-specific variables.
        Override in subclasses.
        """
        pass


class Interceptor(Pkg):
    """
    Base class for interceptors that modify environment variables.
    Interceptors route system and library calls to new functions.
    """

    def __init__(self, pipeline):
        super().__init__(pipeline=pipeline)
        
    def _init(self):
        """
        Initialize interceptor-specific variables.
        Override in subclasses.
        """
        pass
        
    def modify_env(self):
        """
        Override this method to modify the environment for interception.
        This is the main method interceptors should implement.
        """
        pass
```

### `jarvis_cd/core/repository.py`

```python
import os
import shutil
from pathlib import Path
from typing import Dict, Any, List
from jarvis_cd.core.config import Jarvis


class RepositoryManager:
    """
    Manages Jarvis repositories - adding, removing, listing, and creating packages.
    """

    def __init__(self, jarvis_config: Jarvis):
        """
        Initialize repository manager.

        :param jarvis_config: Jarvis configuration singleton
        """
        self.jarvis_config = jarvis_config
        
    def add_repository(self, repo_path: str, force: bool = False):
        """
        Add a repository to Jarvis.
        
        :param repo_path: Path to repository directory
        :param force: Force overwrite if repository already exists
        """
        # Automatically clean up non-existent repositories first
        self.jarvis_config.cleanup_nonexistent_repos()
        
        repo_path = Path(repo_path).absolute()
        
        if not repo_path.exists():
            raise FileNotFoundError(f"Repository path does not exist: {repo_path}")
            
        if not repo_path.is_dir():
            raise ValueError(f"Repository path is not a directory: {repo_path}")
            
        # Check if it looks like a valid repository
        repo_name = repo_path.name
        expected_subdir = repo_path / repo_name

        if not expected_subdir.exists():
            raise ValueError(
                f"Invalid repository structure: {repo_path} does not contain subdirectory '{repo_name}'\n"
                f"Expected structure: {repo_name}/{repo_name}/package_name/pkg.py\n"
                f"Missing directory: {expected_subdir}\n\n"
                f"To fix this, ensure your repository follows the required structure:\n"
                f"  {repo_name}/\n"
                f"  ├── {repo_name}/           # Required subdirectory with same name\n"
                f"  │   ├── package1/\n"
                f"  │   │   └── pkg.py\n"
                f"  │   └── package2/\n"
                f"  │       └── pkg.py\n"
                f"  └── pipelines/          # Optional pipeline index\n"
                f"      └── example.yaml"
            )

        if not expected_subdir.is_dir():
            raise ValueError(f"Expected subdirectory exists but is not a directory: {expected_subdir}")

        self.jarvis_config.add_repo(str(repo_path), force=force)
        
    def remove_repository(self, repo_path: str):
        """
        Remove a repository from Jarvis.
        
        :param repo_path: Path to repository directory
        """
        repo_path = Path(repo_path).absolute()
        self.jarvis_config.remove_repo(str(repo_path))
        
        # Also clean up any other non-existent repositories while we're at it
        self.jarvis_config.cleanup_nonexistent_repos()
        
    def remove_repository_by_name(self, repo_name: str):
        """
        Remove all repositories with the given name from Jarvis.
        
        :param repo_name: Name of repository to remove (not full path)
        :return: Number of repositories removed
        """
        # Automatically clean up non-existent repositories first
        self.jarvis_config.cleanup_nonexistent_repos()
        
        # Remove repositories by name
        removed_count = self.jarvis_config.remove_repo_by_name(repo_name)
        
        # Clean up any other non-existent repositories while we're at it
        self.jarvis_config.cleanup_nonexistent_repos()
        
        return removed_count
        
    def list_repositories(self):
        """List all registered repositories"""
        # Automatically clean up non-existent repositories
        removed_count = self.jarvis_config.cleanup_nonexistent_repos()
        if removed_count > 0:
            print()  # Add spacing after cleanup messages
        
        repos = self.jarvis_config.repos['repos']
        builtin_path = self.jarvis_config.get_builtin_repo_path()
        builtin_path_str = str(builtin_path)
        
        print("Registered repositories:")
        if not repos:
            print("  No repositories registered")
        else:
            repo_count = 0
            for repo_path in repos:
                repo_name = Path(repo_path).name
                exists = "✓" if Path(repo_path).exists() else "✗"
                
                # Check if this is the builtin repository
                if repo_path == builtin_path_str:
                    print(f"  {repo_count+1}. {repo_name} ({repo_path}) {exists} [builtin]")
                else:
                    print(f"  {repo_count+1}. {repo_name} ({repo_path}) {exists}")
                repo_count += 1
                
        # Only show separate builtin entry if it's not in the registered repos
        if builtin_path_str not in repos:
            builtin_exists = "✓" if builtin_path.exists() else "✗"
            print(f"  Built-in: builtin ({builtin_path}) {builtin_exists}")
        
    def create_package(self, package_name: str, package_type: str):
        """
        Create a new package in the first available repository.
        
        :param package_name: Name of package to create
        :param package_type: Type of package (service, app, interceptor)
        """
        if package_type not in ['service', 'app', 'interceptor']:
            raise ValueError(f"Invalid package type: {package_type}. Must be service, app, or interceptor")
            
        repos = self.jarvis_config.repos['repos']
        if not repos:
            raise ValueError("No repositories registered. Add a repository first with 'jarvis repo add'")
            
        # Use the first repository
        repo_path = Path(repos[0])
        repo_name = repo_path.name
        
        if not repo_path.exists():
            raise FileNotFoundError(f"Repository path does not exist: {repo_path}")
            
        # Create package directory structure
        package_dir = repo_path / repo_name / package_name
        package_dir.mkdir(parents=True, exist_ok=True)
        
        # Create package.py file
        package_file = package_dir / 'package.py'
        
        # Generate package template based on type
        template_content = self._generate_package_template(package_name, package_type)
        
        with open(package_file, 'w') as f:
            f.write(template_content)
            
        print(f"Created {package_type} package: {package_name}")
        print(f"Location: {package_file}")
        
    def _generate_package_template(self, package_name: str, package_type: str) -> str:
        """
        Generate package template code based on package type.
        
        :param package_name: Name of the package
        :param package_type: Type of package (service, app, interceptor)
        :return: Template code as string
        """
        class_name = package_name.capitalize()
        
        if package_type == 'service':
            return self._generate_service_template(class_name, package_name)
        elif package_type == 'app':
            return self._generate_app_template(class_name, package_name)
        elif package_type == 'interceptor':
            return self._generate_interceptor_template(class_name, package_name)
        else:
            raise ValueError(f"Unknown package type: {package_type}")
            
    def _generate_service_template(self, class_name: str, package_name: str) -> str:
        """Generate service package template"""
        return f'''"""
{class_name} service package for Jarvis-CD.
This is a long-running service that needs to be manually stopped.
"""
from jarvis_cd.core.pkg import Service


class {class_name}(Service):
    """
    {class_name} service implementation.
    """
    
    def _init(self):
        """
        Initialize service-specific variables.
        Don't assume that self.config is initialized.
        """
        self.port = None
        self.daemon_process = None
        
    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        
        :return: List of argument dictionaries
        """
        return [
            {{
                'name': 'port',
                'msg': 'The port to listen on',
                'type': int,
                'default': 8080
            }},
            {{
                'name': 'config_file',
                'msg': 'Path to configuration file',
                'type': str,
                'default': None
            }}
        ]
        
    def configure(self, **kwargs):
        """
        Configure the service with given parameters.
        
        :param kwargs: Configuration parameters
        """
        self.update_config(kwargs, rebuild=False)
        
        # Generate service-specific configuration files
        config_data = {{
            'port': self.config['port'],
            'service_name': '{package_name}'
        }}
        
        # Save configuration to shared directory
        import yaml
        config_file = f'{{self.shared_dir}}/{package_name}_config.yaml'
        with open(config_file, 'w') as f:
            yaml.dump(config_data, f, default_flow_style=False)
            
        print(f"Generated {package_name} configuration: {{config_file}}")
        
    def start(self):
        """
        Start the {package_name} service.
        """
        print(f"Starting {package_name} service on port {{self.config['port']}}")
        
        # Example service startup - replace with actual service commands
        # self.daemon_process = subprocess.Popen([
        #     '{package_name}_daemon',
        #     '--port', str(self.config['port']),
        #     '--config', f'{{self.shared_dir}}/{package_name}_config.yaml'
        # ])
        
        # Sleep to ensure service starts up
        import time
        time.sleep(self.config.get('sleep', 2))
        print(f"{package_name} service started")
        
    def stop(self):
        """
        Stop the {package_name} service.
        """
        print(f"Stopping {package_name} service")
        
        # Example service shutdown - replace with actual service commands
        # if self.daemon_process:
        #     self.daemon_process.terminate()
        #     self.daemon_process.wait()
        #     self.daemon_process = None
        
        print(f"{package_name} service stopped")
        
    def kill(self):
        """
        Force kill the {package_name} service.
        """
        print(f"Force killing {package_name} service")
        
        # Example force kill - replace with actual kill commands
        # if self.daemon_process:
        #     self.daemon_process.kill()
        #     self.daemon_process = None
        
        print(f"{package_name} service killed")
        
    def clean(self):
        """
        Clean up all {package_name} data and configuration.
        """
        print(f"Cleaning {package_name} service data")
        
        # Remove configuration files
        import os
        config_file = f'{{self.shared_dir}}/{package_name}_config.yaml'
        if os.path.exists(config_file):
            os.remove(config_file)
            
        # Remove any data directories or files
        # os.system(f'rm -rf {{self.private_dir}}/{package_name}_data')
        
        print(f"{package_name} service cleaned")
        
    def status(self):
        """
        Check the status of the {package_name} service.
        
        :return: Service status string
        """
        # Example status check - replace with actual status logic
        # if self.daemon_process and self.daemon_process.poll() is None:
        #     return "running"
        # else:
        #     return "stopped"
        
        return "unknown"
'''
        
    def _generate_app_template(self, class_name: str, package_name: str) -> str:
        """Generate application package template"""
        return f'''"""
{class_name} application package for Jarvis-CD.
This is an application that runs and completes automatically.
"""
from jarvis_cd.core.pkg import Application


class {class_name}(Application):
    """
    {class_name} application implementation.
    """
    
    def _init(self):
        """
        Initialize application-specific variables.
        Don't assume that self.config is initialized.
        """
        self.input_file = None
        self.output_file = None
        self.nprocs = None
        
    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        
        :return: List of argument dictionaries
        """
        return [
            {{
                'name': 'input_file',
                'msg': 'Path to input file',
                'type': str,
                'default': '/tmp/{package_name}_input.dat'
            }},
            {{
                'name': 'output_file',
                'msg': 'Path to output file',
                'type': str,
                'default': '/tmp/{package_name}_output.dat'
            }},
            {{
                'name': 'nprocs',
                'msg': 'Number of processes',
                'type': int,
                'default': 1
            }},
            {{
                'name': 'ppn',
                'msg': 'Processes per node',
                'type': int,
                'default': 1
            }}
        ]
        
    def configure(self, **kwargs):
        """
        Configure the application with given parameters.
        
        :param kwargs: Configuration parameters
        """
        self.update_config(kwargs, rebuild=False)
        
        # Validate configuration
        if self.config['nprocs'] <= 0:
            raise ValueError("Number of processes must be positive")
            
        print(f"Configured {package_name} application:")
        print(f"  Input: {{self.config['input_file']}}")
        print(f"  Output: {{self.config['output_file']}}")
        print(f"  Processes: {{self.config['nprocs']}}")
        
    def start(self):
        """
        Run the {package_name} application.
        """
        print(f"Running {package_name} application")
        
        # Prepare input data if needed
        self._prepare_input()
        
        # Example application execution - replace with actual commands
        # cmd = [
        #     '{package_name}',
        #     '--input', self.config['input_file'],
        #     '--output', self.config['output_file']
        # ]
        
        # Example MPI execution:
        # from jarvis_util import MpiExecInfo, Exec
        # Exec(' '.join(cmd),
        #      MpiExecInfo(env=self.mod_env,
        #                  hostfile=self.jarvis.hostfile,
        #                  nprocs=self.config['nprocs'],
        #                  ppn=self.config['ppn']))
        
        print(f"{package_name} application completed")
        
    def stop(self):
        """
        Stop the application (usually not needed for apps).
        """
        print(f"Stopping {package_name} application")
        
    def clean(self):
        """
        Clean up application data and temporary files.
        """
        print(f"Cleaning {package_name} application data")
        
        # Remove output files
        import os
        if os.path.exists(self.config['output_file']):
            os.remove(self.config['output_file'])
            
        # Remove any temporary files
        # os.system(f'rm -f {{self.config["output_file"]}}*')
        
        print(f"{package_name} application cleaned")
        
    def _prepare_input(self):
        """
        Prepare input data for the application.
        """
        import os
        
        input_file = self.config['input_file']
        
        # Create input directory if needed
        os.makedirs(os.path.dirname(input_file), exist_ok=True)
        
        # Generate or copy input data
        if not os.path.exists(input_file):
            print(f"Generating input file: {{input_file}}")
            with open(input_file, 'w') as f:
                f.write(f"# {package_name} input data\\n")
                f.write(f"# Generated by Jarvis-CD\\n")
'''
        
    def _generate_interceptor_template(self, class_name: str, package_name: str) -> str:
        """Generate interceptor package template"""
        return f'''"""
{class_name} interceptor package for Jarvis-CD.
This modifies environment variables to intercept system/library calls.
"""
from jarvis_cd.core.pkg import Interceptor


class {class_name}(Interceptor):
    """
    {class_name} interceptor implementation.
    """
    
    def _init(self):
        """
        Initialize interceptor-specific variables.
        Don't assume that self.config is initialized.
        """
        self.library_path = None
        
    def _configure_menu(self):
        """
        Create a CLI menu for the configurator method.
        
        :return: List of argument dictionaries
        """
        return [
            {{
                'name': 'library_name',
                'msg': 'Name of the library to intercept',
                'type': str,
                'default': '{package_name}_intercept'
            }},
            {{
                'name': 'enable_logging',
                'msg': 'Enable interception logging',
                'type': bool,
                'default': False
            }}
        ]
        
    def configure(self, **kwargs):
        """
        Configure the interceptor with given parameters.
        
        :param kwargs: Configuration parameters
        """
        self.update_config(kwargs, rebuild=False)
        
        # Find the interception library
        library_name = self.config['library_name']
        self.config['library_path'] = self.find_library(library_name)
        
        if self.config['library_path'] is None:
            raise Exception(f'Could not find {{library_name}} library')
            
        print(f'Found {{library_name}} library at {{self.config["library_path"]}}')
        
        # Set up logging if enabled
        if self.config['enable_logging']:
            self.config['log_file'] = f'{{self.private_dir}}/{package_name}_intercept.log'
            print(f'Interception logging enabled: {{self.config["log_file"]}}')
        
    def modify_env(self):
        """
        Modify the environment to enable interception.
        """
        # Add library to LD_PRELOAD
        self.prepend_env('LD_PRELOAD', self.config['library_path'])
        
        # Set up logging environment if enabled
        if self.config['enable_logging']:
            self.setenv('{package_name.upper()}_LOG_FILE', self.config['log_file'])
            self.setenv('{package_name.upper()}_LOG_LEVEL', 'INFO')
            
        print(f"Environment modified for {package_name} interception")
        print(f"LD_PRELOAD: {{self.mod_env.get('LD_PRELOAD', '')}}")
'''

    def list_packages_in_repo(self, repo_path: str) -> List[str]:
        """
        List all packages in a repository.
        
        :param repo_path: Path to repository
        :return: List of package names
        """
        repo_path = Path(repo_path)
        repo_name = repo_path.name
        packages_dir = repo_path / repo_name
        
        if not packages_dir.exists():
            return []
            
        packages = []
        for item in packages_dir.iterdir():
            if item.is_dir() and (item / 'package.py').exists():
                packages.append(item.name)
                
        return sorted(packages)
        
    def find_all_packages(self) -> Dict[str, List[str]]:
        """
        Find all packages in all registered repositories.
        
        :return: Dictionary mapping repo names to package lists
        """
        all_packages = {}
        
        # Check builtin repository
        builtin_path = self.jarvis_config.get_builtin_repo_path()
        if builtin_path.exists():
            packages = self.list_packages_in_repo(str(builtin_path))
            if packages:
                all_packages['builtin'] = packages
                
        # Check registered repositories
        for repo_path in self.jarvis_config.repos['repos']:
            repo_name = Path(repo_path).name
            if Path(repo_path).exists():
                packages = self.list_packages_in_repo(repo_path)
                if packages:
                    all_packages[repo_name] = packages
                    
        return all_packages
```

### `jarvis_cd/core/resource_graph.py`

```python
"""
Resource graph management for Jarvis.
Coordinates resource collection across nodes and provides analysis capabilities.
"""
import json
import sys
import threading
from pathlib import Path
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from jarvis_cd.core.config import Jarvis
from jarvis_cd.util.resource_graph import ResourceGraph
from jarvis_cd.util.logger import logger
from jarvis_cd.shell import ResourceGraphExec, PsshExecInfo, LocalExec, LocalExecInfo


class ResourceGraphManager:
    """
    Manages resource graph collection and analysis across the Jarvis cluster.
    """
    
    def __init__(self):
        """
        Initialize resource graph manager.
        Gets Jarvis singleton internally.
        """
        self.jarvis = Jarvis.get_instance()
        self.resource_graph = ResourceGraph()

        # Try to load existing resource graph if available
        default_path = Path.home() / '.ppi-jarvis' / 'resource_graph.yaml'
        if default_path.exists():
            try:
                self.resource_graph.load_from_file(default_path)
            except Exception:
                # Silently continue if load fails
                pass
        
    def build(self, benchmark: bool = True, duration: int = 25):
        """
        Build resource graph by collecting information from all nodes in hostfile.

        :param benchmark: Whether to run performance benchmarks
        :param duration: Benchmark duration in seconds
        """
        # Get current hostfile
        jarvis = Jarvis.get_instance()
        if not jarvis.hostfile:
            raise ValueError("No hostfile set. Use 'jarvis hostfile set <path>' first.")
            
        hostfile = jarvis.hostfile
        nodes = hostfile.hosts
        
        if not nodes:
            raise ValueError("Hostfile contains no hosts")
            
        logger.pipeline(f"Building resource graph for {len(nodes)} nodes...")
        
        # Clear existing resource graph
        self.resource_graph = ResourceGraph()
        
        # Collect resources from all nodes in parallel
        self._collect_from_nodes(nodes, benchmark, duration)
        
        # Save resource graph
        self._save()
        
        # Display summary
        self.resource_graph.print_summary()
        self.resource_graph.print_common_storage()
        
    def _collect_from_nodes(self, nodes: List[str], benchmark: bool, duration: int):
        """
        Collect resource information from multiple nodes in parallel.
        
        :param nodes: List of node hostnames/IPs
        :param benchmark: Whether to run benchmarks
        :param duration: Benchmark duration
        """
        def collect_from_node(hostname: str) -> Dict[str, Any]:
            """Collect from a single node."""
            try:
                logger.package(f"Collecting resources from {hostname}...")
                
                # Get jarvis singleton  
                jarvis = Jarvis.get_instance()
                
                # Get current hostname using LocalExec to compare
                exec_info_hostname = LocalExecInfo(collect_output=True, hide_output=True)
                hostname_result = LocalExec('hostname', exec_info_hostname)
                current_hostname = hostname_result.stdout.get('localhost', '').strip()
                
                # Use local execution for localhost, otherwise use SSH
                if hostname in ['localhost', '127.0.0.1'] or hostname == current_hostname:
                    exec_info = LocalExecInfo(
                        collect_output=True,
                        hide_output=True
                    )
                else:
                    # Create a temporary hostfile for this specific node
                    from jarvis_cd.util.hostfile import Hostfile
                    single_host_hostfile = Hostfile([hostname])
                    
                    # Create execution info for this node
                    exec_info = PsshExecInfo(
                        hostfile=single_host_hostfile,
                        collect_output=True,
                        hide_output=True
                    )
                
                # Build command string
                cmd_parts = ['jarvis_resource_graph']
                if not benchmark:
                    cmd_parts.append('--no-benchmark')
                if duration != 25:
                    cmd_parts.extend(['--duration', str(duration)])
                cmd = ' '.join(cmd_parts)
                
                logger.debug(f"Command to execute: {cmd}")
                
                # Execute resource collection based on execution type
                if exec_info.exec_type.name == 'LOCAL':
                    executor = LocalExec(cmd, exec_info)
                else:
                    from jarvis_cd.shell import Exec
                    executor = Exec(cmd, exec_info)
                
                # Get results
                exit_codes = executor.exit_code
                
                logger.debug(f"Exit codes: {exit_codes}")
                logger.debug(f"Stdout: {executor.stdout}")
                logger.debug(f"Stderr: {executor.stderr}")
                
                # Check for errors
                if exit_codes.get(hostname, 1) != 0:
                    error_msg = executor.stderr.get(hostname, "Unknown error")
                    logger.error(f"Failed to collect from {hostname}: {error_msg}")
                    return None
                    
                # Parse JSON output
                json_output = executor.stdout.get(hostname, "")
                if not json_output.strip():
                    logger.error(f"No output from {hostname}")
                    return None
                    
                resource_data = json.loads(json_output)
                logger.package(f"Collected {len(resource_data.get('fs', []))} storage devices from {hostname}")
                
                return resource_data
                
            except Exception as e:
                logger.error(f"Error collecting from {hostname}: {e}")
                return None
                
        # Use ThreadPoolExecutor for parallel collection
        max_workers = min(len(nodes), 10)  # Limit concurrent connections
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all collection tasks
            future_to_hostname = {
                executor.submit(collect_from_node, hostname): hostname 
                for hostname in nodes
            }
            
            # Process results as they complete
            for future in as_completed(future_to_hostname):
                hostname = future_to_hostname[future]
                try:
                    resource_data = future.result()
                    if resource_data:
                        # Add to resource graph
                        self.resource_graph.add_node_data(hostname, resource_data)
                        logger.success(f"Added resources from {hostname} to graph")
                    else:
                        logger.warning(f"No resource data collected from {hostname}")
                        
                except Exception as e:
                    logger.error(f"Exception processing {hostname}: {e}")
                    
    def _save(self):
        """Save resource graph to file."""
        # Save to user's home directory for private machine storage
        output_file = Path.home() / '.ppi-jarvis' / 'resource_graph.yaml'
        output_file.parent.mkdir(exist_ok=True)

        self.resource_graph.save_to_file(output_file)
        
    def load(self, file_path: Optional[Path] = None):
        """
        Load resource graph from file.

        :param file_path: Path to resource graph file (default: ~/.ppi-jarvis/resource_graph.yaml)
        """
        if file_path is None:
            file_path = Path.home() / '.ppi-jarvis' / 'resource_graph.yaml'
            
        if not file_path.exists():
            raise FileNotFoundError(f"Resource graph file not found: {file_path}")
            
        self.resource_graph.load_from_file(file_path)
        logger.success(f"Loaded resource graph from {file_path}")
        
    def show(self):
        """Display the current resource graph YAML file."""
        # Get default resource graph path
        default_path = Path.home() / '.ppi-jarvis' / 'resource_graph.yaml'

        if not default_path.exists():
            logger.warning("No resource graph found. Run 'jarvis rg build' first.")
            return

        # Read and print raw YAML file contents
        with open(default_path, 'r') as f:
            print(f.read())
        
    def show_node_details(self, hostname: str):
        """
        Show detailed storage information for a specific node.
        
        :param hostname: Hostname to show details for
        """
        if not self.resource_graph.get_all_nodes():
            # Try to load the current resource graph
            try:
                self.load()
            except FileNotFoundError:
                logger.warning("No resource graph loaded. Run 'jarvis rg build' first.")
                return
            
        self.resource_graph.print_node_details(hostname)
        
    def list_nodes(self):
        """List all nodes in the resource graph."""
        if not self.resource_graph.get_all_nodes():
            # Try to load the current resource graph
            try:
                self.load()
            except FileNotFoundError:
                logger.warning("No nodes in resource graph. Run 'jarvis rg build' first.")
                return
        
        nodes = self.resource_graph.get_all_nodes()
            
        logger.info(f"Nodes in resource graph ({len(nodes)}):")
        for node in sorted(nodes):
            storage_count = len(self.resource_graph.get_node_storage(node))
            logger.info(f"  {node}: {storage_count} storage devices")
            
    def filter_by_type(self, dev_type: str):
        """
        Show storage devices filtered by type.
        
        :param dev_type: Device type to filter by (ssd, hdd, etc.)
        """
        if not self.resource_graph.get_all_nodes():
            logger.warning("No resource graph loaded. Run 'jarvis rg build' first.")
            return
            
        filtered = self.resource_graph.filter_by_type(dev_type)
        
        if not filtered:
            logger.warning(f"No {dev_type} devices found")
            return
            
        logger.info(f"=== {dev_type.upper()} Storage Devices ===")
        for hostname, devices in filtered.items():
            logger.info(f"\n{hostname}:")
            for device in devices:
                perf_info = ""
                if device.randwrite_4k_bw != 'unknown' and device.seqwrite_1m_bw != 'unknown':
                    perf_info = f" [4K: {device.randwrite_4k_bw}, 1M: {device.seqwrite_1m_bw}]"
                logger.info(f"  {device.mount}: {device.avail}{perf_info}")
                
    def get_common_mounts(self) -> List[str]:
        """Get list of common mount points."""
        return list(self.resource_graph.get_common_storage().keys())
        
    def show_path(self):
        """Show the path to the current resource graph file."""
        # Default path where resource graph is stored
        default_path = Path.home() / '.ppi-jarvis' / 'resource_graph.yaml'
        
        if default_path.exists():
            # Print only the path for shell command substitution
            print(default_path)
        else:
            # Exit with error code for missing file
            print(f"Error: No resource graph found at {default_path}", file=sys.stderr)
            print("Run 'jarvis rg build' to create a resource graph", file=sys.stderr)
            sys.exit(1)
```

### `jarvis_cd/core/route_pkg.py`

```python
"""
Base classes for routed application deployment.
Provides routing functionality that delegates lifecycle methods to implementation-specific subclasses.
"""
from .pkg import Application, Service
from typing import Dict, Any, List


class RouteApp(Application):
    """
    Base class for routed application deployment.

    This class provides automatic delegation of lifecycle methods (start, stop, status, kill, clean)
    to implementation-specific subclasses based on the deploy_mode configuration parameter.

    Subclasses should:
    1. Override _configure_menu() to define package-specific parameters and available deploy modes
    2. Implement specific deployment classes (e.g., MyAppDefault, MyAppContainer)
    """

    def _configure_menu(self) -> List[Dict[str, Any]]:
        """
        Get the configuration menu including deploy_mode parameter.
        Subclasses should override this to specify available deployment modes.

        :return: List of configuration option dictionaries
        """
        return [
            {
                'name': 'deploy_mode',
                'msg': 'Deployment mode',
                'type': str,
                'default': 'default',
                'choices': ['default'],  # Subclasses should override with actual choices
            }
        ]

    def _configure(self, **kwargs):
        """
        Configure the appropriate implementation via delegation.

        :param kwargs: Configuration parameters
        :return: None
        """
        # Call parent to set config
        super()._configure(**kwargs)

        # Delegate to appropriate implementation
        deploy_mode = self.config.get('deploy_mode', 'default')
        delegate = self._get_delegate(deploy_mode)
        delegate._configure(**kwargs)

    def start(self):
        """
        Start the application using the appropriate implementation.

        :return: None
        """
        deploy_mode = self.config.get('deploy_mode', 'default')
        delegate = self._get_delegate(deploy_mode)
        delegate.start()

    def stop(self):
        """
        Stop the application using the appropriate implementation.

        :return: None
        """
        deploy_mode = self.config.get('deploy_mode', 'default')
        delegate = self._get_delegate(deploy_mode)
        delegate.stop()

    def status(self):
        """
        Get status of the application using the appropriate implementation.

        :return: Status information
        """
        deploy_mode = self.config.get('deploy_mode', 'default')
        delegate = self._get_delegate(deploy_mode)
        return delegate.status()

    def kill(self):
        """
        Kill the application using the appropriate implementation.

        :return: None
        """
        deploy_mode = self.config.get('deploy_mode', 'default')
        delegate = self._get_delegate(deploy_mode)
        delegate.kill()

    def clean(self):
        """
        Clean application data using the appropriate implementation.

        :return: None
        """
        deploy_mode = self.config.get('deploy_mode', 'default')
        delegate = self._get_delegate(deploy_mode)
        delegate.clean()

    def augment_container(self) -> str:
        """
        Generate Dockerfile commands to install this package in a container.
        Delegates to the appropriate implementation based on deploy mode.

        :return: Dockerfile commands as a string
        """
        deploy_mode = self.config.get('deploy_mode', 'default')
        delegate = self._get_delegate(deploy_mode)

        # Check if delegate has augment_container method
        if hasattr(delegate, 'augment_container'):
            return delegate.augment_container()
        else:
            # Fall back to base implementation
            return super().augment_container()


class RouteService(RouteApp):
    """
    Alias for RouteApp following service naming conventions.

    This class is identical to RouteApp but follows the naming convention
    where long-running applications are called "services".
    """
    pass
```

### `jarvis_cd/post_install.py`

```python
"""Post-installation script to install builtin packages."""
import os
import shutil
from pathlib import Path


def install_builtin_packages():
    """Install builtin packages to ~/.ppi-jarvis/builtin during pip install."""
    jarvis_root = Path.home() / '.ppi-jarvis'
    builtin_target = jarvis_root / 'builtin'

    # If builtin already exists, nothing to do
    if builtin_target.exists():
        print(f"Builtin packages already installed at {builtin_target}")
        return

    # Create jarvis root directory
    jarvis_root.mkdir(parents=True, exist_ok=True)

    # Find builtin source directory
    try:
        # Get the directory containing this file
        this_file = Path(__file__).resolve()
        project_root = this_file.parent.parent  # Go up from jarvis_cd/post_install.py to project root
        builtin_source = project_root / 'builtin'

        if builtin_source.exists():
            print(f"Installing Jarvis-CD builtin packages...")
            print(f"Source: {builtin_source}")
            print(f"Target: {builtin_target}")

            # Always copy
            shutil.copytree(builtin_source, builtin_target)
            print(f"Copied builtin packages to {builtin_target}")

            # Count packages
            builtin_pkgs = builtin_target / 'builtin'
            if builtin_pkgs.exists():
                packages = [d for d in builtin_pkgs.iterdir()
                           if d.is_dir() and d.name != '__pycache__']
                print(f"Successfully installed {len(packages)} builtin packages")
        else:
            print(f"Warning: Could not find builtin packages directory at {builtin_source}")

    except Exception as e:
        print(f"Warning: Could not install builtin packages: {e}")


if __name__ == '__main__':
    install_builtin_packages()
```

### `jarvis_cd/shell/__init__.py`

```python
"""
Shell execution module for Jarvis-CD.

This module provides comprehensive command execution capabilities including:
- Local execution
- SSH/PSSH execution 
- MPI execution with auto-detection
- SCP/PSCP file transfer
- Utility commands

Usage examples:

# Local execution
from jarvis_cd.shell import LocalExecInfo, LocalExec
exec_info = LocalExecInfo()
result = LocalExec("ls -la", exec_info)

# SSH execution
from jarvis_cd.shell import SshExecInfo, SshExec, Hostfile
exec_info = SshExecInfo(hostfile=Hostfile(["remote.host.com"]))
result = SshExec("ls -la", exec_info)

# MPI execution
from jarvis_cd.shell import MpiExecInfo, MpiExec
exec_info = MpiExecInfo(nprocs=4)
result = MpiExec("./my_mpi_program", exec_info)

# File transfer
from jarvis_cd.shell import ScpExecInfo, ScpExec
exec_info = ScpExecInfo(hostfile=Hostfile(["remote.host.com"]))
result = ScpExec(["/local/file.txt"], exec_info)

# Factory usage
from jarvis_cd.shell import Exec, ExecType, LocalExecInfo
exec_info = LocalExecInfo()
result = Exec("echo hello", exec_info)
"""

# Core classes
from .exec_info import (
    ExecType, ExecInfo, LocalExecInfo, SshExecInfo, PsshExecInfo,
    MpiExecInfo, ScpExecInfo, PscpExecInfo
)
from ..util.hostfile import Hostfile
from .core_exec import CoreExec, LocalExec, MpiVersion
from .ssh_exec import SshExec, PsshExec
from .mpi_exec import (
    LocalMpiExec, OpenMpiExec, MpichExec, IntelMpiExec,
    CrayMpichExec, MpiExec
)
from .scp_exec import ScpExec, PscpExec
from .exec_factory import Exec
from .process import Kill, KillAll, Which, Mkdir, Rm, Chmod, Sleep, Echo
from .resource_graph_exec import ResourceGraphExec
from .container_compose_exec import (
    PodmanComposeExec, DockerComposeExec, ContainerComposeExec,
    PodmanBuildExec, DockerBuildExec, ContainerBuildExec
)
from .container_exec import (
    PodmanContainerExec, DockerContainerExec, ContainerExec
)

__all__ = [
    # Enums and Info classes
    'ExecType', 'ExecInfo', 'LocalExecInfo', 'SshExecInfo', 'PsshExecInfo',
    'MpiExecInfo', 'ScpExecInfo', 'PscpExecInfo',

    # Core execution
    'Hostfile', 'CoreExec', 'LocalExec', 'MpiVersion',

    # SSH execution
    'SshExec', 'PsshExec',

    # MPI execution
    'LocalMpiExec', 'OpenMpiExec', 'MpichExec', 'IntelMpiExec',
    'CrayMpichExec', 'MpiExec',

    # File transfer
    'ScpExec', 'PscpExec',

    # Factory and utilities
    'Exec', 'Kill', 'KillAll', 'Which', 'Mkdir', 'Rm', 'Chmod', 'Sleep', 'Echo',

    # Resource graph
    'ResourceGraphExec',

    # Container compose
    'PodmanComposeExec', 'DockerComposeExec', 'ContainerComposeExec',
    'PodmanBuildExec', 'DockerBuildExec', 'ContainerBuildExec',

    # Container exec
    'PodmanContainerExec', 'DockerContainerExec', 'ContainerExec'
]
```

### `jarvis_cd/shell/container_compose_exec.py`

```python
"""
Container compose execution classes for Docker and Podman.
"""
from pathlib import Path
from typing import Dict, Any, Optional
import hashlib
from .core_exec import CoreExec, LocalExec
from .exec_info import ExecInfo


class PodmanBuildExec(CoreExec):
    """
    Execute podman compose build command.
    """

    def __init__(self, compose_file: str, exec_info: ExecInfo):
        """
        Initialize podman compose build.

        :param compose_file: Path to compose file
        :param exec_info: Execution information
        """
        super().__init__()
        self.compose_file = Path(compose_file)
        self.exec_info = exec_info
        self.local_exec = None

        if not self.compose_file.exists():
            raise FileNotFoundError(f"Compose file not found: {self.compose_file}")

    def get_cmd(self) -> str:
        """Get the podman compose build command string"""
        import shutil
        # Use podman-compose if available, otherwise use podman compose
        if shutil.which('podman-compose'):
            return f"podman-compose -f {self.compose_file} build"
        else:
            # Check if podman has compose subcommand
            from .exec_info import LocalExecInfo
            test_exec = LocalExec('podman compose --help', LocalExecInfo())
            if test_exec.exit_code == 0:
                return f"podman compose -f {self.compose_file} build"

            raise RuntimeError(
                "podman-compose not found and podman compose subcommand not available. "
                "Please install podman-compose: pip install podman-compose"
            )

    def run(self):
        """Execute the podman compose build command"""
        cmd = self.get_cmd()
        self.local_exec = LocalExec(cmd, self.exec_info)

        # Copy state from LocalExec
        self.exit_code = self.local_exec.exit_code
        self.stdout = self.local_exec.stdout
        self.stderr = self.local_exec.stderr
        self.processes = self.local_exec.processes
        self.output_threads = self.local_exec.output_threads


class DockerBuildExec(CoreExec):
    """
    Execute docker compose build command.
    """

    def __init__(self, compose_file: str, exec_info: ExecInfo):
        """
        Initialize docker compose build.

        :param compose_file: Path to compose file
        :param exec_info: Execution information
        """
        super().__init__()
        self.compose_file = Path(compose_file)
        self.exec_info = exec_info
        self.local_exec = None

        if not self.compose_file.exists():
            raise FileNotFoundError(f"Compose file not found: {self.compose_file}")

    def get_cmd(self) -> str:
        """Get the docker compose build command string"""
        return f"docker compose -f {self.compose_file} build"

    def run(self):
        """Execute the docker compose build command"""
        cmd = self.get_cmd()
        self.local_exec = LocalExec(cmd, self.exec_info)

        # Copy state from LocalExec
        self.exit_code = self.local_exec.exit_code
        self.stdout = self.local_exec.stdout
        self.stderr = self.local_exec.stderr
        self.processes = self.local_exec.processes
        self.output_threads = self.local_exec.output_threads


class ContainerBuildExec(CoreExec):
    """
    Router for container build - automatically selects between Docker and Podman.
    """

    def __init__(self, compose_file: str, exec_info: ExecInfo, prefer_podman: bool = False):
        """
        Initialize container build execution.

        :param compose_file: Path to compose file
        :param exec_info: Execution information
        :param prefer_podman: Prefer Podman over Docker if both available
        """
        super().__init__()
        self.compose_file = Path(compose_file)
        self.exec_info = exec_info
        self.prefer_podman = prefer_podman
        self.delegate = None

        # Determine which build implementation to use
        self._select_implementation()

    def _select_implementation(self):
        """Select Docker or Podman based on availability"""
        import shutil

        has_docker = shutil.which('docker') is not None
        has_podman = shutil.which('podman') is not None or shutil.which('podman-compose') is not None

        if self.prefer_podman and has_podman:
            self.delegate = PodmanBuildExec(self.compose_file, self.exec_info)
        elif has_docker:
            self.delegate = DockerBuildExec(self.compose_file, self.exec_info)
        elif has_podman:
            self.delegate = PodmanBuildExec(self.compose_file, self.exec_info)
        else:
            raise RuntimeError("Neither docker nor podman found in PATH")

    def get_cmd(self) -> str:
        """Get the command string from delegate"""
        return self.delegate.get_cmd()

    def run(self):
        """Execute the build command via delegate"""
        self.delegate.run()

        # Copy state from delegate
        self.exit_code = self.delegate.exit_code
        self.stdout = self.delegate.stdout
        self.stderr = self.delegate.stderr
        self.processes = self.delegate.processes
        self.output_threads = self.delegate.output_threads


class PodmanComposeExec(CoreExec):
    """
    Execute podman compose commands.
    """

    def __init__(self, compose_file: str, exec_info: ExecInfo, action: str = 'up'):
        """
        Initialize podman compose execution.

        :param compose_file: Path to compose file
        :param exec_info: Execution information
        :param action: Compose action (up, down, etc.)
        """
        super().__init__()
        self.compose_file = Path(compose_file)
        self.exec_info = exec_info
        self.action = action
        self.local_exec = None

        if not self.compose_file.exists():
            raise FileNotFoundError(f"Compose file not found: {self.compose_file}")

    def get_cmd(self) -> str:
        """Get the podman compose command string"""
        import shutil
        # Use podman-compose if available, otherwise use podman compose
        if shutil.which('podman-compose'):
            cmd = f"podman-compose -f {self.compose_file} {self.action}"
        else:
            # Check if podman has compose subcommand
            from .exec_info import LocalExecInfo
            test_exec = LocalExec('podman compose --help', LocalExecInfo())
            if test_exec.exit_code == 0:
                cmd = f"podman compose -f {self.compose_file} {self.action}"
            else:
                raise RuntimeError(
                    "podman-compose not found and podman compose subcommand not available. "
                    "Please install podman-compose: pip install podman-compose"
                )
        # For 'up', add flags to show output and exit when container stops
        if self.action == 'up':
            cmd += " --abort-on-container-exit"
        return cmd

    def run(self):
        """Execute the podman compose command"""
        cmd = self.get_cmd()
        self.local_exec = LocalExec(cmd, self.exec_info)

        # Copy state from LocalExec
        self.exit_code = self.local_exec.exit_code
        self.stdout = self.local_exec.stdout
        self.stderr = self.local_exec.stderr
        self.processes = self.local_exec.processes
        self.output_threads = self.local_exec.output_threads


class DockerComposeExec(CoreExec):
    """
    Execute docker compose commands.
    """

    def __init__(self, compose_file: str, exec_info: ExecInfo, action: str = 'up'):
        """
        Initialize docker compose execution.

        :param compose_file: Path to compose file
        :param exec_info: Execution information
        :param action: Compose action (up, down, etc.)
        """
        super().__init__()
        self.compose_file = Path(compose_file)
        self.exec_info = exec_info
        self.action = action
        self.local_exec = None

        if not self.compose_file.exists():
            raise FileNotFoundError(f"Compose file not found: {self.compose_file}")

    def get_cmd(self) -> str:
        """Get the docker compose command string"""
        cmd = f"docker compose -f {self.compose_file} {self.action}"
        # For 'up', add flags to show output and exit when container stops
        if self.action == 'up':
            cmd += " --abort-on-container-exit"
        return cmd

    def run(self):
        """Execute the docker compose command"""
        cmd = self.get_cmd()
        self.local_exec = LocalExec(cmd, self.exec_info)

        # Copy state from LocalExec
        self.exit_code = self.local_exec.exit_code
        self.stdout = self.local_exec.stdout
        self.stderr = self.local_exec.stderr
        self.processes = self.local_exec.processes
        self.output_threads = self.local_exec.output_threads


class ContainerComposeExec(CoreExec):
    """
    Router for container compose execution - automatically selects
    between Docker and Podman based on availability or configuration.
    """

    def __init__(self, compose_file: str, exec_info: ExecInfo, action: str = 'up',
                 prefer_podman: bool = False):
        """
        Initialize container compose execution.

        :param compose_file: Path to compose file
        :param exec_info: Execution information
        :param action: Compose action (up, down, etc.)
        :param prefer_podman: Prefer Podman over Docker if both available
        """
        super().__init__()
        self.compose_file = Path(compose_file)
        self.exec_info = exec_info
        self.action = action
        self.prefer_podman = prefer_podman
        self.delegate = None

        # Determine which compose implementation to use
        self._select_implementation()

    def _select_implementation(self):
        """Select Docker or Podman based on availability"""
        import shutil

        has_docker = shutil.which('docker') is not None
        has_podman = shutil.which('podman') is not None or shutil.which('podman-compose') is not None

        if self.prefer_podman and has_podman:
            self.delegate = PodmanComposeExec(self.compose_file, self.exec_info, self.action)
        elif has_docker:
            self.delegate = DockerComposeExec(self.compose_file, self.exec_info, self.action)
        elif has_podman:
            self.delegate = PodmanComposeExec(self.compose_file, self.exec_info, self.action)
        else:
            raise RuntimeError("Neither docker nor podman found in PATH")

    def get_cmd(self) -> str:
        """Get the command string from delegate"""
        return self.delegate.get_cmd()

    def run(self):
        """Execute the compose command via delegate"""
        self.delegate.run()

        # Copy state from delegate
        self.exit_code = self.delegate.exit_code
        self.stdout = self.delegate.stdout
        self.stderr = self.delegate.stderr
        self.processes = self.delegate.processes
        self.output_threads = self.delegate.output_threads
```

### `jarvis_cd/shell/container_exec.py`

```python
"""
Container execution classes for running commands inside Docker and Podman containers.
"""
from typing import Optional
from .core_exec import CoreExec, LocalExec
from .exec_info import ExecInfo


class PodmanContainerExec(CoreExec):
    """
    Execute commands inside a running Podman container.
    """

    def __init__(self, container_name: str, command: str, exec_info: ExecInfo):
        """
        Initialize podman container exec.

        :param container_name: Name of the running container
        :param command: Command to execute inside container
        :param exec_info: Execution information
        """
        super().__init__()
        self.container_name = container_name
        self.command = command
        self.exec_info = exec_info
        self.local_exec = None

    def get_cmd(self) -> str:
        """Get the podman exec command string"""
        return f"podman exec {self.container_name} {self.command}"

    def run(self):
        """Execute the command inside the container"""
        cmd = self.get_cmd()
        self.local_exec = LocalExec(cmd, self.exec_info)

        # Copy state from LocalExec
        self.exit_code = self.local_exec.exit_code
        self.stdout = self.local_exec.stdout
        self.stderr = self.local_exec.stderr
        self.processes = self.local_exec.processes
        self.output_threads = self.local_exec.output_threads


class DockerContainerExec(CoreExec):
    """
    Execute commands inside a running Docker container.
    """

    def __init__(self, container_name: str, command: str, exec_info: ExecInfo):
        """
        Initialize docker container exec.

        :param container_name: Name of the running container
        :param command: Command to execute inside container
        :param exec_info: Execution information
        """
        super().__init__()
        self.container_name = container_name
        self.command = command
        self.exec_info = exec_info
        self.local_exec = None

    def get_cmd(self) -> str:
        """Get the docker exec command string"""
        return f"docker exec {self.container_name} {self.command}"

    def run(self):
        """Execute the command inside the container"""
        cmd = self.get_cmd()
        self.local_exec = LocalExec(cmd, self.exec_info)

        # Copy state from LocalExec
        self.exit_code = self.local_exec.exit_code
        self.stdout = self.local_exec.stdout
        self.stderr = self.local_exec.stderr
        self.processes = self.local_exec.processes
        self.output_threads = self.local_exec.output_threads


class ContainerExec(CoreExec):
    """
    Router for container exec - automatically selects between Docker and Podman
    based on availability or configuration.
    """

    def __init__(self, container_name: str, command: str, exec_info: ExecInfo,
                 prefer_podman: bool = False):
        """
        Initialize container exec.

        :param container_name: Name of the running container
        :param command: Command to execute inside container
        :param exec_info: Execution information
        :param prefer_podman: Prefer Podman over Docker if both available
        """
        super().__init__()
        self.container_name = container_name
        self.command = command
        self.exec_info = exec_info
        self.prefer_podman = prefer_podman
        self.delegate = None

        # Determine which container runtime to use
        self._select_implementation()

    def _select_implementation(self):
        """Select Docker or Podman based on availability"""
        import shutil

        has_docker = shutil.which('docker') is not None
        has_podman = shutil.which('podman') is not None

        if self.prefer_podman and has_podman:
            self.delegate = PodmanContainerExec(self.container_name, self.command, self.exec_info)
        elif has_docker:
            self.delegate = DockerContainerExec(self.container_name, self.command, self.exec_info)
        elif has_podman:
            self.delegate = PodmanContainerExec(self.container_name, self.command, self.exec_info)
        else:
            raise RuntimeError("Neither docker nor podman found in PATH")

    def get_cmd(self) -> str:
        """Get the command string from delegate"""
        return self.delegate.get_cmd()

    def run(self):
        """Execute the command via delegate"""
        self.delegate.run()

        # Copy state from delegate
        self.exit_code = self.delegate.exit_code
        self.stdout = self.delegate.stdout
        self.stderr = self.delegate.stderr
        self.processes = self.delegate.processes
        self.output_threads = self.delegate.output_threads
```

### `jarvis_cd/shell/core_exec.py`

```python
"""
Core execution classes for Jarvis shell execution.
"""
import subprocess
import threading
import time
import os
import signal
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List
from pathlib import Path

from .exec_info import ExecInfo, ExecType
from ..util.hostfile import Hostfile


class CoreExec(ABC):
    """
    An abstract class representing a class which is intended to run
    shell commands. This includes SSH, MPI, etc.
    """

    def __init__(self):
        self.exit_code = {}  # hostname -> exit_code
        self.stdout = {}     # hostname -> stdout 
        self.stderr = {}     # hostname -> stderr
        self.processes = {}  # hostname -> process
        self.output_threads = {}  # hostname -> (stdout_thread, stderr_thread)
        
    @abstractmethod
    def run(self):
        """Execute the command"""
        pass

    @abstractmethod
    def get_cmd(self) -> str:
        """Get the command string"""
        pass
        
    def wait(self, hostname: str = 'localhost') -> int:
        """
        Wait for execution to complete.
        
        :param hostname: Hostname to wait for
        :return: Exit code
        """
        if hostname in self.processes:
            process = self.processes[hostname]
            exit_code = process.wait()
            self.exit_code[hostname] = exit_code
            
            # Wait for output threads to complete
            if hostname in self.output_threads:
                stdout_thread, stderr_thread = self.output_threads[hostname]
                if stdout_thread:
                    stdout_thread.join()
                if stderr_thread:
                    stderr_thread.join()
                    
            return exit_code
        return 0
        
    def wait_all(self) -> Dict[str, int]:
        """
        Wait for all processes to complete.
        
        :return: Dictionary of hostname -> exit_code
        """
        for hostname in list(self.processes.keys()):
            self.wait(hostname)
        return self.exit_code.copy()
        
    def kill(self, hostname: str = 'localhost'):
        """
        Kill the process.
        
        :param hostname: Hostname to kill process on
        """
        if hostname in self.processes:
            process = self.processes[hostname]
            try:
                if process.poll() is None:  # Process is still running
                    process.terminate()
                    # Give it a moment to terminate gracefully
                    time.sleep(0.1)
                    if process.poll() is None:
                        process.kill()
            except ProcessLookupError:
                # Process already terminated
                pass
                
    def kill_all(self):
        """Kill all processes"""
        for hostname in list(self.processes.keys()):
            self.kill(hostname)


class LocalExec(CoreExec):
    """
    Execute commands locally using subprocess.
    """
    
    def __init__(self, cmd: str, exec_info: ExecInfo):
        """
        Initialize local execution.
        
        :param cmd: Command to execute
        :param exec_info: Execution information
        """
        super().__init__()
        self.cmd = cmd
        self.exec_info = exec_info
        self.hostname = 'localhost'
        
        # Initialize output storage
        self.stdout[self.hostname] = ""
        self.stderr[self.hostname] = ""
        self.exit_code[self.hostname] = 0
        
        # Run the command
        self.run()
        
        # If not async, wait for completion
        if not exec_info.exec_async:
            self.wait(self.hostname)
            
    def get_cmd(self) -> str:
        """Get the command string"""
        return self.cmd
        
    def run(self):
        """Execute the command locally"""
        # Set up environment
        if self.exec_info.env:
            env = os.environ.copy()
            # Convert all env values to strings
            for key, value in self.exec_info.env.items():
                env[key] = str(value)
        else:
            env = os.environ
        
        # Prepare stdin
        stdin_pipe = subprocess.PIPE if self.exec_info.stdin else None
        
        # Start process
        try:
            process = subprocess.Popen(
                self.cmd,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                stdin=stdin_pipe,
                env=env,
                cwd=self.exec_info.cwd,
                text=True,
                bufsize=1,  # Line buffered
                universal_newlines=True
            )
            
            self.processes[self.hostname] = process
            
            # Start output monitoring threads
            if self.exec_info.collect_output or not self.exec_info.hide_output:
                stdout_thread = threading.Thread(
                    target=self._monitor_output,
                    args=(process.stdout, 'stdout')
                )
                stderr_thread = threading.Thread(
                    target=self._monitor_output,
                    args=(process.stderr, 'stderr')
                )
                
                stdout_thread.daemon = True
                stderr_thread.daemon = True
                
                stdout_thread.start()
                stderr_thread.start()
                
                self.output_threads[self.hostname] = (stdout_thread, stderr_thread)
            
            # Send stdin if provided
            if self.exec_info.stdin:
                try:
                    process.stdin.write(self.exec_info.stdin)
                    process.stdin.close()
                except BrokenPipeError:
                    # Process may have terminated before we could write
                    pass
                    
        except Exception as e:
            print(f"Error starting process: {e}")
            self.exit_code[self.hostname] = 1
            
    def _monitor_output(self, pipe, output_type: str):
        """
        Monitor stdout or stderr in a separate thread.
        
        :param pipe: The pipe to monitor
        :param output_type: 'stdout' or 'stderr'
        """
        output_buffer = []
        
        try:
            for line in iter(pipe.readline, ''):
                if not line:
                    break
                    
                # Store in buffer if collecting output
                if self.exec_info.collect_output:
                    output_buffer.append(line)
                    
                # Print to console if not hidden
                if not self.exec_info.hide_output:
                    if output_type == 'stdout':
                        print(line, end='')
                    else:
                        print(line, end='', file=subprocess.sys.stderr)
                        
                # Write to file if specified
                pipe_file = (self.exec_info.pipe_stdout if output_type == 'stdout' 
                           else self.exec_info.pipe_stderr)
                if pipe_file:
                    try:
                        with open(pipe_file, 'a') as f:
                            f.write(line)
                    except Exception as e:
                        print(f"Error writing to {pipe_file}: {e}")
                        
        except Exception as e:
            print(f"Error monitoring {output_type}: {e}")
        finally:
            pipe.close()
            
        # Store collected output
        if self.exec_info.collect_output:
            if output_type == 'stdout':
                self.stdout[self.hostname] = ''.join(output_buffer)
            else:
                self.stderr[self.hostname] = ''.join(output_buffer)
                
    def wait(self, hostname: str = 'localhost') -> int:
        """Wait for completion and handle sleep"""
        exit_code = super().wait(hostname)
        
        # Sleep if specified
        if self.exec_info.sleep_ms > 0:
            time.sleep(self.exec_info.sleep_ms / 1000.0)
            
        return exit_code


class MpiVersion(LocalExec):
    """
    Introspect the current MPI implementation from the machine using
    mpiexec --version
    """

    def __init__(self, exec_info: ExecInfo):
        self.cmd = 'mpiexec --version'
        
        # Create modified exec_info for introspection
        introspect_info = exec_info.mod(
            env=exec_info.basic_env,
            collect_output=True,
            hide_output=True,
            do_dbg=False
        )
        
        super().__init__(self.cmd, introspect_info)
        
        # Determine MPI version from output
        vinfo = self.stdout.get('localhost', '')
        
        if 'mpich' in vinfo.lower():
            self.version = ExecType.MPICH
        elif 'Open MPI' in vinfo or 'OpenRTE' in vinfo:
            self.version = ExecType.OPENMPI
        elif 'Intel(R) MPI Library' in vinfo:
            self.version = ExecType.INTEL_MPI
        elif 'mpiexec version' in vinfo:
            self.version = ExecType.CRAY_MPICH
        else:
            # Default to MPICH if we can't determine
            print(f"Warning: Could not identify MPI implementation from: {vinfo}")
            self.version = ExecType.MPICH
```

### `jarvis_cd/shell/exec_factory.py`

```python
"""
Exec base class for Jarvis shell execution.
"""
from typing import Dict
from .exec_info import ExecInfo, ExecType
from .core_exec import CoreExec, LocalExec
from .ssh_exec import SshExec, PsshExec
from .mpi_exec import MpiExec
from .scp_exec import ScpExec, PscpExec


class Exec(CoreExec):
    """
    Base execution class that delegates to appropriate executor based on ExecInfo type.
    """
    
    def __init__(self, cmd: str, exec_info: ExecInfo):
        """
        Initialize executor with command and execution info.
        
        :param cmd: Command to execute
        :param exec_info: Execution information
        """
        super().__init__()
        self.cmd = cmd
        self.exec_info = exec_info
        self._delegate = None
        
    def run(self):
        """Execute the command using appropriate executor"""
        # Create the appropriate executor based on exec_info type
         
        if self.exec_info.exec_type == ExecType.LOCAL:
            self._delegate = LocalExec(self.cmd, self.exec_info)
        elif self.exec_info.exec_type == ExecType.SSH:
            self._delegate = SshExec(self.cmd, self.exec_info)
        elif self.exec_info.exec_type == ExecType.PSSH:
            self._delegate = PsshExec(self.cmd, self.exec_info)
        elif self.exec_info.exec_type in [ExecType.MPI, ExecType.OPENMPI, 
                                         ExecType.MPICH, ExecType.INTEL_MPI, 
                                         ExecType.CRAY_MPICH]:
            self._delegate = MpiExec(self.cmd, self.exec_info)
        else:
            raise ValueError(f"Unsupported execution type: {self.exec_info.exec_type}")
            
        # Copy delegate attributes to self
        self.exit_code = self._delegate.exit_code
        self.stdout = self._delegate.stdout
        self.stderr = self._delegate.stderr
        self.processes = self._delegate.processes
        self.output_threads = self._delegate.output_threads
        
        return self._delegate
        
    def get_cmd(self) -> str:
        """Get the command string"""
        return self.cmd
        
    def wait(self, hostname: str = 'localhost') -> int:
        """Wait for execution to complete"""
        if self._delegate:
            return self._delegate.wait(hostname)
        return 0
        
    def wait_all(self) -> Dict[str, int]:
        """Wait for all executions to complete"""
        if self._delegate:
            return self._delegate.wait_all()
        return {}
```

### `jarvis_cd/shell/exec_info.py`

```python
"""
Execution information classes for Jarvis shell execution.
Contains ExecType enums and ExecInfo data structures.
"""
from enum import Enum
from typing import Dict, Any, Optional, List, Union
from pathlib import Path


class ExecType(Enum):
    """Execution types supported by Jarvis"""
    LOCAL = "local"
    SSH = "ssh"
    PSSH = "pssh"
    MPI = "mpi"
    OPENMPI = "openmpi"
    MPICH = "mpich"
    INTEL_MPI = "intel_mpi"
    CRAY_MPICH = "cray_mpich"
    SCP = "scp"
    PSCP = "pscp"


class ExecInfo:
    """
    Contains all information needed to execute a program. This includes
    parameters such as the path to key-pairs, the hosts to run the program
    on, number of processes, etc.
    """
    
    def __init__(self, exec_type=ExecType.LOCAL, nprocs=None, ppn=None,
                 user=None, pkey=None, port=None,
                 hostfile=None, env=None,
                 sleep_ms=0, sudo=False, sudoenv=True, cwd=None,
                 collect_output=None, pipe_stdout=None, pipe_stderr=None,
                 hide_output=None, exec_async=False, stdin=None,
                 strict_ssh=False, timeout=None, **kwargs):
        """
        Initialize execution information.

        :param exec_type: How to execute a program. SSH, MPI, Local, etc.
        :param nprocs: Number of processes to spawn. E.g., MPI uses this
        :param ppn: Number of processes per node. E.g., MPI uses this
        :param user: The user to execute command under. E.g., SSH, PSSH
        :param pkey: The path to the private key. E.g., SSH, PSSH
        :param port: The port to use for connection. E.g., SSH, PSSH
        :param hostfile: The hosts to launch command on. E.g., PSSH, MPI
        :param env: The environment variables to use for command.
        :param sleep_ms: Sleep for a period of time AFTER executing
        :param sudo: Execute command with root privilege. E.g., SSH, PSSH
        :param sudoenv: Support environment preservation in sudo
        :param cwd: Set current working directory. E.g., SSH, PSSH
        :param collect_output: Collect program output in python buffer
        :param pipe_stdout: Pipe STDOUT into a file. (path string)
        :param pipe_stderr: Pipe STDERR into a file. (path string)
        :param hide_output: Whether to print output to console
        :param exec_async: Whether to execute program asynchronously
        :param stdin: Any input needed by the program. Only local
        :param strict_ssh: Strict ssh host key verification
        :param timeout: Timeout subprocess within timeframe
        :param kwargs: Additional unknown parameters (silently ignored)
        """
        self.exec_type = exec_type
        self.nprocs = nprocs or 1
        self.ppn = ppn
        self.user = user
        self.pkey = pkey
        self.port = port or 22
        self.hostfile = hostfile
        self.env = env or {}
        self.sleep_ms = sleep_ms
        self.sudo = sudo
        self.sudoenv = sudoenv
        self.cwd = cwd
        self.collect_output = collect_output if collect_output is not None else True
        self.pipe_stdout = pipe_stdout
        self.pipe_stderr = pipe_stderr
        self.hide_output = hide_output if hide_output is not None else False
        self.exec_async = exec_async
        self.stdin = stdin
        self.strict_ssh = strict_ssh
        self.timeout = timeout

        # Basic environment for process execution (without LD_PRELOAD)
        # This is used for launching MPI itself, not the MPI processes
        self.basic_env = self.env.copy()
        if 'LD_PRELOAD' in self.basic_env:
            del self.basic_env['LD_PRELOAD']
        
    def mod(self, **kwargs):
        """
        Create a modified copy of this ExecInfo with updated parameters.

        :param kwargs: Parameters to modify
        :return: New ExecInfo instance with modifications
        """
        # Create a copy of current attributes
        current_attrs = {}
        for attr in ['exec_type', 'nprocs', 'ppn', 'user', 'pkey', 'port',
                     'hostfile', 'env', 'sleep_ms', 'sudo', 'sudoenv', 'cwd',
                     'collect_output', 'pipe_stdout', 'pipe_stderr', 'hide_output',
                     'exec_async', 'stdin', 'strict_ssh', 'timeout']:
            current_attrs[attr] = getattr(self, attr)

        # Update with new values
        current_attrs.update(kwargs)

        return ExecInfo(**current_attrs)


class SshExecInfo(ExecInfo):
    """SSH-specific execution information"""
    
    def __init__(self, **kwargs):
        super().__init__(exec_type=ExecType.SSH, **kwargs)


class PsshExecInfo(ExecInfo):
    """PSSH-specific execution information"""
    
    def __init__(self, **kwargs):
        super().__init__(exec_type=ExecType.PSSH, **kwargs)


class MpiExecInfo(ExecInfo):
    """MPI-specific execution information"""
    
    def __init__(self, **kwargs):
        super().__init__(exec_type=ExecType.MPI, **kwargs)


class LocalExecInfo(ExecInfo):
    """Local execution information"""
    
    def __init__(self, **kwargs):
        super().__init__(exec_type=ExecType.LOCAL, **kwargs)


class ScpExecInfo(ExecInfo):
    """SCP-specific execution information"""
    
    def __init__(self, **kwargs):
        super().__init__(exec_type=ExecType.SCP, **kwargs)


class PscpExecInfo(ExecInfo):
    """PSCP-specific execution information"""
    
    def __init__(self, **kwargs):
        super().__init__(exec_type=ExecType.PSCP, **kwargs)
```

### `jarvis_cd/shell/mpi_exec.py`

```python
"""
MPI execution classes for Jarvis shell execution.
"""
from abc import abstractmethod
from typing import Union, List, Dict, Any
from .core_exec import LocalExec, MpiVersion
from .exec_info import ExecInfo, MpiExecInfo, ExecType
from ..util.hostfile import Hostfile


class LocalMpiExec(LocalExec):
    """
    Base class used by all MPI implementations.
    """

    def __init__(self, cmd: Union[str, List[Dict[str, Any]]], exec_info: MpiExecInfo):
        """
        Initialize MPI execution.

        :param cmd: Command to execute with MPI. Can be:
                   - A string: single command
                   - A list of dicts: multiple commands with format:
                     [{'cmd': str, 'nprocs': int, 'disable_preload': bool (optional)}, ...]
                     If 'disable_preload' is True, LD_PRELOAD will be removed for that command
        :param exec_info: MPI execution information
        """
        self.nprocs = exec_info.nprocs
        self.ppn = exec_info.ppn
        self.hostfile = exec_info.hostfile or Hostfile(['localhost'])
        self.mpi_env = exec_info.env
        self.ssh_port = exec_info.port if exec_info.port else None

        # Process command format
        if isinstance(cmd, str):
            # Single command format
            self.original_cmd = cmd
            self.cmd_list = None
        else:
            # Multi-command format: list of dicts
            self.cmd_list = self._process_cmd_list(cmd)
            self.original_cmd = None

        # Build MPI command
        mpi_cmd = self.mpicmd()

        # Create modified exec_info for LocalExec
        local_info = exec_info.mod(
            env=exec_info.basic_env
        )

        super().__init__(mpi_cmd, local_info)

    def _process_cmd_list(self, cmd_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Process list of command dicts, calculating final nprocs for each.

        :param cmd_list: List of dicts with 'cmd', 'nprocs', and optional 'disable_preload' keys
        :return: Processed list with calculated nprocs and environment settings
        """
        if not cmd_list:
            raise ValueError("Command list cannot be empty")

        processed = []
        total_nprocs_allocated = 0

        # Process all but the last command
        for i, cmd_dict in enumerate(cmd_list[:-1]):
            nprocs = cmd_dict.get('nprocs', 0)
            processed.append({
                'cmd': cmd_dict['cmd'],
                'nprocs': nprocs,
                'disable_preload': cmd_dict.get('disable_preload', False)
            })
            total_nprocs_allocated += nprocs

        # Process last command - calculate remaining nprocs
        last_cmd = cmd_list[-1]
        remaining_nprocs = self.nprocs - total_nprocs_allocated

        if remaining_nprocs < 0:
            raise ValueError(f"Total nprocs ({total_nprocs_allocated}) exceeds available ({self.nprocs})")

        processed.append({
            'cmd': last_cmd['cmd'],
            'nprocs': remaining_nprocs,
            'disable_preload': last_cmd.get('disable_preload', False)
        })

        return processed

    @abstractmethod
    def mpicmd(self) -> str:
        """Build MPI command. Must be implemented by subclasses."""
        pass


class OpenMpiExec(LocalMpiExec):
    """
    Execute commands using OpenMPI.
    """

    def mpicmd(self) -> str:
        """Build OpenMPI command"""
        params = ['mpiexec']
        params.append('--oversubscribe')
        params.append('--allow-run-as-root')  # For docker

        # Set SSH port if explicitly specified (SSH config will be used otherwise)
        if self.ssh_port and self.ssh_port != 22:
            params.append(f'--mca plm_rsh_args "-p {self.ssh_port}"')

        if self.ppn is not None:
            params.append(f'-npernode {self.ppn}')

        if len(self.hostfile):
            if self.hostfile.path is None:
                params.append(f'--host {",".join(self.hostfile.hosts)}')
            else:
                params.append(f'--hostfile {self.hostfile.path}')

        # Handle multi-command format
        if self.cmd_list:
            cmd_parts = []
            for cmd_dict in self.cmd_list:
                nprocs = cmd_dict['nprocs']
                cmd = cmd_dict['cmd']
                disable_preload = cmd_dict.get('disable_preload', False)

                # Skip commands with 0 nprocs
                if nprocs > 0:
                    # Build per-command environment variables
                    cmd_env = self.mpi_env.copy()
                    if disable_preload and 'LD_PRELOAD' in cmd_env:
                        del cmd_env['LD_PRELOAD']

                    # Add environment variables for this command
                    env_args = ' '.join([f'-x {key}="{val}"' for key, val in cmd_env.items()])
                    cmd_parts.append(f'{env_args} -n {nprocs} {cmd}')
            # Join with ' : ' for multiple commands
            params.append(' : '.join(cmd_parts))
        else:
            # Single command format - add global environment variables
            params.extend([f'-x {key}="{val}"' for key, val in self.mpi_env.items()])
            params.append(f'-n {self.nprocs}')
            params.append(self.original_cmd)

        return ' '.join(params)


class MpichExec(LocalMpiExec):
    """
    Execute commands using MPICH.
    """

    def mpicmd(self) -> str:
        """Build MPICH command"""
        params = ['mpiexec']

        # Set SSH port if explicitly specified (SSH config will be used otherwise)
        if self.ssh_port and self.ssh_port != 22:
            params.append(f'-bootstrap-exec-args "-p {self.ssh_port}"')

        if self.ppn is not None:
            params.append(f'-ppn {self.ppn}')

        if len(self.hostfile):
            if self.hostfile.path is None:
                params.append(f'--host {",".join(self.hostfile.hosts)}')
            else:
                params.append(f'--hostfile {self.hostfile.path}')

        # Handle multi-command format
        if self.cmd_list:
            cmd_parts = []
            for cmd_dict in self.cmd_list:
                nprocs = cmd_dict['nprocs']
                cmd = cmd_dict['cmd']
                disable_preload = cmd_dict.get('disable_preload', False)

                # Skip commands with 0 nprocs
                if nprocs > 0:
                    # Build per-command environment variables
                    cmd_env = self.mpi_env.copy()
                    if disable_preload and 'LD_PRELOAD' in cmd_env:
                        del cmd_env['LD_PRELOAD']

                    # Add environment variables for this command
                    env_args = ' '.join([f'-env {key}="{val}"' for key, val in cmd_env.items()])
                    cmd_parts.append(f'{env_args} -n {nprocs} {cmd}')
            # Join with ' : ' for multiple commands
            params.append(' : '.join(cmd_parts))
        else:
            # Single command format - add global environment variables
            params.extend([f'-genv {key}="{val}"' for key, val in self.mpi_env.items()])
            params.append(f'-n {self.nprocs}')
            params.append(self.original_cmd)

        return ' '.join(params)


class IntelMpiExec(MpichExec):
    """
    Execute commands using Intel MPI (similar to MPICH).
    """
    pass


class CrayMpichExec(LocalMpiExec):
    """
    Execute commands using Cray MPICH.
    """

    def mpicmd(self) -> str:
        """Build Cray MPICH command"""
        params = ['mpiexec']

        if self.ppn is not None:
            params.append(f'--ppn {self.ppn}')

        if len(self.hostfile):
            if (self.hostfile.hosts[0] == 'localhost' and
                len(self.hostfile) == 1):
                pass  # Skip hostfile for localhost-only
            elif self.hostfile.path is None:
                params.append(f'--hosts {",".join(self.hostfile.hosts)}')
            else:
                params.append(f'--hostfile {self.hostfile.path}')

        # Handle multi-command format
        if self.cmd_list:
            cmd_parts = []
            for cmd_dict in self.cmd_list:
                nprocs = cmd_dict['nprocs']
                cmd = cmd_dict['cmd']
                disable_preload = cmd_dict.get('disable_preload', False)

                # Skip commands with 0 nprocs
                if nprocs > 0:
                    # Build per-command environment variables
                    cmd_env = self.mpi_env.copy()
                    if disable_preload and 'LD_PRELOAD' in cmd_env:
                        del cmd_env['LD_PRELOAD']

                    # Add environment variables for this command
                    env_args = ' '.join([f'--env {key}="{val}"' for key, val in cmd_env.items()])
                    cmd_parts.append(f'{env_args} -n {nprocs} {cmd}')
            # Join with ' : ' for multiple commands
            params.append(' : '.join(cmd_parts))
        else:
            # Single command format - add global environment variables
            params.extend([f'--env {key}="{val}"' for key, val in self.mpi_env.items()])
            params.append(f'-n {self.nprocs}')
            params.append(self.original_cmd)

        return ' '.join(params)


class MpiExec:
    """
    Factory class for MPI execution that automatically detects MPI implementation.
    """

    def __new__(cls, cmd: Union[str, List[Dict[str, Any]]], exec_info: MpiExecInfo):
        """
        Create appropriate MPI executor based on detected MPI implementation.

        :param cmd: Command to execute. Can be:
                   - A string: single command
                   - A list of dicts: multiple commands with format [{'cmd': str, 'nprocs': int}, ...]
        :param exec_info: MPI execution information
        :return: Appropriate MPI executor instance
        """
        # Detect MPI version
        mpi_version_detector = MpiVersion(exec_info)
        mpi_type = mpi_version_detector.version
        
        # Create appropriate executor
        if mpi_type == ExecType.OPENMPI:
            return OpenMpiExec(cmd, exec_info)
        elif mpi_type == ExecType.MPICH:
            return MpichExec(cmd, exec_info)
        elif mpi_type == ExecType.INTEL_MPI:
            return IntelMpiExec(cmd, exec_info)
        elif mpi_type == ExecType.CRAY_MPICH:
            return CrayMpichExec(cmd, exec_info)
        else:
            # Default to MPICH
            print(f"Unknown MPI type {mpi_type}, defaulting to MPICH")
            return MpichExec(cmd, exec_info)
```

### `jarvis_cd/shell/process.py`

```python
"""
Process utility classes for Jarvis shell execution.
"""
from typing import Union, List
from .exec_factory import Exec
from .exec_info import ExecInfo, LocalExecInfo


class Kill(Exec):
    """
    Kill all processes which match the name regex.
    """
    
    def __init__(self, cmd: str, exec_info: ExecInfo = None, partial: bool = True):
        """
        Kill all processes which match the name regex.
        
        :param cmd: A regex for the command to kill
        :param exec_info: Info needed to execute the command
        :param partial: If True, use partial matching (-f flag)
        """
        if exec_info is None:
            exec_info = LocalExecInfo()
            
        partial_flag = "-f" if partial else ""
        kill_cmd = f"pkill -9 {partial_flag} '{cmd}'"
        super().__init__(kill_cmd, exec_info)


class KillAll(Exec):
    """
    Kill all processes owned by the current user.
    """
    
    def __init__(self, exec_info: ExecInfo = None):
        """
        Kill all processes owned by current user.
        
        :param exec_info: Info needed to execute the command
        """
        if exec_info is None:
            exec_info = LocalExecInfo()
            
        kill_cmd = "pkill -9 -u $(whoami)"
        super().__init__(kill_cmd, exec_info)


class Which(Exec):
    """
    Find the location of an executable.
    """
    
    def __init__(self, executable: str, exec_info: ExecInfo = None):
        """
        Find executable location.
        
        :param executable: Name of executable to find
        :param exec_info: Execution information
        """
        if exec_info is None:
            exec_info = LocalExecInfo()
            
        which_cmd = f"which {executable}"
        super().__init__(which_cmd, exec_info)
        self.executable = executable
        
    def get_path(self) -> str:
        """
        Get the path to the executable.
        
        :return: Path to executable or empty string if not found
        """
        return self.stdout.get('localhost', '').strip()
        
    def exists(self) -> bool:
        """
        Check if executable exists.
        
        :return: True if executable was found
        """
        return self.exit_code.get('localhost', 1) == 0 and bool(self.get_path())


class Mkdir(Exec):
    """
    Create directories.
    """
    
    def __init__(self, paths: Union[str, List[str]], exec_info: ExecInfo = None, 
                 parents: bool = True, exist_ok: bool = True):
        """
        Create directories.
        
        :param paths: Directory path(s) to create
        :param exec_info: Execution information
        :param parents: Create parent directories if needed
        :param exist_ok: Don't error if directory already exists
        """
        if exec_info is None:
            exec_info = LocalExecInfo()
            
        if isinstance(paths, str):
            paths = [paths]
            
        flags = []
        if parents:
            flags.append('-p')
            
        # Build command with properly escaped paths
        flag_str = ' '.join(flags)
        path_str = ' '.join([f'"{path}"' for path in paths])
        mkdir_cmd = f"mkdir {flag_str} {path_str}".strip()
            
        super().__init__(mkdir_cmd, exec_info)


class Rm(Exec):
    """
    Remove files and directories.
    """
    
    def __init__(self, paths: Union[str, List[str]], exec_info: ExecInfo = None,
                 recursive: bool = False, force: bool = True):
        """
        Remove files and directories.
        
        :param paths: File/directory path(s) to remove
        :param exec_info: Execution information
        :param recursive: Remove directories recursively
        :param force: Force removal without prompting
        """
        if exec_info is None:
            exec_info = LocalExecInfo()
            
        if isinstance(paths, str):
            paths = [paths]
            
        flags = []
        if recursive:
            flags.append('-r')
        if force:
            flags.append('-f')
            
        # Build command with properly escaped paths
        flag_str = ' '.join(flags)
        path_str = ' '.join([f'"{path}"' for path in paths])
        rm_cmd = f"rm {flag_str} {path_str}".strip()
            
        super().__init__(rm_cmd, exec_info)


class Chmod(Exec):
    """
    Change file permissions.
    """
    
    def __init__(self, paths: Union[str, List[str]], mode: str, exec_info: ExecInfo = None,
                 recursive: bool = False):
        """
        Change file permissions.
        
        :param paths: File/directory path(s) to modify
        :param mode: Permission mode (e.g., '755', '+x', 'u+w')
        :param exec_info: Execution information
        :param recursive: Apply recursively
        """
        if exec_info is None:
            exec_info = LocalExecInfo()
            
        if isinstance(paths, str):
            paths = [paths]
            
        flags = []
        if recursive:
            flags.append('-R')
            
        # Build command with properly escaped paths
        flag_str = ' '.join(flags)
        path_str = ' '.join([f'"{path}"' for path in paths])
        chmod_cmd = f"chmod {flag_str} {mode} {path_str}".strip()
            
        super().__init__(chmod_cmd, exec_info)


class Sleep(Exec):
    """
    Sleep for a specified duration.
    """
    
    def __init__(self, duration: Union[int, float], exec_info: ExecInfo = None):
        """
        Sleep for specified duration.
        
        :param duration: Sleep duration in seconds
        :param exec_info: Execution information
        """
        if exec_info is None:
            exec_info = LocalExecInfo()
            
        sleep_cmd = f"sleep {duration}"
        super().__init__(sleep_cmd, exec_info)


class Echo(Exec):
    """
    Echo text to stdout.
    """

    def __init__(self, text: str, exec_info: ExecInfo = None):
        """
        Echo text to stdout.

        :param text: Text to echo
        :param exec_info: Execution information
        """
        if exec_info is None:
            exec_info = LocalExecInfo()

        echo_cmd = f'echo "{text}"'
        super().__init__(echo_cmd, exec_info)


class GdbServer(Exec):
    """
    Launch a gdbserver for remote debugging.
    """

    def __init__(self, cmd: str, port: int, exec_info: ExecInfo = None):
        """
        Launch a gdbserver for remote debugging.

        :param cmd: Command to run under gdbserver
        :param port: Port number for gdbserver to listen on
        :param exec_info: Execution information
        """
        if exec_info is None:
            exec_info = LocalExecInfo()

        gdbserver_cmd = f"gdbserver :{port} {cmd}"
        super().__init__(gdbserver_cmd, exec_info)
        self.port = port
```

### `jarvis_cd/shell/resource_graph_exec.py`

```python
"""
Shell execution class for resource graph collection.
"""
import sys
from pathlib import Path
from .core_exec import CoreExec
from .exec_info import ExecInfo


class ResourceGraphExec(CoreExec):
    """
    Execute resource graph collection script on remote or local machines.
    """
    
    def __init__(self, exec_info: ExecInfo, benchmark: bool = True, duration: int = 25):
        """
        Initialize resource graph execution.
        
        :param exec_info: Execution information (LocalExecInfo, SshExecInfo, etc.)
        :param benchmark: Whether to run performance benchmarks
        :param duration: Benchmark duration in seconds
        """
        super().__init__()
        self.exec_info = exec_info
        self.benchmark = benchmark
        self.duration = duration
        
        # Find the resource graph script
        # Look for it in the bin directory relative to this module
        current_dir = Path(__file__).parent
        project_root = current_dir.parent.parent
        self.script_path = project_root / 'bin' / 'jarvis_resource_graph'
        
        if not self.script_path.exists():
            raise FileNotFoundError(f"Resource graph script not found at {self.script_path}")
            
        # Build command
        self._build_command()
        
    def _build_command(self):
        """Build the command to execute."""
        cmd_parts = [str(self.script_path)]
        
        if not self.benchmark:
            cmd_parts.append('--no-benchmark')
            
        if self.duration != 25:
            cmd_parts.extend(['--duration', str(self.duration)])
            
        self.cmd = ' '.join(cmd_parts)
        
    def get_cmd(self) -> str:
        """Get the command string."""
        return self.cmd
        
    def run(self):
        """Execute the resource graph collection."""
        from .exec_factory import Exec
        
        # Use the Exec factory to create appropriate executor
        self._executor = Exec(self.cmd, self.exec_info)
        
        # Wait for completion if not async
        if not self.exec_info.exec_async:
            self._executor.wait_all()
        
        # Copy results from executor
        self.exit_code = self._executor.exit_code.copy()
        self.stdout = self._executor.stdout.copy()
        self.stderr = self._executor.stderr.copy()
        self.processes = self._executor.processes.copy()
        
        return self._executor
```

### `jarvis_cd/shell/scp_exec.py`

```python
"""
SCP/file transfer execution classes for Jarvis shell execution.
"""
import os
import threading
from typing import List, Union, Tuple, Dict
from pathlib import Path

from .core_exec import CoreExec, LocalExec
from .exec_info import ScpExecInfo, PscpExecInfo
from ..util.hostfile import Hostfile


class _Scp(LocalExec):
    """
    Internal SCP class for copying files using rsync.
    """
    
    def __init__(self, src_path: str, dst_path: str, exec_info: ScpExecInfo):
        """
        Copy a file or directory from source to destination via rsync.
        
        :param src_path: The path to the file on the host
        :param dst_path: The desired file path on the remote host
        :param exec_info: Info needed to execute command with SSH
        """
        if not exec_info.hostfile or len(exec_info.hostfile) == 0:
            raise ValueError("SCP requires a hostfile with at least one host")
            
        self.addr = exec_info.hostfile.hosts[0]
        
        # Skip if copying to localhost
        if self.addr in ['localhost', '127.0.0.1']:
            # For localhost, just do a local copy if paths are different
            if src_path != dst_path:
                super().__init__(f'cp -r "{src_path}" "{dst_path}"', exec_info)
            else:
                # Same path on localhost - no operation needed
                super().__init__('true', exec_info)  # No-op command
            return
            
        self.src_path = src_path
        self.dst_path = dst_path
        self.user = exec_info.user
        self.pkey = exec_info.pkey
        self.port = exec_info.port
        self.sudo = exec_info.sudo
        
        # Build rsync command
        rsync_cmd = self.build_rsync_cmd(src_path, dst_path)
        
        # Create modified exec_info for LocalExec
        local_info = exec_info.mod(env=exec_info.basic_env)
        
        super().__init__(rsync_cmd, local_info)
        
    def build_rsync_cmd(self, src_path: str, dst_path: str) -> str:
        """
        Build rsync command for file transfer.
        
        :param src_path: Source path
        :param dst_path: Destination path
        :return: Complete rsync command
        """
        lines = ['rsync -ha']
        
        # Add SSH options if needed
        if self.pkey is not None or self.port is not None:
            ssh_lines = ['ssh']
            if self.pkey is not None:
                ssh_lines.append(f'-i {self.pkey}')
            if self.port is not None and self.port != 22:
                ssh_lines.append(f'-p {self.port}')
            ssh_cmd = ' '.join(ssh_lines)
            lines.append(f'-e \'{ssh_cmd}\'')
            
        # Add source path
        lines.append(f'"{src_path}"')
        
        # Add destination
        if self.user is not None:
            lines.append(f'"{self.user}@{self.addr}:{dst_path}"')
        else:
            lines.append(f'"{self.addr}:{dst_path}"')
            
        return ' '.join(lines)


class ScpExec(CoreExec):
    """
    Secure copy data between hosts using rsync.
    """
    
    def __init__(self, paths: Union[str, List[str], List[Tuple[str, str]]], 
                 exec_info: ScpExecInfo):
        """
        Copy files via rsync.
        
        Case 1: Paths is a single file:
        paths = '/tmp/hi.txt'
        '/tmp/hi.txt' will be copied to user@host:/tmp/hi.txt
        
        Case 2: Paths is a list of files:
        paths = ['/tmp/hi1.txt', '/tmp/hi2.txt']
        Repeat Case 1 twice.
        
        Case 3: Paths is a list of tuples of files:
        paths = [('/tmp/hi.txt', '/tmp/remote_hi.txt')]
        '/tmp/hi.txt' will be copied to user@host:'/tmp/remote_hi.txt'
        
        :param paths: Either a path to a file, a list of files, or a list of
        tuples of files.
        :param exec_info: Connection information for SSH
        """
        super().__init__()
        self.paths = paths
        self.exec_info = exec_info
        self.scp_nodes = []
        
        # Determine which execution pattern to use
        if isinstance(paths, str):
            self._exec_single_path(paths)
        elif isinstance(paths, list):
            if len(paths) == 0:
                raise ValueError('Must have at least one path to scp')
            elif isinstance(paths[0], str):
                self._exec_many_paths(paths)
            elif isinstance(paths[0], (tuple, list)):
                self._exec_many_paths_tuple(paths)
        else:
            raise ValueError(f"Invalid paths type: {type(paths)}")
            
        # If not async, wait for completion
        if not self.exec_info.exec_async:
            self.wait_all_scp()
            
    def _exec_single_path(self, path: str):
        """Execute SCP for a single path"""
        self.scp_nodes.append(_Scp(path, path, self.exec_info))
        
    def _exec_many_paths(self, paths: List[str]):
        """Execute SCP for multiple paths (same src and dst names)"""
        for path in paths:
            self.scp_nodes.append(_Scp(path, path, self.exec_info))
            
    def _exec_many_paths_tuple(self, path_tlist: List[Tuple[str, str]]):
        """Execute SCP for list of (src, dst) tuples"""
        for src, dst in path_tlist:
            self.scp_nodes.append(_Scp(src, dst, self.exec_info))

    def run(self):
        """Execute SCP operations (already done in __init__)"""
        # SCP operations are started in __init__ via _exec_* methods
        pass

    def wait_all_scp(self) -> Dict[str, int]:
        """
        Wait for all SCP operations to complete.
        
        :return: Dictionary of exit codes
        """
        self.wait_list(self.scp_nodes)
        self.smash_list_outputs(self.scp_nodes)
        self.set_exit_code()
        return self.exit_code
        
    def wait_list(self, exec_list: List[_Scp]):
        """Wait for a list of executors to complete"""
        for executor in exec_list:
            executor.wait_all()
            
    def smash_list_outputs(self, exec_list: List[_Scp]):
        """Combine outputs from multiple executors"""
        for executor in exec_list:
            for hostname in executor.stdout:
                if hostname not in self.stdout:
                    self.stdout[hostname] = ""
                self.stdout[hostname] += executor.stdout[hostname]
                
            for hostname in executor.stderr:
                if hostname not in self.stderr:
                    self.stderr[hostname] = ""
                self.stderr[hostname] += executor.stderr[hostname]
                
    def set_exit_code(self):
        """Set exit code based on SCP operations"""
        self.set_exit_code_list(self.scp_nodes)
        
    def set_exit_code_list(self, exec_list: List[_Scp]):
        """Set exit code from a list of executors"""
        for executor in exec_list:
            for hostname in executor.exit_code:
                # Use the highest (worst) exit code
                current_code = self.exit_code.get(hostname, 0)
                new_code = executor.exit_code[hostname]
                self.exit_code[hostname] = max(current_code, new_code)
                
    def get_cmd(self) -> str:
        """Get description of SCP operation"""
        if isinstance(self.paths, str):
            return f"scp {self.paths}"
        elif isinstance(self.paths, list):
            if len(self.paths) == 1:
                if isinstance(self.paths[0], str):
                    return f"scp {self.paths[0]}"
                else:
                    return f"scp {self.paths[0][0]} -> {self.paths[0][1]}"
            else:
                return f"scp {len(self.paths)} files"
        return "scp operation"


class PscpExec(CoreExec):
    """
    Parallel SCP execution across multiple hosts.
    """
    
    def __init__(self, paths: Union[str, List[str], List[Tuple[str, str]]], 
                 exec_info: PscpExecInfo):
        """
        Copy files to multiple hosts in parallel.
        
        :param paths: Paths to copy (same format as ScpExec)
        :param exec_info: Parallel SCP execution information
        """
        super().__init__()
        self.paths = paths
        self.exec_info = exec_info
        self.scp_executors = {}
        
        if not exec_info.hostfile or len(exec_info.hostfile) == 0:
            raise ValueError("PSCP requires a hostfile with at least one host")
            
        # Start SCP on each host
        self.run()
        
        # If not async, wait for all to complete
        if not exec_info.exec_async:
            self.wait_all()
            
    def run(self):
        """Execute SCP on all hosts in parallel"""
        threads = []
        
        for hostname in self.exec_info.hostfile.hosts:
            # Create single-host hostfile for this SCP operation
            host_hostfile = Hostfile(hosts=[hostname], find_ips=False)
            
            # Create SCP exec info for this host
            scp_info = ScpExecInfo(
                user=self.exec_info.user,
                pkey=self.exec_info.pkey,
                port=self.exec_info.port,
                hostfile=host_hostfile,
                env=self.exec_info.env,
                sudo=self.exec_info.sudo,
                sudoenv=self.exec_info.sudoenv,
                cwd=self.exec_info.cwd,
                collect_output=self.exec_info.collect_output,
                pipe_stdout=self.exec_info.pipe_stdout,
                pipe_stderr=self.exec_info.pipe_stderr,
                hide_output=self.exec_info.hide_output,
                exec_async=True,  # Always async for parallel execution
                strict_ssh=self.exec_info.strict_ssh,
                timeout=self.exec_info.timeout
            )
            
            # Start SCP execution in a thread
            thread = threading.Thread(
                target=self._execute_on_host,
                args=(hostname, scp_info)
            )
            thread.daemon = True
            thread.start()
            threads.append(thread)
            
        # Wait for all threads to start their SCP processes
        for thread in threads:
            thread.join()
            
    def _execute_on_host(self, hostname: str, scp_info: ScpExecInfo):
        """
        Execute SCP on a specific host.
        
        :param hostname: Target hostname
        :param scp_info: SCP execution information
        """
        try:
            scp_exec = ScpExec(self.paths, scp_info)
            self.scp_executors[hostname] = scp_exec
            
        except Exception as e:
            print(f"Error executing SCP on {hostname}: {e}")
            self.exit_code[hostname] = 1
            self.stdout[hostname] = ""
            self.stderr[hostname] = str(e)
            
    def wait(self, hostname: str) -> int:
        """
        Wait for SCP on a specific host to complete.
        
        :param hostname: Hostname to wait for
        :return: Exit code
        """
        if hostname in self.scp_executors:
            scp_exec = self.scp_executors[hostname]
            exit_codes = scp_exec.wait_all_scp()
            
            # Copy outputs (SCP uses localhost as key)
            self.stdout[hostname] = scp_exec.stdout.get('localhost', "")
            self.stderr[hostname] = scp_exec.stderr.get('localhost', "")
            self.exit_code[hostname] = exit_codes.get('localhost', 0)
            
            return self.exit_code[hostname]
        return 0
        
    def get_cmd(self) -> str:
        """Get description of parallel SCP operation"""
        return f"pscp to {len(self.exec_info.hostfile.hosts)} hosts"
```

### `jarvis_cd/shell/ssh_exec.py`

```python
"""
SSH execution classes for Jarvis shell execution.
"""
import threading
from typing import List, Dict, Any
from .core_exec import CoreExec, LocalExec
from .exec_info import ExecInfo, SshExecInfo, PsshExecInfo
from ..util.hostfile import Hostfile


class SshExec(LocalExec):
    """
    Execute commands on remote hosts using SSH.
    Inherits from LocalExec to reuse subprocess management.
    """
    
    def __init__(self, cmd: str, exec_info: SshExecInfo, hostname: str = None):
        """
        Initialize SSH execution.
        
        :param cmd: Command to execute remotely
        :param exec_info: SSH execution information
        :param hostname: Target hostname (if None, uses first host from hostfile)
        """
        self.original_cmd = cmd
        self.target_hostname = hostname or (exec_info.hostfile.hosts[0] if exec_info.hostfile else 'localhost')
        
        # Build SSH command
        ssh_cmd = self._build_ssh_command(cmd, exec_info)
        
        # Initialize with SSH command
        super().__init__(ssh_cmd, exec_info)
        
        # Override hostname for output tracking
        self.hostname = self.target_hostname
        if 'localhost' in self.stdout:
            self.stdout[self.hostname] = self.stdout.pop('localhost')
        if 'localhost' in self.stderr:
            self.stderr[self.hostname] = self.stderr.pop('localhost')
        if 'localhost' in self.exit_code:
            self.exit_code[self.hostname] = self.exit_code.pop('localhost')
        if 'localhost' in self.processes:
            self.processes[self.hostname] = self.processes.pop('localhost')
        if 'localhost' in self.output_threads:
            self.output_threads[self.hostname] = self.output_threads.pop('localhost')
            
    def _build_ssh_command(self, cmd: str, exec_info: SshExecInfo) -> str:
        """
        Build SSH command with all necessary parameters.
        
        :param cmd: Original command to execute
        :param exec_info: SSH execution information
        :return: Complete SSH command string
        """
        ssh_parts = ['ssh']
        
        # SSH options
        if not exec_info.strict_ssh:
            ssh_parts.extend([
                '-o', 'StrictHostKeyChecking=no',
                '-o', 'UserKnownHostsFile=/dev/null'
            ])
            
        # Port
        if exec_info.port and exec_info.port != 22:
            ssh_parts.extend(['-p', str(exec_info.port)])
            
        # Private key
        if exec_info.pkey:
            ssh_parts.extend(['-i', exec_info.pkey])
            
        # Connection timeout
        if exec_info.timeout:
            ssh_parts.extend(['-o', f'ConnectTimeout={exec_info.timeout}'])
            
        # Target host
        if exec_info.user:
            ssh_parts.append(f'{exec_info.user}@{self.target_hostname}')
        else:
            ssh_parts.append(self.target_hostname)
            
        # Build remote command
        remote_cmd = self._build_remote_command(cmd, exec_info)
        ssh_parts.append(f"'{remote_cmd}'")
        
        return ' '.join(ssh_parts)
        
    def _build_remote_command(self, cmd: str, exec_info: SshExecInfo) -> str:
        """
        Build the command to execute on the remote host.

        :param cmd: Original command
        :param exec_info: SSH execution information
        :return: Remote command string
        """
        cmd_parts = []
        env_prefix = []

        # Change directory if specified (must use && since it's a separate command)
        if exec_info.cwd:
            cmd_parts.append(f'cd {exec_info.cwd}')

        # Set environment variables (these go before the command on same line)
        if exec_info.env:
            for key, value in exec_info.env.items():
                # Escape special characters in environment values
                # Use double quotes to allow spaces, and escape internal double quotes
                escaped_value = str(value).replace('"', '\\"')
                env_prefix.append(f'{key}="{escaped_value}"')

        # Build the final command with env vars, sudo, and the actual command
        final_cmd_parts = []

        # Add environment variables
        if env_prefix:
            final_cmd_parts.append(' '.join(env_prefix))

        # Add sudo if requested
        if exec_info.sudo:
            if exec_info.sudoenv and exec_info.env:
                # Preserve environment with sudo
                final_cmd_parts.append('sudo -E')
            else:
                final_cmd_parts.append('sudo')

        # Add the actual command
        final_cmd_parts.append(cmd)

        # Join env vars, sudo, and command with spaces (they run together)
        final_cmd = ' '.join(final_cmd_parts)

        # Add cd command if needed (joined with &&)
        if cmd_parts:
            cmd_parts.append(final_cmd)
            return ' && '.join(cmd_parts)
        else:
            return final_cmd


class PsshExec(CoreExec):
    """
    Execute commands on multiple hosts using parallel SSH.
    """
    
    def __init__(self, cmd: str, exec_info: PsshExecInfo):
        """
        Initialize parallel SSH execution.
        
        :param cmd: Command to execute on all hosts
        :param exec_info: PSSH execution information
        """
        super().__init__()
        self.cmd = cmd
        self.exec_info = exec_info
        self.ssh_executors = {}
        
        if not exec_info.hostfile or len(exec_info.hostfile) == 0:
            raise ValueError("PSSH requires a hostfile with at least one host")
            
        # Start SSH execution on each host
        self.run()
        
        # If not async, wait for all to complete
        if not exec_info.exec_async:
            self.wait_all()
            
    def run(self):
        """Execute command on all hosts in parallel"""
        threads = []
        
        for hostname in self.exec_info.hostfile.hosts:
            # Create SSH exec info for this host
            ssh_info = SshExecInfo(
                user=self.exec_info.user,
                pkey=self.exec_info.pkey,
                port=self.exec_info.port,
                env=self.exec_info.env,
                sudo=self.exec_info.sudo,
                sudoenv=self.exec_info.sudoenv,
                cwd=self.exec_info.cwd,
                collect_output=self.exec_info.collect_output,
                pipe_stdout=self.exec_info.pipe_stdout,
                pipe_stderr=self.exec_info.pipe_stderr,
                hide_output=self.exec_info.hide_output,
                exec_async=True,  # Always async for parallel execution
                strict_ssh=self.exec_info.strict_ssh,
                timeout=self.exec_info.timeout
            )
            
            # Start SSH execution in a thread
            thread = threading.Thread(
                target=self._execute_on_host,
                args=(hostname, ssh_info)
            )
            thread.daemon = True
            thread.start()
            threads.append(thread)
            
        # Wait for all threads to start their SSH processes
        for thread in threads:
            thread.join()
            
    def _execute_on_host(self, hostname: str, ssh_info: SshExecInfo):
        """
        Execute command on a specific host.
        
        :param hostname: Target hostname
        :param ssh_info: SSH execution information
        """
        try:
            ssh_exec = SshExec(self.cmd, ssh_info, hostname)
            self.ssh_executors[hostname] = ssh_exec
            
            # Copy process reference for management
            if hostname in ssh_exec.processes:
                self.processes[hostname] = ssh_exec.processes[hostname]
                
        except Exception as e:
            print(f"Error executing on {hostname}: {e}")
            self.exit_code[hostname] = 1
            self.stdout[hostname] = ""
            self.stderr[hostname] = str(e)
            
    def wait(self, hostname: str) -> int:
        """
        Wait for execution on a specific host to complete.
        
        :param hostname: Hostname to wait for
        :return: Exit code
        """
        if hostname in self.ssh_executors:
            ssh_exec = self.ssh_executors[hostname]
            exit_code = ssh_exec.wait(hostname)
            
            # Copy outputs
            self.stdout[hostname] = ssh_exec.stdout.get(hostname, "")
            self.stderr[hostname] = ssh_exec.stderr.get(hostname, "")
            self.exit_code[hostname] = exit_code
            
            return exit_code
        return 0
        
    def get_cmd(self) -> str:
        """Get the original command"""
        return self.cmd
```

### `jarvis_cd/util/__init__.py`

```python
"""
Utility classes for Jarvis-CD.

This module provides utility classes including:
- Hostfile: Managing sets of machines with pattern expansion
- Logger: Colored logging utilities
- ArgParse: Command line argument parsing (located in parent directory)
- PkgArgParse: Package configuration argument parsing
"""

from .hostfile import Hostfile
from .logger import Logger, Color, logger
from .resource_graph import ResourceGraph
from .size_type import SizeType, size_to_bytes, human_readable_size
from .pkg_argparse import PkgArgParse

__all__ = [
    'Hostfile',
    'Logger',
    'Color',
    'logger',
    'ResourceGraph',
    'SizeType',
    'size_to_bytes',
    'human_readable_size',
    'PkgArgParse'
]
```

### `jarvis_cd/util/argparse.py`

```python
import re
import ast
from typing import Dict, List, Any, Optional


class ArgParse:
    def __init__(self):
        self.menus = {}
        self.commands = {}
        self.command_args = {}
        self.kwargs = {}
        self.remainder = []
        self.current_menu = None
        self.current_command = None
        
    def add_menu(self, name: str, msg: str = ""):
        """Add a menu to the parser"""
        self.menus[name] = {
            'name': name,
            'msg': msg,
            'commands': []
        }
        
    def add_cmd(self, name: str, msg: str = "", keep_remainder: bool = False, aliases: Optional[List[str]] = None):
        """Add a command to a menu"""
        if aliases is None:
            aliases = []
            
        # Extract menu name from command name
        parts = name.split()
        menu_name = ' '.join(parts[:-1]) if len(parts) > 1 else ''
        cmd_name = parts[-1] if parts else name
        
        self.commands[name] = {
            'name': name,
            'menu': menu_name,
            'cmd_name': cmd_name,
            'msg': msg,
            'keep_remainder': keep_remainder,
            'aliases': aliases,
            'args': []
        }
        
        # Add to menu's command list
        if menu_name in self.menus:
            self.menus[menu_name]['commands'].append(name)
            
        # Add aliases to commands dict
        for alias in aliases:
            self.commands[alias] = self.commands[name]
            
    def add_args(self, args_list: List[Dict[str, Any]]):
        """Add arguments to the most recently added command"""
        if not self.commands:
            raise ValueError("No command to add arguments to")
            
        # Get the last added command (excluding aliases)
        last_cmd = None
        for cmd_name, cmd_info in reversed(list(self.commands.items())):
            # Check if this is not an alias by seeing if it equals itself
            if cmd_name == cmd_info['name']:
                last_cmd = cmd_name
                break
                
        if last_cmd is None:
            last_cmd = list(self.commands.keys())[-1]
                
        self.command_args[last_cmd] = args_list
        
    def _parse_list_value(self, value: str, arg_spec: Dict[str, Any]) -> List[Any]:
        """Parse a list value from string representation"""
        try:
            # Remove surrounding quotes if present
            if value.startswith('"') and value.endswith('"'):
                value = value[1:-1]
            elif value.startswith("'") and value.endswith("'"):
                value = value[1:-1]
                
            # Handle Python-like list notation: "[item1, item2]" or "[(item1, item2), (item3, item4)]"
            if value.startswith('[') and value.endswith(']'):
                parsed = ast.literal_eval(value)
                return self._convert_list_items(parsed, arg_spec)
            else:
                # Handle single item that should be added to list
                return self._convert_list_items([self._parse_single_item(value, arg_spec)], arg_spec)
        except (ValueError, SyntaxError):
            # Fallback: try to parse as single item
            return [self._parse_single_item(value, arg_spec)]
            
    def _parse_single_item(self, value: str, arg_spec: Dict[str, Any]) -> Any:
        """Parse a single item for a list"""
        args_def = arg_spec.get('args', [])
        
        if not args_def:
            return value
        
        # Remove surrounding quotes if present
        if value.startswith('"') and value.endswith('"'):
            value = value[1:-1]
        elif value.startswith("'") and value.endswith("'"):
            value = value[1:-1]
            
        # Handle tuple-like notation: "(item1, item2)"
        if value.startswith('(') and value.endswith(')'):
            try:
                parsed = ast.literal_eval(value)
                if isinstance(parsed, tuple):
                    result = {}
                    for i, arg_def in enumerate(args_def):
                        if i < len(parsed):
                            result[arg_def['name']] = self._cast_value(parsed[i], arg_def.get('type', str))
                    return result
            except (ValueError, SyntaxError):
                pass
                
        # Handle single value
        if len(args_def) == 1:
            return {args_def[0]['name']: self._cast_value(value, args_def[0].get('type', str))}
            
        return value
        
    def _convert_list_items(self, items: List[Any], arg_spec: Dict[str, Any]) -> List[Any]:
        """Convert list items according to the argument specification"""
        args_def = arg_spec.get('args', [])
        result = []

        for item in items:
            if isinstance(item, tuple) and args_def:
                # Convert tuple to dict based on args definition
                item_dict = {}
                for i, arg_def in enumerate(args_def):
                    if i < len(item):
                        item_dict[arg_def['name']] = self._cast_value(item[i], arg_def.get('type', str))
                result.append(item_dict)
            elif isinstance(item, dict) and args_def:
                # Convert dict values based on args definition
                item_dict = {}
                for arg_def in args_def:
                    arg_name = arg_def['name']
                    if arg_name in item:
                        item_dict[arg_name] = self._cast_value(item[arg_name], arg_def.get('type', str))
                    else:
                        # Keep other keys as-is
                        item_dict.update({k: v for k, v in item.items() if k not in [a['name'] for a in args_def]})
                result.append(item_dict)
            elif isinstance(item, dict):
                result.append(item)
            else:
                # Single value - if there's exactly one arg_def, convert it
                if args_def and len(args_def) == 1:
                    arg_def = args_def[0]
                    result.append(self._cast_value(item, arg_def.get('type', str)))
                else:
                    result.append(item)

        return result
        
    def _cast_value(self, value: Any, value_type: type, arg_spec: Dict[str, Any] = None) -> Any:
        """
        Cast a value to the specified type.

        Supported types: str, int, float, bool, list, dict, SizeType, and custom types.

        :param value: Value to cast
        :param value_type: Target type
        :param arg_spec: Full argument specification (optional, used for nested list/dict conversions)
        """
        if value_type == bool:
            if isinstance(value, bool):
                return value
            elif isinstance(value, str):
                return value.lower() in ('true', '1', 'yes', 'on')
            else:
                return bool(value)
        elif value_type == int:
            return int(value)
        elif value_type == float:
            return float(value)
        elif value_type == str:
            return str(value)
        elif value_type == list:
            if isinstance(value, list):
                # If arg_spec provided with nested args, convert list items
                if arg_spec and 'args' in arg_spec:
                    return self._convert_list_items(value, arg_spec)
                return value
            return [value]
        elif value_type == dict:
            if isinstance(value, dict):
                # If arg_spec provided with nested args, convert dict values
                if arg_spec and 'args' in arg_spec:
                    converted = {}
                    args_def = arg_spec.get('args', [])
                    for arg_def in args_def:
                        arg_name = arg_def['name']
                        if arg_name in value:
                            converted[arg_name] = self._cast_value(value[arg_name], arg_def.get('type', str), arg_def)
                    # Keep other keys as-is
                    for k, v in value.items():
                        if k not in converted:
                            converted[k] = v
                    return converted
                return value
            # Try to parse as dict
            if isinstance(value, str):
                # Try Python literal syntax first (JSON-like format)
                if (value.startswith('{') and value.endswith('}')) or (value.startswith('[') and value.endswith(']')):
                    try:
                        import ast
                        result = ast.literal_eval(value)
                        # Apply type conversions if arg_spec has args
                        if isinstance(result, dict) and arg_spec and 'args' in arg_spec:
                            converted = {}
                            args_def = arg_spec.get('args', [])
                            for arg_def in args_def:
                                arg_name = arg_def['name']
                                if arg_name in result:
                                    converted[arg_name] = self._cast_value(result[arg_name], arg_def.get('type', str), arg_def)
                            # Keep other keys as-is
                            for k, v in result.items():
                                if k not in converted:
                                    converted[k] = v
                            return converted if converted else result
                        return result
                    except (ValueError, SyntaxError, TypeError):
                        pass

                # Try key:value,key2:value2 format (only if not JSON-like)
                if ':' in value and ',' in value and not value.startswith('{'):
                    try:
                        result = {}
                        pairs = value.split(',')
                        for pair in pairs:
                            if ':' in pair:
                                k, v = pair.split(':', 1)
                                k = k.strip()
                                v = v.strip()
                                # Apply type conversion if arg_spec has args
                                if arg_spec and 'args' in arg_spec:
                                    for arg_def in arg_spec['args']:
                                        if arg_def['name'] == k:
                                            v = self._cast_value(v, arg_def.get('type', str), arg_def)
                                            break
                                result[k] = v
                        if result:
                            return result
                    except Exception:
                        pass

                # Last resort: return as-is
                return value
            else:
                try:
                    return dict(value)
                except (ValueError, TypeError):
                    return value
        else:
            # For custom types (including SizeType), try direct conversion
            try:
                return value_type(value)
            except (ValueError, TypeError):
                return value
            
    def _find_command(self, args: List[str]) -> tuple[Optional[str], int]:
        """Find the best matching command and return it with the number of args consumed"""
        best_match = None
        best_length = 0
        
        for cmd_name in self.commands.keys():
            # Skip aliases, only check primary command names
            if cmd_name in self.commands and 'aliases' in self.commands[cmd_name] and cmd_name in self.commands[cmd_name]['aliases']:
                continue
                
            cmd_parts = cmd_name.split()
            if len(cmd_parts) <= len(args):
                if args[:len(cmd_parts)] == cmd_parts:
                    if len(cmd_parts) > best_length:
                        best_match = cmd_name
                        best_length = len(cmd_parts)
        
        # Check aliases
        for cmd_name, cmd_info in self.commands.items():
            if 'aliases' in cmd_info:
                for alias in cmd_info['aliases']:
                    alias_parts = alias.split()
                    if len(alias_parts) <= len(args):
                        if args[:len(alias_parts)] == alias_parts:
                            if len(alias_parts) > best_length:
                                best_match = cmd_name
                                best_length = len(alias_parts)
                        
        return best_match, best_length
        
    def _get_argument_info(self, cmd_name: str, arg_name: str) -> Optional[Dict[str, Any]]:
        """Get argument information for a command"""
        if cmd_name not in self.command_args:
            return None

        for arg_spec in self.command_args[cmd_name]:
            if arg_spec['name'] == arg_name:
                return arg_spec
            # Check aliases
            if 'aliases' in arg_spec and arg_name in arg_spec['aliases']:
                return arg_spec

        return None

    def _print_param_error(self, error_msg: str, cmd_name: str):
        """Print parameter error with usage menu and exit"""
        import sys
        print(f"Error: {error_msg}")
        print()
        self.print_command_help(cmd_name)
        sys.exit(1)
        
    def parse(self, args: List[str]) -> Dict[str, Any]:
        """Parse the argument list"""
        self.kwargs = {}
        self.remainder = []
        self.current_menu = None
        self.current_command = None

        # Check for help request
        if args and (args[0] in ['--help', '-h', 'help']):
            if len(args) > 1:
                # Help for specific command/menu
                self.print_help(' '.join(args[1:]))
            else:
                # General help
                self.print_help()
            return {}

        if not args:
            self.current_command = ''
            return self._handle_command('')

        # Find matching command
        cmd_name, consumed = self._find_command(args)

        if cmd_name is None:
            # Check for multi-word menus first (e.g., "ppl env")
            for i in range(min(3, len(args)), 0, -1):  # Check up to 3 words, longest first
                potential_menu = ' '.join(args[:i])
                if potential_menu in self.menus:
                    # Check if there's a help flag in the remaining args
                    remaining_args = args[i:]
                    if '--help' in remaining_args or '-h' in remaining_args:
                        self.print_menu_help(potential_menu)
                        return {}
                    # If no help flag, treat as menu navigation
                    self.print_menu_help(potential_menu)
                    return {}

            # Check if there's an empty command defined (default command)
            if '' in self.commands:
                cmd_name = ''
                consumed = 0
            elif self.commands or self.menus:
                # Unknown command - exit with error if there are menus or commands defined (but no default)
                import sys
                print(f"Error: Unknown command '{' '.join(args)}'")
                print()
                self.print_help()
                sys.exit(1)
            else:
                # Default to empty command (if nothing is defined)
                cmd_name = ''
                consumed = 0

        self.current_command = cmd_name
        remaining_args = args[consumed:]

        # Check for help in remaining args
        if '--help' in remaining_args or '-h' in remaining_args:
            self.print_command_help(cmd_name)
            return {}

        try:
            return self._parse_command_args(cmd_name, remaining_args)
        except ValueError as e:
            self._print_param_error(str(e), cmd_name)
        
    def _parse_command_args(self, cmd_name: str, args: List[str]) -> Dict[str, Any]:
        """Parse arguments for a specific command"""
        if cmd_name not in self.command_args:
            # If no args defined, treat all as remainder
            if cmd_name in self.commands and self.commands[cmd_name]['keep_remainder']:
                self.remainder = args
            return self._handle_command(cmd_name)

        arg_specs = self.command_args[cmd_name]
        keep_remainder = self.commands[cmd_name]['keep_remainder']

        # Initialize defaults
        for arg_spec in arg_specs:
            if 'default' in arg_spec:
                self.kwargs[arg_spec['name']] = arg_spec['default']

        # Separate positional and keyword args by class and rank
        positional_args = []
        for arg_spec in arg_specs:
            if arg_spec.get('pos', False):
                positional_args.append(arg_spec)

        # Sort positional args by class and rank
        # Arguments with a class should be sorted by (class, rank)
        # Arguments without a class should come after classed arguments
        def sort_key(x):
            class_name = x.get('class', '')
            rank = x.get('rank', 0)
            # If no class, put it at the end with a high sort value
            if not class_name:
                return ('zzz_no_class', rank)
            return (class_name, rank)

        positional_args.sort(key=sort_key)

        i = 0
        pos_index = 0

        while i < len(args):
            arg = args[i]

            # Handle key=value format (for any argument, not just --)
            # Only treat as key=value if key looks like a valid argument name (alphanumeric + underscore, no spaces)
            if '=' in arg and not arg.startswith('-'):
                key, value = arg.split('=', 1)
                # Check if key looks like an argument name (no spaces, alphanumeric/underscore)
                if key and not ' ' in key and key.replace('_', '').replace('-', '').isalnum():
                    # Remove surrounding quotes if present
                    if value.startswith('"') and value.endswith('"'):
                        value = value[1:-1]
                    elif value.startswith("'") and value.endswith("'"):
                        value = value[1:-1]

                    arg_spec = self._get_argument_info(cmd_name, key)
                    if arg_spec:
                        if arg_spec.get('type') == list:
                            self.kwargs[arg_spec['name']] = self._parse_list_value(value, arg_spec)
                        else:
                            self.kwargs[arg_spec['name']] = self._cast_value(value, arg_spec.get('type', str), arg_spec)
                        i += 1
                        continue
                    # If not a known arg, fall through to positional handling

            if arg.startswith('--'):
                # Long option
                if '=' in arg:
                    # --key=value format
                    key, value = arg[2:].split('=', 1)
                    arg_spec = self._get_argument_info(cmd_name, key)
                    if arg_spec:
                        if arg_spec.get('type') == list:
                            self.kwargs[arg_spec['name']] = self._parse_list_value(value, arg_spec)
                        else:
                            self.kwargs[arg_spec['name']] = self._cast_value(value, arg_spec.get('type', str), arg_spec)
                    else:
                        # Unknown argument
                        self._print_param_error(f"Unknown argument '{key}'", cmd_name)
                    i += 1
                else:
                    # --key value format
                    key = arg[2:]
                    arg_spec = self._get_argument_info(cmd_name, key)
                    if arg_spec:
                        if i + 1 < len(args):
                            value = args[i + 1]
                            if arg_spec.get('type') == list:
                                # Append mode for lists without =
                                if arg_spec['name'] not in self.kwargs:
                                    self.kwargs[arg_spec['name']] = []
                                elif not isinstance(self.kwargs[arg_spec['name']], list):
                                    self.kwargs[arg_spec['name']] = [self.kwargs[arg_spec['name']]]
                                parsed_item = self._parse_single_item(value, arg_spec)
                                self.kwargs[arg_spec['name']].append(parsed_item)
                            else:
                                self.kwargs[arg_spec['name']] = self._cast_value(value, arg_spec.get('type', str), arg_spec)
                            i += 2
                        else:
                            # Missing value for argument
                            self._print_param_error(f"Argument '{key}' requires a value", cmd_name)
                    else:
                        # Unknown argument
                        self._print_param_error(f"Unknown argument '{key}'", cmd_name)
            elif arg.startswith('+') and len(arg) > 1:
                # +arg format for boolean true
                key = arg[1:]
                arg_spec = self._get_argument_info(cmd_name, key)
                if arg_spec and arg_spec.get('type') == bool:
                    self.kwargs[arg_spec['name']] = True
                    i += 1
                    continue
                else:
                    # If not a boolean arg, treat as positional
                    if pos_index < len(positional_args):
                        arg_spec = positional_args[pos_index]
                        self.kwargs[arg_spec['name']] = self._cast_value(arg, arg_spec.get('type', str), arg_spec)
                        pos_index += 1
                        i += 1
                    else:
                        # Remainder
                        if keep_remainder:
                            self.remainder.extend(args[i:])
                            break
                        i += 1
            elif arg.startswith('-') and len(arg) > 1 and not arg[1:].isdigit():
                # Check for -arg format for boolean false first
                key = arg[1:]
                arg_spec = self._get_argument_info(cmd_name, key)
                if arg_spec and arg_spec.get('type') == bool:
                    self.kwargs[arg_spec['name']] = False
                    i += 1
                    continue

                # Short option with value
                if arg_spec and i + 1 < len(args):
                    value = args[i + 1]
                    if arg_spec.get('type') == list:
                        # Append mode for lists
                        if arg_spec['name'] not in self.kwargs:
                            self.kwargs[arg_spec['name']] = []
                        elif not isinstance(self.kwargs[arg_spec['name']], list):
                            self.kwargs[arg_spec['name']] = [self.kwargs[arg_spec['name']]]
                        parsed_item = self._parse_single_item(value, arg_spec)
                        self.kwargs[arg_spec['name']].append(parsed_item)
                    else:
                        self.kwargs[arg_spec['name']] = self._cast_value(value, arg_spec.get('type', str), arg_spec)
                    i += 2
                else:
                    i += 1
            else:
                # Positional argument
                if pos_index < len(positional_args):
                    arg_spec = positional_args[pos_index]
                    self.kwargs[arg_spec['name']] = self._cast_value(arg, arg_spec.get('type', str), arg_spec)
                    pos_index += 1
                    i += 1
                else:
                    # Remainder
                    if keep_remainder:
                        self.remainder.extend(args[i:])
                        break
                    i += 1

        # Check required args
        for arg_spec in arg_specs:
            if arg_spec.get('required', False) and arg_spec['name'] not in self.kwargs:
                self._print_param_error(f"Required argument '{arg_spec['name']}' not provided", cmd_name)

        # Validate choices
        for arg_spec in arg_specs:
            if 'choices' in arg_spec and arg_spec['choices'] and arg_spec['name'] in self.kwargs:
                value = self.kwargs[arg_spec['name']]
                if value not in arg_spec['choices']:
                    self._print_param_error(f"Argument '{arg_spec['name']}' must be one of {arg_spec['choices']}, got: {value}", cmd_name)

        return self._handle_command(cmd_name)
        
    def _handle_command(self, cmd_name: str) -> Dict[str, Any]:
        """Handle command execution"""
        # Call the appropriate method if it exists
        method_name = cmd_name.replace(' ', '_').replace('-', '_') or 'main_menu'
        if hasattr(self, method_name):
            getattr(self, method_name)()
        return self.kwargs
        
    def print_help(self, target: str = ""):
        """Print help information"""
        if target:
            # Help for specific command or menu
            if target in self.commands:
                self.print_command_help(target)
            elif target in self.menus:
                self.print_menu_help(target)
            else:
                print(f"No help available for '{target}'")
        else:
            # General help - show all top-level menus and commands
            self.print_general_help()
            
    def print_general_help(self):
        """Print general help showing all available menus and commands"""
        print("Usage: [command] [options]")
        print()

        # Show top-level menus and their commands
        top_level_menus = [name for name, menu in self.menus.items() if name and ' ' not in name]
        if top_level_menus:
            print("Available menus:")
            for menu_name in sorted(top_level_menus):
                menu = self.menus[menu_name]
                msg = menu.get('msg', '')
                print(f"  {menu_name:<15} {msg}")

                # Show commands under this menu
                if menu['commands']:
                    for cmd_name in menu['commands']:
                        if cmd_name in self.commands:
                            cmd = self.commands[cmd_name]
                            # Show command with indentation
                            display_name = cmd['cmd_name']
                            cmd_msg = cmd.get('msg', '')
                            print(f"    {display_name:<13} {cmd_msg}")
            print()

        # Show top-level commands (commands with no menu or empty menu)
        top_level_commands = [name for name, cmd in self.commands.items()
                            if cmd['menu'] == '' and name == cmd['name']]  # Exclude aliases
        if top_level_commands:
            print("Available commands:")
            for cmd_name in sorted(top_level_commands):
                cmd = self.commands[cmd_name]
                msg = cmd.get('msg', '')
                aliases_str = f" (aliases: {', '.join(cmd['aliases'])})" if cmd['aliases'] else ""
                print(f"  {cmd_name:<15} {msg}{aliases_str}")
            print()

        print("Use 'help [menu|command]' or '[menu|command] --help' for more information")
        
    def print_menu_help(self, menu_name: str):
        """Print help for a specific menu"""
        if menu_name not in self.menus:
            print(f"Menu '{menu_name}' not found")
            return
            
        menu = self.menus[menu_name]
        print(f"Menu: {menu_name}")
        if menu.get('msg'):
            print(f"Description: {menu['msg']}")
        print()
        
        if menu['commands']:
            print("Available commands:")
            for cmd_name in menu['commands']:
                if cmd_name in self.commands:
                    cmd = self.commands[cmd_name]
                    # Extract just the command part (after the menu name)
                    display_name = cmd['cmd_name']
                    msg = cmd.get('msg', '')
                    aliases_str = f" (aliases: {', '.join(cmd['aliases'])})" if cmd['aliases'] else ""
                    print(f"  {display_name:<15} {msg}{aliases_str}")
        else:
            print("No commands available in this menu")
            
    def print_command_help(self, cmd_name: str):
        """Print help for a specific command"""
        if cmd_name not in self.commands:
            print(f"Command '{cmd_name}' not found")
            return
            
        cmd = self.commands[cmd_name]
        print(f"Command: {cmd_name}")
        if cmd.get('msg'):
            print(f"Description: {cmd['msg']}")
        
        if cmd['aliases']:
            print(f"Aliases: {', '.join(cmd['aliases'])}")
        print()
        
        # Show arguments if any
        if cmd_name in self.command_args:
            args = self.command_args[cmd_name]
            if args:
                print("Arguments:")
                
                # Separate positional and optional arguments
                positional = [arg for arg in args if arg.get('pos', False)]
                optional = [arg for arg in args if not arg.get('pos', False)]
                
                if positional:
                    print("  Positional arguments:")
                    for arg in sorted(positional, key=lambda x: (x.get('class', ''), x.get('rank', 0))):
                        self._print_argument_help(arg, "    ")
                        
                if optional:
                    print("  Optional arguments:")
                    for arg in optional:
                        self._print_argument_help(arg, "    ")
        else:
            print("No arguments defined for this command")
            
    def _print_argument_help(self, arg: Dict[str, Any], indent: str = ""):
        """Print help for a single argument"""
        name = arg['name']
        msg = arg.get('msg', 'No description')
        arg_type = arg.get('type', str).__name__
        default = arg.get('default')
        required = arg.get('required', False)
        choices = arg.get('choices')
        aliases = arg.get('aliases', [])
        
        # Build argument display
        if arg.get('pos', False):
            arg_display = f"{name}"
        else:
            arg_display = f"--{name}"
            if aliases:
                alias_display = ', '.join([f"-{a}" if len(a) == 1 else f"--{a}" for a in aliases])
                arg_display += f", {alias_display}"
            
            # Add +/- syntax for boolean arguments
            if arg.get('type') == bool:
                arg_display += f", +{name}, -{name}"
                
        print(f"{indent}{arg_display}")
        print(f"{indent}  {msg}")
        
        details = []
        if required:
            details.append("required")
        if default is not None:
            details.append(f"default: {default}")
        if choices:
            details.append(f"choices: {choices}")
        details.append(f"type: {arg_type}")
        
        if details:
            print(f"{indent}  ({', '.join(details)})")

    def parse_dict(self, cmd_name: str, arg_dict: Dict[str, Any]) -> Dict[str, Any]:
        """
        Parse arguments from a dictionary with type conversion.

        :param cmd_name: Command name to parse for
        :param arg_dict: Dictionary of argument name -> value
        :return: Parsed arguments with proper types
        """
        self.kwargs = {}
        self.remainder = []  # No remainder support for dict parsing
        self.current_menu = None
        self.current_command = cmd_name

        # Check if command exists
        if cmd_name not in self.commands:
            raise ValueError(f"Command '{cmd_name}' not found")

        try:
            # Get argument specifications for this command
            if cmd_name not in self.command_args:
                # No arguments defined, just return the dict as-is with casting
                self.kwargs = arg_dict.copy()
                return self._handle_command(cmd_name)

            arg_specs = self.command_args[cmd_name]

            # Initialize defaults
            for arg_spec in arg_specs:
                if 'default' in arg_spec:
                    self.kwargs[arg_spec['name']] = arg_spec['default']

            # Process each argument from the dictionary
            for arg_name, arg_value in arg_dict.items():
                # Find the argument specification
                arg_spec = self._get_argument_info(cmd_name, arg_name)

                if arg_spec is None:
                    # Unknown argument - still include it but without type conversion
                    self.kwargs[arg_name] = arg_value
                    continue

                # Convert value to proper type
                if arg_spec.get('type') == list:
                    # Handle list arguments
                    if isinstance(arg_value, list):
                        self.kwargs[arg_spec['name']] = self._convert_list_items(arg_value, arg_spec)
                    else:
                        # Single value for list - wrap in list
                        self.kwargs[arg_spec['name']] = [self._parse_single_item(str(arg_value), arg_spec)]
                else:
                    # Handle scalar arguments
                    self.kwargs[arg_spec['name']] = self._cast_value(arg_value, arg_spec.get('type', str), arg_spec)

            # Check required arguments
            for arg_spec in arg_specs:
                if arg_spec.get('required', False) and arg_spec['name'] not in self.kwargs:
                    self._print_param_error(f"Required argument '{arg_spec['name']}' not provided", cmd_name)

            # Validate choices
            for arg_spec in arg_specs:
                if 'choices' in arg_spec and arg_spec['choices'] and arg_spec['name'] in self.kwargs:
                    value = self.kwargs[arg_spec['name']]
                    if value not in arg_spec['choices']:
                        self._print_param_error(f"Argument '{arg_spec['name']}' must be one of {arg_spec['choices']}, got: {value}", cmd_name)

            return self._handle_command(cmd_name)
        except (ValueError, TypeError) as e:
            self._print_param_error(f"Error converting argument: {e}", cmd_name)

    def define_options(self):
        """Override this method to define your command structure"""
        pass
```

### `jarvis_cd/util/hostfile.py`

```python
import socket
import re
import os
from typing import List, Optional, Union


class Hostfile:
    """
    Parse a hostfile or store a set of hosts passed in manually.
    """

    def __init__(self, path: Optional[str] = None, hosts: Optional[List[str]] = None, 
                 hosts_ip: Optional[List[str]] = None, text: Optional[str] = None, 
                 find_ips: bool = True, load_path: bool = True):
        """
        Constructor. Parse hostfile or store existing host list.

        :param path: The path to the hostfile
        :param hosts: a list of strings representing all hostnames
        :param hosts_ip: a list of strings representing all host IPs
        :param text: Text of a hostfile
        :param find_ips: Whether to construct host_ip and all_host_ip fields
        :param load_path: whether or not path should exist and be read from on init
        """
        self.path = path
        self.find_ips = find_ips
        self.hosts = []
        self.hosts_ip = []
        
        # Initialize from different sources
        if hosts is not None:
            self.hosts = list(hosts)
        elif path is not None and load_path:
            self._load_from_path(path)
        elif text is not None:
            self._load_from_text(text)
        else:
            # Default to localhost
            self.hosts = ['localhost']
            
        # Set hosts_ip if provided, otherwise resolve if find_ips is True
        if hosts_ip is not None:
            self.hosts_ip = list(hosts_ip)
        elif find_ips:
            self._resolve_ips()
            
    def _load_from_path(self, path: str):
        """Load hostfile from filesystem path"""
        if not os.path.exists(path):
            raise FileNotFoundError(f"Hostfile not found: {path}")
            
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            
        self._load_from_text(content)
        
    def _load_from_text(self, text: str):
        """Load hostfile from text content"""
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        self.hosts = []
        
        for line in lines:
            expanded_hosts = self._expand_host_pattern(line)
            self.hosts.extend(expanded_hosts)
            
    def _expand_host_pattern(self, pattern: str) -> List[str]:
        """
        Expand host patterns like:
        - ares-comp-01 -> ['ares-comp-01']
        - ares-comp-[02-04] -> ['ares-comp-02', 'ares-comp-03', 'ares-comp-04']
        - ares-comp-[05-09,11,12-14]-40g -> ['ares-comp-05-40g', ..., 'ares-comp-14-40g']
        """
        if '[' not in pattern:
            return [pattern]
            
        # Find bracket expressions
        bracket_match = re.search(r'\[([^\]]+)\]', pattern)
        if not bracket_match:
            return [pattern]
            
        bracket_content = bracket_match.group(1)
        prefix = pattern[:bracket_match.start()]
        suffix = pattern[bracket_match.end():]
        
        # Parse bracket content (ranges and individual numbers/letters)
        numbers = set()
        for part in bracket_content.split(','):
            part = part.strip()
            if '-' in part and not part.startswith('-'):
                # Range like "02-04", "12-14", or "a-c"
                range_parts = part.split('-')
                if len(range_parts) == 2:
                    start, end = range_parts
                    try:
                        # Try numeric range first
                        start_num = int(start)
                        end_num = int(end)
                        # Preserve zero-padding
                        width = max(len(start), len(end))
                        for i in range(start_num, end_num + 1):
                            numbers.add(str(i).zfill(width))
                    except ValueError:
                        # Try alphabetic range
                        if len(start) == 1 and len(end) == 1 and start.isalpha() and end.isalpha():
                            start_ord = ord(start.lower())
                            end_ord = ord(end.lower())
                            for i in range(start_ord, end_ord + 1):
                                char = chr(i)
                                # Preserve case from start character
                                if start.isupper():
                                    char = char.upper()
                                numbers.add(char)
                        else:
                            # If not numeric or single char range, treat as literal
                            numbers.add(part)
            else:
                # Individual item like "11" or "a"
                numbers.add(part)
                
        # Generate all combinations
        result = []
        for num in sorted(numbers):
            host = f"{prefix}{num}{suffix}"
            # Recursively expand if there are more brackets
            result.extend(self._expand_host_pattern(host))
            
        return result
        
    def _resolve_ips(self):
        """Resolve hostnames to IP addresses"""
        self.hosts_ip = []
        for host in self.hosts:
            try:
                if host == 'localhost':
                    ip = socket.gethostbyname('localhost')
                else:
                    ip = socket.gethostbyname(host)
                self.hosts_ip.append(ip)
            except socket.gaierror:
                # If resolution fails, use the hostname as IP
                self.hosts_ip.append(host)
                
    def subset(self, count: int, path: Optional[str] = None) -> 'Hostfile':
        """Return a subset of the first 'count' hosts"""
        return Hostfile(path=path, hosts=self.hosts[0:count], 
                       hosts_ip=self.hosts_ip[0:count] if self.hosts_ip else None,
                       find_ips=self.find_ips, load_path=False)
                       
    def copy(self) -> 'Hostfile':
        """Return a copy of this hostfile"""
        return self.subset(len(self))
        
    def is_local(self) -> bool:
        """
        Whether this file contains only 'localhost'

        :return: True or false
        """
        if len(self) == 0:
            return True
            
        if len(self.hosts) == 1:
            if self.hosts[0] == 'localhost':
                return True
            try:
                if self.hosts[0] == socket.gethostbyname('localhost'):
                    return True
            except socket.gaierror:
                pass
                
        if len(self.hosts_ip) == 1:
            try:
                if self.hosts_ip[0] == socket.gethostbyname('localhost'):
                    return True
            except socket.gaierror:
                pass
                
        return False
        
    def save(self, path: str) -> 'Hostfile':
        """Save hostfile to filesystem"""
        self.path = path
        with open(path, 'w', encoding='utf-8') as fp:
            fp.write('\n'.join(self.hosts))
        return self
        
    def list(self) -> List['Hostfile']:
        """Return a list of single-host Hostfile objects"""
        return [Hostfile(hosts=[host], find_ips=self.find_ips, load_path=False) 
                for host in self.hosts]
                
    def enumerate(self):
        """Return enumerated list of single-host Hostfile objects"""
        return enumerate(self.list())
        
    def host_str(self, sep: str = ',') -> str:
        """Return hosts as a separated string"""
        return sep.join(self.hosts)
        
    def ip_str(self, sep: str = ',') -> str:
        """Return host IPs as a separated string"""
        return sep.join(self.hosts_ip)
        
    def is_subset(self) -> bool:
        """
        Return True if hostfile was created from a host list rather than a file.
        Used to determine whether to use --host or --hostfile in MPI commands.
        """
        return self.path is None
        
    def __len__(self) -> int:
        """Return number of hosts"""
        return len(self.hosts)
        
    def __iter__(self):
        """Iterate over hostnames"""
        return iter(self.hosts)
        
    def __getitem__(self, index):
        """Get host by index"""
        return self.hosts[index]
        
    def __str__(self) -> str:
        """String representation"""
        return f"Hostfile({len(self.hosts)} hosts: {self.host_str()})"
        
    def __repr__(self) -> str:
        """Detailed string representation"""
        return f"Hostfile(hosts={self.hosts}, hosts_ip={self.hosts_ip})"
```

### `jarvis_cd/util/logger.py`

```python
"""
Logging utilities with color support for Jarvis.
"""
import sys
from enum import Enum


class Color(Enum):
    """Color codes for terminal output"""
    # Basic colors
    BLACK = '\033[30m'
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    MAGENTA = '\033[35m'
    CYAN = '\033[36m'
    WHITE = '\033[37m'
    
    # Light colors
    LIGHT_BLACK = '\033[90m'
    LIGHT_RED = '\033[91m'
    LIGHT_GREEN = '\033[92m'
    LIGHT_YELLOW = '\033[93m'
    LIGHT_BLUE = '\033[94m'
    LIGHT_MAGENTA = '\033[95m'
    LIGHT_CYAN = '\033[96m'
    LIGHT_WHITE = '\033[97m'
    
    # Special
    RESET = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


class Logger:
    """
    Logger class with color support for terminal output.
    """
    
    def __init__(self, enable_colors: bool = True):
        """
        Initialize logger.
        
        :param enable_colors: Whether to enable color output (auto-detects TTY)
        """
        self.enable_colors = enable_colors and sys.stdout.isatty()
        
    def print(self, color: Color, message: str, file=None, end: str = '\n'):
        """
        Print a colored message to the terminal.

        :param color: Color to use for the message
        :param message: Message to print
        :param file: File to write to (default: stdout)
        :param end: String appended after the message
        """
        if file is None:
            file = sys.stdout

        if self.enable_colors:
            formatted_message = f"{color.value}{message}{Color.RESET.value}"
        else:
            formatted_message = message

        print(formatted_message, file=file, end=end, flush=True)
        
    def info(self, message: str, file=None, end: str = '\n'):
        """Print an info message in default color"""
        print(message, file=file, end=end, flush=True)
        
    def success(self, message: str, file=None, end: str = '\n'):
        """Print a success message in green"""
        self.print(Color.GREEN, message, file=file, end=end)
        
    def warning(self, message: str, file=None, end: str = '\n'):
        """Print a warning message in yellow"""
        self.print(Color.YELLOW, message, file=file, end=end)
        
    def error(self, message: str, file=None, end: str = '\n'):
        """Print an error message in red"""
        self.print(Color.RED, message, file=file, end=end)
        
    def debug(self, message: str, file=None, end: str = '\n'):
        """Print a debug message in light black (gray)"""
        self.print(Color.LIGHT_BLACK, message, file=file, end=end)
        
    def pipeline(self, message: str, file=None, end: str = '\n'):
        """Print a pipeline phase message in green"""
        self.print(Color.GREEN, message, file=file, end=end)
        
    def package(self, message: str, file=None, end: str = '\n'):
        """Print a package operation message in light green"""
        self.print(Color.LIGHT_GREEN, message, file=file, end=end)


# Global logger instance
logger = Logger()
```

### `jarvis_cd/util/pkg_argparse.py`

```python
"""
PkgArgParse - Argument parser for package configuration

Supported types in configure_menu:
- str: String values
- int: Integer values
- float: Float values
- bool: Boolean values (supports 'true', '1', 'yes', 'on' for True; 'false', '0', 'no', 'off' for False)
- list: List of values
- dict: Dictionary values (supports Python dict literal syntax)
- SizeType: Size specifications (e.g., "1k", "2M", "10G")
- Custom types: Any type with a constructor that accepts string values
"""
from .argparse import ArgParse
from typing import List, Dict, Any


class PkgArgParse(ArgParse):
    """
    Argument parser specifically for package configuration.
    Provides a single 'configure' command with arguments from package's configure_menu().

    Supported argument types:
    - str: String values
    - int: Integer values
    - float: Float values
    - bool: Boolean values (use +arg/-arg or --arg true/false)
    - list: List of values (supports multiple --arg values or Python list syntax)
    - dict: Dictionary values (supports Python dict literal syntax)
    - SizeType: Size specifications like "1k", "2M", "10G" (binary multipliers)
    - Custom types: Any type with a constructor accepting strings
    """

    def __init__(self, pkg_name: str, configure_menu: List[Dict[str, Any]]):
        """
        Initialize package argument parser.

        :param pkg_name: Name of the package
        :param configure_menu: List of argument specifications from configure_menu()
        """
        super().__init__()

        self.pkg_name = pkg_name

        # Add the configure command
        self.add_cmd(
            'configure',
            msg=f'Configure {pkg_name} package',
            keep_remainder=False
        )

        # Add arguments from configure_menu
        if configure_menu:
            self.add_args(configure_menu)

    def print_help(self, cmd_name: str = None):
        """
        Print help for the package configuration.

        :param cmd_name: Command name (always 'configure' for packages)
        """
        if cmd_name and cmd_name != 'configure':
            print(f"Unknown command: {cmd_name}")
            print("Only 'configure' command is available for packages")
            return

        # Print package header
        print(f"Package: {self.pkg_name}")
        print()
        print("Configuration Parameters:")
        print()

        # Print the configure command help
        self.print_command_help('configure')
```

### `jarvis_cd/util/resource_graph.py`

```python
"""
Resource graph utilities for Jarvis.
Manages storage resource collection and analysis across nodes.
"""
import json
import yaml
import os
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
from collections import defaultdict
from .logger import logger


class ResourceGraph:
    """
    Manages storage resource information across multiple nodes.
    Collects, analyzes, and provides views of storage resources.
    """
    
    def __init__(self):
        """Initialize empty resource graph."""
        self.nodes: Dict[str, List[Dict[str, Any]]] = {}
        self.common_mounts: Dict[str, List[Dict[str, Any]]] = {}
        
    def add_node_data(self, hostname: str, resource_data: Dict[str, Any]):
        """
        Add resource data for a node.

        :param hostname: Hostname of the node
        :param resource_data: Resource data collected from the node
        """
        if hostname not in self.nodes:
            self.nodes[hostname] = []

        # Store filesystem data as dictionaries with hostname field
        for fs_data in resource_data.get('fs', []):
            device = fs_data.copy()
            device['hostname'] = hostname
            # Ensure all expected fields exist with defaults
            device.setdefault('device', '')
            device.setdefault('mount', '')
            device.setdefault('fs_type', 'unknown')
            device.setdefault('avail', '0B')
            device.setdefault('dev_type', 'unknown')
            device.setdefault('model', 'unknown')
            device.setdefault('parent', '')
            device.setdefault('uuid', '')
            device.setdefault('needs_root', False)
            device.setdefault('shared', False)
            device.setdefault('4k_randwrite_bw', 'unknown')
            device.setdefault('1m_seqwrite_bw', 'unknown')
            self.nodes[hostname].append(device)

        # Update common mounts analysis
        self._analyze_common_mounts()
        
    def _analyze_common_mounts(self):
        """Analyze which mount points are common across nodes.

        For multi-node clusters, mounts are common if they exist on multiple nodes.
        For single-node clusters, all mounts are considered common.
        """
        mount_counts: Dict[str, List[Dict[str, Any]]] = defaultdict(list)

        # Group devices by mount point
        for hostname, devices in self.nodes.items():
            for device in devices:
                mount_counts[device['mount']].append(device)

        # Find mounts that exist on multiple nodes, or all mounts if single node
        self.common_mounts = {}
        total_nodes = len(self.nodes)

        for mount_point, devices in mount_counts.items():
            # Consider mount points common if:
            # 1. They exist on multiple nodes (len(devices) > 1), OR
            # 2. There's only one node in the cluster (all its mounts are "common")
            if len(devices) > 1 or total_nodes == 1:
                # Mark as shared if on multiple nodes or single node cluster
                for device in devices:
                    device['shared'] = True
                self.common_mounts[mount_point] = devices
                
    def get_common_storage(self) -> Dict[str, List[Dict[str, Any]]]:
        """
        Get storage devices that are common across nodes (same mount point).

        For multi-node clusters: Returns mount points that exist on multiple nodes.
        For single-node clusters: Returns all mount points (all are considered common).

        :return: Dictionary mapping mount points to list of devices
        """
        return self.common_mounts.copy()

    def get_node_storage(self, hostname: str) -> List[Dict[str, Any]]:
        """
        Get storage devices for a specific node.

        :param hostname: Hostname to get storage for
        :return: List of storage devices
        """
        return self.nodes.get(hostname, [])
        
    def get_all_nodes(self) -> List[str]:
        """Get list of all node hostnames."""
        return list(self.nodes.keys())
        
    def get_storage_summary(self) -> Dict[str, Any]:
        """
        Get summary statistics of storage across all nodes.
        
        :return: Summary dictionary
        """
        total_devices = sum(len(devices) for devices in self.nodes.values())
        common_mounts = len(self.common_mounts)
        
        # Count device types
        dev_type_counts = defaultdict(int)
        fs_type_counts = defaultdict(int)

        for devices in self.nodes.values():
            for device in devices:
                dev_type_counts[device['dev_type']] += 1
                fs_type_counts[device['fs_type']] += 1
                
        return {
            'total_nodes': len(self.nodes),
            'total_devices': total_devices,
            'common_mount_points': common_mounts,
            'device_types': dict(dev_type_counts),
            'filesystem_types': dict(fs_type_counts)
        }
        
    def filter_by_type(self, dev_type: str) -> Dict[str, List[Dict[str, Any]]]:
        """
        Filter devices by device type (ssd, hdd, etc.).

        :param dev_type: Device type to filter by
        :return: Dictionary mapping hostnames to filtered devices
        """
        filtered = {}
        for hostname, devices in self.nodes.items():
            filtered_devices = [d for d in devices if d['dev_type'] == dev_type]
            if filtered_devices:
                filtered[hostname] = filtered_devices
        return filtered

    def filter_by_mount_pattern(self, pattern: str) -> Dict[str, List[Dict[str, Any]]]:
        """
        Filter devices by mount point pattern.

        :param pattern: Pattern to match in mount points
        :return: Dictionary mapping hostnames to filtered devices
        """
        filtered = {}
        for hostname, devices in self.nodes.items():
            filtered_devices = [d for d in devices if pattern in d['mount']]
            if filtered_devices:
                filtered[hostname] = filtered_devices
        return filtered
        
    def save_to_file(self, output_path: Path, format: str = 'yaml'):
        """
        Save resource graph to file.

        :param output_path: Path to save file
        :param format: Output format ('yaml' or 'json')
        """
        # Convert to serializable format - only store common mount points
        fs_data = []

        # Only store common mount points (accessible across nodes or single node)
        for mount_point, devices in self.common_mounts.items():
            # Use the first device as representative of the mount point
            if devices:
                device_dict = devices[0].copy()
                # Remove hostname-specific information
                device_dict.pop('hostname', None)
                fs_data.append(device_dict)

        data = {'fs': fs_data}

        # Save to file
        with open(output_path, 'w') as f:
            if format.lower() == 'json':
                json.dump(data, f, indent=2)
            else:  # Default to YAML
                yaml.dump(data, f, default_flow_style=False)

        logger.success(f"Resource graph saved to {output_path}")
        
    def load_from_file(self, input_path: Path):
        """
        Load resource graph from file.

        :param input_path: Path to load file from
        """
        with open(input_path, 'r') as f:
            if input_path.suffix.lower() == '.json':
                data = json.load(f)
            else:  # Default to YAML
                data = yaml.safe_load(f)

        # Clear existing data
        self.nodes = {}
        self.common_mounts = {}

        # Handle resource graph format with 'fs' section
        if 'fs' in data:
            # Determine hostname for resource graph files
            hostname = input_path.stem  # Use filename as hostname
            self.nodes[hostname] = []

            for device_data in data['fs']:
                # Expand environment variables in mount paths
                mount_path = device_data.get('mount', '')
                if mount_path:
                    mount_path = os.path.expandvars(mount_path)
                    device_data = device_data.copy()
                    device_data['mount'] = mount_path

                device = device_data.copy()
                device['hostname'] = hostname
                # Ensure all expected fields exist with defaults
                device.setdefault('device', '')
                device.setdefault('mount', '')
                device.setdefault('fs_type', 'unknown')
                device.setdefault('avail', '0B')
                device.setdefault('dev_type', 'unknown')
                device.setdefault('model', 'unknown')
                device.setdefault('parent', '')
                device.setdefault('uuid', '')
                device.setdefault('needs_root', False)
                device.setdefault('shared', False)
                device.setdefault('4k_randwrite_bw', 'unknown')
                device.setdefault('1m_seqwrite_bw', 'unknown')
                self.nodes[hostname].append(device)
        else:
            raise ValueError(f"Invalid resource graph format in {input_path}. Expected 'fs' section.")

        # Reanalyze common mounts
        self._analyze_common_mounts()

        logger.success(f"Resource graph loaded from {input_path}")
        
    def print_summary(self):
        """Print a summary of the resource graph."""
        summary = self.get_storage_summary()
        
        logger.info("=== Resource Graph Summary ===")
        logger.info(f"Total nodes: {summary['total_nodes']}")
        logger.info(f"Total storage devices: {summary['total_devices']}")
        logger.info(f"Common mount points: {summary['common_mount_points']}")
        
        if summary['device_types']:
            logger.info("Device types:")
            for dev_type, count in summary['device_types'].items():
                logger.info(f"  {dev_type}: {count}")
                
        if summary['filesystem_types']:
            logger.info("Filesystem types:")
            for fs_type, count in summary['filesystem_types'].items():
                logger.info(f"  {fs_type}: {count}")
                
    def print_common_storage(self):
        """Print information about common storage across nodes."""
        if not self.common_mounts:
            logger.warning("No common storage found across nodes")
            return

        total_nodes = len(self.nodes)
        if total_nodes == 1:
            logger.info("=== Available Storage (Single Node Cluster) ===")
        else:
            logger.info("=== Common Storage Across Nodes ===")

        for mount_point, devices in self.common_mounts.items():
            logger.info(f"\nMount point: {mount_point}")
            if total_nodes == 1:
                logger.info(f"Available on node: {devices[0]['hostname']}")
            else:
                logger.info(f"Available on {len(devices)} nodes:")

            for device in devices:
                perf_info = ""
                if device.get('4k_randwrite_bw', 'unknown') != 'unknown' and device.get('1m_seqwrite_bw', 'unknown') != 'unknown':
                    perf_info = f" [4K: {device.get('4k_randwrite_bw', 'unknown')}, 1M: {device.get('1m_seqwrite_bw', 'unknown')}]"

                logger.info(f"  {device['hostname']}: {device['device']} ({device['avail']}, {device['dev_type']}){perf_info}")
                
    def print_node_details(self, hostname: str):
        """
        Print detailed storage information for a specific node.

        :param hostname: Hostname to print details for
        """
        if hostname not in self.nodes:
            logger.error(f"Node {hostname} not found in resource graph")
            return

        devices = self.nodes[hostname]
        logger.info(f"=== Storage Details for {hostname} ===")
        logger.info(f"Total devices: {len(devices)}")

        for device in devices:
            logger.info(f"\nDevice: {device['device']}")
            logger.info(f"  Mount: {device['mount']}")
            logger.info(f"  Type: {device['dev_type']} ({device['fs_type']})")
            logger.info(f"  Available: {device['avail']}")
            logger.info(f"  Model: {device['model']}")
            logger.info(f"  Shared: {'Yes' if device['shared'] else 'No'}")

            if device.get('4k_randwrite_bw', 'unknown') != 'unknown':
                logger.info(f"  4K Random Write: {device.get('4k_randwrite_bw', 'unknown')}")
            if device.get('1m_seqwrite_bw', 'unknown') != 'unknown':
                logger.info(f"  1M Sequential Write: {device.get('1m_seqwrite_bw', 'unknown')}")
```

### `jarvis_cd/util/size_type.py`

```python
"""
Size type utility for Jarvis-CD.
Converts size strings (like "1k", "2M", "10G") to integer byte values using binary multipliers.
"""

import re
from typing import Union


class SizeType:
    """
    Utility class for converting size strings to integer byte values.
    
    Supports binary multipliers (powers of 2):
    - k/K: 1024 (1 << 10)
    - m/M: 1048576 (1 << 20) 
    - g/G: 1073741824 (1 << 30)
    - t/T: 1099511627776 (1 << 40)
    
    Examples:
        SizeType("1k")    -> 1024
        SizeType("2M")    -> 2097152  
        SizeType("10g")   -> 10737418240
        SizeType("100")   -> 100 (no multiplier)
    """
    
    # Binary multipliers (powers of 2)
    MULTIPLIERS = {
        'k': 1 << 10,  # 1024
        'm': 1 << 20,  # 1048576
        'g': 1 << 30,  # 1073741824
        't': 1 << 40,  # 1099511627776
    }
    
    def __init__(self, size_str: Union[str, int, float]):
        """
        Initialize SizeType with a size string, integer, or float.
        
        :param size_str: Size specification (e.g., "1k", "2M", "100", 1024)
        """
        if isinstance(size_str, (int, float)):
            self._bytes = int(size_str)
        else:
            self._bytes = self._parse_size_string(str(size_str))
    
    def _parse_size_string(self, size_str: str) -> int:
        """
        Parse a size string into integer bytes.
        
        :param size_str: Size string to parse
        :return: Size in bytes
        :raises ValueError: If the size string format is invalid
        """
        # Remove whitespace
        size_str = size_str.strip()
        
        if not size_str:
            raise ValueError("Empty size string")
        
        # Match number followed by optional multiplier (stricter pattern)
        match = re.match(r'^(\d+(?:\.\d+)?)\s*([kmgtKMGT]?)(?:[bB].*)?$', size_str)
        
        if not match:
            raise ValueError(f"Invalid size format: '{size_str}'. Expected format: number[k|m|g|t]")
        
        number_str, multiplier = match.groups()
        
        try:
            # Parse the number (can be float, but must be positive)
            number = float(number_str)
            if number < 0:
                raise ValueError(f"Size cannot be negative: {number}")
        except ValueError as e:
            if "negative" in str(e):
                raise e
            raise ValueError(f"Invalid number: '{number_str}'")
        
        # Get multiplier (case-insensitive, look at first character)
        if multiplier:
            multiplier_key = multiplier[0].lower()
            if multiplier_key in self.MULTIPLIERS:
                multiplier_value = self.MULTIPLIERS[multiplier_key]
            else:
                raise ValueError(f"Unknown size multiplier: '{multiplier}'. Supported: k, m, g, t")
        else:
            multiplier_value = 1
        
        # Calculate final size in bytes
        total_bytes = number * multiplier_value
        
        # Return as integer
        return int(total_bytes)
    
    def __int__(self) -> int:
        """Convert to integer (bytes)."""
        return self._bytes
    
    def __float__(self) -> float:
        """Convert to float (bytes)."""
        return float(self._bytes)
    
    def __str__(self) -> str:
        """String representation shows bytes value."""
        return f"{self._bytes}B"
    
    def __repr__(self) -> str:
        """Developer representation."""
        return f"SizeType({self._bytes} bytes)"
    
    def __eq__(self, other) -> bool:
        """Equality comparison."""
        if isinstance(other, SizeType):
            return self._bytes == other._bytes
        elif isinstance(other, (int, float)):
            return self._bytes == other
        return False
    
    def __lt__(self, other) -> bool:
        """Less than comparison."""
        if isinstance(other, SizeType):
            return self._bytes < other._bytes
        elif isinstance(other, (int, float)):
            return self._bytes < other
        return NotImplemented
    
    def __le__(self, other) -> bool:
        """Less than or equal comparison."""
        return self == other or self < other
    
    def __gt__(self, other) -> bool:
        """Greater than comparison."""
        if isinstance(other, SizeType):
            return self._bytes > other._bytes
        elif isinstance(other, (int, float)):
            return self._bytes > other
        return NotImplemented
    
    def __ge__(self, other) -> bool:
        """Greater than or equal comparison."""
        return self == other or self > other
    
    def __add__(self, other):
        """Addition operation."""
        if isinstance(other, SizeType):
            return SizeType(self._bytes + other._bytes)
        elif isinstance(other, (int, float)):
            return SizeType(self._bytes + other)
        return NotImplemented
    
    def __sub__(self, other):
        """Subtraction operation."""
        if isinstance(other, SizeType):
            return SizeType(self._bytes - other._bytes)
        elif isinstance(other, (int, float)):
            return SizeType(self._bytes - other)
        return NotImplemented
    
    def __mul__(self, other):
        """Multiplication operation."""
        if isinstance(other, (int, float)):
            return SizeType(self._bytes * other)
        return NotImplemented
    
    def __truediv__(self, other):
        """Division operation."""
        if isinstance(other, (int, float)):
            return SizeType(self._bytes / other)
        elif isinstance(other, SizeType):
            return self._bytes / other._bytes  # Return ratio as float
        return NotImplemented
    
    @property
    def bytes(self) -> int:
        """Get size in bytes."""
        return self._bytes
    
    def to_bytes(self) -> int:
        """
        Get size as integer bytes.
        
        :return: Size in bytes as integer
        """
        return self._bytes
    
    @property 
    def kilobytes(self) -> float:
        """Get size in kilobytes (1024 bytes)."""
        return self._bytes / (1 << 10)
    
    @property
    def megabytes(self) -> float:
        """Get size in megabytes (1024^2 bytes)."""
        return self._bytes / (1 << 20)
    
    @property
    def gigabytes(self) -> float:
        """Get size in gigabytes (1024^3 bytes)."""
        return self._bytes / (1 << 30)
    
    @property
    def terabytes(self) -> float:
        """Get size in terabytes (1024^4 bytes)."""
        return self._bytes / (1 << 40)
    
    def to_human_readable(self) -> str:
        """
        Convert to human-readable string format.
        
        :return: Human-readable size string (e.g., "1.5K", "2.0M")
        """
        if self._bytes == 0:
            return "0B"
        
        # Choose the largest unit that results in >= 1
        for unit, multiplier in [('T', 1 << 40), ('G', 1 << 30), ('M', 1 << 20), ('K', 1 << 10)]:
            if self._bytes >= multiplier:
                value = self._bytes / multiplier
                if value == int(value):
                    return f"{int(value)}{unit}"
                else:
                    return f"{value:.1f}{unit}"
        
        # Less than 1024 bytes
        return f"{self._bytes}B"
    
    @classmethod
    def parse(cls, size_str: Union[str, int, float]) -> 'SizeType':
        """
        Class method to parse a size string.
        
        :param size_str: Size specification
        :return: SizeType instance
        """
        return cls(size_str)
    
    @classmethod
    def from_bytes(cls, bytes_value: int) -> 'SizeType':
        """
        Create SizeType from byte value.
        
        :param bytes_value: Size in bytes
        :return: SizeType instance
        """
        return cls(bytes_value)
    
    @classmethod
    def from_kilobytes(cls, kb_value: float) -> 'SizeType':
        """Create SizeType from kilobytes."""
        return cls(int(kb_value * (1 << 10)))
    
    @classmethod
    def from_megabytes(cls, mb_value: float) -> 'SizeType':
        """Create SizeType from megabytes."""
        return cls(int(mb_value * (1 << 20)))
    
    @classmethod
    def from_gigabytes(cls, gb_value: float) -> 'SizeType':
        """Create SizeType from gigabytes."""
        return cls(int(gb_value * (1 << 30)))
    
    @classmethod
    def from_terabytes(cls, tb_value: float) -> 'SizeType':
        """Create SizeType from terabytes."""
        return cls(int(tb_value * (1 << 40)))


# Convenience functions for quick conversions
def size_to_bytes(size_str: Union[str, int, float]) -> int:
    """
    Convert size string to bytes.
    
    :param size_str: Size specification
    :return: Size in bytes
    """
    return SizeType(size_str).bytes


def human_readable_size(bytes_value: int) -> str:
    """
    Convert bytes to human-readable format.
    
    :param bytes_value: Size in bytes
    :return: Human-readable size string
    """
    return SizeType.from_bytes(bytes_value).to_human_readable()
```

### `test/__init__.py`

```python

```

### `test/run_local.sh`

```bash
#!/bin/bash
# Local test runner (non-containerized)
# Usage: ./run_local.sh [test-path] [pytest-args]

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

cd "$PROJECT_ROOT"

# Set PYTHONPATH
export PYTHONPATH="$PROJECT_ROOT:$PYTHONPATH"

# Default test path
TEST_PATH="${1:-test/unit/}"
shift || true

echo -e "${GREEN}=== Jarvis CD Local Test Runner ===${NC}"
echo "Test Path: $TEST_PATH"
echo "Additional Args: $@"
echo ""

# Check if pytest is available
if ! command -v pytest &> /dev/null && ! python3 -m pytest --version &> /dev/null; then
    echo -e "${RED}Error: pytest is not installed${NC}"
    echo "Install with: pip install pytest pytest-cov"
    exit 1
fi

# Compile test binaries if needed
if [ -f "test/unit/shell/test_env_checker.c" ] && [ ! -f "test/unit/shell/test_env_checker" ]; then
    echo -e "${YELLOW}Compiling test binaries...${NC}"
    gcc test/unit/shell/test_env_checker.c -o test/unit/shell/test_env_checker
fi

# Run tests
echo -e "${YELLOW}Running tests...${NC}"
python3 -m pytest "$TEST_PATH" -v "$@"

EXIT_CODE=$?

if [ $EXIT_CODE -eq 0 ]; then
    echo -e "${GREEN}✓ Tests passed!${NC}"
else
    echo -e "${RED}✗ Tests failed with exit code $EXIT_CODE${NC}"
fi

exit $EXIT_CODE
```

### `test/run_tests.sh`

```bash
#!/bin/bash
# Test runner script for Jarvis CD
# Usage: ./run_tests.sh [all|shell|util|core|parallel] [additional pytest args]

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

# Print usage
usage() {
    echo "Usage: $0 [TEST_SUITE] [PYTEST_ARGS]"
    echo ""
    echo "TEST_SUITE options:"
    echo "  all       - Run all tests (default)"
    echo "  shell     - Run only shell module tests"
    echo "  util      - Run only util module tests"
    echo "  core      - Run only core module tests"
    echo "  parallel  - Run all tests in parallel"
    echo ""
    echo "Examples:"
    echo "  $0 all"
    echo "  $0 shell -v"
    echo "  $0 parallel -n 4"
    echo "  $0 core --tb=short"
    exit 1
}

# Check if Docker is available
check_docker() {
    if ! command -v docker &> /dev/null; then
        echo -e "${RED}Error: Docker is not installed or not in PATH${NC}"
        echo "Please install Docker: https://docs.docker.com/get-docker/"
        exit 1
    fi

    if ! command -v docker-compose &> /dev/null && ! docker compose version &> /dev/null; then
        echo -e "${RED}Error: Docker Compose is not installed${NC}"
        echo "Please install Docker Compose"
        exit 1
    fi
}

# Determine docker-compose command
get_compose_cmd() {
    if docker compose version &> /dev/null 2>&1; then
        echo "docker compose"
    else
        echo "docker-compose"
    fi
}

# Main execution
main() {
    local test_suite="${1:-all}"
    shift || true
    local pytest_args="$@"

    # Check for help flag
    if [[ "$test_suite" == "-h" || "$test_suite" == "--help" ]]; then
        usage
    fi

    echo -e "${GREEN}=== Jarvis CD Test Runner ===${NC}"
    echo "Test Suite: $test_suite"
    echo "Additional Args: $pytest_args"
    echo ""

    # Check Docker
    check_docker

    # Get docker-compose command
    COMPOSE_CMD=$(get_compose_cmd)

    cd "$SCRIPT_DIR"

    # Set service name based on test suite
    case "$test_suite" in
        all)
            SERVICE="test"
            ;;
        shell)
            SERVICE="test-shell"
            ;;
        util)
            SERVICE="test-util"
            ;;
        core)
            SERVICE="test-core"
            ;;
        parallel)
            SERVICE="test-parallel"
            ;;
        *)
            echo -e "${RED}Error: Unknown test suite '$test_suite'${NC}"
            usage
            ;;
    esac

    echo -e "${YELLOW}Building Docker image...${NC}"
    $COMPOSE_CMD build $SERVICE

    echo -e "${YELLOW}Running tests...${NC}"
    if [ -n "$pytest_args" ]; then
        PYTEST_ARGS="$pytest_args" $COMPOSE_CMD run --rm $SERVICE
    else
        $COMPOSE_CMD run --rm $SERVICE
    fi

    EXIT_CODE=$?

    if [ $EXIT_CODE -eq 0 ]; then
        echo -e "${GREEN}✓ Tests passed!${NC}"
    else
        echo -e "${RED}✗ Tests failed with exit code $EXIT_CODE${NC}"
    fi

    # Copy coverage report if it exists
    if [ -d "$PROJECT_ROOT/htmlcov" ]; then
        echo -e "${YELLOW}Coverage report available at: $PROJECT_ROOT/htmlcov/index.html${NC}"
    fi

    exit $EXIT_CODE
}

main "$@"
```

### `test/unit/__init__.py`

```python

```

### `test/unit/core/__init__.py`

```python

```

### `test/unit/core/test_cli_base.py`

```python
"""
Base test class for CLI tests.
Provides common setup and utilities for testing Jarvis CLI commands.
"""
import unittest
import sys
import os
import tempfile
import shutil
from pathlib import Path

# Add the project root to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.core.cli import JarvisCLI


class CLITestBase(unittest.TestCase):
    """Base class for CLI tests with common setup/teardown"""

    def setUp(self):
        """Set up test environment"""
        # Create temporary directories for testing
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')

        # Initialize CLI
        self.cli = JarvisCLI()
        self.cli.define_options()

        # Store original environment
        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        # Restore original environment
        os.environ.clear()
        os.environ.update(self.original_env)

        # Clean up temporary directories
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def run_command(self, args):
        """
        Helper to run a CLI command and capture result.

        :param args: List of command arguments
        :return: Result dictionary
        """
        try:
            result = self.cli.parse(args)
            return {
                'success': True,
                'result': result,
                'kwargs': self.cli.kwargs.copy(),
                'remainder': self.cli.remainder.copy()
            }
        except SystemExit as e:
            return {
                'success': False,
                'exit_code': e.code
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'exception': e
            }

    def assert_command_success(self, args):
        """Assert that a command runs successfully"""
        result = self.run_command(args)
        self.assertTrue(result.get('success'), f"Command failed: {args}")
        return result

    def assert_command_fails(self, args):
        """Assert that a command fails"""
        result = self.run_command(args)
        self.assertFalse(result.get('success'), f"Command should have failed: {args}")
        return result

    def create_test_pipeline(self, name='test_pipeline'):
        """Helper to create a test pipeline"""
        # Initialize Jarvis first
        init_args = ['init', self.config_dir, self.private_dir, self.shared_dir]
        self.run_command(init_args)

        # Create pipeline
        create_args = ['ppl', 'create', name]
        return self.run_command(create_args)

    def create_test_repo(self, name='test_repo', path=None):
        """Helper to create a test repository"""
        if path is None:
            path = os.path.join(self.test_dir, 'repos', name)

        # Create repo directory
        os.makedirs(path, exist_ok=True)

        # Initialize Jarvis if not done
        init_args = ['init', self.config_dir, self.private_dir, self.shared_dir]
        self.run_command(init_args)

        # Add repository
        add_args = ['repo', 'add', name, path]
        return self.run_command(add_args)

    def create_test_config_file(self, content, filename='test_config.yaml'):
        """Helper to create a test configuration file"""
        config_path = os.path.join(self.test_dir, filename)
        with open(config_path, 'w') as f:
            f.write(content)
        return config_path
```

### `test/unit/core/test_cli_env_rg.py`

```python
"""
Tests for 'jarvis env', 'jarvis rg', and 'jarvis mod' commands
"""
import os
from test.unit.core.test_cli_base import CLITestBase


class TestCLIEnvironment(CLITestBase):
    """Tests for environment management commands"""

    def test_env_build(self):
        """Test building environment"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['env', 'build', 'test_env']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['env_name'], 'test_env')

    def test_env_list(self):
        """Test listing environments"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['env', 'list']
        result = self.run_command(args)

        # Should execute successfully
        self.assertIsNotNone(result)

    def test_env_show(self):
        """Test showing environment details"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['env', 'show', 'test_env']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['env_name'], 'test_env')

    def test_ppl_env_build(self):
        """Test building pipeline environment"""
        self.create_test_pipeline()

        args = ['ppl', 'env', 'build']
        result = self.run_command(args)

        # Should parse successfully
        self.assertIsNotNone(result)

    def test_ppl_env_copy(self):
        """Test copying pipeline environment"""
        self.create_test_pipeline()

        args = ['ppl', 'env', 'copy', 'new_env']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['new_env_name'], 'new_env')

    def test_ppl_env_show(self):
        """Test showing pipeline environment"""
        self.create_test_pipeline()

        args = ['ppl', 'env', 'show']
        result = self.run_command(args)

        # Should parse successfully
        self.assertIsNotNone(result)


class TestCLIResourceGraph(CLITestBase):
    """Tests for resource graph commands"""

    def test_rg_build(self):
        """Test building resource graph"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['rg', 'build']
        result = self.run_command(args)

        # Should parse successfully
        self.assertIsNotNone(result)

    def test_rg_show(self):
        """Test showing resource graph"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['rg', 'show']
        result = self.run_command(args)

        # Should parse successfully
        self.assertIsNotNone(result)

    def test_rg_nodes(self):
        """Test listing resource graph nodes"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['rg', 'nodes']
        result = self.run_command(args)

        # Should parse successfully
        self.assertIsNotNone(result)

    def test_rg_node(self):
        """Test showing specific node"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['rg', 'node', 'test_node']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['hostname'], 'test_node')

    def test_rg_filter(self):
        """Test filtering resource graph"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['rg', 'filter', 'cpu']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['dev_type'], 'cpu')

    def test_rg_load(self):
        """Test loading resource graph"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        rg_file = self.create_test_config_file('nodes: []', 'rg.yaml')

        args = ['rg', 'load', rg_file]
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['rg_path'], rg_file)

    def test_rg_path(self):
        """Test getting resource graph path"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['rg', 'path']
        result = self.run_command(args)

        # Should parse successfully
        self.assertIsNotNone(result)

    def test_build_profile(self):
        """Test building resource profile"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['build', 'profile']
        result = self.run_command(args)

        # Should parse successfully
        self.assertIsNotNone(result)


class TestCLIModule(CLITestBase):
    """Tests for module commands"""

    def test_mod_create(self):
        """Test creating a module"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['mod', 'create', 'test_module']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['mod_name'], 'test_module')

    def test_mod_cd(self):
        """Test changing to module directory"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['mod', 'cd', 'test_module']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['mod_name'], 'test_module')

    def test_mod_prepend(self):
        """Test prepending to module"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['mod', 'prepend', 'test_module', 'PATH', '/new/path']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['mod_name'], 'test_module')
            # PATH and /new/path will be in remainder args since keep_remainder=True
            self.assertIn('PATH', result['remainder'])
            self.assertIn('/new/path', result['remainder'])


class TestCLIHostfile(CLITestBase):
    """Tests for hostfile commands"""

    def test_hostfile_set(self):
        """Test setting hostfile"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        hostfile = self.create_test_config_file('localhost\n', 'hosts.txt')

        args = ['hostfile', 'set', hostfile]
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['hostfile_path'], hostfile)


class TestCLICD(CLITestBase):
    """Tests for cd command"""

    def test_cd_to_pipeline(self):
        """Test cd to pipeline directory"""
        self.create_test_pipeline()

        args = ['cd']
        result = self.run_command(args)

        # Should parse successfully
        self.assertIsNotNone(result)


if __name__ == '__main__':
    import unittest
    unittest.main()
```

### `test/unit/core/test_cli_init.py`

```python
"""
Tests for 'jarvis init' command
"""
import os
from test.unit.core.test_cli_base import CLITestBase


class TestCLIInit(CLITestBase):
    """Tests for the init command"""

    def test_init_default_directories(self):
        """Test init command with default directories"""
        args = ['init']
        result = self.run_command(args)

        # Command should parse successfully (though may not execute fully in test)
        self.assertTrue(result.get('success') or result.get('exit_code') == 0)

    def test_init_custom_directories(self):
        """Test init command with custom directories"""
        args = ['init', self.config_dir, self.private_dir, self.shared_dir]
        result = self.run_command(args)

        # Verify arguments were parsed correctly
        if result.get('success'):
            self.assertEqual(result['kwargs']['config_dir'], self.config_dir)
            self.assertEqual(result['kwargs']['private_dir'], self.private_dir)
            self.assertEqual(result['kwargs']['shared_dir'], self.shared_dir)

    def test_init_with_force(self):
        """Test init command with force flag"""
        args = ['init', self.config_dir, self.private_dir, self.shared_dir, '--force=true']
        result = self.run_command(args)

        if result.get('success'):
            self.assertTrue(result['kwargs']['force'])

    def test_init_creates_directories(self):
        """Test that init actually creates directories"""
        args = ['init', self.config_dir, self.private_dir, self.shared_dir]
        self.run_command(args)

        # Directories should exist after init (if command executed)
        # Note: This may not work in isolated test environment
        # The test verifies argument parsing at minimum

    def test_init_idempotent(self):
        """Test that init can be run multiple times safely"""
        args = ['init', self.config_dir, self.private_dir, self.shared_dir]

        # Run init twice
        result1 = self.run_command(args)
        result2 = self.run_command(args)

        # Both should succeed (or fail gracefully)
        self.assertIsNotNone(result1)
        self.assertIsNotNone(result2)


if __name__ == '__main__':
    import unittest
    unittest.main()
```

### `test/unit/core/test_cli_pipeline.py`

```python
"""
Tests for 'jarvis ppl' pipeline commands
"""
import os
from test.unit.core.test_cli_base import CLITestBase


class TestCLIPipeline(CLITestBase):
    """Tests for pipeline management commands"""

    def test_ppl_create(self):
        """Test creating a new pipeline"""
        # Initialize first
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        # Create pipeline
        args = ['ppl', 'create', 'test_pipeline']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['pipeline_name'], 'test_pipeline')

    def test_ppl_create_alias(self):
        """Test creating pipeline with alias"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        # Use 'ppl c' alias
        args = ['ppl', 'c', 'test_pipeline_alias']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['pipeline_name'], 'test_pipeline_alias')

    def test_ppl_create_missing_name(self):
        """Test that creating pipeline without name fails"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['ppl', 'create']
        result = self.run_command(args)

        # Should fail due to missing required argument
        self.assertFalse(result.get('success'))

    def test_ppl_append(self):
        """Test appending package to pipeline"""
        self.create_test_pipeline()

        args = ['ppl', 'append', 'test_package']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['pkg_name'], 'test_package')

    def test_ppl_append_alias(self):
        """Test appending with alias"""
        self.create_test_pipeline()

        args = ['ppl', 'a', 'test_package']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['pkg_name'], 'test_package')

    def test_ppl_list(self):
        """Test listing pipelines"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['ppl', 'list']
        result = self.run_command(args)

        # Should succeed
        self.assertIsNotNone(result)

    def test_ppl_load(self):
        """Test loading a pipeline"""
        self.create_test_pipeline('my_pipeline')

        args = ['ppl', 'load', 'my_pipeline']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['pipeline_name'], 'my_pipeline')

    def test_ppl_print(self):
        """Test printing pipeline info"""
        self.create_test_pipeline()

        args = ['ppl', 'print']
        result = self.run_command(args)

        # Should execute without error
        self.assertIsNotNone(result)

    def test_ppl_status(self):
        """Test checking pipeline status"""
        self.create_test_pipeline()

        args = ['ppl', 'status']
        result = self.run_command(args)

        # Should execute without error
        self.assertIsNotNone(result)

    def test_ppl_clean(self):
        """Test cleaning pipeline"""
        self.create_test_pipeline()

        args = ['ppl', 'clean']
        result = self.run_command(args)

        # Should execute
        self.assertIsNotNone(result)

    def test_ppl_rm(self):
        """Test removing package from pipeline"""
        self.create_test_pipeline()

        # Append a package first
        self.run_command(['ppl', 'append', 'test_pkg'])

        # Remove it
        args = ['ppl', 'rm', 'test_pkg']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['package_spec'], 'test_pkg')

    def test_ppl_destroy(self):
        """Test destroying a pipeline"""
        self.create_test_pipeline('destroy_me')

        args = ['ppl', 'destroy', 'destroy_me']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['pipeline_name'], 'destroy_me')

    def test_ppl_run(self):
        """Test running a pipeline"""
        self.create_test_pipeline()

        args = ['ppl', 'run']
        result = self.run_command(args)

        # Should parse successfully
        self.assertIsNotNone(result)

    def test_ppl_start(self):
        """Test starting a pipeline"""
        self.create_test_pipeline()

        args = ['ppl', 'start']
        result = self.run_command(args)

        # Should parse successfully
        self.assertIsNotNone(result)

    def test_ppl_stop(self):
        """Test stopping a pipeline"""
        self.create_test_pipeline()

        args = ['ppl', 'stop']
        result = self.run_command(args)

        # Should parse successfully
        self.assertIsNotNone(result)

    def test_ppl_kill(self):
        """Test killing a pipeline"""
        self.create_test_pipeline()

        args = ['ppl', 'kill']
        result = self.run_command(args)

        # Should parse successfully
        self.assertIsNotNone(result)

    def test_ppl_update(self):
        """Test updating pipeline"""
        self.create_test_pipeline()

        args = ['ppl', 'update']
        result = self.run_command(args)

        # Should parse successfully
        self.assertIsNotNone(result)


if __name__ == '__main__':
    import unittest
    unittest.main()
```

### `test/unit/core/test_cli_repo_pkg.py`

```python
"""
Tests for 'jarvis repo' and 'jarvis pkg' commands
"""
import os
from test.unit.core.test_cli_base import CLITestBase


class TestCLIRepository(CLITestBase):
    """Tests for repository management commands"""

    def test_repo_add(self):
        """Test adding a repository"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        repo_path = os.path.join(self.test_dir, 'test_repo')
        os.makedirs(repo_path, exist_ok=True)

        args = ['repo', 'add', 'myrepo', repo_path]
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['repo_name'], 'myrepo')
            self.assertEqual(result['kwargs']['repo_path'], repo_path)

    def test_repo_remove(self):
        """Test removing a repository"""
        self.create_test_repo('remove_me')

        args = ['repo', 'remove', 'remove_me']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['repo_name'], 'remove_me')

    def test_repo_list(self):
        """Test listing repositories"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        args = ['repo', 'list']
        result = self.run_command(args)

        # Should execute successfully
        self.assertIsNotNone(result)

    def test_repo_create(self):
        """Test creating a new repository"""
        self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])

        repo_path = os.path.join(self.test_dir, 'new_repo')

        args = ['repo', 'create', 'newrepo', repo_path]
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['repo_name'], 'newrepo')
            self.assertEqual(result['kwargs']['repo_path'], repo_path)


class TestCLIPackage(CLITestBase):
    """Tests for package management commands"""

    def test_pkg_configure(self):
        """Test configuring a package"""
        self.create_test_pipeline()

        args = ['pkg', 'configure', 'test_pkg']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['pkg_name'], 'test_pkg')

    def test_pkg_configure_with_options(self):
        """Test package configuration with options"""
        self.create_test_pipeline()

        args = ['pkg', 'configure', 'test_pkg', '--arg1=value1', '--arg2=value2']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['pkg_name'], 'test_pkg')
            # Remainder should contain the options
            self.assertIn('--arg1=value1', result['remainder'])

    def test_pkg_readme(self):
        """Test viewing package README"""
        self.create_test_pipeline()

        args = ['pkg', 'readme', 'test_pkg']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['pkg_name'], 'test_pkg')

    def test_pkg_path(self):
        """Test getting package path"""
        self.create_test_pipeline()

        args = ['pkg', 'path', 'test_pkg']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['pkg_name'], 'test_pkg')

    def test_pkg_help(self):
        """Test package help command"""
        self.create_test_pipeline()

        args = ['pkg', 'help', 'test_pkg']
        result = self.run_command(args)

        if result.get('success'):
            self.assertEqual(result['kwargs']['pkg_name'], 'test_pkg')


if __name__ == '__main__':
    import unittest
    unittest.main()
```

### `test/unit/core/test_environment_integration.py`

```python
"""
Integration tests for environment management operations.
Tests the complete workflow: create named env, copy to pipeline, show, list, verify.
"""
import unittest
import sys
import os
import tempfile
import shutil
import yaml
from pathlib import Path

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.core.cli import JarvisCLI


class TestEnvironmentIntegration(unittest.TestCase):
    """Integration test for environment management workflow"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_env_integration_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')

        # Initialize CLI
        self.cli = JarvisCLI()
        self.cli.define_options()

        # Store original environment
        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def run_command(self, args):
        """Helper to run CLI command"""
        try:
            result = self.cli.parse(args)
            return {
                'success': True,
                'result': result,
                'kwargs': self.cli.kwargs.copy() if hasattr(self.cli, 'kwargs') else {},
                'remainder': self.cli.remainder.copy() if hasattr(self.cli, 'remainder') else []
            }
        except SystemExit as e:
            return {
                'success': False,
                'exit_code': e.code
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'exception': e
            }

    def test_complete_environment_workflow(self):
        """
        Test complete environment workflow:
        1. Create pipeline: jarvis ppl create test
        2. Create named environment: jarvis env build test_env
        3. Copy environment to pipeline: jarvis ppl env copy test_env
        4. Verify: Pipeline environment file exists and equals the named environment file
        5. Show environment: jarvis env show test_env
        6. List environments: jarvis env list
        7. Cleanup
        """

        # Step 0: Initialize Jarvis
        print("\n=== Step 0: Initialize Jarvis ===")
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")
        print("Jarvis initialized successfully")

        # Step 1: Create pipeline
        print("\n=== Step 1: Create pipeline 'test' ===")
        result = self.run_command(['ppl', 'create', 'test'])
        self.assertTrue(result.get('success'), f"Pipeline create failed: {result}")
        self.assertEqual(result['kwargs'].get('pipeline_name'), 'test')

        # Verify pipeline was created
        pipeline_dir = Path(self.shared_dir) / 'test'
        self.assertTrue(pipeline_dir.exists(), f"Pipeline directory not created: {pipeline_dir}")
        print(f"Pipeline 'test' created at: {pipeline_dir}")

        # Step 2: Create named environment with test variables
        print("\n=== Step 2: Create named environment 'test_env' ===")
        # Set some test environment variables in the current environment
        # that will be captured
        test_env_vars = {
            'TEST_VAR': 'test_value_123',
            'PATH': '/test/path:/another/path',
            'MY_CUSTOM_VAR': 'custom_value'
        }

        # Temporarily set these in the environment
        for key, value in test_env_vars.items():
            os.environ[key] = value

        result = self.run_command(['env', 'build', 'test_env', 'EXTRA_VAR=extra_value'])
        self.assertTrue(result.get('success'), f"Env build failed: {result}")
        self.assertEqual(result['kwargs'].get('env_name'), 'test_env')

        # Verify named environment file was created
        # Named environments are stored in ~/.ppi-jarvis/env/, not in the config dir
        from jarvis_cd.core.config import Jarvis
        jarvis = Jarvis.get_instance()
        env_file = jarvis.jarvis_root / 'env' / 'test_env.yaml'
        self.assertTrue(env_file.exists(), f"Named environment file not created: {env_file}")
        print(f"Named environment 'test_env' created at: {env_file}")

        # Verify the environment contains our test variables
        with open(env_file, 'r') as f:
            env_content = yaml.safe_load(f)

        # Should contain PATH (from COMMON_ENV_VARS) and EXTRA_VAR (from args)
        self.assertIn('PATH', env_content, "PATH should be captured")
        self.assertIn('EXTRA_VAR', env_content, "EXTRA_VAR should be added")
        self.assertEqual(env_content['EXTRA_VAR'], 'extra_value')
        print(f"Environment contains {len(env_content)} variables")

        # Step 3: Copy environment to pipeline
        print("\n=== Step 3: Copy environment to pipeline ===")
        result = self.run_command(['ppl', 'env', 'copy', 'test_env'])
        self.assertTrue(result.get('success'), f"Env copy failed: {result}")
        self.assertEqual(result['kwargs'].get('env_name'), 'test_env')

        # Verify pipeline environment file was created
        # Pipeline environment is stored in config directory, not shared directory
        pipeline_config_dir = Path(self.config_dir) / 'pipelines' / 'test'
        pipeline_env_file = pipeline_config_dir / 'env.yaml'
        self.assertTrue(pipeline_env_file.exists(), f"Pipeline env file not created: {pipeline_env_file}")
        print(f"Environment copied to pipeline at: {pipeline_env_file}")

        # Step 4: Verify pipeline environment equals named environment
        print("\n=== Step 4: Verify environment files are identical ===")
        with open(env_file, 'r') as f:
            named_env = yaml.safe_load(f)

        with open(pipeline_env_file, 'r') as f:
            pipeline_env = yaml.safe_load(f)

        self.assertEqual(named_env, pipeline_env,
                        "Pipeline environment should match named environment")
        print(f"Verified: Both environments contain {len(named_env)} identical variables")

        # Verify specific test variables
        self.assertIn('PATH', pipeline_env)
        self.assertIn('EXTRA_VAR', pipeline_env)
        self.assertEqual(pipeline_env['EXTRA_VAR'], 'extra_value')
        print("Verified: Test variables are present and correct")

        # Step 5: Show environment
        print("\n=== Step 5: Show named environment ===")
        # Capture stdout to verify output
        from io import StringIO
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            result = self.run_command(['env', 'show', 'test_env'])
            self.assertTrue(result.get('success'), f"Env show failed: {result}")

            # Get the output
            output = sys.stdout.getvalue()

            # Verify output contains environment name and variables
            self.assertIn('test_env', output, "Output should mention environment name")
            self.assertIn('PATH', output, "Output should show PATH variable")
            self.assertIn('EXTRA_VAR', output, "Output should show EXTRA_VAR")
            print(f"Environment 'test_env' displayed successfully")

        finally:
            sys.stdout = old_stdout

        # Step 6: List environments
        print("\n=== Step 6: List all environments ===")
        sys.stdout = StringIO()

        try:
            result = self.run_command(['env', 'list'])
            self.assertTrue(result.get('success'), f"Env list failed: {result}")

            # Verify test_env is in the list
            # The list command should show available environments
            # We can verify by checking the file exists
            env_dir = jarvis_config.jarvis_root / 'env'
            env_files = list(env_dir.glob('*.yaml'))
            env_names = [f.stem for f in env_files]
            self.assertIn('test_env', env_names, "test_env should be in environment list")
            print(f"Found {len(env_names)} environment(s): {', '.join(env_names)}")

        finally:
            sys.stdout = old_stdout

        # Step 7: Show pipeline environment
        print("\n=== Step 7: Show pipeline environment ===")
        sys.stdout = StringIO()

        try:
            result = self.run_command(['ppl', 'env', 'show'])
            self.assertTrue(result.get('success'), f"Pipeline env show failed: {result}")

            output = sys.stdout.getvalue()
            self.assertIn('test', output, "Output should mention pipeline name")

        finally:
            sys.stdout = old_stdout

        # Step 8: Cleanup - Destroy pipeline
        print("\n=== Step 8: Cleanup ===")
        result = self.run_command(['ppl', 'destroy', 'test'])
        self.assertTrue(result.get('success'), f"Pipeline destroy failed: {result}")

        # Verify pipeline config directory was removed
        # Note: destroy only removes the config directory, not shared/private dirs
        self.assertFalse(pipeline_config_dir.exists(), "Pipeline config directory should be removed")
        print("Pipeline 'test' destroyed successfully")

        # Manually cleanup named environment (no env remove command exists)
        if env_file.exists():
            env_file.unlink()
            print(f"Removed named environment file: {env_file}")

        print("\n=== Test completed successfully ===")


class TestEnvironmentEdgeCases(unittest.TestCase):
    """Test edge cases and error handling for environment operations"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_env_edge_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')

        self.cli = JarvisCLI()
        self.cli.define_options()

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def run_command(self, args):
        """Helper to run CLI command"""
        try:
            result = self.cli.parse(args)
            return {
                'success': True,
                'result': result,
                'kwargs': self.cli.kwargs.copy() if hasattr(self.cli, 'kwargs') else {},
                'remainder': self.cli.remainder.copy() if hasattr(self.cli, 'remainder') else []
            }
        except SystemExit as e:
            return {
                'success': False,
                'exit_code': e.code
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'exception': e
            }

    def test_copy_nonexistent_environment(self):
        """Test copying a non-existent environment to pipeline"""
        # Initialize
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'))

        # Create pipeline
        result = self.run_command(['ppl', 'create', 'test_pipeline'])
        self.assertTrue(result.get('success'))

        # Try to copy non-existent environment
        # This should handle the error gracefully
        result = self.run_command(['ppl', 'env', 'copy', 'nonexistent_env'])
        # The command should parse successfully but the execution may print a warning
        self.assertIsNotNone(result)

        # Verify no env file was created in pipeline
        pipeline_dir = Path(self.shared_dir) / 'test_pipeline'
        pipeline_env_file = pipeline_dir / 'env.yaml'
        # Should not exist since copy should fail
        # (depends on implementation - may create empty or not create at all)

        # Cleanup
        self.run_command(['ppl', 'destroy', 'test_pipeline'])

    def test_show_nonexistent_environment(self):
        """Test showing a non-existent named environment"""
        # Initialize
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'))

        # Try to show non-existent environment
        from io import StringIO
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            result = self.run_command(['env', 'show', 'nonexistent'])
            # Should handle gracefully and print a message
            self.assertIsNotNone(result)

            output = sys.stdout.getvalue()
            # Should mention that it wasn't found
            self.assertTrue('not found' in output.lower() or 'no named environments' in output.lower())

        finally:
            sys.stdout = old_stdout

    def test_build_environment_with_multiple_variables(self):
        """Test building environment with multiple custom variables"""
        # Initialize
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'))

        # Build environment with multiple variables
        result = self.run_command([
            'env', 'build', 'multi_var_env',
            'VAR1=value1',
            'VAR2=value2',
            'VAR3=value3'
        ])
        self.assertTrue(result.get('success'))

        # Verify all variables are in the environment
        # Named environments are stored in jarvis root, not config dir
        from jarvis_cd.core.config import Jarvis
        jarvis = Jarvis.get_instance()
        env_file = jarvis.jarvis_root / 'env' / 'multi_var_env.yaml'
        self.assertTrue(env_file.exists())

        with open(env_file, 'r') as f:
            env_content = yaml.safe_load(f)

        self.assertIn('VAR1', env_content)
        self.assertIn('VAR2', env_content)
        self.assertIn('VAR3', env_content)
        self.assertEqual(env_content['VAR1'], 'value1')
        self.assertEqual(env_content['VAR2'], 'value2')
        self.assertEqual(env_content['VAR3'], 'value3')

        # Cleanup
        env_file.unlink()


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/core/test_ior_delegate.py`

```python
"""
Unit tests for IOR package delegation functionality.
Tests the _get_delegate method in the Pkg base class.
"""
import unittest
import sys
import os
import tempfile
import shutil
from pathlib import Path

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.core.config import Jarvis
from jarvis_cd.core.pipeline import Pipeline


def initialize_jarvis_for_test(config_dir, private_dir, shared_dir):
    """Helper function to properly initialize Jarvis for testing"""
    # Get Jarvis singleton and initialize it
    jarvis = Jarvis.get_instance()
    jarvis.initialize(config_dir, private_dir, shared_dir, force=True)

    return jarvis


class TestIorDelegation(unittest.TestCase):
    """Test the IOR package delegation functionality"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_ior_delegate_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')
        os.makedirs(self.config_dir, exist_ok=True)
        os.makedirs(self.private_dir, exist_ok=True)
        os.makedirs(self.shared_dir, exist_ok=True)

        # Initialize Jarvis config
        os.environ['JARVIS_CONFIG'] = self.config_dir
        os.environ['JARVIS_PRIVATE'] = self.private_dir
        os.environ['JARVIS_SHARED'] = self.shared_dir

        # Initialize Jarvis properly
        self.jarvis = initialize_jarvis_for_test(self.config_dir, self.private_dir, self.shared_dir)

        # Create a test pipeline
        self.pipeline = Pipeline()
        self.pipeline.create('test_ior_pipeline')

    def tearDown(self):
        """Clean up test environment"""
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def test_delegate_default_mode(self):
        """Test delegation to IorDefault implementation"""
        # Create package definition directly to avoid validation
        pkg_def = {
            'pkg_type': 'builtin.ior',
            'pkg_id': 'test_ior',
            'pkg_name': 'ior',
            'global_id': f'{self.pipeline.name}.test_ior',
            'config': {
                'deploy_mode': 'default',
                'nprocs': 1,
                'ppn': 16,
                'block': '32m',
                'xfer': '1m',
                'api': 'posix',
                'out': '/tmp/ior.bin',
                'log': '/tmp/ior.log',
                'write': True,
                'read': False,
                'fpp': False,
                'reps': 1,
                'direct': False,
                'interceptors': []
            }
        }
        self.pipeline.packages.append(pkg_def)
        self.pipeline.save()

        # Find the package in pipeline
        pkg_def = None
        for pkg in self.pipeline.packages:
            if pkg['pkg_id'] == 'test_ior':
                pkg_def = pkg
                break

        self.assertIsNotNone(pkg_def, "IOR package should be in pipeline")

        # Load the package instance
        from jarvis_cd.core.pipeline import Pipeline
        pkg_instance = self.pipeline._load_package_instance(pkg_def, {})

        # Configure with default deploy mode
        pkg_instance.configure(deploy_mode='default')

        # Get delegate for default mode
        delegate = pkg_instance._get_delegate('default')

        # Verify delegate is IorDefault
        self.assertEqual(delegate.__class__.__name__, 'IorDefault',
                        "Delegate should be IorDefault for deploy='default'")

        # Verify delegate has same config
        self.assertEqual(delegate.config.get('deploy_mode'), 'default',
                        "Delegate should have same config")

    def test_delegate_container_mode(self):
        """Test delegation to IorContainer implementation"""
        # Create package definition directly
        pkg_def = {
            'pkg_type': 'builtin.ior',
            'pkg_id': 'test_ior',
            'pkg_name': 'ior',
            'global_id': f'{self.pipeline.name}.test_ior',
            'config': {
                'deploy_mode': 'container',
                'nprocs': 1,
                'ppn': 16,
                'block': '32m',
                'xfer': '1m',
                'api': 'posix',
                'out': '/tmp/ior.bin',
                'log': '/tmp/ior.log',
                'write': True,
                'read': False,
                'fpp': False,
                'reps': 1,
                'direct': False,
                'interceptors': []
            }
        }
        self.pipeline.packages.append(pkg_def)
        self.pipeline.save()

        # Find the package in pipeline
        pkg_def = None
        for pkg in self.pipeline.packages:
            if pkg['pkg_id'] == 'test_ior':
                pkg_def = pkg
                break

        self.assertIsNotNone(pkg_def, "IOR package should be in pipeline")

        # Load the package instance
        pkg_instance = self.pipeline._load_package_instance(pkg_def, {})

        # Configure with container deploy mode
        pkg_instance.configure(deploy_mode='container')

        # Get delegate for container mode
        delegate = pkg_instance._get_delegate('container')

        # Verify delegate is IorContainer
        self.assertEqual(delegate.__class__.__name__, 'IorContainer',
                        "Delegate should be IorContainer for deploy_mode='container'")

        # Verify delegate has same config
        self.assertEqual(delegate.config.get('deploy_mode'), 'container',
                        "Delegate should have same config")

    def test_delegate_caching(self):
        """Test that delegates are cached properly"""
        # Create package definition directly
        pkg_def = {
            'pkg_type': 'builtin.ior',
            'pkg_id': 'test_ior',
            'pkg_name': 'ior',
            'global_id': f'{self.pipeline.name}.test_ior',
            'config': {
                'deploy_mode': 'default',
                'nprocs': 1,
                'ppn': 16,
                'block': '32m',
                'xfer': '1m',
                'api': 'posix',
                'out': '/tmp/ior.bin',
                'log': '/tmp/ior.log',
                'write': True,
                'read': False,
                'fpp': False,
                'reps': 1,
                'direct': False,
                'interceptors': []
            }
        }
        self.pipeline.packages.append(pkg_def)
        self.pipeline.save()

        # Find the package in pipeline
        pkg_def = None
        for pkg in self.pipeline.packages:
            if pkg['pkg_id'] == 'test_ior':
                pkg_def = pkg
                break

        self.assertIsNotNone(pkg_def, "IOR package should be in pipeline")

        # Load the package instance
        pkg_instance = self.pipeline._load_package_instance(pkg_def, {})

        # Configure with default deploy mode
        pkg_instance.configure(deploy_mode='default')

        # Get delegate twice
        delegate1 = pkg_instance._get_delegate('default')
        delegate2 = pkg_instance._get_delegate('default')

        # Verify they are the same instance (cached)
        self.assertIs(delegate1, delegate2,
                     "Delegate should be cached and return same instance")

    def test_delegate_multiple_modes(self):
        """Test that different deploy modes create different delegates"""
        # Create package definition directly
        pkg_def = {
            'pkg_type': 'builtin.ior',
            'pkg_id': 'test_ior',
            'pkg_name': 'ior',
            'global_id': f'{self.pipeline.name}.test_ior',
            'config': {
                'deploy_mode': 'default',
                'nprocs': 1,
                'ppn': 16,
                'block': '32m',
                'xfer': '1m',
                'api': 'posix',
                'out': '/tmp/ior.bin',
                'log': '/tmp/ior.log',
                'write': True,
                'read': False,
                'fpp': False,
                'reps': 1,
                'direct': False,
                'interceptors': []
            }
        }
        self.pipeline.packages.append(pkg_def)
        self.pipeline.save()

        # Find the package in pipeline
        pkg_def = None
        for pkg in self.pipeline.packages:
            if pkg['pkg_id'] == 'test_ior':
                pkg_def = pkg
                break

        self.assertIsNotNone(pkg_def, "IOR package should be in pipeline")

        # Load the package instance
        pkg_instance = self.pipeline._load_package_instance(pkg_def, {})

        # Configure
        pkg_instance.configure(deploy_mode='default')

        # Get delegates for different modes
        delegate_default = pkg_instance._get_delegate('default')
        delegate_container = pkg_instance._get_delegate('container')

        # Verify they are different instances
        self.assertIsNot(delegate_default, delegate_container,
                        "Different deploy modes should create different delegates")

        # Verify correct types
        self.assertEqual(delegate_default.__class__.__name__, 'IorDefault')
        self.assertEqual(delegate_container.__class__.__name__, 'IorContainer')

    def test_delegate_invalid_mode(self):
        """Test that invalid deploy mode raises proper error"""
        # Create package definition directly
        pkg_def = {
            'pkg_type': 'builtin.ior',
            'pkg_id': 'test_ior',
            'pkg_name': 'ior',
            'global_id': f'{self.pipeline.name}.test_ior',
            'config': {
                'deploy_mode': 'default',
                'nprocs': 1,
                'ppn': 16,
                'block': '32m',
                'xfer': '1m',
                'api': 'posix',
                'out': '/tmp/ior.bin',
                'log': '/tmp/ior.log',
                'write': True,
                'read': False,
                'fpp': False,
                'reps': 1,
                'direct': False,
                'interceptors': []
            }
        }
        self.pipeline.packages.append(pkg_def)
        self.pipeline.save()

        # Find the package in pipeline
        pkg_def = None
        for pkg in self.pipeline.packages:
            if pkg['pkg_id'] == 'test_ior':
                pkg_def = pkg
                break

        self.assertIsNotNone(pkg_def, "IOR package should be in pipeline")

        # Load the package instance
        pkg_instance = self.pipeline._load_package_instance(pkg_def, {})

        # Configure
        pkg_instance.configure(deploy_mode='default')

        # Try to get delegate with invalid mode
        with self.assertRaises(ImportError) as context:
            pkg_instance._get_delegate('invalid_mode')

        self.assertIn('invalid_mode', str(context.exception),
                     "Error should mention the invalid mode")

    def test_delegate_state_sharing(self):
        """Test that delegate shares state with parent"""
        # Create package definition directly
        pkg_def = {
            'pkg_type': 'builtin.ior',
            'pkg_id': 'test_ior',
            'pkg_name': 'ior',
            'global_id': f'{self.pipeline.name}.test_ior',
            'config': {
                'deploy_mode': 'default',
                'nprocs': 4,
                'ppn': 16,
                'block': '64m',
                'xfer': '1m',
                'api': 'posix',
                'out': '/tmp/ior.bin',
                'log': '/tmp/ior.log',
                'write': True,
                'read': False,
                'fpp': False,
                'reps': 1,
                'direct': False,
                'interceptors': []
            }
        }
        self.pipeline.packages.append(pkg_def)
        self.pipeline.save()

        # Find the package in pipeline
        pkg_def = None
        for pkg in self.pipeline.packages:
            if pkg['pkg_id'] == 'test_ior':
                pkg_def = pkg
                break

        self.assertIsNotNone(pkg_def, "IOR package should be in pipeline")

        # Load the package instance
        pkg_instance = self.pipeline._load_package_instance(pkg_def, {})

        # Configure
        pkg_instance.configure(deploy_mode='default', nprocs=4, block='64m')

        # Get delegate
        delegate = pkg_instance._get_delegate('default')

        # Verify delegate has same pkg_id, global_id, and config
        self.assertEqual(delegate.pkg_id, pkg_instance.pkg_id,
                        "Delegate should have same pkg_id")
        self.assertEqual(delegate.global_id, pkg_instance.global_id,
                        "Delegate should have same global_id")
        self.assertEqual(delegate.config.get('nprocs'), 4,
                        "Delegate should have same config values")
        self.assertEqual(delegate.config.get('block'), '64m',
                        "Delegate should have same config values")


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/core/test_module_integration.py`

```python
"""
Module (Lmod) integration tests using Docker container with Lmod support.
Tests jarvis mod commands and verifies file creation in ~/.ppi-jarvis-mods.
"""
import unittest
import os
import sys
import subprocess
import shutil
from pathlib import Path

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.core.cli import JarvisCLI


class TestModuleIntegrationDocker(unittest.TestCase):
    """Module integration tests using Docker container with Lmod"""

    @classmethod
    def setUpClass(cls):
        """Set up Docker container for testing"""
        cls.use_docker = cls._check_docker_available()
        cls.container_name = 'jarvis_mod_test'

        if cls.use_docker:
            # Check if iowarp/iowarp-base:latest exists
            result = subprocess.run(
                ['docker', 'images', '-q', 'iowarp/iowarp-base:latest'],
                capture_output=True,
                text=True
            )
            if not result.stdout.strip():
                print("Warning: iowarp/iowarp-base:latest not found, skipping Docker tests")
                cls.use_docker = False

    @staticmethod
    def _check_docker_available():
        """Check if Docker is available"""
        try:
            result = subprocess.run(
                ['docker', '--version'],
                capture_output=True,
                timeout=5
            )
            return result.returncode == 0
        except (FileNotFoundError, subprocess.TimeoutExpired):
            return False

    def setUp(self):
        """Set up test environment"""
        self.test_dir = Path(__file__).parent / 'test_mod_workspace'
        self.test_dir.mkdir(exist_ok=True)

        self.config_dir = self.test_dir / 'config'
        self.private_dir = self.test_dir / 'private'
        self.shared_dir = self.test_dir / 'shared'
        self.mods_dir = Path.home() / '.ppi-jarvis-mods'

        # Initialize CLI and define options
        self.cli = JarvisCLI()
        self.cli.define_options()

    def tearDown(self):
        """Clean up test environment"""
        # Remove test workspace
        if self.test_dir.exists():
            shutil.rmtree(self.test_dir)

        # Clean up test modules
        if self.mods_dir.exists():
            for test_mod in ['test1', 'test2', 'test_dep_mod', 'test_import', 'test_update']:
                mod_yaml = self.mods_dir / 'modules' / f'{test_mod}.yaml'
                mod_tcl = self.mods_dir / 'modules' / test_mod
                mod_pkg = self.mods_dir / 'packages' / test_mod

                if mod_yaml.exists():
                    mod_yaml.unlink()
                if mod_tcl.exists():
                    mod_tcl.unlink()
                if mod_pkg.exists():
                    shutil.rmtree(mod_pkg)

        # Stop and remove Docker container if used
        if hasattr(self, 'container_name') and self.use_docker:
            subprocess.run(
                ['docker', 'rm', '-f', self.container_name],
                capture_output=True
            )

    def run_command(self, args):
        """Helper to run CLI command"""
        try:
            result = self.cli.parse(args)
            return {
                'success': True,
                'result': result,
                'kwargs': self.cli.kwargs.copy() if hasattr(self.cli, 'kwargs') else {},
                'remainder': self.cli.remainder.copy() if hasattr(self.cli, 'remainder') else []
            }
        except SystemExit as e:
            return {'success': False, 'exit_code': e.code}
        except Exception as e:
            return {'success': False, 'error': str(e), 'exception': e}

    def test_mod_create_test1(self):
        """Test: jarvis mod create test1"""
        # Initialize Jarvis
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Create module test1
        result = self.run_command(['mod', 'create', 'test1'])

        # Verify module was created (check kwargs or file existence)
        if result.get('success'):
            self.assertEqual(result['kwargs'].get('mod_name'), 'test1')

        # Verify files created
        packages_dir = self.mods_dir / 'packages' / 'test1'
        src_dir = packages_dir / 'src'
        yaml_file = self.mods_dir / 'modules' / 'test1.yaml'
        tcl_file = self.mods_dir / 'modules' / 'test1'

        self.assertTrue(packages_dir.exists(), f"Package directory not created: {packages_dir}")
        self.assertTrue(src_dir.exists(), f"Source directory not created: {src_dir}")
        self.assertTrue(yaml_file.exists(), f"YAML file not created: {yaml_file}")
        self.assertTrue(tcl_file.exists(), f"TCL file not created: {tcl_file}")

        # Verify YAML content
        import yaml
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)

        self.assertIn('prepends', config)
        self.assertIn('setenvs', config)
        self.assertIn('deps', config)
        self.assertIn('doc', config)

        # Verify default paths include package root
        package_root = str(packages_dir)
        self.assertIn('PATH', config['prepends'])
        self.assertIn(f'{package_root}/bin', config['prepends']['PATH'])

        print(f"Module test1 created successfully at {packages_dir}")

    def test_mod_create_test2(self):
        """Test: jarvis mod create test2"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Create module test2
        result = self.run_command(['mod', 'create', 'test2'])

        if result.get('success'):
            self.assertEqual(result['kwargs'].get('mod_name'), 'test2')

        # Verify files created
        packages_dir = self.mods_dir / 'packages' / 'test2'
        src_dir = packages_dir / 'src'
        yaml_file = self.mods_dir / 'modules' / 'test2.yaml'
        tcl_file = self.mods_dir / 'modules' / 'test2'

        self.assertTrue(packages_dir.exists(), "Package directory not created")
        self.assertTrue(src_dir.exists(), "Source directory not created")
        self.assertTrue(yaml_file.exists(), "YAML file not created")
        self.assertTrue(tcl_file.exists(), "TCL file not created")

        print(f"Module test2 created successfully at {packages_dir}")

    def test_mod_directory_structure(self):
        """Test: Verify ~/.ppi-jarvis-mods directory structure"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create modules
        self.run_command(['mod', 'create', 'test1'])
        self.run_command(['mod', 'create', 'test2'])

        # Verify root directory structure
        self.assertTrue(self.mods_dir.exists(), "Modules root directory not created")
        self.assertTrue((self.mods_dir / 'packages').exists(), "Packages directory not created")
        self.assertTrue((self.mods_dir / 'modules').exists(), "Modules directory not created")

        # Verify test1 structure
        test1_pkg = self.mods_dir / 'packages' / 'test1'
        self.assertTrue(test1_pkg.exists())
        self.assertTrue((test1_pkg / 'src').exists())

        # Verify test2 structure
        test2_pkg = self.mods_dir / 'packages' / 'test2'
        self.assertTrue(test2_pkg.exists())
        self.assertTrue((test2_pkg / 'src').exists())

        print("Module directory structure verified")

    def test_mod_cd(self):
        """Test: jarvis mod cd test1"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create module
        self.run_command(['mod', 'create', 'test1'])

        # Change to module
        result = self.run_command(['mod', 'cd', 'test1'])
        if result.get('success'):
            self.assertEqual(result['kwargs'].get('mod_name'), 'test1')

        print("Successfully changed to module test1")

    def test_mod_prepend(self):
        """Test: jarvis mod prepend test1 PATH=/custom/path"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create module
        self.run_command(['mod', 'create', 'test1'])

        # Prepend environment variable
        result = self.run_command(['mod', 'prepend', 'test1', 'PATH=/custom/path'])

        # Verify YAML was updated
        yaml_file = self.mods_dir / 'modules' / 'test1.yaml'
        import yaml
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)

        self.assertIn('PATH', config['prepends'])
        self.assertIn('/custom/path', config['prepends']['PATH'])

        # Verify TCL was regenerated
        tcl_file = self.mods_dir / 'modules' / 'test1'
        with open(tcl_file, 'r') as f:
            tcl_content = f.read()

        self.assertIn('prepend-path PATH /custom/path', tcl_content)

        print("Successfully prepended PATH to test1")

    def test_mod_setenv(self):
        """Test: jarvis mod setenv test1 MY_VAR=hello"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create module
        self.run_command(['mod', 'create', 'test1'])

        # Set environment variable
        result = self.run_command(['mod', 'setenv', 'test1', 'MY_VAR=hello'])

        # Verify YAML was updated
        yaml_file = self.mods_dir / 'modules' / 'test1.yaml'
        import yaml
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)

        self.assertIn('MY_VAR', config['setenvs'])
        self.assertEqual(config['setenvs']['MY_VAR'], 'hello')

        # Verify TCL was regenerated
        tcl_file = self.mods_dir / 'modules' / 'test1'
        with open(tcl_file, 'r') as f:
            tcl_content = f.read()

        self.assertIn('setenv MY_VAR hello', tcl_content)

        print("Successfully set MY_VAR in test1")

    def test_mod_destroy(self):
        """Test: jarvis mod destroy test1"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create module
        self.run_command(['mod', 'create', 'test1'])

        # Verify creation
        packages_dir = self.mods_dir / 'packages' / 'test1'
        yaml_file = self.mods_dir / 'modules' / 'test1.yaml'
        tcl_file = self.mods_dir / 'modules' / 'test1'

        self.assertTrue(packages_dir.exists())
        self.assertTrue(yaml_file.exists())
        self.assertTrue(tcl_file.exists())

        # Destroy module
        result = self.run_command(['mod', 'destroy', 'test1'])

        # Verify deletion
        self.assertFalse(packages_dir.exists(), "Package directory still exists")
        self.assertFalse(yaml_file.exists(), "YAML file still exists")
        self.assertFalse(tcl_file.exists(), "TCL file still exists")

        print("Successfully destroyed test1")

    def test_mod_clear(self):
        """Test: jarvis mod clear test1"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create module
        self.run_command(['mod', 'create', 'test1'])

        # Add files to package directory
        packages_dir = self.mods_dir / 'packages' / 'test1'
        bin_dir = packages_dir / 'bin'
        bin_dir.mkdir(exist_ok=True)
        test_file = bin_dir / 'test_exec'
        test_file.write_text('#!/bin/bash\necho test')

        # Verify files exist
        self.assertTrue(bin_dir.exists())
        self.assertTrue(test_file.exists())

        # Clear module (preserves src/)
        result = self.run_command(['mod', 'clear', 'test1'])

        # Verify bin/ was removed but src/ preserved
        self.assertFalse(bin_dir.exists(), "bin/ directory still exists")
        self.assertTrue((packages_dir / 'src').exists(), "src/ directory was removed")

        print("Successfully cleared test1 (preserved src/)")

    def test_mod_list(self):
        """Test: jarvis mod list"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create multiple modules
        self.run_command(['mod', 'create', 'test1'])
        self.run_command(['mod', 'create', 'test2'])

        # List modules
        result = self.run_command(['mod', 'list'])

        # Verify command executed (listing is printed)
        self.assertIsNotNone(result)

        print("Successfully listed modules")

    def test_mod_src_dir(self):
        """Test: jarvis mod src test1"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create module
        self.run_command(['mod', 'create', 'test1'])

        # Get src directory
        result = self.run_command(['mod', 'src', 'test1'])

        # Verify result
        if result.get('success'):
            expected_src = str(self.mods_dir / 'packages' / 'test1' / 'src')
            # The src path is printed, not returned in kwargs
            self.assertTrue((self.mods_dir / 'packages' / 'test1' / 'src').exists())

        print("Successfully retrieved src directory")

    def test_mod_root_dir(self):
        """Test: jarvis mod root test1"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create module
        self.run_command(['mod', 'create', 'test1'])

        # Get root directory
        result = self.run_command(['mod', 'root', 'test1'])

        # Verify result
        if result.get('success'):
            self.assertTrue((self.mods_dir / 'packages' / 'test1').exists())

        print("Successfully retrieved root directory")

    def test_mod_tcl_path(self):
        """Test: jarvis mod tcl test1"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create module
        self.run_command(['mod', 'create', 'test1'])

        # Get TCL path
        result = self.run_command(['mod', 'tcl', 'test1'])

        # Verify file exists
        tcl_file = self.mods_dir / 'modules' / 'test1'
        self.assertTrue(tcl_file.exists())

        print("Successfully retrieved TCL path")

    def test_mod_yaml_path(self):
        """Test: jarvis mod yaml test1"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create module
        self.run_command(['mod', 'create', 'test1'])

        # Get YAML path
        result = self.run_command(['mod', 'yaml', 'test1'])

        # Verify file exists
        yaml_file = self.mods_dir / 'modules' / 'test1.yaml'
        self.assertTrue(yaml_file.exists())

        print("Successfully retrieved YAML path")

    def test_mod_dir(self):
        """Test: jarvis mod dir"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Get modules directory
        result = self.run_command(['mod', 'dir'])

        # Verify directory exists
        self.assertTrue(self.mods_dir.exists())

        print("Successfully retrieved modules directory")

    def test_mod_dep_add(self):
        """Test: jarvis mod dep add test_dep_mod test1"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create modules
        self.run_command(['mod', 'create', 'test1'])
        self.run_command(['mod', 'create', 'test_dep_mod'])

        # Add dependency (dep_name first, then mod_name)
        result = self.run_command(['mod', 'dep', 'add', 'test_dep_mod', 'test1'])

        # Verify YAML was updated
        yaml_file = self.mods_dir / 'modules' / 'test1.yaml'
        import yaml
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)

        self.assertIn('deps', config)
        self.assertIn('test_dep_mod', config['deps'])
        self.assertTrue(config['deps']['test_dep_mod'])

        # Verify TCL was regenerated with module load
        tcl_file = self.mods_dir / 'modules' / 'test1'
        with open(tcl_file, 'r') as f:
            tcl_content = f.read()

        self.assertIn('module load test_dep_mod', tcl_content)

        print("Successfully added dependency test_dep_mod to test1")

    def test_mod_dep_remove(self):
        """Test: jarvis mod dep remove test_dep_mod test1"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create modules
        self.run_command(['mod', 'create', 'test1'])
        self.run_command(['mod', 'create', 'test_dep_mod'])

        # Add dependency first (dep_name first, then mod_name)
        self.run_command(['mod', 'dep', 'add', 'test_dep_mod', 'test1'])

        # Verify it was added
        yaml_file = self.mods_dir / 'modules' / 'test1.yaml'
        import yaml
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)
        self.assertIn('test_dep_mod', config['deps'])

        # Remove dependency (dep_name first, then mod_name)
        result = self.run_command(['mod', 'dep', 'remove', 'test_dep_mod', 'test1'])

        # Verify YAML was updated
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)

        self.assertNotIn('test_dep_mod', config['deps'])

        # Verify TCL was regenerated without module load
        tcl_file = self.mods_dir / 'modules' / 'test1'
        with open(tcl_file, 'r') as f:
            tcl_content = f.read()

        self.assertNotIn('module load test_dep_mod', tcl_content)

        print("Successfully removed dependency test_dep_mod from test1")

    def test_mod_prepend_multiple_values(self):
        """Test: jarvis mod prepend with semicolon-separated values"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create module
        self.run_command(['mod', 'create', 'test1'])

        # Prepend multiple paths
        result = self.run_command(['mod', 'prepend', 'test1', 'PATH=/path1;/path2;/path3'])

        # Verify YAML was updated
        yaml_file = self.mods_dir / 'modules' / 'test1.yaml'
        import yaml
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)

        self.assertIn('PATH', config['prepends'])
        self.assertIn('/path1', config['prepends']['PATH'])
        self.assertIn('/path2', config['prepends']['PATH'])
        self.assertIn('/path3', config['prepends']['PATH'])

        print("Successfully prepended multiple paths to test1")

    def test_mod_profile_default(self):
        """Test: jarvis mod profile (default dotenv format)"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Capture stdout to verify profile output
        import sys
        from io import StringIO
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            # Run profile command
            result = self.run_command(['mod', 'profile'])

            # Get output
            output = sys.stdout.getvalue()

            # Verify environment variables are printed
            expected_vars = ['PATH', 'LD_LIBRARY_PATH', 'LIBRARY_PATH',
                           'INCLUDE', 'CPATH', 'PKG_CONFIG_PATH', 'CMAKE_PREFIX_PATH',
                           'JAVA_HOME', 'PYTHONPATH']

            for var in expected_vars:
                self.assertIn(var, output, f"{var} not found in profile output")

            print(f"Profile output contains all expected environment variables")

        finally:
            sys.stdout = old_stdout

    def test_mod_profile_clion(self):
        """Test: jarvis mod profile with clion format"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Capture stdout
        import sys
        from io import StringIO
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            # Run profile command with clion format (use m= not method=)
            result = self.run_command(['mod', 'profile', 'm=clion'])

            # Get output
            output = sys.stdout.getvalue()

            # Verify output is semicolon-separated
            self.assertIn(';', output, "CLion format should be semicolon-separated")
            self.assertIn('PATH=', output)

        finally:
            sys.stdout = old_stdout

    def test_mod_profile_vscode(self):
        """Test: jarvis mod profile with vscode format"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Capture stdout
        import sys
        from io import StringIO
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            # Run profile command with vscode format (use m= not method=)
            result = self.run_command(['mod', 'profile', 'm=vscode'])

            # Get output
            output = sys.stdout.getvalue()

            # Verify output is JSON-like
            self.assertIn('"environment"', output, "VSCode format should have environment key")
            self.assertIn('"PATH"', output)

        finally:
            sys.stdout = old_stdout

    def test_mod_profile_to_file(self):
        """Test: jarvis mod profile path=/tmp/test_profile.env"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create temp file path
        profile_path = self.test_dir / 'test_profile.env'

        # Run profile command with output file
        result = self.run_command(['mod', 'profile', f'path={profile_path}'])

        # Verify file was created
        self.assertTrue(profile_path.exists(), "Profile file was not created")

        # Verify file contents
        with open(profile_path, 'r') as f:
            content = f.read()

        # Check for environment variables
        self.assertIn('PATH=', content)
        self.assertIn('LD_LIBRARY_PATH=', content)

        print(f"Profile written to {profile_path}")

    def test_mod_profile_cmake(self):
        """Test: jarvis mod profile with cmake format to file"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Create temp file path
        profile_path = self.test_dir / 'test_profile.cmake'

        # Run profile command with cmake format (use m= not method=)
        result = self.run_command(['mod', 'profile', f'path={profile_path}', 'm=cmake'])

        # Verify file was created
        self.assertTrue(profile_path.exists(), "CMake profile file was not created")

        # Verify file contents
        with open(profile_path, 'r') as f:
            content = f.read()

        # Check for CMake set commands
        self.assertIn('set(ENV{PATH}', content)
        self.assertIn('set(ENV{LD_LIBRARY_PATH}', content)

        print(f"CMake profile written to {profile_path}")

    def test_mod_build_profile(self):
        """Test: jarvis mod build profile (alternate command)"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Capture stdout
        import sys
        from io import StringIO
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            # Run build profile command with vscode format (which prints to stdout)
            # Note: dotenv without path doesn't print in build_profile (only in build_profile_new)
            result = self.run_command(['mod', 'build', 'profile', '--m=vscode'])

            # Get output
            output = sys.stdout.getvalue()

            # Verify environment variables are printed
            self.assertIn('PATH', output)
            self.assertIn('"environment"', output)

        finally:
            sys.stdout = old_stdout

        print("Build profile command executed successfully")

    def test_mod_import_simple(self):
        """Test: jarvis mod import with simple echo command"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Import module with a simple command that modifies PATH
        # Use a command that sets an environment variable
        result = self.run_command(['mod', 'import', 'test_import', 'export PATH=/custom/test/path:$PATH'])

        # Verify module was created
        yaml_file = self.mods_dir / 'modules' / 'test_import.yaml'
        self.assertTrue(yaml_file.exists(), "Import did not create module YAML")

        # Verify command was stored
        import yaml
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)

        self.assertIn('command', config)
        self.assertEqual(config['command'], 'export PATH=/custom/test/path:$PATH')

        print("Module imported successfully with stored command")

    def test_mod_update(self):
        """Test: jarvis mod update"""
        result = self.run_command(['init', str(self.config_dir), str(self.private_dir), str(self.shared_dir)])
        self.assertTrue(result.get('success'))

        # Import module first
        self.run_command(['mod', 'import', 'test_update', 'export MY_VAR=initial_value'])

        # Verify initial import
        yaml_file = self.mods_dir / 'modules' / 'test_update.yaml'
        self.assertTrue(yaml_file.exists())

        # Update the module (re-runs stored command)
        result = self.run_command(['mod', 'update', 'test_update'])

        # Verify module still exists and has command
        import yaml
        with open(yaml_file, 'r') as f:
            config = yaml.safe_load(f)

        self.assertIn('command', config)
        self.assertEqual(config['command'], 'export MY_VAR=initial_value')

        print("Module updated successfully")


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/core/test_pipeline_hostfile.py`

```python
"""
Test pipeline hostfile functionality.
"""
import pytest
import tempfile
import os
from pathlib import Path
from jarvis_cd.core.pipeline import Pipeline
from jarvis_cd.core.config import Jarvis
from jarvis_cd.util.hostfile import Hostfile


@pytest.fixture
def jarvis_env(tmp_path):
    """Setup Jarvis environment for testing"""
    # Create Jarvis directories
    config_dir = tmp_path / "config"
    private_dir = tmp_path / "private"
    shared_dir = tmp_path / "shared"

    config_dir.mkdir(parents=True, exist_ok=True)
    private_dir.mkdir(parents=True, exist_ok=True)
    shared_dir.mkdir(parents=True, exist_ok=True)

    # Initialize Jarvis
    Jarvis._instance = None  # Reset singleton

    # Get Jarvis singleton and initialize it
    jarvis = Jarvis.get_instance()
    jarvis.initialize(str(config_dir), str(private_dir), str(shared_dir), force=True)

    yield jarvis, tmp_path

    # Cleanup
    Jarvis._instance = None


def test_pipeline_localhost_hostfile(jarvis_env):
    """Test pipeline with localhost hostfile"""
    jarvis, tmp_path = jarvis_env

    # Create a localhost hostfile
    hostfile_path = tmp_path / "localhost_hostfile"
    with open(hostfile_path, 'w') as f:
        f.write("localhost\n")

    # Create pipeline
    pipeline = Pipeline()
    pipeline.create("test_pipeline")

    # Set pipeline hostfile
    pipeline.hostfile = Hostfile(path=str(hostfile_path))
    pipeline.save()

    # Verify hostfile is set
    assert pipeline.hostfile is not None
    assert len(pipeline.hostfile.hosts) == 1
    assert pipeline.hostfile.hosts[0] == "localhost"

    # Load pipeline and verify hostfile persists
    pipeline2 = Pipeline("test_pipeline")
    assert pipeline2.hostfile is not None
    assert len(pipeline2.hostfile.hosts) == 1
    assert pipeline2.hostfile.hosts[0] == "localhost"

    # Test get_hostfile method
    effective_hostfile = pipeline2.get_hostfile()
    assert effective_hostfile is not None
    assert len(effective_hostfile.hosts) == 1
    assert effective_hostfile.hosts[0] == "localhost"


def test_pipeline_hostfile_fallback_to_jarvis(jarvis_env):
    """Test pipeline falls back to jarvis global hostfile"""
    jarvis, tmp_path = jarvis_env

    # Create pipeline without hostfile
    pipeline = Pipeline()
    pipeline.create("test_pipeline2")

    # Verify pipeline hostfile is None
    assert pipeline.hostfile is None

    # But get_hostfile() should return jarvis hostfile (defaults to localhost)
    effective_hostfile = pipeline.get_hostfile()
    assert effective_hostfile is not None
    assert len(effective_hostfile.hosts) >= 1  # At least localhost


def test_pipeline_hostfile_container_path(jarvis_env):
    """Test hostfile path is updated for containerized pipelines"""
    jarvis, tmp_path = jarvis_env

    # Create a hostfile
    hostfile_path = tmp_path / "test_hostfile"
    with open(hostfile_path, 'w') as f:
        f.write("localhost\n")

    # Create containerized pipeline
    pipeline = Pipeline()
    pipeline.create("test_container_pipeline")
    pipeline.container_name = "test_container"
    pipeline.hostfile = Hostfile(path=str(hostfile_path))
    pipeline.save()

    # Load pipeline config
    config_dir = jarvis.get_pipeline_dir("test_container_pipeline")
    config_file = config_dir / "pipeline.yaml"

    import yaml
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)

    # Verify hostfile path is set to container path
    assert config['hostfile'] == "/root/.ppi-jarvis/hostfile"


def test_package_hostfile_fallback(jarvis_env):
    """Test package hostfile falls back to pipeline hostfile"""
    jarvis, tmp_path = jarvis_env

    # Create pipeline with hostfile
    pipeline = Pipeline()
    pipeline.create("test_pkg_pipeline")

    hostfile_path = tmp_path / "pipeline_hostfile"
    with open(hostfile_path, 'w') as f:
        f.write("localhost\n")

    pipeline.hostfile = Hostfile(path=str(hostfile_path))
    pipeline.save()

    # Create a simple package class
    from jarvis_cd.core.pkg import Pkg

    class TestPkg(Pkg):
        pass

    # Create package instance
    pkg = TestPkg(pipeline)
    pkg.config = {}  # Empty config means no package-specific hostfile

    # Package should fall back to pipeline hostfile
    pkg_hostfile = pkg.get_hostfile()
    assert pkg_hostfile is not None
    assert len(pkg_hostfile.hosts) == 1
    assert pkg_hostfile.hosts[0] == "localhost"
```

### `test/unit/core/test_pipeline_index.py`

```python
"""
Tests for pipeline_index.py - Pipeline Index Manager
"""
import unittest
import sys
import os
import tempfile
import shutil
from pathlib import Path

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.core.pipeline_index import PipelineIndexManager
from jarvis_cd.core.config import Jarvis


class TestPipelineIndexManager(unittest.TestCase):
    """Tests for PipelineIndexManager class"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_ppl_index_')
        self.jarvis_root = os.path.join(self.test_dir, '.ppi-jarvis')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')

        os.makedirs(self.jarvis_root, exist_ok=True)
        os.makedirs(self.config_dir, exist_ok=True)
        os.makedirs(self.private_dir, exist_ok=True)
        os.makedirs(self.shared_dir, exist_ok=True)

        # Reset and initialize Jarvis singleton
        Jarvis._instance = None
        self.config = Jarvis(self.jarvis_root)
        self.config.initialize(self.config_dir, self.private_dir, self.shared_dir)
        self.manager = PipelineIndexManager(self.config)

    def tearDown(self):
        """Clean up"""
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def test_parse_index_query_simple(self):
        """Test parsing simple index query"""
        repo, subdirs, script = self.manager.parse_index_query('myrepo.script')
        self.assertEqual(repo, 'myrepo')
        self.assertEqual(subdirs, [])
        self.assertEqual(script, 'script')

    def test_parse_index_query_with_subdirs(self):
        """Test parsing index query with subdirectories"""
        repo, subdirs, script = self.manager.parse_index_query('myrepo.sub1.sub2.script')
        self.assertEqual(repo, 'myrepo')
        self.assertEqual(subdirs, ['sub1', 'sub2'])
        self.assertEqual(script, 'script')

    def test_parse_index_query_invalid_no_dot(self):
        """Test invalid query without dot"""
        with self.assertRaises(ValueError) as context:
            self.manager.parse_index_query('nodotquery')
        self.assertIn('Invalid index query', str(context.exception))

    def test_parse_index_query_invalid_empty(self):
        """Test invalid empty query"""
        with self.assertRaises(ValueError):
            self.manager.parse_index_query('')

    def test_find_repo_path_builtin(self):
        """Test finding builtin repo path"""
        path = self.manager.find_repo_path('builtin')
        self.assertIsNotNone(path)
        self.assertTrue(isinstance(path, Path))

    def test_find_repo_path_nonexistent(self):
        """Test finding non-existent repo"""
        path = self.manager.find_repo_path('nonexistent_repo_xyz')
        self.assertIsNone(path)

    def test_find_pipeline_script_nonexistent_repo(self):
        """Test finding script in non-existent repo"""
        script = self.manager.find_pipeline_script('nonexistent.script')
        self.assertIsNone(script)

    def test_initialization(self):
        """Test PipelineIndexManager initialization"""
        self.assertIsNotNone(self.manager.jarvis_config)
        self.assertEqual(self.manager.jarvis_config, self.config)


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/core/test_pipeline_integration.py`

```python
"""
Integration tests for pipeline operations using example_app and test_interceptor
"""
import unittest
import sys
import os
import tempfile
import shutil
from pathlib import Path

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.core.cli import JarvisCLI


class TestPipelineIntegration(unittest.TestCase):
    """Integration tests for pipeline create, append, configure, and run"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_pipeline_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')

        # Initialize CLI
        self.cli = JarvisCLI()
        self.cli.define_options()

        # Store original environment
        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def run_command(self, args):
        """Helper to run CLI command"""
        try:
            result = self.cli.parse(args)
            return {
                'success': True,
                'result': result,
                'kwargs': self.cli.kwargs.copy() if hasattr(self.cli, 'kwargs') else {},
                'remainder': self.cli.remainder.copy() if hasattr(self.cli, 'remainder') else []
            }
        except SystemExit as e:
            return {
                'success': False,
                'exit_code': e.code
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'exception': e
            }

    def test_pipeline_create_append_configure_run(self):
        """Test: jarvis ppl create test, jarvis ppl append example_app, jarvis pkg conf example_app, jarvis ppl run"""
        # Step 1: Initialize Jarvis
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Step 2: Create pipeline
        result = self.run_command(['ppl', 'create', 'test'])
        if result.get('success'):
            self.assertEqual(result['kwargs'].get('pipeline_name'), 'test')
        else:
            print(f"Pipeline create result: {result}")

        # Step 3: Append example_app to pipeline
        result = self.run_command(['ppl', 'append', 'example_app'])
        if result.get('success'):
            self.assertEqual(result['kwargs'].get('package_spec'), 'example_app')
        else:
            print(f"Pipeline append result: {result}")

        # Step 4: Configure example_app package
        result = self.run_command(['pkg', 'configure', 'example_app'])
        self.assertIsNotNone(result)

        # Verify configure marker was created
        configure_marker = os.path.join(self.shared_dir, 'test', 'example_app', 'configure.marker')
        self.assertTrue(os.path.exists(configure_marker), f"Configure marker not found at {configure_marker}")
        print(f"Verified configure marker exists: {configure_marker}")

        # Step 5: Start the pipeline
        result = self.run_command(['ppl', 'start'])
        self.assertIsNotNone(result)
        print("Pipeline start command executed")

        # Verify start marker was created
        start_marker = os.path.join(self.shared_dir, 'test', 'example_app', 'start.marker')
        self.assertTrue(os.path.exists(start_marker), f"Start marker not found at {start_marker}")
        print(f"Verified start marker exists: {start_marker}")

        # Step 6: Check pipeline status
        result = self.run_command(['ppl', 'status'])
        self.assertIsNotNone(result)
        print("Pipeline status command executed")

        # Step 7: Run the pipeline
        result = self.run_command(['ppl', 'run'])
        self.assertIsNotNone(result)
        print("Pipeline run command executed")

        # Step 8: Stop the pipeline
        result = self.run_command(['ppl', 'stop'])
        self.assertIsNotNone(result)
        print("Pipeline stop command executed")

        # Verify stop marker was created
        stop_marker = os.path.join(self.shared_dir, 'test', 'example_app', 'stop.marker')
        self.assertTrue(os.path.exists(stop_marker), f"Stop marker not found at {stop_marker}")
        print(f"Verified stop marker exists: {stop_marker}")

        # Step 9: Kill the pipeline
        result = self.run_command(['ppl', 'kill'])
        self.assertIsNotNone(result)
        print("Pipeline kill command executed")

        # Verify kill marker was created
        kill_marker = os.path.join(self.shared_dir, 'test', 'example_app', 'kill.marker')
        self.assertTrue(os.path.exists(kill_marker), f"Kill marker not found at {kill_marker}")
        print(f"Verified kill marker exists: {kill_marker}")

        # Step 10: Clean the pipeline
        result = self.run_command(['ppl', 'clean'])
        self.assertIsNotNone(result)
        print("Pipeline clean command executed")

        # Verify all markers were removed by clean
        self.assertFalse(os.path.exists(configure_marker), f"Configure marker should be removed after clean")
        self.assertFalse(os.path.exists(start_marker), f"Start marker should be removed after clean")
        self.assertFalse(os.path.exists(stop_marker), f"Stop marker should be removed after clean")
        self.assertFalse(os.path.exists(kill_marker), f"Kill marker should be removed after clean")
        print("Verified all markers were removed by clean")

        # Step 11: Destroy the pipeline
        result = self.run_command(['ppl', 'destroy', 'test'])
        if result.get('success'):
            self.assertEqual(result['kwargs'].get('pipeline_name'), 'test')
        self.assertIsNotNone(result)
        print("Pipeline destroy command executed")

        print("Pipeline full lifecycle test completed with marker verification")


class TestPipelineLoadYAML(unittest.TestCase):
    """Test loading pipeline from YAML file"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_yaml_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')

        self.cli = JarvisCLI()
        self.cli.define_options()

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def run_command(self, args):
        """Helper to run CLI command"""
        try:
            result = self.cli.parse(args)
            return {
                'success': True,
                'result': result,
                'kwargs': self.cli.kwargs.copy() if hasattr(self.cli, 'kwargs') else {},
                'remainder': self.cli.remainder.copy() if hasattr(self.cli, 'remainder') else []
            }
        except SystemExit as e:
            return {
                'success': False,
                'exit_code': e.code
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'exception': e
            }

    def test_pipeline_load_yaml(self):
        """Test: jarvis ppl load builtin/pipelines/unit_tests/test_interceptor.yaml"""
        # Initialize Jarvis
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Find the YAML file
        yaml_path = os.path.join(os.getcwd(), 'builtin', 'pipelines', 'unit_tests', 'test_interceptor.yaml')

        if not os.path.exists(yaml_path):
            # Try alternative paths
            yaml_path = 'builtin/pipelines/unit_tests/test_interceptor.yaml'

        # Load pipeline from YAML
        result = self.run_command(['ppl', 'load', yaml_path])

        if result.get('success'):
            self.assertIn('pipeline_path', result['kwargs'])
            self.assertIn('test_interceptor', result['kwargs']['pipeline_path'])
        else:
            # YAML loading may fail in test environment, but parsing should work
            print(f"Pipeline load YAML result: {result}")

        # Verify command was parsed correctly
        self.assertIsNotNone(result)
        print("Pipeline load YAML test completed")


class TestPipelineIndexLoad(unittest.TestCase):
    """Test loading pipeline from index"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_index_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')

        self.cli = JarvisCLI()
        self.cli.define_options()

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def run_command(self, args):
        """Helper to run CLI command"""
        try:
            result = self.cli.parse(args)
            return {
                'success': True,
                'result': result,
                'kwargs': self.cli.kwargs.copy() if hasattr(self.cli, 'kwargs') else {},
                'remainder': self.cli.remainder.copy() if hasattr(self.cli, 'remainder') else []
            }
        except SystemExit as e:
            return {
                'success': False,
                'exit_code': e.code
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'exception': e
            }

    def test_pipeline_index_load(self):
        """Test: jarvis ppl index load builtin.unit_tests.test_interceptor"""
        # Initialize Jarvis
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Load pipeline from index using dotted notation
        result = self.run_command(['ppl', 'index', 'load', 'builtin.unit_tests.test_interceptor'])

        if result.get('success'):
            self.assertIn('index_query', result['kwargs'])
            self.assertEqual(result['kwargs']['index_query'], 'builtin.unit_tests.test_interceptor')
        else:
            # May fail if pipeline index not set up, but parsing should work
            print(f"Pipeline index load result: {result}")

        # Verify command was parsed
        self.assertIsNotNone(result)
        print("Pipeline index load test completed")


class TestPipelineIndexCopy(unittest.TestCase):
    """Test copying pipeline from index"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_copy_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')

        self.cli = JarvisCLI()
        self.cli.define_options()

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def run_command(self, args):
        """Helper to run CLI command"""
        try:
            result = self.cli.parse(args)
            return {
                'success': True,
                'result': result,
                'kwargs': self.cli.kwargs.copy() if hasattr(self.cli, 'kwargs') else {},
                'remainder': self.cli.remainder.copy() if hasattr(self.cli, 'remainder') else []
            }
        except SystemExit as e:
            return {
                'success': False,
                'exit_code': e.code
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'exception': e
            }

    def test_pipeline_index_copy(self):
        """Test: jarvis ppl index copy builtin.unit_tests.test_interceptor"""
        # Initialize Jarvis
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Copy pipeline from index
        result = self.run_command(['ppl', 'index', 'copy', 'builtin.unit_tests.test_interceptor'])

        if result.get('success'):
            self.assertIn('index_query', result['kwargs'])
            self.assertEqual(result['kwargs']['index_query'], 'builtin.unit_tests.test_interceptor')
        else:
            # May fail if pipeline index not set up, but parsing should work
            print(f"Pipeline index copy result: {result}")

        # Verify command was parsed
        self.assertIsNotNone(result)
        print("Pipeline index copy test completed")


class TestPipelineIndexList(unittest.TestCase):
    """Test listing pipeline indexes"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_list_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')

        self.cli = JarvisCLI()
        self.cli.define_options()

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def run_command(self, args):
        """Helper to run CLI command"""
        try:
            result = self.cli.parse(args)
            return {
                'success': True,
                'result': result,
                'kwargs': self.cli.kwargs.copy() if hasattr(self.cli, 'kwargs') else {},
                'remainder': self.cli.remainder.copy() if hasattr(self.cli, 'remainder') else []
            }
        except SystemExit as e:
            return {
                'success': False,
                'exit_code': e.code
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'exception': e
            }

    def test_pipeline_index_list(self):
        """Test: jarvis ppl index list"""
        # Initialize Jarvis
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # List pipeline indexes
        result = self.run_command(['ppl', 'index', 'list'])

        # Should execute (may not have results in test env)
        self.assertIsNotNone(result)
        print("Pipeline index list test completed")


class TestEnvironmentOperations(unittest.TestCase):
    """Test environment build, copy, and remove operations"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_env_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')

        self.cli = JarvisCLI()
        self.cli.define_options()

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def run_command(self, args):
        """Helper to run CLI command"""
        try:
            result = self.cli.parse(args)
            return {
                'success': True,
                'result': result,
                'kwargs': self.cli.kwargs.copy() if hasattr(self.cli, 'kwargs') else {},
                'remainder': self.cli.remainder.copy() if hasattr(self.cli, 'remainder') else []
            }
        except SystemExit as e:
            return {
                'success': False,
                'exit_code': e.code
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'exception': e
            }

    def test_env_build_simple(self):
        """Test: jarvis env build test"""
        # Initialize Jarvis
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Build environment
        result = self.run_command(['env', 'build', 'test'])

        if result.get('success'):
            self.assertEqual(result['kwargs'].get('env_name'), 'test')
            print("Environment 'test' build command parsed successfully")
        else:
            # May fail if Spack not available, but parsing should work
            print(f"Env build result: {result}")

        self.assertIsNotNone(result)
        print("Environment build test completed")

    def test_env_build_with_variable(self):
        """Test: jarvis env build test X=1024 (verify X was set)"""
        # Initialize Jarvis
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Build environment with variable
        result = self.run_command(['env', 'build', 'test', 'X=1024'])

        if result.get('success'):
            self.assertEqual(result['kwargs'].get('env_name'), 'test')
            # X=1024 should be in remainder args since env build uses keep_remainder
            if result.get('remainder'):
                self.assertIn('X=1024', result['remainder'])
                print("Environment variable X=1024 captured in remainder")
        else:
            print(f"Env build with var result: {result}")

        self.assertIsNotNone(result)
        print("Environment build with variable test completed")

    def test_env_build_with_multiple_variables(self):
        """Test: jarvis env build test with multiple variables"""
        # Initialize Jarvis
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Build environment with multiple variables
        result = self.run_command(['env', 'build', 'test_multi', 'X=1024', 'Y=2048', 'DEBUG=true'])

        if result.get('success'):
            self.assertEqual(result['kwargs'].get('env_name'), 'test_multi')
            if result.get('remainder'):
                self.assertIn('X=1024', result['remainder'])
                self.assertIn('Y=2048', result['remainder'])
                self.assertIn('DEBUG=true', result['remainder'])
                print("Multiple environment variables captured")

        self.assertIsNotNone(result)
        print("Environment build with multiple variables test completed")

    def test_ppl_env_copy(self):
        """Test: jarvis ppl env copy test (requires pipeline to exist)"""
        # Initialize Jarvis
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Create a pipeline first
        result = self.run_command(['ppl', 'create', 'test_pipeline'])
        if not result.get('success'):
            print(f"Pipeline creation for env copy test: {result}")

        # Copy pipeline environment
        result = self.run_command(['ppl', 'env', 'copy', 'test_env_copy'])

        if result.get('success'):
            self.assertEqual(result['kwargs'].get('new_env_name'), 'test_env_copy')
            print("Pipeline environment copy command parsed successfully")
        else:
            print(f"Ppl env copy result: {result}")

        self.assertIsNotNone(result)
        print("Pipeline environment copy test completed")

    def test_env_list(self):
        """Test: jarvis env list"""
        # Initialize Jarvis
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # List environments
        result = self.run_command(['env', 'list'])

        # Should execute without error
        self.assertIsNotNone(result)
        print("Environment list test completed")

    def test_env_show(self):
        """Test: jarvis env show test"""
        # Initialize Jarvis
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Show environment details
        result = self.run_command(['env', 'show', 'test'])

        if result.get('success'):
            self.assertEqual(result['kwargs'].get('env_name'), 'test')

        self.assertIsNotNone(result)
        print("Environment show test completed")

    def test_env_build_and_list(self):
        """Test: jarvis env build + env list workflow"""
        # Initialize Jarvis
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Build multiple environments
        result1 = self.run_command(['env', 'build', 'env1', 'VAR1=100'])
        result2 = self.run_command(['env', 'build', 'env2', 'VAR2=200'])

        print(f"Env1 build: {result1.get('success')}")
        print(f"Env2 build: {result2.get('success')}")

        # List all environments
        result = self.run_command(['env', 'list'])
        self.assertIsNotNone(result)
        print("Environment build and list workflow completed")


class TestPipelineEnvironmentIntegration(unittest.TestCase):
    """Test pipeline environment integration workflows"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_ppl_env_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')

        self.cli = JarvisCLI()
        self.cli.define_options()

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def run_command(self, args):
        """Helper to run CLI command"""
        try:
            result = self.cli.parse(args)
            return {
                'success': True,
                'result': result,
                'kwargs': self.cli.kwargs.copy() if hasattr(self.cli, 'kwargs') else {},
                'remainder': self.cli.remainder.copy() if hasattr(self.cli, 'remainder') else []
            }
        except SystemExit as e:
            return {
                'success': False,
                'exit_code': e.code
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'exception': e
            }

    def test_pipeline_with_environment_workflow(self):
        """Test complete pipeline + environment workflow"""
        # Initialize Jarvis
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Create pipeline
        result = self.run_command(['ppl', 'create', 'env_test_pipeline'])
        print(f"Pipeline created: {result.get('success')}")

        # Append example_app
        result = self.run_command(['ppl', 'append', 'example_app'])
        print(f"Package appended: {result.get('success')}")

        # Build pipeline environment
        result = self.run_command(['ppl', 'env', 'build'])
        self.assertIsNotNone(result)
        print("Pipeline env build executed")

        # Show pipeline environment
        result = self.run_command(['ppl', 'env', 'show'])
        self.assertIsNotNone(result)
        print("Pipeline env show executed")

        # Copy pipeline environment to new name
        result = self.run_command(['ppl', 'env', 'copy', 'copied_env'])
        if result.get('success'):
            self.assertEqual(result['kwargs'].get('new_env_name'), 'copied_env')
        print("Pipeline env copy executed")

        # Don't destroy pipeline - leave it for env copy test
        print("Pipeline + environment workflow test completed")


class TestPackageLifecycle(unittest.TestCase):
    """Test package lifecycle methods through pipeline operations"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_lifecycle_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')

        # Initialize CLI
        self.cli = JarvisCLI()
        self.cli.define_options()

        # Store original environment
        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def run_command(self, args):
        """Helper to run CLI command"""
        try:
            result = self.cli.parse(args)
            return {
                'success': True,
                'result': result,
                'kwargs': self.cli.kwargs.copy() if hasattr(self.cli, 'kwargs') else {},
                'remainder': self.cli.remainder.copy() if hasattr(self.cli, 'remainder') else []
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'exception': e
            }

    def test_full_package_lifecycle(self):
        """Test complete package lifecycle: create → append → configure → start → run → stop → kill → clean → destroy"""
        # Initialize
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'), f"Init failed: {result}")

        # Create pipeline
        result = self.run_command(['ppl', 'create', 'lifecycle_test'])
        self.assertTrue(result.get('success') or result.get('kwargs', {}).get('pipeline_name') == 'lifecycle_test')

        # Append example_app (Application type)
        result = self.run_command(['ppl', 'append', 'example_app'])
        self.assertTrue(result.get('success') or result.get('kwargs', {}).get('package_spec') == 'example_app')

        # Configure the package (tests pkg.configure())
        result = self.run_command(['pkg', 'configure', 'example_app'])
        self.assertIsNotNone(result)
        print("Package configured")

        # Start pipeline (tests pkg.start() for all packages)
        result = self.run_command(['ppl', 'start'])
        self.assertIsNotNone(result)
        print("Pipeline started - pkg.start() called")

        # Run pipeline (tests pkg.start() again for Applications)
        result = self.run_command(['ppl', 'run'])
        self.assertIsNotNone(result)
        print("Pipeline run - pkg.start() called for apps")

        # Stop pipeline (tests pkg.stop())
        result = self.run_command(['ppl', 'stop'])
        self.assertIsNotNone(result)
        print("Pipeline stopped - pkg.stop() called")

        # Start again to test multiple start/stop cycles
        result = self.run_command(['ppl', 'start'])
        self.assertIsNotNone(result)
        print("Pipeline restarted")

        # Kill pipeline (tests pkg.kill())
        result = self.run_command(['ppl', 'kill'])
        self.assertIsNotNone(result)
        print("Pipeline killed - pkg.kill() called")

        # Clean pipeline (tests pkg.clean())
        result = self.run_command(['ppl', 'clean'])
        self.assertIsNotNone(result)
        print("Pipeline cleaned - pkg.clean() called")

        # Destroy pipeline
        result = self.run_command(['ppl', 'destroy', 'lifecycle_test'])
        if result.get('success'):
            self.assertEqual(result['kwargs'].get('pipeline_name'), 'lifecycle_test')
        print("Pipeline destroyed")

    def test_pipeline_with_interceptor(self):
        """Test pipeline with interceptor package (tests interceptor.modify_env())"""
        # Initialize
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'))

        # Create pipeline
        result = self.run_command(['ppl', 'create', 'interceptor_test'])
        self.assertTrue(result.get('success') or result.get('kwargs', {}).get('pipeline_name') == 'interceptor_test')

        # Append example_app
        result = self.run_command(['ppl', 'append', 'example_app'])
        self.assertIsNotNone(result)

        # Append example_interceptor
        result = self.run_command(['ppl', 'append', 'example_interceptor'])
        self.assertIsNotNone(result)

        # Configure packages
        result = self.run_command(['pkg', 'configure', 'example_app'])
        self.assertIsNotNone(result)

        result = self.run_command(['pkg', 'configure', 'example_interceptor'])
        self.assertIsNotNone(result)
        print("Interceptor configured")

        # Start pipeline (should call interceptor.modify_env())
        result = self.run_command(['ppl', 'start'])
        self.assertIsNotNone(result)
        print("Pipeline with interceptor started - modify_env() called")

        # Run pipeline
        result = self.run_command(['ppl', 'run'])
        self.assertIsNotNone(result)

        # Clean up
        result = self.run_command(['ppl', 'clean'])
        self.assertIsNotNone(result)

        result = self.run_command(['ppl', 'destroy', 'interceptor_test'])
        self.assertIsNotNone(result)

    def test_package_status(self):
        """Test package status command"""
        # Initialize
        result = self.run_command(['init', self.config_dir, self.private_dir, self.shared_dir])
        self.assertTrue(result.get('success'))

        # Create pipeline with package
        result = self.run_command(['ppl', 'create', 'status_test'])
        self.assertTrue(result.get('success') or result.get('kwargs', {}).get('pipeline_name') == 'status_test')

        result = self.run_command(['ppl', 'append', 'example_app'])
        self.assertIsNotNone(result)

        result = self.run_command(['pkg', 'configure', 'example_app'])
        self.assertIsNotNone(result)

        # Check status before start
        result = self.run_command(['ppl', 'status'])
        self.assertIsNotNone(result)
        print("Status checked before start")

        # Start and check status again
        result = self.run_command(['ppl', 'start'])
        self.assertIsNotNone(result)

        result = self.run_command(['ppl', 'status'])
        self.assertIsNotNone(result)
        print("Status checked after start")

        # Clean up
        result = self.run_command(['ppl', 'destroy', 'status_test'])
        self.assertIsNotNone(result)


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/core/test_pkg_methods.py`

```python
"""
Comprehensive unit tests for jarvis_cd/core/pkg.py
Focuses on methods with low test coverage to improve overall coverage from 34% to 70%+
"""
import unittest
import sys
import os
import tempfile
import shutil
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.core.pkg import Pkg, Application, Service, Interceptor
from jarvis_cd.core.config import Jarvis


def initialize_jarvis_for_test(config_dir, private_dir, shared_dir):
    """Helper function to properly initialize Jarvis for testing"""
    # Get Jarvis singleton and initialize it
    jarvis = Jarvis.get_instance()
    jarvis.initialize(config_dir, private_dir, shared_dir, force=True)

    return jarvis


class TestPkgLoadStandalone(unittest.TestCase):
    """Test the load_standalone() class method"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_standalone_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')
        os.makedirs(self.config_dir, exist_ok=True)
        os.makedirs(self.private_dir, exist_ok=True)
        os.makedirs(self.shared_dir, exist_ok=True)

        # Initialize Jarvis config
        os.environ['JARVIS_CONFIG'] = self.config_dir
        os.environ['JARVIS_PRIVATE'] = self.private_dir
        os.environ['JARVIS_SHARED'] = self.shared_dir

        # Initialize Jarvis properly
        initialize_jarvis_for_test(self.config_dir, self.private_dir, self.shared_dir)

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

        # Reset Jarvis singleton
        if hasattr(Jarvis, '_instance'):
            Jarvis._instance = None

    def test_load_standalone_with_full_spec(self):
        """Test load_standalone() with full package specification (repo.pkg)"""
        # Load example_app with full specification
        pkg = Pkg.load_standalone('builtin.example_app')

        self.assertIsNotNone(pkg)
        self.assertEqual(pkg.pkg_id, 'example_app')
        self.assertEqual(pkg.global_id, 'standalone.example_app')
        self.assertIsNotNone(pkg.pipeline)
        self.assertEqual(pkg.pipeline.name, 'standalone')
        self.assertIsNotNone(pkg.config_dir)
        self.assertIsNotNone(pkg.shared_dir)
        self.assertIsNotNone(pkg.private_dir)

    def test_load_standalone_with_package_name_only(self):
        """Test load_standalone() with just package name (searches repos)"""
        # This should find example_app in builtin repo
        pkg = Pkg.load_standalone('example_app')

        self.assertIsNotNone(pkg)
        self.assertEqual(pkg.pkg_id, 'example_app')
        self.assertEqual(pkg.global_id, 'standalone.example_app')

    def test_load_standalone_nonexistent_package(self):
        """Test load_standalone() with non-existent package"""
        with self.assertRaises(ValueError) as context:
            Pkg.load_standalone('nonexistent.package')

        self.assertIn('Repository not found', str(context.exception))

    def test_load_standalone_invalid_package_name(self):
        """Test load_standalone() with invalid package name"""
        with self.assertRaises(ValueError) as context:
            Pkg.load_standalone('completely_nonexistent_pkg')

        self.assertIn('Package not found', str(context.exception))

    def test_load_standalone_interceptor(self):
        """Test load_standalone() with interceptor package"""
        pkg = Pkg.load_standalone('builtin.example_interceptor')

        self.assertIsNotNone(pkg)
        self.assertIsInstance(pkg, Interceptor)
        self.assertEqual(pkg.pkg_id, 'example_interceptor')


class TestPkgEnvironmentMethods(unittest.TestCase):
    """Test environment manipulation methods"""

    def setUp(self):
        """Set up test environment with mock pipeline"""
        self.mock_pipeline = Mock()
        self.mock_pipeline.name = 'test_pipeline'

        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_env_')
        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')
        os.makedirs(self.config_dir, exist_ok=True)
        os.makedirs(self.private_dir, exist_ok=True)
        os.makedirs(self.shared_dir, exist_ok=True)

        os.environ['JARVIS_CONFIG'] = self.config_dir
        os.environ['JARVIS_PRIVATE'] = self.private_dir
        os.environ['JARVIS_SHARED'] = self.shared_dir

        # Initialize Jarvis properly
        initialize_jarvis_for_test(self.config_dir, self.private_dir, self.shared_dir)

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

        if hasattr(Jarvis, '_instance'):
            Jarvis._instance = None

    def test_track_env_basic(self):
        """Test track_env() with basic environment variables"""
        pkg = Pkg(pipeline=self.mock_pipeline)

        env_dict = {
            'PATH': '/usr/bin:/bin',
            'MY_VAR': 'test_value',
            'ANOTHER_VAR': 'another_value'
        }

        pkg.track_env(env_dict)

        # Check that variables were added to env
        self.assertEqual(pkg.env['PATH'], '/usr/bin:/bin')
        self.assertEqual(pkg.env['MY_VAR'], 'test_value')
        self.assertEqual(pkg.env['ANOTHER_VAR'], 'another_value')

        # Check that mod_env is a copy of env
        self.assertEqual(pkg.mod_env['PATH'], '/usr/bin:/bin')
        self.assertEqual(pkg.mod_env['MY_VAR'], 'test_value')

    def test_track_env_with_ld_preload(self):
        """Test track_env() with LD_PRELOAD (should only go to mod_env)"""
        pkg = Pkg(pipeline=self.mock_pipeline)

        env_dict = {
            'PATH': '/usr/bin',
            'LD_PRELOAD': '/lib/interceptor.so'
        }

        pkg.track_env(env_dict)

        # LD_PRELOAD should NOT be in env
        self.assertNotIn('LD_PRELOAD', pkg.env)

        # But should be in mod_env
        self.assertEqual(pkg.mod_env['LD_PRELOAD'], '/lib/interceptor.so')

        # Other vars should be in both
        self.assertEqual(pkg.env['PATH'], '/usr/bin')
        self.assertEqual(pkg.mod_env['PATH'], '/usr/bin')

    def test_prepend_env_regular_variable(self):
        """Test prepend_env() with regular environment variable"""
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.env['PATH'] = '/usr/bin'
        pkg.mod_env['PATH'] = '/usr/bin'

        pkg.prepend_env('PATH', '/custom/bin')

        self.assertEqual(pkg.env['PATH'], '/custom/bin:/usr/bin')
        self.assertEqual(pkg.mod_env['PATH'], '/custom/bin:/usr/bin')

    def test_prepend_env_empty_variable(self):
        """Test prepend_env() when variable doesn't exist yet"""
        pkg = Pkg(pipeline=self.mock_pipeline)

        pkg.prepend_env('NEW_PATH', '/some/path')

        self.assertEqual(pkg.env['NEW_PATH'], '/some/path')
        self.assertEqual(pkg.mod_env['NEW_PATH'], '/some/path')

    def test_prepend_env_ld_preload(self):
        """Test prepend_env() with LD_PRELOAD (should only modify mod_env)"""
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.mod_env['LD_PRELOAD'] = '/lib/existing.so'

        pkg.prepend_env('LD_PRELOAD', '/lib/new.so')

        # LD_PRELOAD should NOT be in env
        self.assertNotIn('LD_PRELOAD', pkg.env)

        # Should be prepended in mod_env
        self.assertEqual(pkg.mod_env['LD_PRELOAD'], '/lib/new.so:/lib/existing.so')

    def test_setenv_regular_variable(self):
        """Test setenv() with regular environment variable"""
        pkg = Pkg(pipeline=self.mock_pipeline)

        pkg.setenv('MY_VAR', 'my_value')

        self.assertEqual(pkg.env['MY_VAR'], 'my_value')
        self.assertEqual(pkg.mod_env['MY_VAR'], 'my_value')

    def test_setenv_ld_preload(self):
        """Test setenv() with LD_PRELOAD (should only set mod_env)"""
        pkg = Pkg(pipeline=self.mock_pipeline)

        pkg.setenv('LD_PRELOAD', '/lib/interceptor.so')

        # LD_PRELOAD should NOT be in env
        self.assertNotIn('LD_PRELOAD', pkg.env)

        # Should be in mod_env
        self.assertEqual(pkg.mod_env['LD_PRELOAD'], '/lib/interceptor.so')

    def test_setenv_overwrites_existing(self):
        """Test setenv() overwrites existing values"""
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.env['VAR'] = 'old_value'
        pkg.mod_env['VAR'] = 'old_value'

        pkg.setenv('VAR', 'new_value')

        self.assertEqual(pkg.env['VAR'], 'new_value')
        self.assertEqual(pkg.mod_env['VAR'], 'new_value')


class TestPkgFindLibrary(unittest.TestCase):
    """Test find_library() method"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_lib_')
        self.lib_dir = os.path.join(self.test_dir, 'lib')
        os.makedirs(self.lib_dir, exist_ok=True)

        self.mock_pipeline = Mock()
        self.mock_pipeline.name = 'test_pipeline'

        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')
        os.makedirs(self.config_dir, exist_ok=True)
        os.makedirs(self.private_dir, exist_ok=True)
        os.makedirs(self.shared_dir, exist_ok=True)

        os.environ['JARVIS_CONFIG'] = self.config_dir
        os.environ['JARVIS_PRIVATE'] = self.private_dir
        os.environ['JARVIS_SHARED'] = self.shared_dir

        # Initialize Jarvis properly
        initialize_jarvis_for_test(self.config_dir, self.private_dir, self.shared_dir)

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

        if hasattr(Jarvis, '_instance'):
            Jarvis._instance = None

    def test_find_library_with_standard_name(self):
        """Test find_library() finds library with standard libXXX.so naming"""
        # Create a test library file
        lib_path = os.path.join(self.lib_dir, 'libtest.so')
        Path(lib_path).touch()

        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.setenv('LD_LIBRARY_PATH', self.lib_dir)

        result = pkg.find_library('test')

        self.assertIsNotNone(result)
        self.assertEqual(result, lib_path)

    def test_find_library_with_so_extension(self):
        """Test find_library() finds library with .so extension"""
        lib_path = os.path.join(self.lib_dir, 'mylib.so')
        Path(lib_path).touch()

        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.setenv('LD_LIBRARY_PATH', self.lib_dir)

        result = pkg.find_library('mylib')

        self.assertIsNotNone(result)
        self.assertEqual(result, lib_path)

    def test_find_library_static_library(self):
        """Test find_library() finds static library (.a)"""
        lib_path = os.path.join(self.lib_dir, 'libstatic.a')
        Path(lib_path).touch()

        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.setenv('LD_LIBRARY_PATH', self.lib_dir)

        result = pkg.find_library('static')

        self.assertIsNotNone(result)
        self.assertEqual(result, lib_path)

    def test_find_library_not_found(self):
        """Test find_library() returns None when library not found"""
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.setenv('LD_LIBRARY_PATH', self.lib_dir)

        result = pkg.find_library('nonexistent')

        self.assertIsNone(result)

    def test_find_library_multiple_paths(self):
        """Test find_library() searches multiple paths in LD_LIBRARY_PATH"""
        lib_dir2 = os.path.join(self.test_dir, 'lib2')
        os.makedirs(lib_dir2, exist_ok=True)

        # Create library in second directory
        lib_path = os.path.join(lib_dir2, 'libfound.so')
        Path(lib_path).touch()

        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.setenv('LD_LIBRARY_PATH', f'{self.lib_dir}:{lib_dir2}')

        result = pkg.find_library('found')

        self.assertIsNotNone(result)
        self.assertEqual(result, lib_path)

    def test_find_library_uses_mod_env(self):
        """Test find_library() checks mod_env for LD_LIBRARY_PATH"""
        lib_path = os.path.join(self.lib_dir, 'libmodenv.so')
        Path(lib_path).touch()

        pkg = Pkg(pipeline=self.mock_pipeline)
        # Set in mod_env directly (simulating LD_PRELOAD scenario)
        pkg.mod_env['LD_LIBRARY_PATH'] = self.lib_dir

        result = pkg.find_library('modenv')

        self.assertIsNotNone(result)


class TestPkgCopyTemplateFile(unittest.TestCase):
    """Test copy_template_file() method"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_template_')
        self.template_dir = os.path.join(self.test_dir, 'templates')
        os.makedirs(self.template_dir, exist_ok=True)

        self.mock_pipeline = Mock()
        self.mock_pipeline.name = 'test_pipeline'

        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')
        os.makedirs(self.config_dir, exist_ok=True)
        os.makedirs(self.private_dir, exist_ok=True)
        os.makedirs(self.shared_dir, exist_ok=True)

        os.environ['JARVIS_CONFIG'] = self.config_dir
        os.environ['JARVIS_PRIVATE'] = self.private_dir
        os.environ['JARVIS_SHARED'] = self.shared_dir

        # Initialize Jarvis properly
        initialize_jarvis_for_test(self.config_dir, self.private_dir, self.shared_dir)

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

        if hasattr(Jarvis, '_instance'):
            Jarvis._instance = None

    def test_copy_template_basic(self):
        """Test copy_template_file() with basic template"""
        # Create template file
        template_path = os.path.join(self.template_dir, 'test.txt')
        with open(template_path, 'w') as f:
            f.write('Hello World')

        dest_path = os.path.join(self.test_dir, 'output.txt')

        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.copy_template_file(template_path, dest_path)

        # Verify file was copied
        self.assertTrue(os.path.exists(dest_path))
        with open(dest_path, 'r') as f:
            content = f.read()
        self.assertEqual(content, 'Hello World')

    def test_copy_template_with_replacements(self):
        """Test copy_template_file() with template replacements"""
        template_path = os.path.join(self.template_dir, 'config.xml')
        with open(template_path, 'w') as f:
            f.write('<config>\n  <host>##HOST##</host>\n  <port>##PORT##</port>\n</config>')

        dest_path = os.path.join(self.test_dir, 'config_output.xml')

        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.copy_template_file(
            template_path,
            dest_path,
            replacements={'HOST': 'localhost', 'PORT': '8080'}
        )

        # Verify replacements were made
        with open(dest_path, 'r') as f:
            content = f.read()

        self.assertIn('<host>localhost</host>', content)
        self.assertIn('<port>8080</port>', content)
        self.assertNotIn('##HOST##', content)
        self.assertNotIn('##PORT##', content)

    def test_copy_template_creates_dest_directory(self):
        """Test copy_template_file() creates destination directory if needed"""
        template_path = os.path.join(self.template_dir, 'test.txt')
        with open(template_path, 'w') as f:
            f.write('Test content')

        # Destination in non-existent directory
        dest_path = os.path.join(self.test_dir, 'subdir', 'deep', 'output.txt')

        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.copy_template_file(template_path, dest_path)

        # Verify file was created and directories were made
        self.assertTrue(os.path.exists(dest_path))

    def test_copy_template_with_numeric_replacements(self):
        """Test copy_template_file() with numeric replacement values"""
        template_path = os.path.join(self.template_dir, 'numeric.txt')
        with open(template_path, 'w') as f:
            f.write('Threads: ##THREADS##, Memory: ##MEMORY##MB')

        dest_path = os.path.join(self.test_dir, 'numeric_output.txt')

        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.copy_template_file(
            template_path,
            dest_path,
            replacements={'THREADS': 16, 'MEMORY': 4096}
        )

        with open(dest_path, 'r') as f:
            content = f.read()

        self.assertEqual(content, 'Threads: 16, Memory: 4096MB')

    def test_copy_template_file_not_found(self):
        """Test copy_template_file() raises error when template not found"""
        pkg = Pkg(pipeline=self.mock_pipeline)

        with self.assertRaises(FileNotFoundError):
            pkg.copy_template_file(
                '/nonexistent/template.txt',
                os.path.join(self.test_dir, 'output.txt')
            )


class TestPkgDisplayMethods(unittest.TestCase):
    """Test show_readme() and show_paths() methods"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_display_')
        self.mock_pipeline = Mock()
        self.mock_pipeline.name = 'test_pipeline'

        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')
        os.makedirs(self.config_dir, exist_ok=True)
        os.makedirs(self.private_dir, exist_ok=True)
        os.makedirs(self.shared_dir, exist_ok=True)

        os.environ['JARVIS_CONFIG'] = self.config_dir
        os.environ['JARVIS_PRIVATE'] = self.private_dir
        os.environ['JARVIS_SHARED'] = self.shared_dir

        # Initialize Jarvis properly
        initialize_jarvis_for_test(self.config_dir, self.private_dir, self.shared_dir)

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

        if hasattr(Jarvis, '_instance'):
            Jarvis._instance = None

    def test_show_readme_exists(self):
        """Test show_readme() when README.md exists"""
        # Use the actual example_app package which has pkg_dir set
        pkg = Pkg.load_standalone('builtin.example_app')

        # Create README in pkg_dir
        readme_path = os.path.join(pkg.pkg_dir, 'README.md')
        with open(readme_path, 'w') as f:
            f.write('# Example App\n\nThis is a test README.')

        # Capture output
        import io
        from contextlib import redirect_stdout

        f = io.StringIO()
        with redirect_stdout(f):
            pkg.show_readme()

        output = f.getvalue()
        self.assertIn('Example App', output)
        self.assertIn('test README', output)

        # Clean up
        os.remove(readme_path)

    def test_show_readme_not_exists(self):
        """Test show_readme() when README.md doesn't exist"""
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.pkg_dir = self.test_dir

        import io
        from contextlib import redirect_stdout

        f = io.StringIO()
        with redirect_stdout(f):
            pkg.show_readme()

        output = f.getvalue()
        self.assertIn('No README found', output)

    def test_show_readme_no_pkg_dir(self):
        """Test show_readme() when pkg_dir is not set"""
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.pkg_dir = None

        import io
        from contextlib import redirect_stdout

        f = io.StringIO()
        with redirect_stdout(f):
            pkg.show_readme()

        output = f.getvalue()
        self.assertIn('Package directory not set', output)

    def test_show_paths_config(self):
        """Test show_paths() with conf flag"""
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.pkg_id = 'test_pkg'
        pkg._ensure_directories()

        import io
        from contextlib import redirect_stdout

        f = io.StringIO()
        with redirect_stdout(f):
            pkg.show_paths({'conf': True})

        output = f.getvalue()
        self.assertIn('config.yaml', output)

    def test_show_paths_multiple_flags(self):
        """Test show_paths() with multiple flags"""
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.pkg_id = 'test_pkg'
        pkg._ensure_directories()

        import io
        from contextlib import redirect_stdout

        f = io.StringIO()
        with redirect_stdout(f):
            pkg.show_paths({
                'conf_dir': True,
                'shared_dir': True,
                'priv_dir': True
            })

        output = f.getvalue()
        lines = output.strip().split('\n')

        # Should output 3 paths
        self.assertEqual(len(lines), 3)
        self.assertTrue(any('config' in line for line in lines))
        self.assertTrue(any('shared' in line for line in lines))
        self.assertTrue(any('private' in line for line in lines))

    def test_show_paths_pkg_dir(self):
        """Test show_paths() with pkg_dir flag"""
        pkg = Pkg.load_standalone('builtin.example_app')

        import io
        from contextlib import redirect_stdout

        f = io.StringIO()
        with redirect_stdout(f):
            pkg.show_paths({'pkg_dir': True})

        output = f.getvalue().strip()
        self.assertTrue(os.path.exists(output))
        self.assertIn('example_app', output)


class TestPkgConfigurationMethods(unittest.TestCase):
    """Test configuration-related methods"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_config_')
        self.mock_pipeline = Mock()
        self.mock_pipeline.name = 'test_pipeline'

        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')
        os.makedirs(self.config_dir, exist_ok=True)
        os.makedirs(self.private_dir, exist_ok=True)
        os.makedirs(self.shared_dir, exist_ok=True)

        os.environ['JARVIS_CONFIG'] = self.config_dir
        os.environ['JARVIS_PRIVATE'] = self.private_dir
        os.environ['JARVIS_SHARED'] = self.shared_dir

        # Initialize Jarvis properly
        initialize_jarvis_for_test(self.config_dir, self.private_dir, self.shared_dir)

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

        if hasattr(Jarvis, '_instance'):
            Jarvis._instance = None

    def test_apply_menu_defaults(self):
        """Test _apply_menu_defaults() applies default values from menu"""
        # Create a custom package class with menu
        class TestPkg(Pkg):
            def _configure_menu(self):
                return [
                    {'name': 'option1', 'default': 'default1'},
                    {'name': 'option2', 'default': 42},
                    {'name': 'option3', 'default': True}
                ]

        pkg = TestPkg(pipeline=self.mock_pipeline)
        pkg._apply_menu_defaults()

        self.assertEqual(pkg.config['option1'], 'default1')
        self.assertEqual(pkg.config['option2'], 42)
        self.assertEqual(pkg.config['option3'], True)

    def test_apply_menu_defaults_doesnt_overwrite(self):
        """Test _apply_menu_defaults() doesn't overwrite existing config"""
        class TestPkg(Pkg):
            def _configure_menu(self):
                return [
                    {'name': 'option1', 'default': 'default1'}
                ]

        pkg = TestPkg(pipeline=self.mock_pipeline)
        pkg.config['option1'] = 'existing_value'
        pkg._apply_menu_defaults()

        # Should not overwrite
        self.assertEqual(pkg.config['option1'], 'existing_value')

    def test_update_config(self):
        """Test update_config() method"""
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.config = {'existing': 'value'}

        pkg.update_config({'new_key': 'new_value', 'existing': 'updated'}, rebuild=False)

        self.assertEqual(pkg.config['new_key'], 'new_value')
        self.assertEqual(pkg.config['existing'], 'updated')

    def test_configure_menu_includes_common_params(self):
        """Test configure_menu() includes common parameters"""
        pkg = Pkg(pipeline=self.mock_pipeline)
        menu = pkg.configure_menu()

        # Check for common parameters
        param_names = [item['name'] for item in menu]
        self.assertIn('interceptors', param_names)
        self.assertIn('sleep', param_names)
        self.assertIn('do_dbg', param_names)
        self.assertIn('timeout', param_names)

    def test_get_argparse(self):
        """Test get_argparse() returns PkgArgParse instance"""
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.pkg_id = 'test_pkg'

        argparse = pkg.get_argparse()

        self.assertIsNotNone(argparse)
        self.assertEqual(argparse.pkg_name, 'test_pkg')


class TestPkgUtilityMethods(unittest.TestCase):
    """Test utility methods like log(), sleep(), etc."""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_util_')
        self.mock_pipeline = Mock()
        self.mock_pipeline.name = 'test_pipeline'

        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')
        os.makedirs(self.config_dir, exist_ok=True)
        os.makedirs(self.private_dir, exist_ok=True)
        os.makedirs(self.shared_dir, exist_ok=True)

        os.environ['JARVIS_CONFIG'] = self.config_dir
        os.environ['JARVIS_PRIVATE'] = self.private_dir
        os.environ['JARVIS_SHARED'] = self.shared_dir

        # Initialize Jarvis properly
        initialize_jarvis_for_test(self.config_dir, self.private_dir, self.shared_dir)

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

        if hasattr(Jarvis, '_instance'):
            Jarvis._instance = None

    def test_sleep_with_config(self):
        """Test sleep() uses config value"""
        import time
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.config['sleep'] = 0.1

        start = time.time()
        pkg.sleep()
        elapsed = time.time() - start

        self.assertGreaterEqual(elapsed, 0.1)
        self.assertLess(elapsed, 0.2)  # Should not take much longer

    def test_sleep_with_parameter(self):
        """Test sleep() with explicit parameter overrides config"""
        import time
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.config['sleep'] = 10  # Would take too long

        start = time.time()
        pkg.sleep(time_sec=0.05)
        elapsed = time.time() - start

        self.assertGreaterEqual(elapsed, 0.05)
        self.assertLess(elapsed, 0.2)

    def test_sleep_zero(self):
        """Test sleep(0) completes immediately"""
        import time
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.config['sleep'] = 0

        start = time.time()
        pkg.sleep()
        elapsed = time.time() - start

        self.assertLess(elapsed, 0.01)

    def test_ensure_directories_creates_dirs(self):
        """Test _ensure_directories() creates all necessary directories"""
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.pkg_id = 'test_pkg'
        pkg._ensure_directories()

        self.assertTrue(os.path.exists(pkg.config_dir))
        self.assertTrue(os.path.exists(pkg.shared_dir))
        self.assertTrue(os.path.exists(pkg.private_dir))

    def test_ensure_directories_idempotent(self):
        """Test _ensure_directories() can be called multiple times safely"""
        pkg = Pkg(pipeline=self.mock_pipeline)
        pkg.pkg_id = 'test_pkg'

        pkg._ensure_directories()
        dir1 = pkg.config_dir

        pkg._ensure_directories()
        dir2 = pkg.config_dir

        self.assertEqual(dir1, dir2)


class TestPkgSubclasses(unittest.TestCase):
    """Test Service, Application, and Interceptor subclasses"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_subclass_')
        self.mock_pipeline = Mock()
        self.mock_pipeline.name = 'test_pipeline'

        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')
        os.makedirs(self.config_dir, exist_ok=True)
        os.makedirs(self.private_dir, exist_ok=True)
        os.makedirs(self.shared_dir, exist_ok=True)

        os.environ['JARVIS_CONFIG'] = self.config_dir
        os.environ['JARVIS_PRIVATE'] = self.private_dir
        os.environ['JARVIS_SHARED'] = self.shared_dir

        # Initialize Jarvis properly
        initialize_jarvis_for_test(self.config_dir, self.private_dir, self.shared_dir)

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

        if hasattr(Jarvis, '_instance'):
            Jarvis._instance = None

    def test_service_initialization(self):
        """Test Service class initialization"""
        service = Service(pipeline=self.mock_pipeline)

        self.assertIsNotNone(service)
        self.assertIsInstance(service, Pkg)
        self.assertEqual(service.pipeline, self.mock_pipeline)

    def test_application_initialization(self):
        """Test Application class initialization"""
        app = Application(pipeline=self.mock_pipeline)

        self.assertIsNotNone(app)
        self.assertIsInstance(app, Pkg)
        self.assertEqual(app.pipeline, self.mock_pipeline)

    def test_interceptor_initialization(self):
        """Test Interceptor class initialization"""
        interceptor = Interceptor(pipeline=self.mock_pipeline)

        self.assertIsNotNone(interceptor)
        self.assertIsInstance(interceptor, Pkg)
        self.assertEqual(interceptor.pipeline, self.mock_pipeline)

    def test_interceptor_has_modify_env(self):
        """Test Interceptor has modify_env() method"""
        interceptor = Interceptor(pipeline=self.mock_pipeline)

        self.assertTrue(hasattr(interceptor, 'modify_env'))
        self.assertTrue(callable(interceptor.modify_env))

    def test_example_interceptor_modify_env(self):
        """Test example_interceptor's modify_env() implementation"""
        pkg = Pkg.load_standalone('builtin.example_interceptor')
        pkg.configure(library_path='/lib/test.so', custom_env_var='test123')

        pkg.modify_env()

        # Check environment was modified
        self.assertEqual(pkg.env.get('EXAMPLE_INTERCEPTOR_ACTIVE'), 'true')
        self.assertEqual(pkg.env.get('EXAMPLE_CUSTOM_VAR'), 'test123')
        self.assertIn('/lib/test.so', pkg.mod_env.get('LD_PRELOAD', ''))


class TestPkgLifecycleMethods(unittest.TestCase):
    """Test lifecycle methods (start, stop, kill, clean, status)"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_lifecycle_')
        self.mock_pipeline = Mock()
        self.mock_pipeline.name = 'test_pipeline'

        self.config_dir = os.path.join(self.test_dir, 'config')
        self.private_dir = os.path.join(self.test_dir, 'private')
        self.shared_dir = os.path.join(self.test_dir, 'shared')
        os.makedirs(self.config_dir, exist_ok=True)
        os.makedirs(self.private_dir, exist_ok=True)
        os.makedirs(self.shared_dir, exist_ok=True)

        os.environ['JARVIS_CONFIG'] = self.config_dir
        os.environ['JARVIS_PRIVATE'] = self.private_dir
        os.environ['JARVIS_SHARED'] = self.shared_dir

        # Initialize Jarvis properly
        initialize_jarvis_for_test(self.config_dir, self.private_dir, self.shared_dir)

        self.original_env = os.environ.copy()

    def tearDown(self):
        """Clean up test environment"""
        os.environ.clear()
        os.environ.update(self.original_env)

        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

        if hasattr(Jarvis, '_instance'):
            Jarvis._instance = None

    def test_default_lifecycle_methods_exist(self):
        """Test default lifecycle methods exist and are callable"""
        pkg = Pkg(pipeline=self.mock_pipeline)

        self.assertTrue(hasattr(pkg, 'start'))
        self.assertTrue(hasattr(pkg, 'stop'))
        self.assertTrue(hasattr(pkg, 'kill'))
        self.assertTrue(hasattr(pkg, 'clean'))
        self.assertTrue(hasattr(pkg, 'status'))

        # Should not raise errors
        pkg.start()
        pkg.stop()
        pkg.kill()
        pkg.clean()
        status = pkg.status()

        self.assertEqual(status, "unknown")

    def test_example_app_lifecycle(self):
        """Test example_app implements lifecycle methods correctly"""
        pkg = Pkg.load_standalone('builtin.example_app')
        pkg.configure(message='test message', output_file='test.txt')

        # Start should create marker
        pkg.start()
        start_marker = os.path.join(pkg.shared_dir, 'start.marker')
        self.assertTrue(os.path.exists(start_marker))

        # Stop should create marker
        pkg.stop()
        stop_marker = os.path.join(pkg.shared_dir, 'stop.marker')
        self.assertTrue(os.path.exists(stop_marker))

        # Kill should create marker
        pkg.kill()
        kill_marker = os.path.join(pkg.shared_dir, 'kill.marker')
        self.assertTrue(os.path.exists(kill_marker))

        # Clean should remove all markers
        pkg.clean()
        self.assertFalse(os.path.exists(start_marker))
        self.assertFalse(os.path.exists(stop_marker))
        self.assertFalse(os.path.exists(kill_marker))


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/core/test_repository_additional.py`

```python
"""
Additional repository manager tests for improved coverage
"""
import unittest
import os
import sys
import tempfile
import shutil
from pathlib import Path

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.core.repository import RepositoryManager
from jarvis_cd.core.config import Jarvis


class TestRepositoryManagerAdditional(unittest.TestCase):
    """Additional tests for RepositoryManager"""

    def setUp(self):
        """Set up test environment"""
        self.test_dir = Path(tempfile.mkdtemp())
        self.config_dir = self.test_dir / 'config'
        self.private_dir = self.test_dir / 'private'
        self.shared_dir = self.test_dir / 'shared'
        self.jarvis_root = self.test_dir / '.ppi-jarvis'

        # Reset and initialize Jarvis singleton
        Jarvis._instance = None
        self.jarvis_config = Jarvis(jarvis_root=str(self.jarvis_root))
        self.jarvis_config.initialize(
            str(self.config_dir),
            str(self.private_dir),
            str(self.shared_dir)
        )

        self.repo_manager = RepositoryManager(self.jarvis_config)

    def tearDown(self):
        """Clean up test environment"""
        if self.test_dir.exists():
            shutil.rmtree(self.test_dir)

    def test_add_repository_not_exists(self):
        """Test adding a repository that doesn't exist"""
        fake_path = self.test_dir / 'nonexistent_repo'

        with self.assertRaises(FileNotFoundError) as context:
            self.repo_manager.add_repository(str(fake_path))

        self.assertIn('does not exist', str(context.exception))

    def test_add_repository_not_directory(self):
        """Test adding a repository that is a file, not a directory"""
        file_path = self.test_dir / 'test_file.txt'
        file_path.write_text('test')

        with self.assertRaises(ValueError) as context:
            self.repo_manager.add_repository(str(file_path))

        self.assertIn('not a directory', str(context.exception))

    def test_add_repository_invalid_structure(self):
        """Test adding a repository with invalid structure"""
        repo_dir = self.test_dir / 'bad_repo'
        repo_dir.mkdir()

        # Missing the required subdirectory with same name
        with self.assertRaises(ValueError) as context:
            self.repo_manager.add_repository(str(repo_dir))

        self.assertIn('Invalid repository structure', str(context.exception))
        self.assertIn('does not contain subdirectory', str(context.exception))

    def test_add_repository_subdirectory_is_file(self):
        """Test adding a repository where expected subdirectory is actually a file"""
        repo_dir = self.test_dir / 'file_repo'
        repo_dir.mkdir()

        # Create a file instead of directory
        (repo_dir / 'file_repo').write_text('not a directory')

        with self.assertRaises(ValueError) as context:
            self.repo_manager.add_repository(str(repo_dir))

        self.assertIn('not a directory', str(context.exception))

    def test_add_repository_valid(self):
        """Test adding a valid repository"""
        repo_dir = self.test_dir / 'valid_repo'
        repo_subdir = repo_dir / 'valid_repo'
        repo_subdir.mkdir(parents=True)

        # Should succeed
        self.repo_manager.add_repository(str(repo_dir))

        # Verify it's in the config
        self.assertIn(str(repo_dir.absolute()), self.jarvis_config.repos['repos'])

    def test_remove_repository_by_name(self):
        """Test removing repository by name"""
        # Create and add a repository
        repo_dir = self.test_dir / 'test_repo'
        repo_subdir = repo_dir / 'test_repo'
        repo_subdir.mkdir(parents=True)

        self.repo_manager.add_repository(str(repo_dir))

        # Remove by name
        removed_count = self.repo_manager.remove_repository_by_name('test_repo')

        self.assertEqual(removed_count, 1)
        self.assertNotIn(str(repo_dir.absolute()), self.jarvis_config.repos['repos'])

    def test_create_package_invalid_type(self):
        """Test creating package with invalid type"""
        with self.assertRaises(ValueError) as context:
            self.repo_manager.create_package('test_pkg', 'invalid_type')

        self.assertIn('Invalid package type', str(context.exception))
        self.assertIn('service, app, or interceptor', str(context.exception))

    def test_create_package_no_repos(self):
        """Test creating package when no repositories are registered"""
        # Clear all repos including builtin
        self.jarvis_config.repos['repos'] = []

        with self.assertRaises(ValueError) as context:
            self.repo_manager.create_package('test_pkg', 'app')

        self.assertIn('No repositories registered', str(context.exception))

    def test_create_package_repo_not_exists(self):
        """Test creating package when repository doesn't exist"""
        # Clear all repos and add only a non-existent repo
        fake_repo = self.test_dir / 'fake_repo'
        self.jarvis_config.repos['repos'] = [str(fake_repo)]

        with self.assertRaises(FileNotFoundError) as context:
            self.repo_manager.create_package('test_pkg', 'app')

        self.assertIn('does not exist', str(context.exception))

    def test_create_package_service(self):
        """Test creating a service package"""
        # Create and add a repository
        repo_dir = self.test_dir / 'test_repo'
        repo_subdir = repo_dir / 'test_repo'
        repo_subdir.mkdir(parents=True)
        self.repo_manager.add_repository(str(repo_dir))

        # Create service package
        self.repo_manager.create_package('my_service', 'service')

        # Verify package file was created
        package_file = repo_subdir / 'my_service' / 'package.py'
        self.assertTrue(package_file.exists())

        # Verify content has Service base class
        content = package_file.read_text()
        self.assertIn('from jarvis_cd.core.pkg import Service', content)
        self.assertIn('class My_service(Service):', content)
        self.assertIn('def start(self):', content)
        self.assertIn('def stop(self):', content)

    def test_create_package_app(self):
        """Test creating an application package"""
        repo_dir = self.test_dir / 'test_repo'
        repo_subdir = repo_dir / 'test_repo'
        repo_subdir.mkdir(parents=True)
        self.repo_manager.add_repository(str(repo_dir))

        # Create app package
        self.repo_manager.create_package('my_app', 'app')

        # Verify package file was created
        package_file = repo_subdir / 'my_app' / 'package.py'
        self.assertTrue(package_file.exists())

        # Verify content has Application base class
        content = package_file.read_text()
        self.assertIn('from jarvis_cd.core.pkg import Application', content)
        self.assertIn('class My_app(Application):', content)
        self.assertIn('def _prepare_input(self):', content)

    def test_create_package_interceptor(self):
        """Test creating an interceptor package"""
        repo_dir = self.test_dir / 'test_repo'
        repo_subdir = repo_dir / 'test_repo'
        repo_subdir.mkdir(parents=True)
        self.repo_manager.add_repository(str(repo_dir))

        # Create interceptor package
        self.repo_manager.create_package('my_interceptor', 'interceptor')

        # Verify package file was created
        package_file = repo_subdir / 'my_interceptor' / 'package.py'
        self.assertTrue(package_file.exists())

        # Verify content has Interceptor base class
        content = package_file.read_text()
        self.assertIn('from jarvis_cd.core.pkg import Interceptor', content)
        self.assertIn('class My_interceptor(Interceptor):', content)
        self.assertIn('def modify_env(self):', content)
        self.assertIn('LD_PRELOAD', content)

    def test_list_packages_in_repo_empty(self):
        """Test listing packages in empty repository"""
        repo_dir = self.test_dir / 'empty_repo'
        repo_subdir = repo_dir / 'empty_repo'
        repo_subdir.mkdir(parents=True)

        packages = self.repo_manager.list_packages_in_repo(str(repo_dir))

        self.assertEqual(packages, [])

    def test_list_packages_in_repo_nonexistent(self):
        """Test listing packages in nonexistent repository"""
        fake_repo = self.test_dir / 'fake_repo'

        packages = self.repo_manager.list_packages_in_repo(str(fake_repo))

        self.assertEqual(packages, [])

    def test_list_packages_in_repo_with_packages(self):
        """Test listing packages in repository with packages"""
        repo_dir = self.test_dir / 'pkg_repo'
        repo_subdir = repo_dir / 'pkg_repo'
        repo_subdir.mkdir(parents=True)

        # Create some package directories
        pkg1_dir = repo_subdir / 'package1'
        pkg1_dir.mkdir()
        (pkg1_dir / 'package.py').write_text('# Package 1')

        pkg2_dir = repo_subdir / 'package2'
        pkg2_dir.mkdir()
        (pkg2_dir / 'package.py').write_text('# Package 2')

        # Create a directory without package.py (should be ignored)
        not_pkg_dir = repo_subdir / 'not_a_package'
        not_pkg_dir.mkdir()

        packages = self.repo_manager.list_packages_in_repo(str(repo_dir))

        self.assertEqual(sorted(packages), ['package1', 'package2'])

    def test_find_all_packages(self):
        """Test finding all packages across repositories"""
        # Create builtin-like structure
        builtin_dir = self.jarvis_config.get_builtin_repo_path()
        builtin_subdir = builtin_dir / 'builtin'
        builtin_subdir.mkdir(parents=True, exist_ok=True)

        # Add a builtin package
        builtin_pkg = builtin_subdir / 'builtin_pkg'
        builtin_pkg.mkdir(exist_ok=True)
        (builtin_pkg / 'package.py').write_text('# Builtin')

        # Create custom repo
        custom_dir = self.test_dir / 'custom_repo'
        custom_subdir = custom_dir / 'custom_repo'
        custom_subdir.mkdir(parents=True)

        custom_pkg = custom_subdir / 'custom_pkg'
        custom_pkg.mkdir()
        (custom_pkg / 'package.py').write_text('# Custom')

        self.repo_manager.add_repository(str(custom_dir))

        # Find all packages
        all_packages = self.repo_manager.find_all_packages()

        # Should have both repos
        self.assertIn('builtin', all_packages)
        self.assertIn('custom_repo', all_packages)

        # Check package lists
        self.assertIn('builtin_pkg', all_packages['builtin'])
        self.assertIn('custom_pkg', all_packages['custom_repo'])


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/shell/__init__.py`

```python

```

### `test/unit/shell/print_env.py`

```python
#!/usr/bin/env python3
"""
Simple script to print environment variables.
Usage: print_env.py VAR1 VAR2 VAR3 ...
"""
import sys
import os

if len(sys.argv) < 2:
    print("Usage: print_env.py VAR1 [VAR2 ...]", file=sys.stderr)
    sys.exit(1)

for var_name in sys.argv[1:]:
    value = os.environ.get(var_name)
    if value is not None:
        print(f"{var_name}={value}")
    else:
        print(f"ERROR: {var_name} not found", file=sys.stderr)
        sys.exit(1)

sys.exit(0)
```

### `test/unit/shell/test_env_checker.c`

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

int main(int argc, char *argv[]) {
    if (argc < 2) {
        fprintf(stderr, "Usage: %s <ENV_VAR_NAME> [<ENV_VAR_NAME> ...]\n", argv[0]);
        return 1;
    }

    int all_found = 1;
    for (int i = 1; i < argc; i++) {
        const char *env_name = argv[i];
        const char *env_value = getenv(env_name);

        if (env_value != NULL) {
            printf("%s=%s\n", env_name, env_value);
        } else {
            fprintf(stderr, "ERROR: Environment variable '%s' not found\n", env_name);
            all_found = 0;
        }
    }

    return all_found ? 0 : 1;
}
```

### `test/unit/shell/test_env_forwarding.py`

```python
import unittest
import sys
import os
import subprocess

# Add the project root to the path so we can import jarvis_cd
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from jarvis_cd.shell.core_exec import LocalExec
from jarvis_cd.shell.ssh_exec import SshExec, PsshExec
from jarvis_cd.shell.mpi_exec import MpiExec
from jarvis_cd.shell.exec_info import LocalExecInfo, SshExecInfo, PsshExecInfo, MpiExecInfo
from jarvis_cd.util.hostfile import Hostfile


class TestLocalExecEnvForwarding(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.print_env = os.path.join(os.path.dirname(__file__), 'print_env.py')

    def test_single_custom_env_var(self):
        """Test LocalExec forwards single custom environment variable"""
        exec_info = LocalExecInfo(env={'CUSTOM_VAR': 'HELLO'})
        local_exec = LocalExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)
        self.assertIn('CUSTOM_VAR=HELLO', local_exec.stdout['localhost'])

    def test_multiple_custom_env_vars(self):
        """Test LocalExec forwards multiple custom environment variables"""
        exec_info = LocalExecInfo(env={
            'CUSTOM_VAR': 'HELLO',
            'ANOTHER_VAR': 'WORLD',
            'THIRD_VAR': 'TEST'
        })
        local_exec = LocalExec(f'python3 {self.print_env} CUSTOM_VAR ANOTHER_VAR THIRD_VAR', exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)
        stdout = local_exec.stdout['localhost']
        self.assertIn('CUSTOM_VAR=HELLO', stdout)
        self.assertIn('ANOTHER_VAR=WORLD', stdout)
        self.assertIn('THIRD_VAR=TEST', stdout)

    def test_env_var_with_spaces(self):
        """Test LocalExec forwards environment variable with spaces"""
        exec_info = LocalExecInfo(env={'CUSTOM_VAR': 'HELLO WORLD'})
        local_exec = LocalExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)
        self.assertIn('CUSTOM_VAR=HELLO WORLD', local_exec.stdout['localhost'])

    def test_env_var_with_special_chars(self):
        """Test LocalExec forwards environment variable with special characters"""
        exec_info = LocalExecInfo(env={'CUSTOM_VAR': 'HELLO@#$%'})
        local_exec = LocalExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)
        self.assertIn('CUSTOM_VAR=HELLO@#$%', local_exec.stdout['localhost'])

    def test_numeric_env_var(self):
        """Test LocalExec forwards numeric environment variable"""
        exec_info = LocalExecInfo(env={'CUSTOM_VAR': 12345})
        local_exec = LocalExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)
        self.assertIn('CUSTOM_VAR=12345', local_exec.stdout['localhost'])

    def test_env_not_forwarded_without_setting(self):
        """Test that custom env var is not available without setting it"""
        exec_info = LocalExecInfo(env={})
        local_exec = LocalExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        # Should fail because CUSTOM_VAR is not set
        self.assertNotEqual(local_exec.exit_code['localhost'], 0)
        self.assertIn('CUSTOM_VAR not found', local_exec.stderr['localhost'])


class TestSshExecEnvForwarding(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.print_env = os.path.join(os.path.dirname(__file__), 'print_env.py')
        # Use localhost for SSH tests to avoid needing remote hosts
        self.hostfile = Hostfile(hosts=['localhost'], find_ips=False)

    def test_single_custom_env_var(self):
        """Test SshExec forwards single custom environment variable"""
        exec_info = SshExecInfo(
            hostfile=self.hostfile,
            env={'CUSTOM_VAR': 'HELLO'}
        )
        ssh_exec = SshExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        self.assertEqual(ssh_exec.exit_code['localhost'], 0)
        self.assertIn('CUSTOM_VAR=HELLO', ssh_exec.stdout['localhost'])

    def test_multiple_custom_env_vars(self):
        """Test SshExec forwards multiple custom environment variables"""
        exec_info = SshExecInfo(
            hostfile=self.hostfile,
            env={
                'CUSTOM_VAR': 'HELLO',
                'ANOTHER_VAR': 'WORLD',
                'THIRD_VAR': 'TEST'
            }
        )
        ssh_exec = SshExec(f'python3 {self.print_env} CUSTOM_VAR ANOTHER_VAR THIRD_VAR', exec_info)

        self.assertEqual(ssh_exec.exit_code['localhost'], 0)
        stdout = ssh_exec.stdout['localhost']
        self.assertIn('CUSTOM_VAR=HELLO', stdout)
        self.assertIn('ANOTHER_VAR=WORLD', stdout)
        self.assertIn('THIRD_VAR=TEST', stdout)

    def test_env_var_with_spaces(self):
        """Test SshExec forwards environment variable with spaces"""
        exec_info = SshExecInfo(
            hostfile=self.hostfile,
            env={'CUSTOM_VAR': 'HELLO WORLD'}
        )
        ssh_exec = SshExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        self.assertEqual(ssh_exec.exit_code['localhost'], 0)
        self.assertIn('CUSTOM_VAR=HELLO WORLD', ssh_exec.stdout['localhost'])

    def test_env_var_with_quotes(self):
        """Test SshExec forwards environment variable with quotes"""
        exec_info = SshExecInfo(
            hostfile=self.hostfile,
            env={'CUSTOM_VAR': 'HELLO "QUOTED" WORLD'}
        )
        ssh_exec = SshExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        self.assertEqual(ssh_exec.exit_code['localhost'], 0)
        # The quotes might be escaped differently, so just check the key parts
        stdout = ssh_exec.stdout['localhost']
        self.assertIn('CUSTOM_VAR=', stdout)
        self.assertIn('HELLO', stdout)
        self.assertIn('WORLD', stdout)

    def test_numeric_env_var(self):
        """Test SshExec forwards numeric environment variable"""
        exec_info = SshExecInfo(
            hostfile=self.hostfile,
            env={'CUSTOM_VAR': 12345}
        )
        ssh_exec = SshExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        self.assertEqual(ssh_exec.exit_code['localhost'], 0)
        self.assertIn('CUSTOM_VAR=12345', ssh_exec.stdout['localhost'])


class TestPsshExecEnvForwarding(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.print_env = os.path.join(os.path.dirname(__file__), 'print_env.py')
        # Use localhost for PSSH tests
        self.hostfile = Hostfile(hosts=['localhost'], find_ips=False)

    def test_single_custom_env_var(self):
        """Test PsshExec forwards single custom environment variable"""
        exec_info = PsshExecInfo(
            hostfile=self.hostfile,
            env={'CUSTOM_VAR': 'HELLO'}
        )
        pssh_exec = PsshExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        self.assertEqual(pssh_exec.exit_code['localhost'], 0)
        self.assertIn('CUSTOM_VAR=HELLO', pssh_exec.stdout['localhost'])

    def test_multiple_custom_env_vars(self):
        """Test PsshExec forwards multiple custom environment variables"""
        exec_info = PsshExecInfo(
            hostfile=self.hostfile,
            env={
                'CUSTOM_VAR': 'HELLO',
                'ANOTHER_VAR': 'WORLD'
            }
        )
        pssh_exec = PsshExec(f'python3 {self.print_env} CUSTOM_VAR ANOTHER_VAR', exec_info)

        self.assertEqual(pssh_exec.exit_code['localhost'], 0)
        stdout = pssh_exec.stdout['localhost']
        self.assertIn('CUSTOM_VAR=HELLO', stdout)
        self.assertIn('ANOTHER_VAR=WORLD', stdout)


class TestMpiExecEnvForwarding(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.print_env = os.path.join(os.path.dirname(__file__), 'print_env.py')
        self.hostfile = Hostfile(hosts=['localhost'], find_ips=False)

        # Check if mpiexec is available
        try:
            subprocess.run(['mpiexec', '--version'], capture_output=True, check=True)
            self.mpi_available = True
        except (subprocess.CalledProcessError, FileNotFoundError):
            self.mpi_available = False

    @unittest.skipIf(not hasattr(unittest.TestCase, 'mpi_available'), "MPI not available")
    @unittest.skipIf(not hasattr(unittest.TestCase, 'mpi_available'), "MPI not available")
    def test_single_custom_env_var(self):
        """Test MpiExec forwards single custom environment variable"""
        if not self.mpi_available:
            self.skipTest("MPI not available")

        exec_info = MpiExecInfo(
            nprocs=1,
            hostfile=self.hostfile,
            env={'CUSTOM_VAR': 'HELLO'}
        )
        mpi_exec = MpiExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        self.assertEqual(mpi_exec.exit_code['localhost'], 0)
        self.assertIn('CUSTOM_VAR=HELLO', mpi_exec.stdout['localhost'])

    def test_multiple_custom_env_vars(self):
        """Test MpiExec forwards multiple custom environment variables"""
        if not self.mpi_available:
            self.skipTest("MPI not available")

        exec_info = MpiExecInfo(
            nprocs=1,
            hostfile=self.hostfile,
            env={
                'CUSTOM_VAR': 'HELLO',
                'ANOTHER_VAR': 'WORLD',
                'THIRD_VAR': 'TEST'
            }
        )
        mpi_exec = MpiExec(f'python3 {self.print_env} CUSTOM_VAR ANOTHER_VAR THIRD_VAR', exec_info)

        self.assertEqual(mpi_exec.exit_code['localhost'], 0)
        stdout = mpi_exec.stdout['localhost']
        self.assertIn('CUSTOM_VAR=HELLO', stdout)
        self.assertIn('ANOTHER_VAR=WORLD', stdout)
        self.assertIn('THIRD_VAR=TEST', stdout)

    def test_env_var_with_spaces(self):
        """Test MpiExec forwards environment variable with spaces"""
        if not self.mpi_available:
            self.skipTest("MPI not available")

        exec_info = MpiExecInfo(
            nprocs=1,
            hostfile=self.hostfile,
            env={'CUSTOM_VAR': 'HELLO WORLD'}
        )
        mpi_exec = MpiExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        self.assertEqual(mpi_exec.exit_code['localhost'], 0)
        self.assertIn('CUSTOM_VAR=HELLO WORLD', mpi_exec.stdout['localhost'])

    def test_numeric_env_var(self):
        """Test MpiExec forwards numeric environment variable"""
        if not self.mpi_available:
            self.skipTest("MPI not available")

        exec_info = MpiExecInfo(
            nprocs=1,
            hostfile=self.hostfile,
            env={'CUSTOM_VAR': 54321}
        )
        mpi_exec = MpiExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        self.assertEqual(mpi_exec.exit_code['localhost'], 0)
        self.assertIn('CUSTOM_VAR=54321', mpi_exec.stdout['localhost'])

    def test_env_forwarding_with_multiple_procs(self):
        """Test MpiExec forwards env vars with multiple processes"""
        if not self.mpi_available:
            self.skipTest("MPI not available")

        exec_info = MpiExecInfo(
            nprocs=2,
            hostfile=self.hostfile,
            env={'CUSTOM_VAR': 'HELLO'}
        )
        mpi_exec = MpiExec(f'python3 {self.print_env} CUSTOM_VAR', exec_info)

        self.assertEqual(mpi_exec.exit_code['localhost'], 0)
        # With 2 processes, we should see the output twice (or at least once)
        self.assertIn('CUSTOM_VAR=HELLO', mpi_exec.stdout['localhost'])


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/shell/test_exec_factory.py`

```python
"""
Tests for exec_factory.py - Exec factory class
"""
import unittest
import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.shell.exec_factory import Exec
from jarvis_cd.shell.exec_info import (
    LocalExecInfo, SshExecInfo, PsshExecInfo, MpiExecInfo,
    ExecType
)


class TestExecFactory(unittest.TestCase):
    """Tests for Exec factory pattern"""

    def test_local_exec_delegation(self):
        """Test Exec delegates to LocalExec"""
        exec_info = LocalExecInfo()
        exec_obj = Exec('echo "test"', exec_info)
        delegate = exec_obj.run()

        self.assertIsNotNone(delegate)
        self.assertEqual(exec_obj.exit_code['localhost'], 0)
        self.assertIn('test', exec_obj.stdout['localhost'])

    def test_ssh_exec_delegation(self):
        """Test Exec delegates to SshExec"""
        exec_info = SshExecInfo(hostnames=['mock_host'])
        exec_obj = Exec('echo "ssh test"', exec_info)

        # Test delegation without running (mock host doesn't exist)
        self.assertEqual(exec_obj.exec_info.exec_type, ExecType.SSH)
        self.assertIsNotNone(exec_obj.cmd)

    def test_pssh_exec_delegation(self):
        """Test Exec delegates to PsshExec"""
        exec_info = PsshExecInfo(hostnames=['mock1', 'mock2'])
        exec_obj = Exec('echo "pssh test"', exec_info)

        # Test delegation without running
        self.assertEqual(exec_obj.exec_info.exec_type, ExecType.PSSH)
        self.assertIsNotNone(exec_obj.cmd)

    def test_mpi_exec_delegation(self):
        """Test Exec delegates to MpiExec"""
        exec_info = MpiExecInfo(nprocs=4)
        exec_obj = Exec('echo "mpi test"', exec_info)

        # Test delegation without running (MPI may not be available)
        self.assertIn(exec_obj.exec_info.exec_type,
                     [ExecType.MPI, ExecType.OPENMPI, ExecType.MPICH,
                      ExecType.INTEL_MPI, ExecType.CRAY_MPICH])
        self.assertIsNotNone(exec_obj.cmd)

    def test_get_cmd(self):
        """Test get_cmd method"""
        cmd = 'python3 -c "print(42)"'
        exec_info = LocalExecInfo()
        exec_obj = Exec(cmd, exec_info)

        self.assertEqual(exec_obj.get_cmd(), cmd)

    def test_wait(self):
        """Test wait method"""
        exec_info = LocalExecInfo()
        exec_obj = Exec('echo "wait test"', exec_info)
        exec_obj.run()

        exit_code = exec_obj.wait('localhost')
        self.assertEqual(exit_code, 0)

    def test_wait_all(self):
        """Test wait_all method"""
        exec_info = LocalExecInfo()
        exec_obj = Exec('echo "wait all test"', exec_info)
        exec_obj.run()

        exit_codes = exec_obj.wait_all()
        self.assertIn('localhost', exit_codes)
        self.assertEqual(exit_codes['localhost'], 0)

    def test_exec_type_unsupported(self):
        """Test unsupported exec type raises error"""
        # Create a custom exec_info with invalid type
        exec_info = LocalExecInfo()
        # Manually set invalid type
        exec_info.exec_type = None

        exec_obj = Exec('echo "test"', exec_info)

        with self.assertRaises(ValueError) as context:
            exec_obj.run()

        self.assertIn('Unsupported execution type', str(context.exception))

    def test_delegate_attributes_copied(self):
        """Test that delegate attributes are copied to parent"""
        exec_info = LocalExecInfo()
        exec_obj = Exec('echo "attribute test"', exec_info)
        exec_obj.run()

        # Check attributes are copied
        self.assertIsNotNone(exec_obj.exit_code)
        self.assertIsNotNone(exec_obj.stdout)
        self.assertIsNotNone(exec_obj.stderr)
        self.assertIsNotNone(exec_obj.processes)
        self.assertIsNotNone(exec_obj.output_threads)

    def test_wait_without_run(self):
        """Test wait before run returns 0"""
        exec_info = LocalExecInfo()
        exec_obj = Exec('echo "test"', exec_info)

        # Don't run, just wait
        exit_code = exec_obj.wait('localhost')
        self.assertEqual(exit_code, 0)

    def test_wait_all_without_run(self):
        """Test wait_all before run returns empty dict"""
        exec_info = LocalExecInfo()
        exec_obj = Exec('echo "test"', exec_info)

        # Don't run, just wait_all
        exit_codes = exec_obj.wait_all()
        self.assertEqual(exit_codes, {})


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/shell/test_local_exec.py`

```python
import unittest
import sys
import os
import tempfile
import time

# Add the project root to the path so we can import jarvis_cd
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from jarvis_cd.shell.core_exec import LocalExec
from jarvis_cd.shell.exec_info import LocalExecInfo


class TestLocalExec(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.test_binary = os.path.join(os.path.dirname(__file__), 'test_env_checker')

        # Verify the test binary exists
        if not os.path.exists(self.test_binary):
            raise RuntimeError(f"Test binary not found: {self.test_binary}")

    def test_basic_execution(self):
        """Test basic command execution"""
        exec_info = LocalExecInfo()
        local_exec = LocalExec('echo "hello world"', exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)
        self.assertIn('hello world', local_exec.stdout['localhost'])

    def test_single_env_variable(self):
        """Test execution with a single environment variable"""
        exec_info = LocalExecInfo(env={'TEST_VAR': 'test_value'})
        local_exec = LocalExec(f'{self.test_binary} TEST_VAR', exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)
        self.assertIn('TEST_VAR=test_value', local_exec.stdout['localhost'])

    def test_multiple_env_variables(self):
        """Test execution with multiple environment variables"""
        exec_info = LocalExecInfo(env={
            'VAR1': 'value1',
            'VAR2': 'value2',
            'VAR3': 'value3'
        })
        local_exec = LocalExec(f'{self.test_binary} VAR1 VAR2 VAR3', exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)
        stdout = local_exec.stdout['localhost']
        self.assertIn('VAR1=value1', stdout)
        self.assertIn('VAR2=value2', stdout)
        self.assertIn('VAR3=value3', stdout)

    def test_env_variable_with_special_chars(self):
        """Test environment variables with special characters"""
        special_value = "value with spaces and 'quotes' and \"double quotes\""
        exec_info = LocalExecInfo(env={'SPECIAL_VAR': special_value})
        local_exec = LocalExec(f'{self.test_binary} SPECIAL_VAR', exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)
        self.assertIn('SPECIAL_VAR=', local_exec.stdout['localhost'])

    def test_env_variable_override(self):
        """Test that custom env variables override system variables"""
        exec_info = LocalExecInfo(env={'PATH': '/custom/path'})
        local_exec = LocalExec(f'{self.test_binary} PATH', exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)
        self.assertIn('PATH=/custom/path', local_exec.stdout['localhost'])

    def test_empty_env(self):
        """Test execution with empty env dict"""
        exec_info = LocalExecInfo(env={})
        local_exec = LocalExec('echo "test"', exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)

    def test_none_env(self):
        """Test execution with None env (should use system env)"""
        exec_info = LocalExecInfo(env=None)
        local_exec = LocalExec(f'{self.test_binary} PATH', exec_info)

        # PATH should exist in system environment
        self.assertEqual(local_exec.exit_code['localhost'], 0)
        self.assertIn('PATH=', local_exec.stdout['localhost'])

    def test_collect_output(self):
        """Test output collection"""
        exec_info = LocalExecInfo(collect_output=True)
        local_exec = LocalExec('echo "collected output"', exec_info)

        self.assertIn('collected output', local_exec.stdout['localhost'])

    def test_hide_output(self):
        """Test hiding output (should still collect)"""
        exec_info = LocalExecInfo(hide_output=True, collect_output=True)
        local_exec = LocalExec('echo "hidden output"', exec_info)

        self.assertIn('hidden output', local_exec.stdout['localhost'])

    def test_stderr_collection(self):
        """Test stderr collection"""
        exec_info = LocalExecInfo(collect_output=True)
        local_exec = LocalExec('echo "error message" >&2', exec_info)

        self.assertIn('error message', local_exec.stderr['localhost'])

    def test_pipe_stdout_to_file(self):
        """Test piping stdout to a file"""
        with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:
            temp_file = f.name

        try:
            exec_info = LocalExecInfo(pipe_stdout=temp_file)
            local_exec = LocalExec('echo "piped output"', exec_info)

            with open(temp_file, 'r') as f:
                content = f.read()
            self.assertIn('piped output', content)
        finally:
            os.unlink(temp_file)

    def test_pipe_stderr_to_file(self):
        """Test piping stderr to a file"""
        with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:
            temp_file = f.name

        try:
            exec_info = LocalExecInfo(pipe_stderr=temp_file)
            local_exec = LocalExec('echo "error output" >&2', exec_info)

            with open(temp_file, 'r') as f:
                content = f.read()
            self.assertIn('error output', content)
        finally:
            os.unlink(temp_file)

    def test_cwd_change(self):
        """Test changing working directory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            exec_info = LocalExecInfo(cwd=tmpdir)
            local_exec = LocalExec('pwd', exec_info)

            self.assertIn(tmpdir, local_exec.stdout['localhost'])

    def test_async_execution(self):
        """Test asynchronous execution"""
        exec_info = LocalExecInfo(exec_async=True)
        local_exec = LocalExec('sleep 0.1 && echo "async done"', exec_info)

        # Process should still be running or just finished
        # Wait for it to complete
        exit_code = local_exec.wait('localhost')
        self.assertEqual(exit_code, 0)
        self.assertIn('async done', local_exec.stdout['localhost'])

    def test_sleep_ms(self):
        """Test sleep after execution"""
        exec_info = LocalExecInfo(sleep_ms=100)
        start_time = time.time()
        local_exec = LocalExec('echo "test"', exec_info)
        elapsed = time.time() - start_time

        # Should have slept for at least 0.1 seconds
        self.assertGreaterEqual(elapsed, 0.1)

    def test_stdin_input(self):
        """Test providing stdin to command"""
        exec_info = LocalExecInfo(stdin="test input\n")
        local_exec = LocalExec('cat', exec_info)

        self.assertIn('test input', local_exec.stdout['localhost'])

    def test_exit_code_on_failure(self):
        """Test that exit code is captured on command failure"""
        exec_info = LocalExecInfo()
        local_exec = LocalExec('false', exec_info)

        self.assertNotEqual(local_exec.exit_code['localhost'], 0)

    def test_env_preserved_across_commands(self):
        """Test that environment variables persist across shell commands"""
        exec_info = LocalExecInfo(env={'CUSTOM_VAR': 'custom_value'})
        # Use shell to execute multiple commands
        cmd = f'{self.test_binary} CUSTOM_VAR && echo "second command"'
        local_exec = LocalExec(cmd, exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)
        stdout = local_exec.stdout['localhost']
        self.assertIn('CUSTOM_VAR=custom_value', stdout)
        self.assertIn('second command', stdout)

    def test_numeric_env_values(self):
        """Test environment variables with numeric values"""
        exec_info = LocalExecInfo(env={
            'INT_VAR': 42,
            'FLOAT_VAR': 3.14,
            'BOOL_VAR': True
        })
        local_exec = LocalExec(f'{self.test_binary} INT_VAR FLOAT_VAR BOOL_VAR', exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)
        stdout = local_exec.stdout['localhost']
        self.assertIn('INT_VAR=42', stdout)
        self.assertIn('FLOAT_VAR=3.14', stdout)
        self.assertIn('BOOL_VAR=True', stdout)

    def test_env_with_equals_in_value(self):
        """Test environment variable with equals sign in value"""
        exec_info = LocalExecInfo(env={'CONFIG': 'key=value'})
        local_exec = LocalExec(f'{self.test_binary} CONFIG', exec_info)

        self.assertEqual(local_exec.exit_code['localhost'], 0)
        self.assertIn('CONFIG=key=value', local_exec.stdout['localhost'])

    def test_get_cmd(self):
        """Test get_cmd returns the original command"""
        cmd = 'echo "test command"'
        exec_info = LocalExecInfo()
        local_exec = LocalExec(cmd, exec_info)

        self.assertEqual(local_exec.get_cmd(), cmd)


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/shell/test_mpi_exec.py`

```python
import unittest
from unittest.mock import Mock, patch, MagicMock
import sys
import os

# Add the project root to the path so we can import jarvis_cd
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from jarvis_cd.shell.mpi_exec import MpiExec, OpenMpiExec, MpichExec, CrayMpichExec, IntelMpiExec
from jarvis_cd.shell.exec_info import MpiExecInfo, ExecType
from jarvis_cd.util.hostfile import Hostfile


class TestMpiExec(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.hostfile = Hostfile(hosts=['localhost'], find_ips=False)

    def test_single_command_format(self):
        """Test MPI execution with a single command string"""
        exec_info = MpiExecInfo(
            nprocs=4,
            ppn=2,
            hostfile=self.hostfile,
            env={'TEST_VAR': 'test_value'}
        )

        mpi_exec = MpiExec('echo "hello world"', exec_info)
        cmd = mpi_exec.get_cmd()

        # Verify basic command structure
        self.assertIn('mpiexec', cmd)
        self.assertIn('echo "hello world"', cmd)

    def test_multi_command_format(self):
        """Test MPI execution with multiple commands"""
        exec_info = MpiExecInfo(
            nprocs=10,
            hostfile=self.hostfile
        )

        cmd_list = [
            {'cmd': 'gdbserver :1234 ./myapp', 'nprocs': 1},
            {'cmd': './myapp', 'nprocs': None}  # Should get remaining 9 procs
        ]

        mpi_exec = MpiExec(cmd_list, exec_info)
        cmd = mpi_exec.get_cmd()

        # Verify multi-command structure
        self.assertIn('mpiexec', cmd)
        self.assertIn('gdbserver', cmd)
        self.assertIn('./myapp', cmd)

    def test_multi_command_with_zero_nprocs(self):
        """Test that commands with 0 nprocs are skipped"""
        exec_info = MpiExecInfo(
            nprocs=4,
            hostfile=self.hostfile
        )

        cmd_list = [
            {'cmd': 'gdbserver :1234 ./myapp', 'nprocs': 0},  # Should be skipped
            {'cmd': './myapp', 'nprocs': None}  # Should get all 4 procs
        ]

        mpi_exec = MpiExec(cmd_list, exec_info)
        cmd = mpi_exec.get_cmd()

        # Verify gdbserver command is not included
        self.assertNotIn('gdbserver', cmd)
        self.assertIn('./myapp', cmd)

    def test_environment_variables(self):
        """Test that environment variables are properly forwarded"""
        exec_info = MpiExecInfo(
            nprocs=2,
            hostfile=self.hostfile,
            env={'MY_VAR': 'my_value', 'ANOTHER_VAR': 'another_value'}
        )

        mpi_exec = MpiExec('./myapp', exec_info)
        cmd = mpi_exec.get_cmd()

        # Verify environment variables are in the command
        # The exact format depends on MPI implementation
        self.assertTrue(
            'MY_VAR' in cmd or 'ANOTHER_VAR' in cmd,
            "Environment variables should be included in MPI command"
        )

    def test_single_env_variable(self):
        """Test MPI execution with a single environment variable"""
        test_binary = os.path.join(os.path.dirname(__file__), 'test_env_checker')
        exec_info = MpiExecInfo(
            nprocs=1,
            hostfile=self.hostfile,
            env={'TEST_VAR': 'test_value'}
        )

        if os.path.exists(test_binary):
            mpi_exec = MpiExec(f'{test_binary} TEST_VAR', exec_info)
            cmd = mpi_exec.get_cmd()

            self.assertIn('TEST_VAR', cmd)
            self.assertIn('test_value', cmd)

    def test_multiple_env_variables(self):
        """Test MPI execution with multiple environment variables"""
        test_binary = os.path.join(os.path.dirname(__file__), 'test_env_checker')
        exec_info = MpiExecInfo(
            nprocs=1,
            hostfile=self.hostfile,
            env={
                'VAR1': 'value1',
                'VAR2': 'value2',
                'VAR3': 'value3'
            }
        )

        if os.path.exists(test_binary):
            mpi_exec = MpiExec(f'{test_binary} VAR1 VAR2 VAR3', exec_info)
            cmd = mpi_exec.get_cmd()

            self.assertIn('VAR1', cmd)
            self.assertIn('value1', cmd)
            self.assertIn('VAR2', cmd)
            self.assertIn('value2', cmd)
            self.assertIn('VAR3', cmd)
            self.assertIn('value3', cmd)

    def test_env_with_special_characters(self):
        """Test environment variables with special characters"""
        exec_info = MpiExecInfo(
            nprocs=1,
            hostfile=self.hostfile,
            env={'SPECIAL_VAR': 'value with "quotes" and spaces'}
        )

        mpi_exec = MpiExec('echo $SPECIAL_VAR', exec_info)
        cmd = mpi_exec.get_cmd()

        self.assertIn('SPECIAL_VAR', cmd)

    def test_numeric_env_values(self):
        """Test environment variables with numeric values"""
        exec_info = MpiExecInfo(
            nprocs=1,
            hostfile=self.hostfile,
            env={
                'INT_VAR': 42,
                'FLOAT_VAR': 3.14
            }
        )

        mpi_exec = MpiExec('echo test', exec_info)
        cmd = mpi_exec.get_cmd()

        self.assertIn('INT_VAR', cmd)
        self.assertIn('42', cmd)
        self.assertIn('FLOAT_VAR', cmd)
        self.assertIn('3.14', cmd)

    def test_basic_env_without_ld_preload(self):
        """Test that basic_env removes LD_PRELOAD"""
        exec_info = MpiExecInfo(
            nprocs=1,
            hostfile=self.hostfile,
            env={'LD_PRELOAD': '/lib/test.so', 'OTHER_VAR': 'value'}
        )

        # basic_env should not have LD_PRELOAD
        self.assertNotIn('LD_PRELOAD', exec_info.basic_env)
        self.assertIn('OTHER_VAR', exec_info.basic_env)

    def test_multi_command_env_per_command(self):
        """Test environment variables in multi-command MPI execution"""
        exec_info = MpiExecInfo(
            nprocs=4,
            hostfile=self.hostfile,
            env={'GLOBAL_VAR': 'global_value'}
        )

        cmd_list = [
            {'cmd': 'echo cmd1', 'nprocs': 2},
            {'cmd': 'echo cmd2', 'nprocs': 2}
        ]

        mpi_exec = MpiExec(cmd_list, exec_info)
        cmd = mpi_exec.get_cmd()

        # Environment should be included for both commands
        self.assertIn('GLOBAL_VAR', cmd)

    def test_disable_preload_in_multi_command(self):
        """Test that disable_preload removes LD_PRELOAD for specific commands"""
        exec_info = MpiExecInfo(
            nprocs=4,
            hostfile=self.hostfile,
            env={'LD_PRELOAD': '/lib/test.so', 'OTHER_VAR': 'value'}
        )

        cmd_list = [
            {'cmd': 'echo cmd1', 'nprocs': 2, 'disable_preload': True},
            {'cmd': 'echo cmd2', 'nprocs': 2, 'disable_preload': False}
        ]

        mpi_exec = MpiExec(cmd_list, exec_info)
        # The first command should not have LD_PRELOAD in its env
        # This is implementation-specific, so we just verify it doesn't crash
        cmd = mpi_exec.get_cmd()
        self.assertIsNotNone(cmd)

    def test_ppn_option(self):
        """Test processes per node option"""
        exec_info = MpiExecInfo(
            nprocs=8,
            ppn=4,
            hostfile=self.hostfile
        )

        mpi_exec = MpiExec('./myapp', exec_info)
        cmd = mpi_exec.get_cmd()

        # Verify ppn option is included (format varies by MPI)
        self.assertTrue(
            'ppn' in cmd or 'npernode' in cmd,
            "PPN option should be in MPI command"
        )

    def test_hostfile_option(self):
        """Test hostfile option with multiple hosts"""
        multi_host = Hostfile(hosts=['host1', 'host2', 'host3'], find_ips=False)
        exec_info = MpiExecInfo(
            nprocs=6,
            hostfile=multi_host
        )

        mpi_exec = MpiExec('./myapp', exec_info)
        cmd = mpi_exec.get_cmd()

        # Verify hostfile or host option is included
        self.assertTrue(
            'host' in cmd.lower(),
            "Hostfile option should be in MPI command"
        )

    def test_remainder_calculation(self):
        """Test that remainder nprocs are calculated correctly"""
        exec_info = MpiExecInfo(
            nprocs=10,
            hostfile=self.hostfile
        )

        cmd_list = [
            {'cmd': 'cmd1', 'nprocs': 2},
            {'cmd': 'cmd2', 'nprocs': 3},
            {'cmd': 'cmd3', 'nprocs': None}  # Should get 10 - 2 - 3 = 5
        ]

        mpi_exec = MpiExec(cmd_list, exec_info)

        # Access internal cmd_list to verify calculation
        processed_list = mpi_exec.cmd_list
        self.assertEqual(processed_list[2]['nprocs'], 5)

    def test_nprocs_overflow(self):
        """Test error when total nprocs exceeds available"""
        exec_info = MpiExecInfo(
            nprocs=5,
            hostfile=self.hostfile
        )

        cmd_list = [
            {'cmd': 'cmd1', 'nprocs': 3},
            {'cmd': 'cmd2', 'nprocs': 3},  # Total = 6, exceeds 5
            {'cmd': 'cmd3', 'nprocs': None}
        ]

        with self.assertRaises(ValueError):
            MpiExec(cmd_list, exec_info)


class TestOpenMpiExec(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.hostfile = Hostfile(hosts=['localhost'], find_ips=False)

    def test_openmpi_specific_flags(self):
        """Test OpenMPI-specific flags"""
        exec_info = MpiExecInfo(
            nprocs=2,
            hostfile=self.hostfile
        )

        mpi_exec = OpenMpiExec('./myapp', exec_info)
        cmd = mpi_exec.get_cmd()

        # Verify OpenMPI-specific flags
        self.assertIn('--oversubscribe', cmd)
        self.assertIn('--allow-run-as-root', cmd)

    def test_openmpi_env_format(self):
        """Test OpenMPI environment variable format"""
        exec_info = MpiExecInfo(
            nprocs=2,
            hostfile=self.hostfile,
            env={'TEST_VAR': 'value'}
        )

        mpi_exec = OpenMpiExec('./myapp', exec_info)
        cmd = mpi_exec.get_cmd()

        # OpenMPI uses -x for environment variables
        self.assertIn('-x', cmd)


class TestMpichExec(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.hostfile = Hostfile(hosts=['localhost'], find_ips=False)

    def test_mpich_env_format(self):
        """Test MPICH environment variable format"""
        exec_info = MpiExecInfo(
            nprocs=2,
            hostfile=self.hostfile,
            env={'TEST_VAR': 'value'}
        )

        mpi_exec = MpichExec('./myapp', exec_info)
        cmd = mpi_exec.get_cmd()

        # MPICH uses -genv for environment variables
        self.assertIn('-genv', cmd)


class TestCrayMpichExec(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.hostfile = Hostfile(hosts=['localhost'], find_ips=False)

    def test_cray_env_format(self):
        """Test Cray MPICH environment variable format"""
        exec_info = MpiExecInfo(
            nprocs=2,
            hostfile=self.hostfile,
            env={'TEST_VAR': 'value'}
        )

        mpi_exec = CrayMpichExec('./myapp', exec_info)
        cmd = mpi_exec.get_cmd()

        # Cray MPICH uses --env for environment variables
        self.assertIn('--env', cmd)

    def test_cray_localhost_only_no_hostfile(self):
        """Test that Cray skips hostfile for localhost-only"""
        localhost_only = Hostfile(hosts=['localhost'], find_ips=False)
        exec_info = MpiExecInfo(
            nprocs=2,
            hostfile=localhost_only,
            env={}
        )

        mpi_exec = CrayMpichExec('./myapp', exec_info)
        cmd = mpi_exec.get_cmd()

        # Should not include hostfile option for localhost-only
        self.assertNotIn('--hostfile', cmd)
        self.assertNotIn('--hosts', cmd)

    def test_cray_multi_host(self):
        """Test Cray with multiple hosts"""
        multi_host = Hostfile(hosts=['host1', 'host2'], find_ips=False)
        exec_info = MpiExecInfo(
            nprocs=4,
            hostfile=multi_host,
            env={}
        )

        mpi_exec = CrayMpichExec('./myapp', exec_info)
        cmd = mpi_exec.get_cmd()

        # Should include hosts option
        self.assertTrue('--hosts' in cmd or '--hostfile' in cmd)

    def test_cray_ppn_option(self):
        """Test Cray MPICH ppn option"""
        exec_info = MpiExecInfo(
            nprocs=8,
            ppn=4,
            hostfile=self.hostfile,
            env={}
        )

        mpi_exec = CrayMpichExec('./myapp', exec_info)
        cmd = mpi_exec.get_cmd()

        # Cray uses --ppn
        self.assertIn('--ppn', cmd)

    def test_cray_multi_command(self):
        """Test Cray MPICH multi-command format"""
        exec_info = MpiExecInfo(
            nprocs=4,
            hostfile=self.hostfile,
            env={'MY_VAR': 'value'}
        )

        cmd_list = [
            {'cmd': 'cmd1', 'nprocs': 2},
            {'cmd': 'cmd2', 'nprocs': 2}
        ]

        mpi_exec = CrayMpichExec(cmd_list, exec_info)
        cmd = mpi_exec.get_cmd()

        # Verify multi-command structure with ' : ' separator
        self.assertIn(':', cmd)
        self.assertIn('cmd1', cmd)
        self.assertIn('cmd2', cmd)

    def test_cray_multi_command_with_disable_preload(self):
        """Test Cray multi-command with disable_preload"""
        exec_info = MpiExecInfo(
            nprocs=4,
            hostfile=self.hostfile,
            env={'LD_PRELOAD': '/lib/test.so', 'OTHER_VAR': 'value'}
        )

        cmd_list = [
            {'cmd': 'cmd1', 'nprocs': 2, 'disable_preload': True},
            {'cmd': 'cmd2', 'nprocs': 2, 'disable_preload': False}
        ]

        mpi_exec = CrayMpichExec(cmd_list, exec_info)
        # Should not crash and should produce valid command
        cmd = mpi_exec.get_cmd()
        self.assertIsNotNone(cmd)

    def test_cray_hostfile_path(self):
        """Test Cray MPICH with hostfile path"""
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.hosts') as f:
            f.write('host1\nhost2\n')
            hostfile_path = f.name

        try:
            hostfile = Hostfile(path=hostfile_path)
            exec_info = MpiExecInfo(
                nprocs=4,
                hostfile=hostfile,
                env={}
            )

            mpi_exec = CrayMpichExec('./myapp', exec_info)
            cmd = mpi_exec.get_cmd()

            # Should include hostfile path
            self.assertIn('--hostfile', cmd)
            self.assertIn(hostfile_path, cmd)
        finally:
            os.unlink(hostfile_path)


class TestIntelMpiExec(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.hostfile = Hostfile(hosts=['localhost'], find_ips=False)

    def test_intel_mpi_inherits_mpich(self):
        """Test that Intel MPI uses MPICH-style commands"""
        exec_info = MpiExecInfo(
            nprocs=2,
            hostfile=self.hostfile,
            env={'TEST_VAR': 'value'}
        )

        mpi_exec = IntelMpiExec('./myapp', exec_info)
        cmd = mpi_exec.get_cmd()

        # Intel MPI inherits from MPICH, so uses -genv
        self.assertIn('-genv', cmd)


class TestOpenMpiMultiCommand(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.hostfile = Hostfile(hosts=['localhost'], find_ips=False)

    def test_openmpi_multi_command(self):
        """Test OpenMPI multi-command format"""
        exec_info = MpiExecInfo(
            nprocs=6,
            hostfile=self.hostfile,
            env={'MY_VAR': 'value'}
        )

        cmd_list = [
            {'cmd': 'cmd1', 'nprocs': 2},
            {'cmd': 'cmd2', 'nprocs': 4}
        ]

        mpi_exec = OpenMpiExec(cmd_list, exec_info)
        cmd = mpi_exec.get_cmd()

        # Verify multi-command structure with ' : ' separator
        self.assertIn(':', cmd)
        self.assertIn('cmd1', cmd)
        self.assertIn('cmd2', cmd)
        self.assertIn('-x', cmd)  # OpenMPI env format

    def test_openmpi_ppn_with_hostfile_path(self):
        """Test OpenMPI with ppn and hostfile path"""
        # Create a temporary hostfile path
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.hosts') as f:
            f.write('host1\nhost2\n')
            hostfile_path = f.name

        try:
            hostfile = Hostfile(path=hostfile_path)
            exec_info = MpiExecInfo(
                nprocs=4,
                ppn=2,
                hostfile=hostfile,
                env={}
            )

            mpi_exec = OpenMpiExec('./myapp', exec_info)
            cmd = mpi_exec.get_cmd()

            # Should include hostfile path
            self.assertIn('--hostfile', cmd)
            self.assertIn(hostfile_path, cmd)
            self.assertIn('-npernode', cmd)
        finally:
            os.unlink(hostfile_path)

    def test_openmpi_multi_command_with_ld_preload(self):
        """Test OpenMPI multi-command with LD_PRELOAD in env and disable_preload"""
        exec_info = MpiExecInfo(
            nprocs=4,
            hostfile=self.hostfile,
            env={'LD_PRELOAD': '/lib/test.so', 'OTHER_VAR': 'value'}
        )

        cmd_list = [
            {'cmd': 'cmd1', 'nprocs': 2, 'disable_preload': True},
            {'cmd': 'cmd2', 'nprocs': 2, 'disable_preload': False}
        ]

        mpi_exec = OpenMpiExec(cmd_list, exec_info)
        cmd = mpi_exec.get_cmd()

        # First command should not have LD_PRELOAD, second should have it
        # This tests the LD_PRELOAD deletion logic
        self.assertIsNotNone(cmd)
        self.assertIn('cmd1', cmd)
        self.assertIn('cmd2', cmd)


class TestMpichMultiCommand(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.hostfile = Hostfile(hosts=['localhost'], find_ips=False)

    def test_mpich_multi_command(self):
        """Test MPICH multi-command format"""
        exec_info = MpiExecInfo(
            nprocs=6,
            hostfile=self.hostfile,
            env={'MY_VAR': 'value'}
        )

        cmd_list = [
            {'cmd': 'cmd1', 'nprocs': 2},
            {'cmd': 'cmd2', 'nprocs': 4}
        ]

        mpi_exec = MpichExec(cmd_list, exec_info)
        cmd = mpi_exec.get_cmd()

        # Verify multi-command structure with ' : ' separator
        self.assertIn(':', cmd)
        self.assertIn('cmd1', cmd)
        self.assertIn('cmd2', cmd)
        self.assertIn('-env', cmd)  # MPICH env format for multi-command

    def test_mpich_ppn_option(self):
        """Test MPICH ppn option"""
        exec_info = MpiExecInfo(
            nprocs=8,
            ppn=4,
            hostfile=self.hostfile,
            env={}
        )

        mpi_exec = MpichExec('./myapp', exec_info)
        cmd = mpi_exec.get_cmd()

        # MPICH uses -ppn
        self.assertIn('-ppn', cmd)

    def test_mpich_hostfile_path(self):
        """Test MPICH with hostfile path"""
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.hosts') as f:
            f.write('host1\nhost2\n')
            hostfile_path = f.name

        try:
            hostfile = Hostfile(path=hostfile_path)
            exec_info = MpiExecInfo(
                nprocs=4,
                hostfile=hostfile,
                env={}
            )

            mpi_exec = MpichExec('./myapp', exec_info)
            cmd = mpi_exec.get_cmd()

            # Should include hostfile path
            self.assertIn('--hostfile', cmd)
            self.assertIn(hostfile_path, cmd)
        finally:
            os.unlink(hostfile_path)


class TestMpiExecFactory(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.hostfile = Hostfile(hosts=['localhost'], find_ips=False)

    @patch('jarvis_cd.shell.mpi_exec.MpiVersion')
    def test_factory_openmpi_detection(self, mock_mpi_version):
        """Test factory creates OpenMpiExec when OpenMPI is detected"""
        # Mock MPI version detection to return OpenMPI
        mock_version = Mock()
        mock_version.version = ExecType.OPENMPI
        mock_mpi_version.return_value = mock_version

        exec_info = MpiExecInfo(
            nprocs=2,
            hostfile=self.hostfile,
            env={}
        )

        mpi_exec = MpiExec('./myapp', exec_info)

        # Should be an instance of OpenMpiExec
        self.assertIsInstance(mpi_exec, OpenMpiExec)

    @patch('jarvis_cd.shell.mpi_exec.MpiVersion')
    def test_factory_mpich_detection(self, mock_mpi_version):
        """Test factory creates MpichExec when MPICH is detected"""
        # Mock MPI version detection to return MPICH
        mock_version = Mock()
        mock_version.version = ExecType.MPICH
        mock_mpi_version.return_value = mock_version

        exec_info = MpiExecInfo(
            nprocs=2,
            hostfile=self.hostfile,
            env={}
        )

        mpi_exec = MpiExec('./myapp', exec_info)

        # Should be an instance of MpichExec
        self.assertIsInstance(mpi_exec, MpichExec)

    @patch('jarvis_cd.shell.mpi_exec.MpiVersion')
    def test_factory_intel_mpi_detection(self, mock_mpi_version):
        """Test factory creates IntelMpiExec when Intel MPI is detected"""
        # Mock MPI version detection to return Intel MPI
        mock_version = Mock()
        mock_version.version = ExecType.INTEL_MPI
        mock_mpi_version.return_value = mock_version

        exec_info = MpiExecInfo(
            nprocs=2,
            hostfile=self.hostfile,
            env={}
        )

        mpi_exec = MpiExec('./myapp', exec_info)

        # Should be an instance of IntelMpiExec
        self.assertIsInstance(mpi_exec, IntelMpiExec)

    @patch('jarvis_cd.shell.mpi_exec.MpiVersion')
    def test_factory_cray_mpich_detection(self, mock_mpi_version):
        """Test factory creates CrayMpichExec when Cray MPICH is detected"""
        # Mock MPI version detection to return Cray MPICH
        mock_version = Mock()
        mock_version.version = ExecType.CRAY_MPICH
        mock_mpi_version.return_value = mock_version

        exec_info = MpiExecInfo(
            nprocs=2,
            hostfile=self.hostfile,
            env={}
        )

        mpi_exec = MpiExec('./myapp', exec_info)

        # Should be an instance of CrayMpichExec
        self.assertIsInstance(mpi_exec, CrayMpichExec)

    @patch('jarvis_cd.shell.mpi_exec.MpiVersion')
    def test_factory_unknown_mpi_defaults_to_mpich(self, mock_mpi_version):
        """Test factory defaults to MPICH for unknown MPI type"""
        # Mock MPI version detection to return unknown type
        mock_version = Mock()
        mock_version.version = ExecType.LOCAL  # Unknown MPI type
        mock_mpi_version.return_value = mock_version

        exec_info = MpiExecInfo(
            nprocs=2,
            hostfile=self.hostfile,
            env={}
        )

        # Capture print output
        import io
        import sys
        captured_output = io.StringIO()
        sys.stdout = captured_output

        mpi_exec = MpiExec('./myapp', exec_info)

        sys.stdout = sys.__stdout__

        # Should default to MpichExec
        self.assertIsInstance(mpi_exec, MpichExec)
        # Should print warning
        self.assertIn('Unknown MPI type', captured_output.getvalue())


class TestEmptyCmdList(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.hostfile = Hostfile(hosts=['localhost'], find_ips=False)

    def test_empty_cmd_list_error(self):
        """Test that empty command list raises ValueError"""
        exec_info = MpiExecInfo(
            nprocs=4,
            hostfile=self.hostfile,
            env={}
        )

        cmd_list = []

        with self.assertRaises(ValueError) as ctx:
            OpenMpiExec(cmd_list, exec_info)
        self.assertIn('empty', str(ctx.exception).lower())


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/shell/test_process.py`

```python
"""
Tests for process utility classes in jarvis_cd.shell.process
"""
import unittest
import sys
import os
import tempfile

# Add project root to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.shell.process import (
    Kill, KillAll, Which, Mkdir, Rm, Chmod, Sleep, Echo, GdbServer
)
from jarvis_cd.shell.exec_info import LocalExecInfo


class TestKill(unittest.TestCase):
    """Tests for Kill class"""

    def test_kill_with_partial(self):
        """Test kill command construction with partial matching"""
        # Just test command construction, don't run it
        kill = Kill('nonexistent_test_process_xyz', partial=True)
        cmd = kill.cmd
        self.assertIn('pkill', cmd)
        self.assertIn('-9', cmd)
        self.assertIn('-f', cmd)
        self.assertIn('nonexistent_test_process_xyz', cmd)

    def test_kill_without_partial(self):
        """Test kill command construction without partial matching"""
        # Just test command construction, don't run it
        kill = Kill('nonexistent_test_process_abc', partial=False)
        cmd = kill.cmd
        self.assertIn('pkill', cmd)
        self.assertIn('-9', cmd)
        self.assertNotIn('-f', cmd)
        self.assertIn('nonexistent_test_process_abc', cmd)

    def test_kill_with_exec_info(self):
        """Test kill with custom exec info"""
        exec_info = LocalExecInfo()
        kill = Kill('nonexistent_myprocess_123', exec_info=exec_info)
        # Just verify the object is created properly
        self.assertIsNotNone(kill.cmd)
        self.assertIn('pkill', kill.cmd)


class TestKillAll(unittest.TestCase):
    """Tests for KillAll class"""

    def test_killall_command(self):
        """Test killall command construction"""
        # Just test command construction, don't run it (dangerous!)
        killall = KillAll()
        cmd = killall.cmd
        self.assertIn('pkill', cmd)
        self.assertIn('-9', cmd)
        self.assertIn('-u', cmd)
        self.assertIn('$(whoami)', cmd)

    def test_killall_with_exec_info(self):
        """Test killall with custom exec info"""
        exec_info = LocalExecInfo()
        killall = KillAll(exec_info=exec_info)
        # Just verify object creation
        self.assertIsNotNone(killall.cmd)
        self.assertIn('pkill', killall.cmd)


class TestWhich(unittest.TestCase):
    """Tests for Which class"""

    def test_which_python(self):
        """Test finding python executable"""
        which = Which('python3')
        which.run()

        self.assertEqual(which.exit_code['localhost'], 0)
        self.assertTrue(which.exists())
        self.assertIn('python', which.get_path().lower())

    def test_which_nonexistent(self):
        """Test finding non-existent executable"""
        which = Which('nonexistent_executable_12345')
        which.run()

        self.assertFalse(which.exists())
        self.assertEqual(which.get_path(), '')

    def test_which_command_construction(self):
        """Test which command construction"""
        which = Which('bash')
        which.run()
        cmd = which.get_cmd()
        self.assertEqual(cmd, 'which bash')


class TestMkdir(unittest.TestCase):
    """Tests for Mkdir class"""

    def setUp(self):
        """Set up test directory"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_mkdir_')

    def tearDown(self):
        """Clean up test directory"""
        import shutil
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def test_mkdir_single_path(self):
        """Test creating single directory"""
        new_dir = os.path.join(self.test_dir, 'test_dir')
        mkdir = Mkdir(new_dir)
        mkdir.run()

        self.assertEqual(mkdir.exit_code['localhost'], 0)
        self.assertTrue(os.path.exists(new_dir))

    def test_mkdir_multiple_paths(self):
        """Test creating multiple directories"""
        paths = [
            os.path.join(self.test_dir, 'dir1'),
            os.path.join(self.test_dir, 'dir2'),
            os.path.join(self.test_dir, 'dir3')
        ]
        mkdir = Mkdir(paths)
        mkdir.run()

        self.assertEqual(mkdir.exit_code['localhost'], 0)
        for path in paths:
            self.assertTrue(os.path.exists(path))

    def test_mkdir_with_parents(self):
        """Test creating nested directories with parent flag"""
        nested_dir = os.path.join(self.test_dir, 'parent', 'child', 'grandchild')
        mkdir = Mkdir(nested_dir, parents=True)
        mkdir.run()

        self.assertEqual(mkdir.exit_code['localhost'], 0)
        self.assertTrue(os.path.exists(nested_dir))

    def test_mkdir_command_construction(self):
        """Test mkdir command construction"""
        mkdir = Mkdir('/tmp/test', parents=True)
        mkdir.run()
        cmd = mkdir.get_cmd()
        self.assertIn('mkdir', cmd)
        self.assertIn('-p', cmd)
        self.assertIn('/tmp/test', cmd)


class TestRm(unittest.TestCase):
    """Tests for Rm class"""

    def setUp(self):
        """Set up test files/directories"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_rm_')
        self.test_file = os.path.join(self.test_dir, 'test_file.txt')
        with open(self.test_file, 'w') as f:
            f.write('test content')

    def tearDown(self):
        """Clean up"""
        import shutil
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def test_rm_single_file(self):
        """Test removing single file"""
        rm = Rm(self.test_file)
        rm.run()

        self.assertEqual(rm.exit_code['localhost'], 0)
        self.assertFalse(os.path.exists(self.test_file))

    def test_rm_multiple_files(self):
        """Test removing multiple files"""
        file1 = os.path.join(self.test_dir, 'file1.txt')
        file2 = os.path.join(self.test_dir, 'file2.txt')

        with open(file1, 'w') as f:
            f.write('test1')
        with open(file2, 'w') as f:
            f.write('test2')

        rm = Rm([file1, file2])
        rm.run()

        self.assertEqual(rm.exit_code['localhost'], 0)
        self.assertFalse(os.path.exists(file1))
        self.assertFalse(os.path.exists(file2))

    def test_rm_directory_recursive(self):
        """Test removing directory recursively"""
        test_subdir = os.path.join(self.test_dir, 'subdir')
        os.makedirs(test_subdir)

        rm = Rm(test_subdir, recursive=True)
        rm.run()

        self.assertEqual(rm.exit_code['localhost'], 0)
        self.assertFalse(os.path.exists(test_subdir))

    def test_rm_command_construction(self):
        """Test rm command construction"""
        rm = Rm('/tmp/test', recursive=True, force=True)
        rm.run()
        cmd = rm.get_cmd()
        self.assertIn('rm', cmd)
        self.assertIn('-r', cmd)
        self.assertIn('-f', cmd)


class TestChmod(unittest.TestCase):
    """Tests for Chmod class"""

    def setUp(self):
        """Set up test file"""
        self.test_dir = tempfile.mkdtemp(prefix='jarvis_test_chmod_')
        self.test_file = os.path.join(self.test_dir, 'test_file.txt')
        with open(self.test_file, 'w') as f:
            f.write('test content')

    def tearDown(self):
        """Clean up"""
        import shutil
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def test_chmod_single_file(self):
        """Test changing permissions on single file"""
        chmod = Chmod(self.test_file, '755')
        chmod.run()

        self.assertEqual(chmod.exit_code['localhost'], 0)

        # Check permissions changed
        import stat
        st = os.stat(self.test_file)
        mode = st.st_mode
        self.assertTrue(mode & stat.S_IRUSR)  # User read
        self.assertTrue(mode & stat.S_IWUSR)  # User write
        self.assertTrue(mode & stat.S_IXUSR)  # User execute

    def test_chmod_multiple_files(self):
        """Test changing permissions on multiple files"""
        file1 = os.path.join(self.test_dir, 'file1.txt')
        file2 = os.path.join(self.test_dir, 'file2.txt')

        with open(file1, 'w') as f:
            f.write('test1')
        with open(file2, 'w') as f:
            f.write('test2')

        chmod = Chmod([file1, file2], '+x')
        chmod.run()

        self.assertEqual(chmod.exit_code['localhost'], 0)

    def test_chmod_command_construction(self):
        """Test chmod command construction"""
        chmod = Chmod('/tmp/test', '644', recursive=True)
        chmod.run()
        cmd = chmod.get_cmd()
        self.assertIn('chmod', cmd)
        self.assertIn('-R', cmd)
        self.assertIn('644', cmd)


class TestSleep(unittest.TestCase):
    """Tests for Sleep class"""

    def test_sleep_integer(self):
        """Test sleep with integer duration"""
        sleep = Sleep(1)
        sleep.run()
        cmd = sleep.get_cmd()
        self.assertEqual(cmd, 'sleep 1')
        self.assertEqual(sleep.exit_code['localhost'], 0)

    def test_sleep_float(self):
        """Test sleep with float duration"""
        sleep = Sleep(0.1)
        sleep.run()
        cmd = sleep.get_cmd()
        self.assertEqual(cmd, 'sleep 0.1')
        self.assertEqual(sleep.exit_code['localhost'], 0)

    def test_sleep_with_exec_info(self):
        """Test sleep with custom exec info"""
        exec_info = LocalExecInfo()
        sleep = Sleep(1, exec_info=exec_info)
        sleep.run()
        self.assertEqual(sleep.exit_code['localhost'], 0)


class TestEcho(unittest.TestCase):
    """Tests for Echo class"""

    def test_echo_simple_text(self):
        """Test echoing simple text"""
        echo = Echo("Hello World")
        echo.run()

        self.assertEqual(echo.exit_code['localhost'], 0)
        self.assertIn("Hello World", echo.stdout['localhost'])

    def test_echo_with_special_chars(self):
        """Test echoing text with special characters"""
        text = "Test: $VAR ${PATH} `date`"
        echo = Echo(text)
        echo.run()

        self.assertEqual(echo.exit_code['localhost'], 0)
        # The output will have shell expansion, just check command succeeds

    def test_echo_command_construction(self):
        """Test echo command construction"""
        echo = Echo("test message")
        echo.run()
        cmd = echo.get_cmd()
        self.assertIn('echo', cmd)
        self.assertIn('test message', cmd)


class TestGdbServer(unittest.TestCase):
    """Tests for GdbServer class"""

    def test_gdbserver_command_construction(self):
        """Test gdbserver command construction"""
        gdbserver = GdbServer("./myapp", 1234)
        # Don't run it since gdbserver may not be installed
        cmd = gdbserver.cmd
        self.assertIn('gdbserver', cmd)
        self.assertIn(':1234', cmd)
        self.assertIn('./myapp', cmd)
        self.assertEqual(gdbserver.port, 1234)

    def test_gdbserver_with_exec_info(self):
        """Test gdbserver with custom exec info"""
        exec_info = LocalExecInfo()
        gdbserver = GdbServer("/bin/true", 5555, exec_info=exec_info)
        cmd = gdbserver.cmd
        self.assertIn('gdbserver', cmd)
        self.assertIn(':5555', cmd)
        self.assertEqual(gdbserver.port, 5555)


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/shell/test_resource_graph_exec.py`

```python
"""
Tests for resource_graph_exec.py
"""
import unittest
import sys
import os
import tempfile
from pathlib import Path

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.shell.resource_graph_exec import ResourceGraphExec
from jarvis_cd.shell.exec_info import LocalExecInfo


class TestResourceGraphExec(unittest.TestCase):
    """Tests for ResourceGraphExec class"""

    def test_initialization(self):
        """Test ResourceGraphExec initialization"""
        exec_info = LocalExecInfo()
        # This will raise FileNotFoundError if script doesn't exist, which is expected
        try:
            rg_exec = ResourceGraphExec(exec_info, benchmark=True, duration=25)
            self.assertIsNotNone(rg_exec.cmd)
        except FileNotFoundError as e:
            # Expected if jarvis_resource_graph script doesn't exist
            self.assertIn('Resource graph script not found', str(e))

    def test_command_building_with_benchmark(self):
        """Test command building with benchmark enabled"""
        exec_info = LocalExecInfo()
        try:
            rg_exec = ResourceGraphExec(exec_info, benchmark=True, duration=30)
            cmd = rg_exec.get_cmd()
            self.assertIn('jarvis_resource_graph', cmd)
            self.assertIn('--duration', cmd)
            self.assertIn('30', cmd)
        except FileNotFoundError:
            pass  # Expected if script doesn't exist

    def test_command_building_without_benchmark(self):
        """Test command building with benchmark disabled"""
        exec_info = LocalExecInfo()
        try:
            rg_exec = ResourceGraphExec(exec_info, benchmark=False, duration=25)
            cmd = rg_exec.get_cmd()
            self.assertIn('jarvis_resource_graph', cmd)
            self.assertIn('--no-benchmark', cmd)
        except FileNotFoundError:
            pass  # Expected if script doesn't exist

    def test_custom_duration(self):
        """Test custom duration parameter"""
        exec_info = LocalExecInfo()
        try:
            rg_exec = ResourceGraphExec(exec_info, benchmark=True, duration=60)
            cmd = rg_exec.get_cmd()
            self.assertIn('--duration', cmd)
            self.assertIn('60', cmd)
        except FileNotFoundError:
            pass  # Expected


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/shell/test_scp_exec.py`

```python
import unittest
import sys
import os
import tempfile
from unittest.mock import Mock, patch, MagicMock

# Add the project root to the path so we can import jarvis_cd
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from jarvis_cd.shell.scp_exec import ScpExec, PscpExec, _Scp
from jarvis_cd.shell.exec_info import ScpExecInfo, PscpExecInfo
from jarvis_cd.util.hostfile import Hostfile


class TestScpExec(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.test_file = tempfile.NamedTemporaryFile(delete=False)
        self.test_file.write(b"test content")
        self.test_file.close()
        self.hostfile = Hostfile(hosts=['testhost'], find_ips=False)

    def tearDown(self):
        """Clean up test files"""
        if os.path.exists(self.test_file.name):
            os.unlink(self.test_file.name)

    def test_scp_single_path(self):
        """Test SCP with single path"""
        exec_info = ScpExecInfo(hostfile=self.hostfile, exec_async=True)
        scp_exec = ScpExec(self.test_file.name, exec_info)

        self.assertEqual(len(scp_exec.scp_nodes), 1)
        cmd = scp_exec.get_cmd()
        self.assertIn('scp', cmd)
        self.assertIn(self.test_file.name, cmd)

    def test_scp_multiple_paths(self):
        """Test SCP with multiple paths"""
        exec_info = ScpExecInfo(hostfile=self.hostfile, exec_async=True)
        paths = [self.test_file.name, '/tmp/file2.txt']
        scp_exec = ScpExec(paths, exec_info)

        self.assertEqual(len(scp_exec.scp_nodes), 2)

    def test_scp_tuple_paths(self):
        """Test SCP with tuple paths (different src and dst)"""
        exec_info = ScpExecInfo(hostfile=self.hostfile, exec_async=True)
        paths = [(self.test_file.name, '/tmp/remote_file.txt')]
        scp_exec = ScpExec(paths, exec_info)

        self.assertEqual(len(scp_exec.scp_nodes), 1)
        cmd = scp_exec.get_cmd()
        self.assertIn('scp', cmd)

    def test_scp_requires_hostfile(self):
        """Test that SCP requires a hostfile"""
        exec_info = ScpExecInfo(hostfile=None, exec_async=True)

        with self.assertRaises(ValueError):
            ScpExec(self.test_file.name, exec_info)

    def test_scp_empty_paths_list(self):
        """Test that SCP requires at least one path"""
        exec_info = ScpExecInfo(hostfile=self.hostfile, exec_async=True)

        with self.assertRaises(ValueError):
            ScpExec([], exec_info)


class TestInternalScp(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.test_file = tempfile.NamedTemporaryFile(delete=False)
        self.test_file.write(b"test content")
        self.test_file.close()
        self.hostfile = Hostfile(hosts=['testhost'], find_ips=False)

    def tearDown(self):
        """Clean up test files"""
        if os.path.exists(self.test_file.name):
            os.unlink(self.test_file.name)

    def test_rsync_basic_command(self):
        """Test basic rsync command construction"""
        exec_info = ScpExecInfo(hostfile=self.hostfile, exec_async=True)
        scp = _Scp(self.test_file.name, '/tmp/remote.txt', exec_info)

        cmd = scp.get_cmd()
        self.assertIn('rsync', cmd)
        self.assertIn(self.test_file.name, cmd)
        self.assertIn('testhost', cmd)

    def test_rsync_with_user(self):
        """Test rsync command with user"""
        exec_info = ScpExecInfo(
            user='testuser',
            hostfile=self.hostfile,
            exec_async=True
        )
        scp = _Scp(self.test_file.name, '/tmp/remote.txt', exec_info)

        cmd = scp.get_cmd()
        self.assertIn('testuser@testhost', cmd)

    def test_rsync_with_port(self):
        """Test rsync command with custom port"""
        exec_info = ScpExecInfo(
            port=2222,
            hostfile=self.hostfile,
            exec_async=True
        )
        scp = _Scp(self.test_file.name, '/tmp/remote.txt', exec_info)

        cmd = scp.get_cmd()
        self.assertIn('-p 2222', cmd)

    def test_rsync_with_pkey(self):
        """Test rsync command with private key"""
        exec_info = ScpExecInfo(
            pkey='/path/to/key.pem',
            hostfile=self.hostfile,
            exec_async=True
        )
        scp = _Scp(self.test_file.name, '/tmp/remote.txt', exec_info)

        cmd = scp.get_cmd()
        self.assertIn('-i /path/to/key.pem', cmd)

    def test_rsync_localhost_no_copy_same_path(self):
        """Test that rsync to localhost with same path does nothing"""
        localhost_hostfile = Hostfile(hosts=['localhost'], find_ips=False)
        exec_info = ScpExecInfo(hostfile=localhost_hostfile, exec_async=True)
        scp = _Scp(self.test_file.name, self.test_file.name, exec_info)

        # Should execute 'true' (no-op)
        cmd = scp.get_cmd()
        self.assertEqual(cmd, 'true')

    def test_rsync_localhost_copy_different_path(self):
        """Test that rsync to localhost with different path uses cp"""
        localhost_hostfile = Hostfile(hosts=['localhost'], find_ips=False)
        exec_info = ScpExecInfo(hostfile=localhost_hostfile, exec_async=True)
        scp = _Scp(self.test_file.name, '/tmp/copy.txt', exec_info)

        cmd = scp.get_cmd()
        self.assertIn('cp -r', cmd)
        self.assertNotIn('rsync', cmd)

    def test_rsync_combined_options(self):
        """Test rsync with multiple options"""
        exec_info = ScpExecInfo(
            user='testuser',
            port=2222,
            pkey='/path/to/key.pem',
            hostfile=self.hostfile,
            exec_async=True
        )
        scp = _Scp(self.test_file.name, '/tmp/remote.txt', exec_info)

        cmd = scp.get_cmd()
        self.assertIn('rsync', cmd)
        self.assertIn('testuser@testhost', cmd)
        self.assertIn('-p 2222', cmd)
        self.assertIn('-i /path/to/key.pem', cmd)


class TestPscpExec(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.test_file = tempfile.NamedTemporaryFile(delete=False)
        self.test_file.write(b"test content")
        self.test_file.close()
        self.multi_host = Hostfile(hosts=['host1', 'host2', 'host3'], find_ips=False)

    def tearDown(self):
        """Clean up test files"""
        if os.path.exists(self.test_file.name):
            os.unlink(self.test_file.name)

    def test_pscp_requires_hostfile(self):
        """Test that PSCP requires a hostfile"""
        exec_info = PscpExecInfo(hostfile=None, exec_async=True)

        with self.assertRaises(ValueError):
            PscpExec(self.test_file.name, exec_info)

    def test_pscp_empty_hostfile(self):
        """Test that PSCP requires non-empty hostfile"""
        empty_hostfile = Hostfile(hosts=[], find_ips=False)
        exec_info = PscpExecInfo(hostfile=empty_hostfile, exec_async=True)

        with self.assertRaises(ValueError):
            PscpExec(self.test_file.name, exec_info)

    @patch('jarvis_cd.shell.scp_exec.ScpExec')
    def test_pscp_creates_scp_for_each_host(self, mock_scp_exec):
        """Test that PSCP creates SCP executor for each host"""
        exec_info = PscpExecInfo(hostfile=self.multi_host, exec_async=True)

        # Mock ScpExec to avoid actual execution
        mock_instances = []
        for i in range(3):
            mock_instance = MagicMock()
            mock_instance.wait_all_scp.return_value = {'localhost': 0}
            mock_instance.stdout = {'localhost': ''}
            mock_instance.stderr = {'localhost': ''}
            mock_instances.append(mock_instance)

        mock_scp_exec.side_effect = mock_instances

        pscp_exec = PscpExec(self.test_file.name, exec_info)

        # Should create 3 SCP executors, one for each host
        self.assertEqual(mock_scp_exec.call_count, 3)

    @patch('jarvis_cd.shell.scp_exec.ScpExec')
    def test_pscp_parallel_execution(self, mock_scp_exec):
        """Test that PSCP executes on all hosts in parallel"""
        exec_info = PscpExecInfo(hostfile=self.multi_host, exec_async=True)

        # Create mock instances for each host
        mock_instances = []
        for hostname in self.multi_host.hosts:
            mock_instance = MagicMock()
            mock_instance.wait_all_scp.return_value = {'localhost': 0}
            mock_instance.stdout = {'localhost': ''}
            mock_instance.stderr = {'localhost': ''}
            mock_instances.append(mock_instance)

        mock_scp_exec.side_effect = mock_instances

        pscp_exec = PscpExec(self.test_file.name, exec_info)

        # Verify SCP was created for each host
        self.assertEqual(mock_scp_exec.call_count, 3)

        # Check that each call had the correct single-host hostfile
        for i, call in enumerate(mock_scp_exec.call_args_list):
            scp_info = call[0][1]
            self.assertEqual(len(scp_info.hostfile.hosts), 1)
            self.assertIn(scp_info.hostfile.hosts[0], self.multi_host.hosts)

    def test_pscp_get_cmd(self):
        """Test that PSCP get_cmd returns description"""
        exec_info = PscpExecInfo(hostfile=self.multi_host, exec_async=True)

        with patch('jarvis_cd.shell.scp_exec.ScpExec'):
            pscp_exec = PscpExec(self.test_file.name, exec_info)
            cmd = pscp_exec.get_cmd()
            self.assertIn('pscp', cmd)
            self.assertIn('3 hosts', cmd)


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/shell/test_ssh_exec.py`

```python
import unittest
import sys
import os
from unittest.mock import Mock, patch, MagicMock

# Add the project root to the path so we can import jarvis_cd
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from jarvis_cd.shell.ssh_exec import SshExec, PsshExec
from jarvis_cd.shell.exec_info import SshExecInfo, PsshExecInfo
from jarvis_cd.util.hostfile import Hostfile


class TestSshExec(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.test_binary = os.path.join(os.path.dirname(__file__), 'test_env_checker')
        self.hostfile = Hostfile(hosts=['testhost'], find_ips=False)

    def test_basic_ssh_command(self):
        """Test basic SSH command construction"""
        exec_info = SshExecInfo(hostfile=self.hostfile, exec_async=True)
        ssh_exec = SshExec('echo "hello"', exec_info)

        cmd = ssh_exec.get_cmd()
        self.assertIn('ssh', cmd)
        self.assertIn('testhost', cmd)
        self.assertIn('echo "hello"', cmd)

    def test_ssh_with_user(self):
        """Test SSH command with user"""
        exec_info = SshExecInfo(
            user='testuser',
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec('echo "test"', exec_info)

        cmd = ssh_exec.get_cmd()
        self.assertIn('testuser@testhost', cmd)

    def test_ssh_with_port(self):
        """Test SSH command with custom port"""
        exec_info = SshExecInfo(
            port=2222,
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec('echo "test"', exec_info)

        cmd = ssh_exec.get_cmd()
        self.assertIn('-p 2222', cmd)

    def test_ssh_with_pkey(self):
        """Test SSH command with private key"""
        exec_info = SshExecInfo(
            pkey='/path/to/key.pem',
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec('echo "test"', exec_info)

        cmd = ssh_exec.get_cmd()
        self.assertIn('-i /path/to/key.pem', cmd)

    def test_ssh_strict_mode(self):
        """Test SSH with strict host key checking"""
        exec_info = SshExecInfo(
            strict_ssh=True,
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec('echo "test"', exec_info)

        cmd = ssh_exec.get_cmd()
        # Should NOT contain StrictHostKeyChecking=no
        self.assertNotIn('StrictHostKeyChecking=no', cmd)

    def test_ssh_non_strict_mode(self):
        """Test SSH with non-strict host key checking"""
        exec_info = SshExecInfo(
            strict_ssh=False,
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec('echo "test"', exec_info)

        cmd = ssh_exec.get_cmd()
        self.assertIn('StrictHostKeyChecking=no', cmd)

    def test_ssh_with_single_env_variable(self):
        """Test SSH command with single environment variable"""
        exec_info = SshExecInfo(
            env={'TEST_VAR': 'test_value'},
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec(self.test_binary + ' TEST_VAR', exec_info)

        cmd = ssh_exec.get_cmd()
        self.assertIn('TEST_VAR', cmd)
        self.assertIn('test_value', cmd)

    def test_ssh_with_multiple_env_variables(self):
        """Test SSH command with multiple environment variables"""
        exec_info = SshExecInfo(
            env={
                'VAR1': 'value1',
                'VAR2': 'value2',
                'VAR3': 'value3'
            },
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec(self.test_binary + ' VAR1 VAR2 VAR3', exec_info)

        cmd = ssh_exec.get_cmd()
        self.assertIn('VAR1', cmd)
        self.assertIn('value1', cmd)
        self.assertIn('VAR2', cmd)
        self.assertIn('value2', cmd)
        self.assertIn('VAR3', cmd)
        self.assertIn('value3', cmd)

    def test_ssh_env_with_special_characters(self):
        """Test SSH environment variables with special characters"""
        exec_info = SshExecInfo(
            env={'SPECIAL_VAR': "value with 'quotes'"},
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec('echo $SPECIAL_VAR', exec_info)

        cmd = ssh_exec.get_cmd()
        # Should properly escape quotes
        self.assertIn('SPECIAL_VAR', cmd)

    def test_ssh_with_cwd(self):
        """Test SSH command with working directory change"""
        exec_info = SshExecInfo(
            cwd='/tmp/test',
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec('pwd', exec_info)

        cmd = ssh_exec.get_cmd()
        self.assertIn('cd /tmp/test', cmd)

    def test_ssh_with_sudo(self):
        """Test SSH command with sudo"""
        exec_info = SshExecInfo(
            sudo=True,
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec('whoami', exec_info)

        cmd = ssh_exec.get_cmd()
        self.assertIn('sudo', cmd)

    def test_ssh_with_sudo_and_env(self):
        """Test SSH command with sudo preserving environment"""
        exec_info = SshExecInfo(
            sudo=True,
            sudoenv=True,
            env={'TEST_VAR': 'value'},
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec('echo $TEST_VAR', exec_info)

        cmd = ssh_exec.get_cmd()
        self.assertIn('sudo -E', cmd)
        self.assertIn('TEST_VAR', cmd)

    def test_ssh_with_sudo_no_env(self):
        """Test SSH command with sudo not preserving environment"""
        exec_info = SshExecInfo(
            sudo=True,
            sudoenv=False,
            env={'TEST_VAR': 'value'},
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec('echo $TEST_VAR', exec_info)

        cmd = ssh_exec.get_cmd()
        # Should have 'sudo' but not 'sudo -E'
        self.assertIn('sudo', cmd)
        self.assertNotIn('sudo -E', cmd)

    def test_ssh_with_timeout(self):
        """Test SSH command with connection timeout"""
        exec_info = SshExecInfo(
            timeout=30,
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec('echo "test"', exec_info)

        cmd = ssh_exec.get_cmd()
        self.assertIn('ConnectTimeout=30', cmd)

    def test_ssh_hostname_override(self):
        """Test SSH execution with explicit hostname override"""
        multi_host = Hostfile(hosts=['host1', 'host2'], find_ips=False)
        exec_info = SshExecInfo(hostfile=multi_host, exec_async=True)
        ssh_exec = SshExec('echo "test"', exec_info, hostname='host2')

        cmd = ssh_exec.get_cmd()
        self.assertIn('host2', cmd)
        self.assertNotIn('host1', cmd)

    def test_ssh_env_numeric_values(self):
        """Test SSH environment variables with numeric values"""
        exec_info = SshExecInfo(
            env={
                'INT_VAR': 42,
                'FLOAT_VAR': 3.14
            },
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec(self.test_binary + ' INT_VAR FLOAT_VAR', exec_info)

        cmd = ssh_exec.get_cmd()
        self.assertIn('INT_VAR', cmd)
        self.assertIn('42', cmd)
        self.assertIn('FLOAT_VAR', cmd)
        self.assertIn('3.14', cmd)

    def test_ssh_combined_options(self):
        """Test SSH with multiple options combined"""
        exec_info = SshExecInfo(
            user='testuser',
            port=2222,
            pkey='/path/to/key',
            cwd='/tmp',
            env={'VAR1': 'value1', 'VAR2': 'value2'},
            sudo=True,
            sudoenv=True,
            hostfile=self.hostfile,
            exec_async=True
        )
        ssh_exec = SshExec('echo "test"', exec_info)

        cmd = ssh_exec.get_cmd()
        self.assertIn('testuser@testhost', cmd)
        self.assertIn('-p 2222', cmd)
        self.assertIn('-i /path/to/key', cmd)
        self.assertIn('cd /tmp', cmd)
        self.assertIn('VAR1', cmd)
        self.assertIn('VAR2', cmd)
        self.assertIn('sudo -E', cmd)


class TestPsshExec(unittest.TestCase):

    def setUp(self):
        """Set up test fixtures"""
        self.test_binary = os.path.join(os.path.dirname(__file__), 'test_env_checker')
        self.multi_host = Hostfile(hosts=['host1', 'host2', 'host3'], find_ips=False)

    def test_pssh_requires_hostfile(self):
        """Test that PSSH requires a hostfile"""
        exec_info = PsshExecInfo(hostfile=None, exec_async=True)

        with self.assertRaises(ValueError):
            PsshExec('echo "test"', exec_info)

    def test_pssh_empty_hostfile(self):
        """Test that PSSH requires non-empty hostfile"""
        empty_hostfile = Hostfile(hosts=[], find_ips=False)
        exec_info = PsshExecInfo(hostfile=empty_hostfile, exec_async=True)

        with self.assertRaises(ValueError):
            PsshExec('echo "test"', exec_info)

    @patch('jarvis_cd.shell.ssh_exec.SshExec')
    def test_pssh_creates_ssh_for_each_host(self, mock_ssh_exec):
        """Test that PSSH creates SSH executor for each host"""
        exec_info = PsshExecInfo(hostfile=self.multi_host, exec_async=True)

        # Mock SshExec to avoid actual execution
        mock_instances = []
        for i in range(3):
            mock_instance = MagicMock()
            mock_instance.processes = {f'host{i+1}': MagicMock()}
            mock_instances.append(mock_instance)

        mock_ssh_exec.side_effect = mock_instances

        pssh_exec = PsshExec('echo "test"', exec_info)

        # Should create 3 SSH executors, one for each host
        self.assertEqual(mock_ssh_exec.call_count, 3)

    @patch('jarvis_cd.shell.ssh_exec.SshExec')
    def test_pssh_env_forwarding(self, mock_ssh_exec):
        """Test that PSSH forwards environment variables to SSH executors"""
        exec_info = PsshExecInfo(
            env={'TEST_VAR': 'test_value'},
            hostfile=self.multi_host,
            exec_async=True
        )

        # Mock SshExec
        mock_instance = MagicMock()
        mock_instance.processes = {'host1': MagicMock()}
        mock_ssh_exec.return_value = mock_instance

        pssh_exec = PsshExec('echo $TEST_VAR', exec_info)

        # Check that environment was passed to SSH executors
        for call in mock_ssh_exec.call_args_list:
            ssh_info = call[0][1]
            self.assertIn('TEST_VAR', ssh_info.env)
            self.assertEqual(ssh_info.env['TEST_VAR'], 'test_value')

    @patch('jarvis_cd.shell.ssh_exec.SshExec')
    def test_pssh_parallel_execution(self, mock_ssh_exec):
        """Test that PSSH executes on all hosts in parallel"""
        exec_info = PsshExecInfo(hostfile=self.multi_host, exec_async=True)

        # Create mock instances for each host
        mock_instances = []
        for hostname in self.multi_host.hosts:
            mock_instance = MagicMock()
            mock_instance.processes = {hostname: MagicMock()}
            mock_instances.append(mock_instance)

        mock_ssh_exec.side_effect = mock_instances

        pssh_exec = PsshExec('echo "test"', exec_info)

        # Verify SSH was created for each host with correct hostname
        self.assertEqual(mock_ssh_exec.call_count, 3)

        # Extract hostnames from call args (positional arg 2 or keyword arg 'hostname')
        call_hostnames = []
        for call in mock_ssh_exec.call_args_list:
            args, kwargs = call
            if 'hostname' in kwargs:
                call_hostnames.append(kwargs['hostname'])
            elif len(args) > 2:
                call_hostnames.append(args[2])

        self.assertIn('host1', call_hostnames)
        self.assertIn('host2', call_hostnames)
        self.assertIn('host3', call_hostnames)

    def test_pssh_get_cmd(self):
        """Test that PSSH get_cmd returns original command"""
        exec_info = PsshExecInfo(hostfile=self.multi_host, exec_async=True)

        with patch('jarvis_cd.shell.ssh_exec.SshExec'):
            pssh_exec = PsshExec('echo "test"', exec_info)
            self.assertEqual(pssh_exec.get_cmd(), 'echo "test"')


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/util/__init__.py`

```python

```

### `test/unit/util/test_argparse.py`

```python
import unittest
import sys
import os

# Add the project root to the path so we can import jarvis_cd
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from jarvis_cd.util.argparse import ArgParse


class MyAppArgParse(ArgParse):
    def define_options(self):
        self.add_menu('')
        self.add_cmd('', keep_remainder=True)
        self.add_args([
            {
                'name': 'hi',
                'msg': 'hello',
                'type': str,
                'default': None
            }
        ])

        self.add_menu('vpic', msg="The VPIC application")
        self.add_cmd('vpic run',
                      keep_remainder=False,
                      aliases=['vpic r', 'vpic runner'])
        self.add_args([
            {
                'name': 'steps',
                'msg': 'Number of checkpoints',
                'type': int,
                'required': True,
                'pos': True,
                'class': 'sim',
                'rank': 0
            },
            {
                'name': 'x',
                'msg': 'The length of the x-axis',
                'type': int,
                'required': False,
                'default': 256,
                'pos': True,
                'class': 'sim',
                'rank': 1
            },
            {
                'name': 'do_io',
                'msg': 'Whether to perform I/O or not',
                'type': bool,
                'required': False,
                'default': False,
                'pos': True,
            },
            {
                'name': 'make_figures',
                'msg': 'Whether to make a figure',
                'type': bool,
                'default': False,
            },
            {
                'name': 'data_size',
                'msg': 'Total amount of data to produce',
                'type': int,
                'default': 1024,
            },
            {
                'name': 'hosts',
                'msg': 'A list of hosts',
                'type': list,
                'args': [
                    {
                        'name': 'host',
                        'msg': 'A string representing a host',
                        'type': str,
                    }
                ],
                'aliases': ['x']
            },
            {
                'name': 'devices',
                'msg': 'A list of devices and counts',
                'type': list,
                'aliases': ['d'],
                'args': [
                    {
                        'name': 'path',
                        'msg': 'The mount point of device',
                        'type': str,
                    },
                    {
                        'name': 'count',
                        'msg': 'The number of devices to search for',
                        'type': int,
                    }
                ]
            }
        ])

    def main_menu(self):
        pass
        
    def vpic_run(self):
        pass


class TestArgParse(unittest.TestCase):
    
    def setUp(self):
        self.parser = MyAppArgParse()
        self.parser.define_options()
    
    def test_empty_menu_with_remainder(self):
        """Test: my_app hi="hi" rem1 rem2 rem3"""
        args = ['hi=hi', 'rem1', 'rem2', 'rem3']
        result = self.parser.parse(args)
        
        self.assertEqual(self.parser.kwargs['hi'], 'hi')
        self.assertEqual(self.parser.remainder, ['rem1', 'rem2', 'rem3'])
    
    def test_vpic_run_basic(self):
        """Test basic vpic run command with required positional args"""
        args = ['vpic', 'run', '10']
        result = self.parser.parse(args)
        
        self.assertEqual(self.parser.kwargs['steps'], 10)
        self.assertEqual(self.parser.kwargs['x'], 256)  # default value
        self.assertEqual(self.parser.kwargs['do_io'], False)  # default value
        
    def test_vpic_run_with_positional_args(self):
        """Test vpic run with multiple positional arguments"""
        args = ['vpic', 'run', '10', '512', 'true']
        result = self.parser.parse(args)
        
        self.assertEqual(self.parser.kwargs['steps'], 10)
        self.assertEqual(self.parser.kwargs['x'], 512)
        self.assertEqual(self.parser.kwargs['do_io'], True)
        
    def test_vpic_run_with_keyword_args(self):
        """Test vpic run with keyword arguments"""
        args = ['vpic', 'run', '10', '--make_figures=true', '--data_size=2048']
        result = self.parser.parse(args)
        
        self.assertEqual(self.parser.kwargs['steps'], 10)
        self.assertEqual(self.parser.kwargs['make_figures'], True)
        self.assertEqual(self.parser.kwargs['data_size'], 2048)
        
    def test_list_args_set_mode(self):
        """Test: my_app vpic run 1 --devices="[(/mnt/home, 5), (/mnt/home2, 6)]" """
        args = ['vpic', 'run', '1', '--devices=[("/mnt/home", 5), ("/mnt/home2", 6)]']
        result = self.parser.parse(args)
        
        expected_devices = [
            {'path': '/mnt/home', 'count': 5},
            {'path': '/mnt/home2', 'count': 6}
        ]
        self.assertEqual(self.parser.kwargs['devices'], expected_devices)
        
    def test_list_args_append_mode(self):
        """Test: my_app vpic run 1 --d "(/mnt/home, 5)" --d "(/mnt/home2, 6)" """
        args = ['vpic', 'run', '1', '--d', '("/mnt/home", 5)', '--d', '("/mnt/home2", 6)']
        result = self.parser.parse(args)
        
        expected_devices = [
            {'path': '/mnt/home', 'count': 5},
            {'path': '/mnt/home2', 'count': 6}
        ]
        self.assertEqual(self.parser.kwargs['devices'], expected_devices)
        
    def test_short_options(self):
        """Test short option aliases"""
        args = ['vpic', 'run', '1', '-d', '("/mnt/home", 5)']
        result = self.parser.parse(args)
        
        expected_devices = [{'path': '/mnt/home', 'count': 5}]
        self.assertEqual(self.parser.kwargs['devices'], expected_devices)
        
    def test_command_aliases(self):
        """Test command aliases work"""
        args = ['vpic', 'r', '10']
        result = self.parser.parse(args)
        
        self.assertEqual(self.parser.kwargs['steps'], 10)
        
        # Test another alias
        args = ['vpic', 'runner', '20']
        result = self.parser.parse(args)
        
        self.assertEqual(self.parser.kwargs['steps'], 20)
        
    def test_required_argument_missing(self):
        """Test that missing required arguments raise an error"""
        args = ['vpic', 'run']  # missing required 'steps' argument

        with self.assertRaises(SystemExit):
            self.parser.parse(args)
        
    def test_type_casting(self):
        """Test various type casting"""
        args = ['vpic', 'run', '10', '512', 'false', '--make_figures=true', '--data_size=4096']
        result = self.parser.parse(args)
        
        self.assertEqual(type(self.parser.kwargs['steps']), int)
        self.assertEqual(type(self.parser.kwargs['x']), int)
        self.assertEqual(type(self.parser.kwargs['do_io']), bool)
        self.assertEqual(type(self.parser.kwargs['make_figures']), bool)
        self.assertEqual(type(self.parser.kwargs['data_size']), int)
        
        self.assertEqual(self.parser.kwargs['steps'], 10)
        self.assertEqual(self.parser.kwargs['x'], 512)
        self.assertEqual(self.parser.kwargs['do_io'], False)
        self.assertEqual(self.parser.kwargs['make_figures'], True)
        self.assertEqual(self.parser.kwargs['data_size'], 4096)
        
    def test_argument_ranking(self):
        """Test that arguments are processed in class/rank order"""
        args = ['vpic', 'run', '10', '512']
        result = self.parser.parse(args)
        
        # steps (class='sim', rank=0) should be filled first
        # x (class='sim', rank=1) should be filled second
        self.assertEqual(self.parser.kwargs['steps'], 10)
        self.assertEqual(self.parser.kwargs['x'], 512)
        
    def test_empty_command_defaults(self):
        """Test empty command with no arguments"""
        args = []
        result = self.parser.parse(args)
        
        # Should use default value for 'hi'
        self.assertEqual(self.parser.kwargs.get('hi'), None)
        self.assertEqual(self.parser.remainder, [])
        
    def test_invalid_menu_command(self):
        """Test behavior with invalid menu commands"""
        # This test might print error messages, but shouldn't crash
        args = ['vpic', 'invalid_command']
        result = self.parser.parse(args)
        
        # Should not crash and return empty kwargs
        self.assertEqual(result, {})


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/util/test_argparse_complete.py`

```python
"""
Complete argparse tests for 100% coverage
"""
import unittest
import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.util.argparse import ArgParse


class TestArgParseEdgeCases(unittest.TestCase):
    """Tests for ArgParse edge cases and uncovered lines"""

    def test_add_args_no_command_error(self):
        """Test add_args raises error when no command exists"""
        parser = ArgParse()
        with self.assertRaises(ValueError) as context:
            parser.add_args([{'name': 'test', 'type': str}])
        self.assertIn("No command", str(context.exception))

    def test_add_args_with_only_aliases(self):
        """Test add_args when only aliases exist"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test menu")
        parser.add_cmd('test cmd', msg="Test", aliases=['tc'])
        parser.add_args([{'name': 'arg1', 'type': str, 'pos': True, 'required': True}])

        # Should not crash
        self.assertIn('test cmd', parser.command_args)

    def test_parse_list_value_with_quotes(self):
        """Test _parse_list_value with quoted strings"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {'name': 'items', 'type': list, 'default': []}
        ])

        # Test with double quotes - the parser removes quotes but treats as single value
        args = ['test', 'cmd', '--items="a,b,c"']
        parser.parse(args)
        # When quotes are removed, it's treated as comma-separated
        self.assertIsInstance(parser.kwargs['items'], list)

    def test_parse_list_value_with_single_quotes(self):
        """Test _parse_list_value with single quotes"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {'name': 'items', 'type': list, 'default': []}
        ])

        args = ["test", "cmd", "--items='x,y,z'"]
        parser.parse(args)
        # Single quotes are handled by the parser
        self.assertIsInstance(parser.kwargs['items'], list)

    def test_parse_list_with_python_notation(self):
        """Test parsing list with Python notation"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {'name': 'items', 'type': list, 'default': []}
        ])

        args = ['test', 'cmd', '--items=[1,2,3]']
        parser.parse(args)
        # Should parse as list
        self.assertIsInstance(parser.kwargs['items'], list)

    def test_parse_list_with_tuple_args(self):
        """Test parsing list with tuple elements and args definition"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {
                'name': 'pairs',
                'type': list,
                'args': [
                    {'name': 'key', 'type': str},
                    {'name': 'value', 'type': int}
                ]
            }
        ])

        args = ['test', 'cmd', "--pairs=[(a,1),(b,2)]"]
        parser.parse(args)
        # Should convert tuples to dicts
        self.assertIsInstance(parser.kwargs['pairs'], list)

    def test_parse_dict_with_nested_args(self):
        """Test parsing dict with nested args definition"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {
                'name': 'config',
                'type': dict,
                'args': [
                    {'name': 'host', 'type': str},
                    {'name': 'port', 'type': int}
                ]
            }
        ])

        # Use simpler dict notation that works
        args = ['test', 'cmd', '--config=host:localhost,port:8080']
        parser.parse(args)
        result = parser.kwargs['config']
        self.assertIsInstance(result, dict)
        self.assertIn('host', result)
        self.assertIn('port', result)

    def test_cast_value_dict_failure(self):
        """Test _cast_value dict parsing failure"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {'name': 'config', 'type': dict}
        ])

        # Invalid dict string
        args = ['test', 'cmd', '--config=invalid{dict}']
        parser.parse(args)
        # Should return the value as-is when parsing fails
        self.assertIn('config', parser.kwargs)

    def test_convert_list_items_with_dict_items(self):
        """Test _convert_list_items with dict items"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {
                'name': 'items',
                'type': list,
                'args': [
                    {'name': 'id', 'type': int},
                    {'name': 'name', 'type': str}
                ]
            }
        ])

        args = ['test', 'cmd', '--items=[{"id":"1","name":"foo"},{"id":"2","name":"bar"}]']
        parser.parse(args)
        items = parser.kwargs['items']
        self.assertEqual(items[0]['id'], 1)  # Should be int

    def test_convert_list_items_single_value_with_args(self):
        """Test _convert_list_items with single value and args def"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {
                'name': 'nums',
                'type': list,
                'args': [{'name': 'value', 'type': int}]
            }
        ])

        args = ['test', 'cmd', '--nums=["1","2","3"]']
        parser.parse(args)
        # Should convert string values to int
        self.assertEqual(parser.kwargs['nums'], [1, 2, 3])

    def test_boolean_arg_plus_format(self):
        """Test boolean argument with + prefix"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {'name': 'verbose', 'type': bool, 'default': False}
        ])

        args = ['test', 'cmd', '+verbose']
        parser.parse(args)
        self.assertTrue(parser.kwargs['verbose'])

    def test_boolean_arg_minus_format(self):
        """Test boolean argument with - prefix"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {'name': 'debug', 'type': bool, 'default': True}
        ])

        args = ['test', 'cmd', '-debug']
        parser.parse(args)
        self.assertFalse(parser.kwargs['debug'])

    def test_positional_after_non_boolean_plus(self):
        """Test positional argument after non-boolean + arg"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {'name': 'file', 'type': str, 'pos': True, 'required': True}
        ])

        # +something that's not a boolean arg should be treated as positional
        args = ['test', 'cmd', '+notabool']
        parser.parse(args)
        self.assertEqual(parser.kwargs['file'], '+notabool')

    def test_remainder_with_keep_remainder_false(self):
        """Test remainder handling when keep_remainder is False"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test", keep_remainder=False)
        parser.add_args([
            {'name': 'arg1', 'type': str, 'pos': True, 'required': True}
        ])

        # Extra args when keep_remainder=False should be ignored
        args = ['test', 'cmd', 'value1', 'extra', 'args']
        parser.parse(args)
        self.assertEqual(parser.kwargs['arg1'], 'value1')
        # Remainder should be empty since keep_remainder=False
        self.assertEqual(len(parser.remainder), 0)

    def test_negative_number_as_value(self):
        """Test negative number is not treated as flag"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {'name': 'value', 'type': int, 'pos': True, 'required': True}
        ])

        args = ['test', 'cmd', '-42']
        parser.parse(args)
        self.assertEqual(parser.kwargs['value'], -42)

    def test_menu_help(self):
        """Test menu help printing"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test menu")
        parser.add_cmd('test cmd1', msg="Command 1")
        parser.add_cmd('test cmd2', msg="Command 2")

        # Should not crash
        import io
        old_stdout = sys.stdout
        sys.stdout = io.StringIO()
        try:
            parser.print_menu_help('test')
            output = sys.stdout.getvalue()
            self.assertIn('test', output.lower())
        finally:
            sys.stdout = old_stdout

    def test_command_help(self):
        """Test command help printing"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test menu")
        parser.add_cmd('test cmd', msg="Command")
        parser.add_args([
            {'name': 'arg1', 'type': str, 'msg': 'First argument', 'required': True},
            {'name': 'arg2', 'type': int, 'msg': 'Second argument', 'default': 10}
        ])

        import io
        old_stdout = sys.stdout
        sys.stdout = io.StringIO()
        try:
            parser.print_command_help('test cmd')
            output = sys.stdout.getvalue()
            self.assertIn('arg1', output.lower())
            self.assertIn('arg2', output.lower())
        finally:
            sys.stdout = old_stdout

    def test_choices_validation(self):
        """Test choices validation"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {'name': 'option', 'type': str, 'choices': ['a', 'b', 'c']}
        ])

        # Valid choice
        args = ['test', 'cmd', '--option=a']
        parser.parse(args)
        self.assertEqual(parser.kwargs['option'], 'a')

        # Invalid choice should raise error
        args = ['test', 'cmd', '--option=z']
        with self.assertRaises(SystemExit):
            parser.parse(args)


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/util/test_argparse_comprehensive.py`

```python
import unittest
import sys
import os

# Add the project root to the path so we can import jarvis_cd
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from jarvis_cd.util.argparse import ArgParse


class ComprehensiveArgParse(ArgParse):
    def define_options(self):
        # Test command with all type conversions
        self.add_menu('test', msg="Test menu")
        self.add_cmd('test types', msg="Test type conversions")
        self.add_args([
            {'name': 'str_arg', 'type': str, 'default': 'default_str'},
            {'name': 'int_arg', 'type': int, 'default': 0},
            {'name': 'float_arg', 'type': float, 'default': 0.0},
            {'name': 'bool_arg', 'type': bool, 'default': False},
            {'name': 'list_arg', 'type': list, 'default': []},
            {'name': 'dict_arg', 'type': dict, 'default': {}},
        ])

        # Test command with required arguments
        self.add_cmd('test required', msg="Test required arguments")
        self.add_args([
            {'name': 'required_str', 'type': str, 'required': True},
            {'name': 'required_int', 'type': int, 'required': True},
            {'name': 'optional_str', 'type': str, 'default': 'optional'},
        ])

        # Test command with remainder arguments
        self.add_cmd('test remainder', msg="Test remainder arguments", keep_remainder=True)
        self.add_args([
            {'name': 'first_arg', 'type': str, 'pos': True},
        ])

        # Test command with list of dictionaries
        self.add_cmd('test listdict', msg="Test list of dictionaries")
        self.add_args([
            {
                'name': 'items',
                'type': list,
                'args': [
                    {'name': 'name', 'type': str},
                    {'name': 'value', 'type': int},
                    {'name': 'enabled', 'type': bool}
                ]
            }
        ])

        # Test command with positional arguments
        self.add_cmd('test positional', msg="Test positional arguments")
        self.add_args([
            {'name': 'pos1', 'type': str, 'pos': True, 'required': True},
            {'name': 'pos2', 'type': int, 'pos': True, 'required': True},
            {'name': 'pos3', 'type': float, 'pos': True, 'default': 3.14},
        ])

        # Test command with no remainder (should reject undefined args)
        self.add_cmd('test strict', msg="Test strict argument checking", keep_remainder=False)
        self.add_args([
            {'name': 'known_arg', 'type': str},
        ])

        # Test command with dict argument
        self.add_cmd('test dict', msg="Test dict argument")
        self.add_args([
            {
                'name': 'config',
                'type': dict,
                'args': [
                    {'name': 'host', 'type': str},
                    {'name': 'port', 'type': int},
                ]
            }
        ])

        # Test command with choices
        self.add_cmd('test choices', msg="Test argument with choices")
        self.add_args([
            {'name': 'mode', 'type': str, 'choices': ['read', 'write', 'append'], 'required': True},
        ])

        # Test boolean flags
        self.add_cmd('test bool', msg="Test boolean flags")
        self.add_args([
            {'name': 'flag1', 'type': bool, 'default': False},
            {'name': 'flag2', 'type': bool, 'default': True},
        ])

    def test_types(self):
        pass

    def test_required(self):
        pass

    def test_remainder(self):
        pass

    def test_listdict(self):
        pass

    def test_positional(self):
        pass

    def test_strict(self):
        pass

    def test_dict(self):
        pass

    def test_choices(self):
        pass

    def test_bool(self):
        pass


class TestArgParseTypeConversions(unittest.TestCase):

    def setUp(self):
        self.parser = ComprehensiveArgParse()
        self.parser.define_options()

    def test_str_conversion(self):
        """Test string type conversion"""
        args = ['test', 'types', '--str_arg=hello']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['str_arg'], 'hello')
        self.assertIsInstance(self.parser.kwargs['str_arg'], str)

    def test_int_conversion(self):
        """Test int type conversion"""
        args = ['test', 'types', '--int_arg=42']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['int_arg'], 42)
        self.assertIsInstance(self.parser.kwargs['int_arg'], int)

    def test_float_conversion(self):
        """Test float type conversion"""
        args = ['test', 'types', '--float_arg=3.14159']
        result = self.parser.parse(args)

        self.assertAlmostEqual(self.parser.kwargs['float_arg'], 3.14159)
        self.assertIsInstance(self.parser.kwargs['float_arg'], float)

    def test_bool_conversion_true(self):
        """Test bool type conversion to True"""
        args = ['test', 'types', '--bool_arg=true']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['bool_arg'], True)
        self.assertIsInstance(self.parser.kwargs['bool_arg'], bool)

    def test_bool_conversion_false(self):
        """Test bool type conversion to False"""
        args = ['test', 'types', '--bool_arg=false']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['bool_arg'], False)
        self.assertIsInstance(self.parser.kwargs['bool_arg'], bool)

    def test_list_conversion(self):
        """Test list type conversion"""
        args = ['test', 'types', '--list_arg=["item1", "item2", "item3"]']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['list_arg'], ["item1", "item2", "item3"])
        self.assertIsInstance(self.parser.kwargs['list_arg'], list)

    def test_dict_conversion(self):
        """Test dict type conversion"""
        args = ['test', 'types', '--dict_arg={"key1": "value1", "key2": 42}']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['dict_arg'], {"key1": "value1", "key2": 42})
        self.assertIsInstance(self.parser.kwargs['dict_arg'], dict)

    def test_int_from_numeric_string(self):
        """Test int conversion from numeric string"""
        args = ['test', 'types', '--int_arg', '123']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['int_arg'], 123)

    def test_float_from_numeric_string(self):
        """Test float conversion from numeric string"""
        args = ['test', 'types', '--float_arg', '2.718']
        result = self.parser.parse(args)

        self.assertAlmostEqual(self.parser.kwargs['float_arg'], 2.718)

    def test_negative_int(self):
        """Test negative integer conversion"""
        args = ['test', 'types', '--int_arg=-42']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['int_arg'], -42)

    def test_negative_float(self):
        """Test negative float conversion"""
        args = ['test', 'types', '--float_arg=-3.14']
        result = self.parser.parse(args)

        self.assertAlmostEqual(self.parser.kwargs['float_arg'], -3.14)


class TestArgParseRequiredArguments(unittest.TestCase):

    def setUp(self):
        self.parser = ComprehensiveArgParse()
        self.parser.define_options()

    def test_required_arguments_provided(self):
        """Test that required arguments are accepted when provided"""
        args = ['test', 'required', '--required_str=hello', '--required_int=42']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['required_str'], 'hello')
        self.assertEqual(self.parser.kwargs['required_int'], 42)
        self.assertEqual(self.parser.kwargs['optional_str'], 'optional')

    def test_required_str_missing(self):
        """Test that missing required string argument raises error"""
        args = ['test', 'required', '--required_int=42']

        with self.assertRaises(SystemExit):
            self.parser.parse(args)

    def test_required_int_missing(self):
        """Test that missing required int argument raises error"""
        args = ['test', 'required', '--required_str=hello']

        with self.assertRaises(SystemExit):
            self.parser.parse(args)

    def test_all_required_missing(self):
        """Test that all missing required arguments raise error"""
        args = ['test', 'required']

        with self.assertRaises(SystemExit):
            self.parser.parse(args)

    def test_optional_can_be_omitted(self):
        """Test that optional arguments can be omitted"""
        args = ['test', 'required', '--required_str=hello', '--required_int=42']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['optional_str'], 'optional')


class TestArgParseRemainderArguments(unittest.TestCase):

    def setUp(self):
        self.parser = ComprehensiveArgParse()
        self.parser.define_options()

    def test_remainder_arguments_collected(self):
        """Test that remainder arguments are collected"""
        args = ['test', 'remainder', 'first', 'extra1', 'extra2', 'extra3']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['first_arg'], 'first')
        self.assertEqual(self.parser.remainder, ['extra1', 'extra2', 'extra3'])

    def test_remainder_with_no_extras(self):
        """Test remainder when no extra arguments provided"""
        args = ['test', 'remainder', 'first']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['first_arg'], 'first')
        self.assertEqual(self.parser.remainder, [])

    def test_undefined_args_without_remainder(self):
        """Test that undefined arguments without keep_remainder raise an error"""
        args = ['test', 'strict', '--known_arg=value', '--unknown_arg=bad']

        # The command is marked as strict (keep_remainder=False), so undefined args should raise error
        # This matches the command definition comment: "should reject undefined args"
        with self.assertRaises(SystemExit):
            self.parser.parse(args)


class TestArgParseListArguments(unittest.TestCase):

    def setUp(self):
        self.parser = ComprehensiveArgParse()
        self.parser.define_options()

    def test_list_of_dicts_set_mode(self):
        """Test list of dictionaries in set mode"""
        args = ['test', 'listdict', '--items=[("item1", 10, True), ("item2", 20, False)]']
        result = self.parser.parse(args)

        expected = [
            {'name': 'item1', 'value': 10, 'enabled': True},
            {'name': 'item2', 'value': 20, 'enabled': False}
        ]
        self.assertEqual(self.parser.kwargs['items'], expected)

    def test_list_of_dicts_append_mode(self):
        """Test list of dictionaries in append mode"""
        args = ['test', 'listdict', '--items', '("item1", 10, True)', '--items', '("item2", 20, False)']
        result = self.parser.parse(args)

        expected = [
            {'name': 'item1', 'value': 10, 'enabled': True},
            {'name': 'item2', 'value': 20, 'enabled': False}
        ]
        self.assertEqual(self.parser.kwargs['items'], expected)

    def test_empty_list_default(self):
        """Test empty list default value"""
        args = ['test', 'types']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['list_arg'], [])

    def test_list_type_conversion(self):
        """Test that list items are converted to proper types"""
        args = ['test', 'listdict', '--items=[("test", 123, True)]']
        result = self.parser.parse(args)

        item = self.parser.kwargs['items'][0]
        self.assertIsInstance(item['name'], str)
        self.assertIsInstance(item['value'], int)
        self.assertIsInstance(item['enabled'], bool)


class TestArgParseDictArguments(unittest.TestCase):

    def setUp(self):
        self.parser = ComprehensiveArgParse()
        self.parser.define_options()

    def test_dict_argument(self):
        """Test dict argument parsing"""
        args = ['test', 'dict', '--config={"host": "localhost", "port": 8080}']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['config']['host'], 'localhost')
        self.assertEqual(self.parser.kwargs['config']['port'], 8080)

    def test_dict_type_conversion(self):
        """Test that dict values are converted to proper types"""
        args = ['test', 'dict', '--config={"host": "server.com", "port": 443}']
        result = self.parser.parse(args)

        config = self.parser.kwargs['config']
        self.assertIsInstance(config['host'], str)
        self.assertIsInstance(config['port'], int)


class TestArgParsePositionalArguments(unittest.TestCase):

    def setUp(self):
        self.parser = ComprehensiveArgParse()
        self.parser.define_options()

    def test_positional_arguments_order(self):
        """Test that positional arguments are parsed in order"""
        args = ['test', 'positional', 'hello', '42', '2.718']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['pos1'], 'hello')
        self.assertEqual(self.parser.kwargs['pos2'], 42)
        self.assertAlmostEqual(self.parser.kwargs['pos3'], 2.718)

    def test_positional_with_defaults(self):
        """Test positional arguments with defaults"""
        args = ['test', 'positional', 'hello', '42']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['pos1'], 'hello')
        self.assertEqual(self.parser.kwargs['pos2'], 42)
        self.assertAlmostEqual(self.parser.kwargs['pos3'], 3.14)

    def test_positional_required_missing(self):
        """Test that missing required positional argument raises error"""
        args = ['test', 'positional', 'hello']

        with self.assertRaises(SystemExit):
            self.parser.parse(args)

    def test_positional_type_conversion(self):
        """Test type conversion for positional arguments"""
        args = ['test', 'positional', 'test', '100', '1.5']
        result = self.parser.parse(args)

        self.assertIsInstance(self.parser.kwargs['pos1'], str)
        self.assertIsInstance(self.parser.kwargs['pos2'], int)
        self.assertIsInstance(self.parser.kwargs['pos3'], float)


class TestArgParseChoices(unittest.TestCase):

    def setUp(self):
        self.parser = ComprehensiveArgParse()
        self.parser.define_options()

    def test_valid_choice(self):
        """Test that valid choice is accepted"""
        args = ['test', 'choices', '--mode=read']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['mode'], 'read')

    def test_invalid_choice(self):
        """Test that invalid choice raises error"""
        args = ['test', 'choices', '--mode=execute']

        with self.assertRaises(SystemExit):
            self.parser.parse(args)

    def test_all_valid_choices(self):
        """Test all valid choices"""
        for mode in ['read', 'write', 'append']:
            self.parser = ComprehensiveArgParse()
            self.parser.define_options()
            args = ['test', 'choices', f'--mode={mode}']
            result = self.parser.parse(args)
            self.assertEqual(self.parser.kwargs['mode'], mode)


class TestArgParseBooleanFlags(unittest.TestCase):

    def setUp(self):
        self.parser = ComprehensiveArgParse()
        self.parser.define_options()

    def test_plus_flag_sets_true(self):
        """Test +flag sets boolean to true"""
        args = ['test', 'bool', '+flag1']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['flag1'], True)

    def test_minus_flag_sets_false(self):
        """Test -flag sets boolean to false"""
        args = ['test', 'bool', '-flag2']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['flag2'], False)

    def test_bool_keyword_true(self):
        """Test boolean keyword argument set to true"""
        args = ['test', 'bool', '--flag1=true']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['flag1'], True)

    def test_bool_keyword_false(self):
        """Test boolean keyword argument set to false"""
        args = ['test', 'bool', '--flag1=false']
        result = self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['flag1'], False)

    def test_bool_variations(self):
        """Test various boolean value representations"""
        true_values = ['true', 'True', '1', 'yes', 'on']
        for val in true_values:
            self.parser = ComprehensiveArgParse()
            self.parser.define_options()
            args = ['test', 'bool', f'--flag1={val}']
            result = self.parser.parse(args)
            self.assertEqual(self.parser.kwargs['flag1'], True, f"Failed for value: {val}")


class TestArgParseDictMode(unittest.TestCase):

    def setUp(self):
        self.parser = ComprehensiveArgParse()
        self.parser.define_options()

    def test_parse_dict_basic(self):
        """Test parse_dict with basic types"""
        arg_dict = {
            'str_arg': 'hello',
            'int_arg': 42,
            'float_arg': 3.14,
            'bool_arg': True
        }
        result = self.parser.parse_dict('test types', arg_dict)

        self.assertEqual(self.parser.kwargs['str_arg'], 'hello')
        self.assertEqual(self.parser.kwargs['int_arg'], 42)
        self.assertAlmostEqual(self.parser.kwargs['float_arg'], 3.14)
        self.assertEqual(self.parser.kwargs['bool_arg'], True)

    def test_parse_dict_type_conversion(self):
        """Test parse_dict performs type conversion"""
        arg_dict = {
            'int_arg': '123',  # String that should be converted to int
            'float_arg': '2.5',  # String that should be converted to float
        }
        result = self.parser.parse_dict('test types', arg_dict)

        self.assertEqual(self.parser.kwargs['int_arg'], 123)
        self.assertIsInstance(self.parser.kwargs['int_arg'], int)
        self.assertAlmostEqual(self.parser.kwargs['float_arg'], 2.5)
        self.assertIsInstance(self.parser.kwargs['float_arg'], float)

    def test_parse_dict_with_list(self):
        """Test parse_dict with list argument"""
        arg_dict = {
            'items': [
                ('item1', 10, True),
                ('item2', 20, False)
            ]
        }
        result = self.parser.parse_dict('test listdict', arg_dict)

        expected = [
            {'name': 'item1', 'value': 10, 'enabled': True},
            {'name': 'item2', 'value': 20, 'enabled': False}
        ]
        self.assertEqual(self.parser.kwargs['items'], expected)

    def test_parse_dict_required_args(self):
        """Test parse_dict validates required arguments"""
        arg_dict = {
            'required_str': 'hello'
            # Missing required_int
        }

        with self.assertRaises(SystemExit):
            self.parser.parse_dict('test required', arg_dict)

    def test_parse_dict_with_defaults(self):
        """Test parse_dict uses default values"""
        arg_dict = {}
        result = self.parser.parse_dict('test types', arg_dict)

        self.assertEqual(self.parser.kwargs['str_arg'], 'default_str')
        self.assertEqual(self.parser.kwargs['int_arg'], 0)
        self.assertEqual(self.parser.kwargs['float_arg'], 0.0)
        self.assertEqual(self.parser.kwargs['bool_arg'], False)


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/util/test_argparse_final.py`

```python
"""
Final argparse tests for remaining coverage
"""
import unittest
import sys
import os
from io import StringIO

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.util.argparse import ArgParse


class TestArgParseFinal(unittest.TestCase):
    """Tests for remaining uncovered ArgParse lines"""

    def test_help_flag_specific_command(self):
        """Test --help for specific command"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Command")
        parser.add_args([{'name': 'arg1', 'type': str}])

        old_stdout = sys.stdout
        sys.stdout = StringIO()
        try:
            parser.parse(['--help', 'test cmd'])
            output = sys.stdout.getvalue()
            self.assertIn('test', output.lower())
        finally:
            sys.stdout = old_stdout

    def test_dash_h_flag(self):
        """Test -h flag for help"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")

        old_stdout = sys.stdout
        sys.stdout = StringIO()
        try:
            parser.parse(['-h'])
            output = sys.stdout.getvalue()
            self.assertIn('test', output.lower())
        finally:
            sys.stdout = old_stdout

    def test_help_command(self):
        """Test help command"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")

        old_stdout = sys.stdout
        sys.stdout = StringIO()
        try:
            parser.parse(['help'])
            output = sys.stdout.getvalue()
            self.assertIsInstance(output, str)
        finally:
            sys.stdout = old_stdout

    def test_empty_args(self):
        """Test parsing empty args list"""
        parser = ArgParse()
        result = parser.parse([])
        self.assertEqual(result, {})

    def test_unknown_command(self):
        """Test unknown command handling"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")

        with self.assertRaises(SystemExit):
            parser.parse(['unknown_command'])

    def test_list_value_conversion_edge_case(self):
        """Test list value conversion with special format"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {'name': 'items', 'type': list}
        ])

        # Test single value to list
        args = ['test', 'cmd', '--items=single']
        parser.parse(args)
        self.assertIsInstance(parser.kwargs['items'], list)

    def test_dict_parse_error_handling(self):
        """Test dict parsing with invalid input"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {'name': 'config', 'type': dict}
        ])

        # Invalid dict
        args = ['test', 'cmd', '--config={invalid}']
        parser.parse(args)
        # Should handle gracefully
        self.assertIn('config', parser.kwargs)

    def test_get_argument_info_none(self):
        """Test _get_argument_info returns None for unknown arg"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([{'name': 'known', 'type': str}])

        # This should not crash
        args = ['test', 'cmd', '--unknown=value']
        with self.assertRaises(SystemExit):
            parser.parse(args)

    def test_remainder_with_positionals(self):
        """Test remainder when positionals are exhausted"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test", keep_remainder=True)
        parser.add_args([
            {'name': 'arg1', 'type': str, 'pos': True, 'required': True}
        ])

        args = ['test', 'cmd', 'value1', 'extra1', 'extra2']
        parser.parse(args)
        self.assertEqual(parser.kwargs['arg1'], 'value1')
        self.assertIn('extra1', parser.remainder)
        self.assertIn('extra2', parser.remainder)

    def test_convert_list_items_without_args(self):
        """Test _convert_list_items when no args definition"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="Test")
        parser.add_args([
            {'name': 'items', 'type': list}  # No args sub-definition
        ])

        args = ['test', 'cmd', '--items=[{"key":"val"},{"key2":"val2"}]']
        parser.parse(args)
        # Should preserve dict items
        self.assertIsInstance(parser.kwargs['items'], list)

    def test_print_command_help_non_existent(self):
        """Test print_command_help for non-existent command"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")

        old_stdout = sys.stdout
        sys.stdout = StringIO()
        try:
            parser.print_command_help('nonexistent')
            output = sys.stdout.getvalue()
            # Should indicate command not found
            self.assertIsInstance(output, str)
        finally:
            sys.stdout = old_stdout

    def test_multiline_help_description(self):
        """Test command with multi-line description"""
        parser = ArgParse()
        parser.add_menu('test', msg="Test")
        parser.add_cmd('test cmd', msg="This is a\nmulti-line\ndescription")

        old_stdout = sys.stdout
        sys.stdout = StringIO()
        try:
            parser.print_help('test cmd')
            output = sys.stdout.getvalue()
            self.assertIn('test', output.lower())
        finally:
            sys.stdout = old_stdout


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/util/test_argparse_help.py`

```python
"""
Tests for argparse help system
"""
import unittest
import sys
import os
from io import StringIO

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.util.argparse import ArgParse


class TestArgParseHelp(unittest.TestCase):
    """Tests for ArgParse help system"""

    def setUp(self):
        """Set up test parser"""
        self.parser = ArgParse()
        self.parser.add_menu('test', msg="Test menu")
        self.parser.add_cmd('test cmd1', msg="Test command 1")
        self.parser.add_args([
            {'name': 'arg1', 'msg': 'First argument', 'type': str, 'required': True, 'pos': True},
            {'name': 'arg2', 'msg': 'Second argument', 'type': int, 'default': 42}
        ])
        self.parser.add_cmd('test cmd2', msg="Test command 2")
        self.parser.add_args([
            {'name': 'flag', 'msg': 'Boolean flag', 'type': bool, 'default': False}
        ])

    def test_print_help_to_stdout(self):
        """Test that print_help outputs to stdout"""
        # Capture stdout
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            self.parser.print_help()
            output = sys.stdout.getvalue()

            # Check output contains expected elements
            self.assertIn('test', output.lower())
            self.assertIn('cmd1', output.lower())
            self.assertIn('cmd2', output.lower())

        finally:
            sys.stdout = old_stdout

    def test_print_help_for_specific_command(self):
        """Test print_help for a specific command"""
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            self.parser.print_help('test cmd1')
            output = sys.stdout.getvalue()

            # Should contain command-specific help
            self.assertIn('arg1', output.lower())
            self.assertIn('first argument', output.lower())

        finally:
            sys.stdout = old_stdout

    def test_print_menu_help(self):
        """Test print_menu_help method"""
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            self.parser.print_menu_help('test')
            output = sys.stdout.getvalue()

            # Should contain menu help
            self.assertIn('test', output.lower())

        finally:
            sys.stdout = old_stdout

    def test_help_shows_required_args(self):
        """Test that help indicates required arguments"""
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            self.parser.print_help('test cmd1')
            output = sys.stdout.getvalue()

            # Should indicate arg1 is required
            self.assertIn('arg1', output.lower())
            self.assertIn('required', output.lower())

        finally:
            sys.stdout = old_stdout

    def test_help_shows_default_values(self):
        """Test that help shows default values"""
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            self.parser.print_help('test cmd1')
            output = sys.stdout.getvalue()

            # Should show default value for arg2
            self.assertIn('arg2', output.lower())
            self.assertIn('42', output)

        finally:
            sys.stdout = old_stdout

    def test_help_with_no_commands(self):
        """Test help when no commands are defined"""
        empty_parser = ArgParse()
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            empty_parser.print_help()
            output = sys.stdout.getvalue()

            # Should not crash
            self.assertIsInstance(output, str)

        finally:
            sys.stdout = old_stdout


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/util/test_hostfile.py`

```python
import unittest
import tempfile
import os
import socket
import sys

# Add the project root to the path so we can import jarvis_cd
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from jarvis_cd.util.hostfile import Hostfile


class TestHostfile(unittest.TestCase):
    
    def setUp(self):
        """Set up test fixtures"""
        self.temp_dir = tempfile.mkdtemp()
        
    def tearDown(self):
        """Clean up test fixtures"""
        # Clean up any temporary files
        for file in os.listdir(self.temp_dir):
            os.remove(os.path.join(self.temp_dir, file))
        os.rmdir(self.temp_dir)
        
    def test_default_constructor(self):
        """Test default constructor creates localhost hostfile"""
        hostfile = Hostfile()
        
        self.assertEqual(len(hostfile), 1)
        self.assertEqual(hostfile.hosts, ['localhost'])
        self.assertEqual(len(hostfile.hosts_ip), 1)
        self.assertTrue(hostfile.is_local())
        
    def test_constructor_with_hosts_list(self):
        """Test constructor with explicit hosts list"""
        hosts = ['host1', 'host2', 'host3']
        hostfile = Hostfile(hosts=hosts, find_ips=False)
        
        self.assertEqual(len(hostfile), 3)
        self.assertEqual(hostfile.hosts, hosts)
        self.assertEqual(hostfile.hosts_ip, [])
        self.assertFalse(hostfile.is_local())
        
    def test_constructor_with_hosts_and_ips(self):
        """Test constructor with both hosts and IPs"""
        hosts = ['host1', 'host2']
        ips = ['192.168.1.1', '192.168.1.2']
        hostfile = Hostfile(hosts=hosts, hosts_ip=ips, find_ips=False)
        
        self.assertEqual(hostfile.hosts, hosts)
        self.assertEqual(hostfile.hosts_ip, ips)
        
    def test_simple_host_pattern(self):
        """Test simple hostname without brackets"""
        text = "ares-comp-01"
        hostfile = Hostfile(text=text, find_ips=False)
        
        self.assertEqual(len(hostfile), 1)
        self.assertEqual(hostfile.hosts, ['ares-comp-01'])
        
    def test_bracket_range_pattern(self):
        """Test bracket range expansion like [02-04]"""
        text = "ares-comp-[02-04]"
        hostfile = Hostfile(text=text, find_ips=False)
        
        expected = ['ares-comp-02', 'ares-comp-03', 'ares-comp-04']
        self.assertEqual(len(hostfile), 3)
        self.assertEqual(hostfile.hosts, expected)
        
    def test_bracket_list_pattern(self):
        """Test bracket list expansion like [05,07,09]"""
        text = "ares-comp-[05,07,09]"
        hostfile = Hostfile(text=text, find_ips=False)
        
        expected = ['ares-comp-05', 'ares-comp-07', 'ares-comp-09']
        self.assertEqual(len(hostfile), 3)
        self.assertEqual(hostfile.hosts, expected)
        
    def test_complex_bracket_pattern(self):
        """Test complex bracket pattern with ranges and lists"""
        text = "ares-comp-[05-09,11,12-14]-40g"
        hostfile = Hostfile(text=text, find_ips=False)
        
        expected = [
            'ares-comp-05-40g', 'ares-comp-06-40g', 'ares-comp-07-40g',
            'ares-comp-08-40g', 'ares-comp-09-40g', 'ares-comp-11-40g',
            'ares-comp-12-40g', 'ares-comp-13-40g', 'ares-comp-14-40g'
        ]
        self.assertEqual(len(hostfile), 9)
        self.assertEqual(hostfile.hosts, expected)
        
    def test_zero_padded_ranges(self):
        """Test that zero-padding is preserved in ranges"""
        text = "node-[001-003]"
        hostfile = Hostfile(text=text, find_ips=False)
        
        expected = ['node-001', 'node-002', 'node-003']
        self.assertEqual(hostfile.hosts, expected)
        
    def test_alphabetic_ranges(self):
        """Test alphabetic range expansion like [a-c]"""
        text = "server-[a-c]"
        hostfile = Hostfile(text=text, find_ips=False)
        
        expected = ['server-a', 'server-b', 'server-c']
        self.assertEqual(hostfile.hosts, expected)
        
    def test_uppercase_alphabetic_ranges(self):
        """Test uppercase alphabetic range expansion like [A-C]"""
        text = "server-[A-C]"
        hostfile = Hostfile(text=text, find_ips=False)
        
        expected = ['server-A', 'server-B', 'server-C']
        self.assertEqual(hostfile.hosts, expected)
        
    def test_multiple_lines(self):
        """Test hostfile with multiple lines"""
        text = """ares-comp-01
ares-comp-[02-04]
ares-comp-[05-09,11,12-14]-40g"""
        hostfile = Hostfile(text=text, find_ips=False)
        
        # Should have 1 + 3 + 9 = 13 hosts
        self.assertEqual(len(hostfile), 13)
        self.assertIn('ares-comp-01', hostfile.hosts)
        self.assertIn('ares-comp-02', hostfile.hosts)
        self.assertIn('ares-comp-05-40g', hostfile.hosts)
        
    def test_file_loading(self):
        """Test loading hostfile from filesystem"""
        content = """host1
host2
host-[10-12]"""
        
        # Create temporary hostfile
        hostfile_path = os.path.join(self.temp_dir, 'test_hostfile.txt')
        with open(hostfile_path, 'w') as f:
            f.write(content)
            
        hostfile = Hostfile(path=hostfile_path, find_ips=False)
        
        expected = ['host1', 'host2', 'host-10', 'host-11', 'host-12']
        self.assertEqual(len(hostfile), 5)
        self.assertEqual(hostfile.hosts, expected)
        
    def test_file_not_found(self):
        """Test error when hostfile doesn't exist"""
        with self.assertRaises(FileNotFoundError):
            Hostfile(path='/nonexistent/hostfile.txt')
            
    def test_subset(self):
        """Test subset functionality"""
        hosts = ['host1', 'host2', 'host3', 'host4']
        hostfile = Hostfile(hosts=hosts, find_ips=False)
        
        subset = hostfile.subset(2)
        
        self.assertEqual(len(subset), 2)
        self.assertEqual(subset.hosts, ['host1', 'host2'])
        
    def test_copy(self):
        """Test copy functionality"""
        hosts = ['host1', 'host2']
        hostfile = Hostfile(hosts=hosts, find_ips=False)
        
        copy = hostfile.copy()
        
        self.assertEqual(len(copy), len(hostfile))
        self.assertEqual(copy.hosts, hostfile.hosts)
        self.assertIsNot(copy.hosts, hostfile.hosts)  # Should be different objects
        
    def test_is_local_localhost(self):
        """Test is_local with localhost"""
        hostfile = Hostfile(hosts=['localhost'], find_ips=False)
        self.assertTrue(hostfile.is_local())
        
    def test_is_local_empty(self):
        """Test is_local with empty hostfile"""
        hostfile = Hostfile(hosts=[], find_ips=False)
        self.assertTrue(hostfile.is_local())
        
    def test_is_local_multiple_hosts(self):
        """Test is_local with multiple hosts"""
        hostfile = Hostfile(hosts=['host1', 'host2'], find_ips=False)
        self.assertFalse(hostfile.is_local())
        
    def test_save(self):
        """Test saving hostfile to filesystem"""
        hosts = ['host1', 'host2', 'host3']
        hostfile = Hostfile(hosts=hosts, find_ips=False)
        
        save_path = os.path.join(self.temp_dir, 'saved_hostfile.txt')
        hostfile.save(save_path)
        
        # Verify file was created and has correct content
        self.assertTrue(os.path.exists(save_path))
        with open(save_path, 'r') as f:
            content = f.read().strip()
        
        self.assertEqual(content, 'host1\nhost2\nhost3')
        
    def test_list(self):
        """Test list functionality"""
        hosts = ['host1', 'host2']
        hostfile = Hostfile(hosts=hosts, find_ips=False)
        
        host_list = hostfile.list()
        
        self.assertEqual(len(host_list), 2)
        self.assertIsInstance(host_list[0], Hostfile)
        self.assertEqual(host_list[0].hosts, ['host1'])
        self.assertEqual(host_list[1].hosts, ['host2'])
        
    def test_enumerate(self):
        """Test enumerate functionality"""
        hosts = ['host1', 'host2']
        hostfile = Hostfile(hosts=hosts, find_ips=False)
        
        enumerated = list(hostfile.enumerate())
        
        self.assertEqual(len(enumerated), 2)
        self.assertEqual(enumerated[0][0], 0)
        self.assertEqual(enumerated[0][1].hosts, ['host1'])
        self.assertEqual(enumerated[1][0], 1)
        self.assertEqual(enumerated[1][1].hosts, ['host2'])
        
    def test_host_str(self):
        """Test host_str functionality"""
        hosts = ['host1', 'host2', 'host3']
        hostfile = Hostfile(hosts=hosts, find_ips=False)
        
        self.assertEqual(hostfile.host_str(), 'host1,host2,host3')
        self.assertEqual(hostfile.host_str('|'), 'host1|host2|host3')
        
    def test_ip_str(self):
        """Test ip_str functionality"""
        hosts = ['host1', 'host2']
        ips = ['192.168.1.1', '192.168.1.2']
        hostfile = Hostfile(hosts=hosts, hosts_ip=ips, find_ips=False)
        
        self.assertEqual(hostfile.ip_str(), '192.168.1.1,192.168.1.2')
        self.assertEqual(hostfile.ip_str('|'), '192.168.1.1|192.168.1.2')
        
    def test_len(self):
        """Test __len__ functionality"""
        hostfile = Hostfile(hosts=['host1', 'host2', 'host3'], find_ips=False)
        self.assertEqual(len(hostfile), 3)
        
    def test_iter(self):
        """Test __iter__ functionality"""
        hosts = ['host1', 'host2', 'host3']
        hostfile = Hostfile(hosts=hosts, find_ips=False)
        
        iterated_hosts = list(hostfile)
        self.assertEqual(iterated_hosts, hosts)
        
    def test_getitem(self):
        """Test __getitem__ functionality"""
        hosts = ['host1', 'host2', 'host3']
        hostfile = Hostfile(hosts=hosts, find_ips=False)
        
        self.assertEqual(hostfile[0], 'host1')
        self.assertEqual(hostfile[1], 'host2')
        self.assertEqual(hostfile[-1], 'host3')
        
    def test_str_repr(self):
        """Test string representations"""
        hosts = ['host1', 'host2']
        hostfile = Hostfile(hosts=hosts, find_ips=False)
        
        str_repr = str(hostfile)
        self.assertIn('2 hosts', str_repr)
        self.assertIn('host1,host2', str_repr)
        
        detailed_repr = repr(hostfile)
        self.assertIn('hosts=', detailed_repr)
        self.assertIn('hosts_ip=', detailed_repr)
        
    def test_ip_resolution_localhost(self):
        """Test IP resolution for localhost"""
        hostfile = Hostfile()  # Default constructor with find_ips=True
        
        self.assertEqual(len(hostfile.hosts_ip), 1)
        # Should resolve to some localhost IP
        self.assertIsNotNone(hostfile.hosts_ip[0])
        
    def test_empty_lines_ignored(self):
        """Test that empty lines are ignored in hostfile text"""
        text = """host1

host2

host3
"""
        hostfile = Hostfile(text=text, find_ips=False)
        
        self.assertEqual(len(hostfile), 3)
        self.assertEqual(hostfile.hosts, ['host1', 'host2', 'host3'])
        
    def test_no_find_ips(self):
        """Test constructor with find_ips=False"""
        hostfile = Hostfile(hosts=['host1', 'host2'], find_ips=False)
        
        self.assertEqual(hostfile.hosts_ip, [])


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/util/test_pkg_argparse.py`

```python
"""
Tests for pkg_argparse.py - Package-specific argument parser
"""
import unittest
import sys
import os
from io import StringIO

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.util.pkg_argparse import PkgArgParse


class TestPkgArgParse(unittest.TestCase):
    """Tests for PkgArgParse class"""

    def setUp(self):
        """Set up test parser with sample configure menu"""
        self.configure_menu = [
            {'name': 'install_dir', 'msg': 'Installation directory', 'type': str, 'default': '/usr/local'},
            {'name': 'num_threads', 'msg': 'Number of threads', 'type': int, 'default': 4},
            {'name': 'enable_debug', 'msg': 'Enable debug mode', 'type': bool, 'default': False},
        ]
        self.parser = PkgArgParse('test_package', self.configure_menu)

    def test_initialization(self):
        """Test PkgArgParse initialization"""
        self.assertEqual(self.parser.pkg_name, 'test_package')
        self.assertIsNotNone(self.parser.cmds)

    def test_configure_command_exists(self):
        """Test that configure command is automatically added"""
        self.assertIn('configure', self.parser.cmds)

    def test_parse_configure_with_args(self):
        """Test parsing configure command with arguments"""
        args = ['configure', '--install_dir=/opt', '--num_threads=8']
        self.parser.parse(args)

        self.assertEqual(self.parser.kwargs['install_dir'], '/opt')
        self.assertEqual(self.parser.kwargs['num_threads'], 8)

    def test_parse_configure_with_defaults(self):
        """Test that default values are used"""
        args = ['configure']
        self.parser.parse(args)

        # Should have defaults
        self.assertIn('install_dir', self.parser.kwargs)
        self.assertIn('num_threads', self.parser.kwargs)

    def test_print_help(self):
        """Test print_help for package"""
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            self.parser.print_help()
            output = sys.stdout.getvalue()

            # Should contain package name and parameters
            self.assertIn('test_package', output)
            self.assertIn('install_dir', output.lower())
            self.assertIn('num_threads', output.lower())

        finally:
            sys.stdout = old_stdout

    def test_print_help_unknown_command(self):
        """Test print_help with unknown command"""
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            self.parser.print_help('unknown_cmd')
            output = sys.stdout.getvalue()

            # Should indicate unknown command
            self.assertIn('unknown', output.lower())

        finally:
            sys.stdout = old_stdout

    def test_print_help_configure_command(self):
        """Test print_help for configure command"""
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            self.parser.print_help('configure')
            output = sys.stdout.getvalue()

            # Should show configure help
            self.assertIn('test_package', output)

        finally:
            sys.stdout = old_stdout

    def test_empty_configure_menu(self):
        """Test PkgArgParse with empty configure menu"""
        parser = PkgArgParse('empty_package', [])
        args = ['configure']
        parser.parse(args)

        # Should not crash
        self.assertIsNotNone(parser.kwargs)


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/util/test_resource_graph.py`

```python
"""
Comprehensive unit tests for resource_graph.py module
Tests resource graph construction, analysis, filtering, and serialization
"""
import unittest
import sys
import os
import tempfile
import json
import yaml
from pathlib import Path

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.util.resource_graph import ResourceGraph


class TestResourceGraphInitialization(unittest.TestCase):
    """Tests for ResourceGraph initialization"""

    def test_init_empty_graph(self):
        """Test ResourceGraph initialization creates empty structures"""
        graph = ResourceGraph()
        self.assertEqual(graph.nodes, {})
        self.assertEqual(graph.common_mounts, {})

    def test_init_no_arguments(self):
        """Test ResourceGraph can be initialized without arguments"""
        graph = ResourceGraph()
        self.assertIsInstance(graph.nodes, dict)
        self.assertIsInstance(graph.common_mounts, dict)


class TestResourceGraphAddNodeData(unittest.TestCase):
    """Tests for adding node data to resource graph"""

    def setUp(self):
        """Set up test resource graph"""
        self.graph = ResourceGraph()

    def test_add_single_node_single_device(self):
        """Test adding a single device to a single node"""
        resource_data = {
            'fs': [{
                'device': '/dev/sda1',
                'mount': '/mnt/data',
                'fs_type': 'ext4',
                'avail': '100G',
                'dev_type': 'ssd',
                'model': 'Samsung 970'
            }]
        }
        self.graph.add_node_data('node1', resource_data)

        self.assertIn('node1', self.graph.nodes)
        self.assertEqual(len(self.graph.nodes['node1']), 1)
        self.assertEqual(self.graph.nodes['node1'][0]['device'], '/dev/sda1')
        self.assertEqual(self.graph.nodes['node1'][0]['hostname'], 'node1')

    def test_add_single_node_multiple_devices(self):
        """Test adding multiple devices to a single node"""
        resource_data = {
            'fs': [
                {'device': '/dev/sda1', 'mount': '/mnt/data1', 'dev_type': 'ssd'},
                {'device': '/dev/sdb1', 'mount': '/mnt/data2', 'dev_type': 'hdd'},
                {'device': '/dev/sdc1', 'mount': '/mnt/data3', 'dev_type': 'nvme'}
            ]
        }
        self.graph.add_node_data('node1', resource_data)

        self.assertEqual(len(self.graph.nodes['node1']), 3)
        device_names = [d['device'] for d in self.graph.nodes['node1']]
        self.assertIn('/dev/sda1', device_names)
        self.assertIn('/dev/sdb1', device_names)
        self.assertIn('/dev/sdc1', device_names)

    def test_add_multiple_nodes(self):
        """Test adding data for multiple nodes"""
        data1 = {'fs': [{'device': '/dev/sda1', 'mount': '/shared'}]}
        data2 = {'fs': [{'device': '/dev/sdb1', 'mount': '/shared'}]}

        self.graph.add_node_data('node1', data1)
        self.graph.add_node_data('node2', data2)

        self.assertEqual(len(self.graph.nodes), 2)
        self.assertIn('node1', self.graph.nodes)
        self.assertIn('node2', self.graph.nodes)

    def test_default_field_values(self):
        """Test that default field values are set correctly"""
        resource_data = {'fs': [{'device': '/dev/sda1'}]}
        self.graph.add_node_data('node1', resource_data)

        device = self.graph.nodes['node1'][0]
        self.assertEqual(device['mount'], '')
        self.assertEqual(device['fs_type'], 'unknown')
        self.assertEqual(device['avail'], '0B')
        self.assertEqual(device['dev_type'], 'unknown')
        self.assertEqual(device['model'], 'unknown')
        self.assertEqual(device['parent'], '')
        self.assertEqual(device['uuid'], '')
        self.assertFalse(device['needs_root'])
        # In single-node cluster, all mounts are marked as shared
        self.assertTrue(device['shared'])
        self.assertEqual(device['4k_randwrite_bw'], 'unknown')
        self.assertEqual(device['1m_seqwrite_bw'], 'unknown')

    def test_add_empty_fs_list(self):
        """Test adding node with empty filesystem list"""
        resource_data = {'fs': []}
        self.graph.add_node_data('node1', resource_data)

        self.assertIn('node1', self.graph.nodes)
        self.assertEqual(len(self.graph.nodes['node1']), 0)

    def test_add_node_without_fs_key(self):
        """Test adding node data without 'fs' key"""
        resource_data = {}
        self.graph.add_node_data('node1', resource_data)

        self.assertIn('node1', self.graph.nodes)
        self.assertEqual(len(self.graph.nodes['node1']), 0)


class TestResourceGraphCommonMounts(unittest.TestCase):
    """Tests for common mount analysis"""

    def setUp(self):
        """Set up test resource graph"""
        self.graph = ResourceGraph()

    def test_single_node_all_common(self):
        """Test that all mounts are common in single-node cluster"""
        data = {
            'fs': [
                {'device': '/dev/sda1', 'mount': '/mnt/data1'},
                {'device': '/dev/sdb1', 'mount': '/mnt/data2'}
            ]
        }
        self.graph.add_node_data('node1', data)

        common = self.graph.get_common_storage()
        self.assertEqual(len(common), 2)
        self.assertIn('/mnt/data1', common)
        self.assertIn('/mnt/data2', common)

    def test_multi_node_common_mounts(self):
        """Test detection of common mounts across multiple nodes"""
        data1 = {
            'fs': [
                {'device': '/dev/sda1', 'mount': '/shared'},
                {'device': '/dev/sdb1', 'mount': '/local1'}
            ]
        }
        data2 = {
            'fs': [
                {'device': '/dev/sdc1', 'mount': '/shared'},
                {'device': '/dev/sdd1', 'mount': '/local2'}
            ]
        }

        self.graph.add_node_data('node1', data1)
        self.graph.add_node_data('node2', data2)

        common = self.graph.get_common_storage()
        self.assertEqual(len(common), 1)
        self.assertIn('/shared', common)
        self.assertEqual(len(common['/shared']), 2)

    def test_shared_flag_multi_node(self):
        """Test that shared flag is set for common mounts in multi-node"""
        data1 = {'fs': [{'device': '/dev/sda1', 'mount': '/shared'}]}
        data2 = {'fs': [{'device': '/dev/sdb1', 'mount': '/shared'}]}

        self.graph.add_node_data('node1', data1)
        self.graph.add_node_data('node2', data2)

        # Check that shared flag is True
        for device in self.graph.nodes['node1']:
            if device['mount'] == '/shared':
                self.assertTrue(device['shared'])
        for device in self.graph.nodes['node2']:
            if device['mount'] == '/shared':
                self.assertTrue(device['shared'])

    def test_shared_flag_single_node(self):
        """Test that shared flag is set for all mounts in single-node"""
        data = {'fs': [{'device': '/dev/sda1', 'mount': '/data'}]}
        self.graph.add_node_data('node1', data)

        self.assertTrue(self.graph.nodes['node1'][0]['shared'])

    def test_no_common_mounts(self):
        """Test when there are no common mounts across nodes"""
        data1 = {'fs': [{'device': '/dev/sda1', 'mount': '/local1'}]}
        data2 = {'fs': [{'device': '/dev/sdb1', 'mount': '/local2'}]}

        self.graph.add_node_data('node1', data1)
        self.graph.add_node_data('node2', data2)

        common = self.graph.get_common_storage()
        self.assertEqual(len(common), 0)

    def test_three_nodes_partial_overlap(self):
        """Test common mounts with three nodes and partial overlap"""
        data1 = {'fs': [{'device': '/dev/sda1', 'mount': '/shared'}]}
        data2 = {'fs': [{'device': '/dev/sdb1', 'mount': '/shared'}]}
        data3 = {'fs': [{'device': '/dev/sdc1', 'mount': '/local'}]}

        self.graph.add_node_data('node1', data1)
        self.graph.add_node_data('node2', data2)
        self.graph.add_node_data('node3', data3)

        common = self.graph.get_common_storage()
        self.assertEqual(len(common), 1)
        self.assertIn('/shared', common)
        self.assertEqual(len(common['/shared']), 2)


class TestResourceGraphGetters(unittest.TestCase):
    """Tests for getter methods"""

    def setUp(self):
        """Set up test resource graph with sample data"""
        self.graph = ResourceGraph()
        data1 = {
            'fs': [
                {'device': '/dev/sda1', 'mount': '/shared', 'dev_type': 'ssd'},
                {'device': '/dev/sdb1', 'mount': '/local1', 'dev_type': 'hdd'}
            ]
        }
        data2 = {
            'fs': [
                {'device': '/dev/sdc1', 'mount': '/shared', 'dev_type': 'ssd'}
            ]
        }
        self.graph.add_node_data('node1', data1)
        self.graph.add_node_data('node2', data2)

    def test_get_node_storage(self):
        """Test getting storage for specific node"""
        storage = self.graph.get_node_storage('node1')
        self.assertEqual(len(storage), 2)

        storage2 = self.graph.get_node_storage('node2')
        self.assertEqual(len(storage2), 1)

    def test_get_node_storage_nonexistent(self):
        """Test getting storage for non-existent node"""
        storage = self.graph.get_node_storage('nonexistent')
        self.assertEqual(storage, [])

    def test_get_all_nodes(self):
        """Test getting list of all nodes"""
        nodes = self.graph.get_all_nodes()
        self.assertEqual(len(nodes), 2)
        self.assertIn('node1', nodes)
        self.assertIn('node2', nodes)

    def test_get_all_nodes_empty(self):
        """Test getting nodes from empty graph"""
        empty_graph = ResourceGraph()
        nodes = empty_graph.get_all_nodes()
        self.assertEqual(nodes, [])

    def test_get_common_storage_returns_copy(self):
        """Test that get_common_storage returns a copy"""
        common1 = self.graph.get_common_storage()
        common2 = self.graph.get_common_storage()

        # Modify one copy
        if common1:
            common1.clear()

        # Original should still have data
        self.assertGreater(len(self.graph.common_mounts), 0)


class TestResourceGraphSummary(unittest.TestCase):
    """Tests for storage summary statistics"""

    def setUp(self):
        """Set up test resource graph"""
        self.graph = ResourceGraph()

    def test_summary_empty_graph(self):
        """Test summary of empty graph"""
        summary = self.graph.get_storage_summary()

        self.assertEqual(summary['total_nodes'], 0)
        self.assertEqual(summary['total_devices'], 0)
        self.assertEqual(summary['common_mount_points'], 0)
        self.assertEqual(summary['device_types'], {})
        self.assertEqual(summary['filesystem_types'], {})

    def test_summary_single_node(self):
        """Test summary with single node"""
        data = {
            'fs': [
                {'device': '/dev/sda1', 'mount': '/data1', 'dev_type': 'ssd', 'fs_type': 'ext4'},
                {'device': '/dev/sdb1', 'mount': '/data2', 'dev_type': 'hdd', 'fs_type': 'xfs'}
            ]
        }
        self.graph.add_node_data('node1', data)

        summary = self.graph.get_storage_summary()
        self.assertEqual(summary['total_nodes'], 1)
        self.assertEqual(summary['total_devices'], 2)
        self.assertEqual(summary['common_mount_points'], 2)
        self.assertEqual(summary['device_types']['ssd'], 1)
        self.assertEqual(summary['device_types']['hdd'], 1)
        self.assertEqual(summary['filesystem_types']['ext4'], 1)
        self.assertEqual(summary['filesystem_types']['xfs'], 1)

    def test_summary_multiple_nodes(self):
        """Test summary with multiple nodes"""
        data1 = {
            'fs': [
                {'device': '/dev/sda1', 'mount': '/shared', 'dev_type': 'ssd', 'fs_type': 'ext4'},
                {'device': '/dev/sdb1', 'mount': '/local1', 'dev_type': 'hdd', 'fs_type': 'xfs'}
            ]
        }
        data2 = {
            'fs': [
                {'device': '/dev/sdc1', 'mount': '/shared', 'dev_type': 'ssd', 'fs_type': 'ext4'},
                {'device': '/dev/sdd1', 'mount': '/local2', 'dev_type': 'nvme', 'fs_type': 'btrfs'}
            ]
        }

        self.graph.add_node_data('node1', data1)
        self.graph.add_node_data('node2', data2)

        summary = self.graph.get_storage_summary()
        self.assertEqual(summary['total_nodes'], 2)
        self.assertEqual(summary['total_devices'], 4)
        self.assertEqual(summary['common_mount_points'], 1)
        self.assertEqual(summary['device_types']['ssd'], 2)
        self.assertEqual(summary['device_types']['hdd'], 1)
        self.assertEqual(summary['device_types']['nvme'], 1)

    def test_summary_device_type_aggregation(self):
        """Test that device types are properly aggregated"""
        data1 = {'fs': [{'device': '/dev/sda', 'dev_type': 'ssd', 'fs_type': 'ext4'}]}
        data2 = {'fs': [{'device': '/dev/sdb', 'dev_type': 'ssd', 'fs_type': 'ext4'}]}
        data3 = {'fs': [{'device': '/dev/sdc', 'dev_type': 'hdd', 'fs_type': 'ext4'}]}

        self.graph.add_node_data('node1', data1)
        self.graph.add_node_data('node2', data2)
        self.graph.add_node_data('node3', data3)

        summary = self.graph.get_storage_summary()
        self.assertEqual(summary['device_types']['ssd'], 2)
        self.assertEqual(summary['device_types']['hdd'], 1)


class TestResourceGraphFiltering(unittest.TestCase):
    """Tests for filtering operations"""

    def setUp(self):
        """Set up test resource graph with diverse devices"""
        self.graph = ResourceGraph()
        data1 = {
            'fs': [
                {'device': '/dev/sda1', 'mount': '/mnt/ssd1', 'dev_type': 'ssd'},
                {'device': '/dev/sdb1', 'mount': '/mnt/hdd1', 'dev_type': 'hdd'},
                {'device': '/dev/sdc1', 'mount': '/mnt/nvme1', 'dev_type': 'nvme'}
            ]
        }
        data2 = {
            'fs': [
                {'device': '/dev/sdd1', 'mount': '/mnt/ssd2', 'dev_type': 'ssd'},
                {'device': '/dev/sde1', 'mount': '/home/data', 'dev_type': 'hdd'}
            ]
        }
        self.graph.add_node_data('node1', data1)
        self.graph.add_node_data('node2', data2)

    def test_filter_by_type_ssd(self):
        """Test filtering by SSD device type"""
        filtered = self.graph.filter_by_type('ssd')

        self.assertEqual(len(filtered), 2)
        self.assertEqual(len(filtered['node1']), 1)
        self.assertEqual(len(filtered['node2']), 1)
        self.assertEqual(filtered['node1'][0]['device'], '/dev/sda1')

    def test_filter_by_type_hdd(self):
        """Test filtering by HDD device type"""
        filtered = self.graph.filter_by_type('hdd')

        self.assertEqual(len(filtered), 2)
        self.assertEqual(len(filtered['node1']), 1)
        self.assertEqual(len(filtered['node2']), 1)

    def test_filter_by_type_nvme(self):
        """Test filtering by NVMe device type"""
        filtered = self.graph.filter_by_type('nvme')

        self.assertEqual(len(filtered), 1)
        self.assertIn('node1', filtered)
        self.assertEqual(len(filtered['node1']), 1)

    def test_filter_by_type_nonexistent(self):
        """Test filtering by non-existent device type"""
        filtered = self.graph.filter_by_type('tape')
        self.assertEqual(filtered, {})

    def test_filter_by_mount_pattern_mnt(self):
        """Test filtering by mount pattern '/mnt'"""
        filtered = self.graph.filter_by_mount_pattern('/mnt')

        self.assertEqual(len(filtered), 2)
        self.assertEqual(len(filtered['node1']), 3)
        self.assertEqual(len(filtered['node2']), 1)

    def test_filter_by_mount_pattern_home(self):
        """Test filtering by mount pattern '/home'"""
        filtered = self.graph.filter_by_mount_pattern('/home')

        self.assertEqual(len(filtered), 1)
        self.assertIn('node2', filtered)
        self.assertEqual(len(filtered['node2']), 1)

    def test_filter_by_mount_pattern_ssd(self):
        """Test filtering by mount pattern containing 'ssd'"""
        filtered = self.graph.filter_by_mount_pattern('ssd')

        self.assertEqual(len(filtered), 2)
        self.assertEqual(len(filtered['node1']), 1)
        self.assertEqual(len(filtered['node2']), 1)

    def test_filter_by_mount_pattern_no_match(self):
        """Test filtering with pattern that matches nothing"""
        filtered = self.graph.filter_by_mount_pattern('/nonexistent')
        self.assertEqual(filtered, {})


class TestResourceGraphSerialization(unittest.TestCase):
    """Tests for save/load operations"""

    def setUp(self):
        """Set up test resource graph and temp directory"""
        self.graph = ResourceGraph()
        self.temp_dir = tempfile.mkdtemp()

    def tearDown(self):
        """Clean up temp directory"""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    def test_save_to_yaml(self):
        """Test saving resource graph to YAML file"""
        data = {
            'fs': [
                {'device': '/dev/sda1', 'mount': '/data', 'dev_type': 'ssd', 'avail': '100G'}
            ]
        }
        self.graph.add_node_data('node1', data)

        output_path = Path(self.temp_dir) / 'test.yaml'
        self.graph.save_to_file(output_path, format='yaml')

        self.assertTrue(output_path.exists())

        # Verify content
        with open(output_path, 'r') as f:
            loaded_data = yaml.safe_load(f)

        self.assertIn('fs', loaded_data)
        self.assertEqual(len(loaded_data['fs']), 1)

    def test_save_to_json(self):
        """Test saving resource graph to JSON file"""
        data = {
            'fs': [
                {'device': '/dev/sda1', 'mount': '/data', 'dev_type': 'ssd'}
            ]
        }
        self.graph.add_node_data('node1', data)

        output_path = Path(self.temp_dir) / 'test.json'
        self.graph.save_to_file(output_path, format='json')

        self.assertTrue(output_path.exists())

        # Verify content
        with open(output_path, 'r') as f:
            loaded_data = json.load(f)

        self.assertIn('fs', loaded_data)

    def test_save_only_common_mounts(self):
        """Test that only common mounts are saved"""
        data1 = {'fs': [{'device': '/dev/sda1', 'mount': '/shared'}]}
        data2 = {'fs': [
            {'device': '/dev/sdb1', 'mount': '/shared'},
            {'device': '/dev/sdc1', 'mount': '/local'}
        ]}

        self.graph.add_node_data('node1', data1)
        self.graph.add_node_data('node2', data2)

        output_path = Path(self.temp_dir) / 'test.yaml'
        self.graph.save_to_file(output_path)

        with open(output_path, 'r') as f:
            loaded_data = yaml.safe_load(f)

        # Should only save /shared (common mount)
        self.assertEqual(len(loaded_data['fs']), 1)
        self.assertEqual(loaded_data['fs'][0]['mount'], '/shared')

    def test_save_removes_hostname(self):
        """Test that hostname is removed from saved data"""
        data = {'fs': [{'device': '/dev/sda1', 'mount': '/data'}]}
        self.graph.add_node_data('node1', data)

        output_path = Path(self.temp_dir) / 'test.yaml'
        self.graph.save_to_file(output_path)

        with open(output_path, 'r') as f:
            loaded_data = yaml.safe_load(f)

        self.assertNotIn('hostname', loaded_data['fs'][0])

    def test_load_from_yaml(self):
        """Test loading resource graph from YAML file"""
        yaml_content = {
            'fs': [
                {
                    'device': '/dev/sda1',
                    'mount': '/data',
                    'dev_type': 'ssd',
                    'fs_type': 'ext4',
                    'avail': '100G'
                }
            ]
        }

        yaml_path = Path(self.temp_dir) / 'test.yaml'
        with open(yaml_path, 'w') as f:
            yaml.dump(yaml_content, f)

        self.graph.load_from_file(yaml_path)

        self.assertEqual(len(self.graph.nodes), 1)
        self.assertIn('test', self.graph.nodes)  # Filename becomes hostname
        self.assertEqual(len(self.graph.nodes['test']), 1)
        self.assertEqual(self.graph.nodes['test'][0]['device'], '/dev/sda1')

    def test_load_from_json(self):
        """Test loading resource graph from JSON file"""
        json_content = {
            'fs': [
                {
                    'device': '/dev/sda1',
                    'mount': '/data',
                    'dev_type': 'ssd'
                }
            ]
        }

        json_path = Path(self.temp_dir) / 'test.json'
        with open(json_path, 'w') as f:
            json.dump(json_content, f)

        self.graph.load_from_file(json_path)

        self.assertEqual(len(self.graph.nodes), 1)
        self.assertIn('test', self.graph.nodes)

    def test_load_clears_existing_data(self):
        """Test that loading clears existing data"""
        data = {'fs': [{'device': '/dev/existing', 'mount': '/old'}]}
        self.graph.add_node_data('oldnode', data)

        yaml_content = {'fs': [{'device': '/dev/new', 'mount': '/new'}]}
        yaml_path = Path(self.temp_dir) / 'new.yaml'
        with open(yaml_path, 'w') as f:
            yaml.dump(yaml_content, f)

        self.graph.load_from_file(yaml_path)

        self.assertNotIn('oldnode', self.graph.nodes)
        self.assertIn('new', self.graph.nodes)

    def test_load_expands_environment_variables(self):
        """Test that environment variables in mount paths are expanded"""
        os.environ['TEST_MOUNT'] = '/expanded/path'

        yaml_content = {'fs': [{'device': '/dev/sda1', 'mount': '$TEST_MOUNT/data'}]}
        yaml_path = Path(self.temp_dir) / 'test.yaml'
        with open(yaml_path, 'w') as f:
            yaml.dump(yaml_content, f)

        self.graph.load_from_file(yaml_path)

        self.assertEqual(self.graph.nodes['test'][0]['mount'], '/expanded/path/data')

    def test_load_invalid_format_raises_error(self):
        """Test that loading invalid format raises ValueError"""
        yaml_content = {'invalid': 'no fs section'}
        yaml_path = Path(self.temp_dir) / 'invalid.yaml'
        with open(yaml_path, 'w') as f:
            yaml.dump(yaml_content, f)

        with self.assertRaises(ValueError) as context:
            self.graph.load_from_file(yaml_path)

        self.assertIn('Invalid resource graph format', str(context.exception))

    def test_save_load_roundtrip(self):
        """Test that save and load preserve data correctly"""
        data = {
            'fs': [
                {
                    'device': '/dev/sda1',
                    'mount': '/data',
                    'dev_type': 'ssd',
                    'fs_type': 'ext4',
                    'avail': '500G',
                    'model': 'Samsung 970'
                }
            ]
        }
        self.graph.add_node_data('node1', data)

        # Save
        save_path = Path(self.temp_dir) / 'roundtrip.yaml'
        self.graph.save_to_file(save_path)

        # Load into new graph
        new_graph = ResourceGraph()
        new_graph.load_from_file(save_path)

        # Verify data
        self.assertEqual(len(new_graph.nodes), 1)
        loaded_device = new_graph.nodes['roundtrip'][0]
        self.assertEqual(loaded_device['device'], '/dev/sda1')
        self.assertEqual(loaded_device['mount'], '/data')
        self.assertEqual(loaded_device['dev_type'], 'ssd')

    def test_save_empty_graph(self):
        """Test saving empty graph"""
        output_path = Path(self.temp_dir) / 'empty.yaml'
        self.graph.save_to_file(output_path)

        self.assertTrue(output_path.exists())

        with open(output_path, 'r') as f:
            loaded_data = yaml.safe_load(f)

        self.assertEqual(loaded_data['fs'], [])


class TestResourceGraphPrintMethods(unittest.TestCase):
    """Tests for print/display methods"""

    def setUp(self):
        """Set up test resource graph"""
        self.graph = ResourceGraph()

    def test_print_summary_empty(self):
        """Test print_summary with empty graph"""
        # Should not raise any errors
        self.graph.print_summary()

    def test_print_summary_with_data(self):
        """Test print_summary with actual data"""
        data = {
            'fs': [
                {'device': '/dev/sda1', 'mount': '/data', 'dev_type': 'ssd', 'fs_type': 'ext4'}
            ]
        }
        self.graph.add_node_data('node1', data)

        # Should not raise any errors
        self.graph.print_summary()

    def test_print_common_storage_empty(self):
        """Test print_common_storage with no common storage"""
        data1 = {'fs': [{'device': '/dev/sda1', 'mount': '/local1'}]}
        data2 = {'fs': [{'device': '/dev/sdb1', 'mount': '/local2'}]}

        self.graph.add_node_data('node1', data1)
        self.graph.add_node_data('node2', data2)

        # Should handle gracefully
        self.graph.print_common_storage()

    def test_print_common_storage_single_node(self):
        """Test print_common_storage with single node"""
        data = {'fs': [{'device': '/dev/sda1', 'mount': '/data'}]}
        self.graph.add_node_data('node1', data)

        # Should not raise any errors
        self.graph.print_common_storage()

    def test_print_common_storage_multi_node(self):
        """Test print_common_storage with multiple nodes"""
        data1 = {'fs': [{'device': '/dev/sda1', 'mount': '/shared'}]}
        data2 = {'fs': [{'device': '/dev/sdb1', 'mount': '/shared'}]}

        self.graph.add_node_data('node1', data1)
        self.graph.add_node_data('node2', data2)

        # Should not raise any errors
        self.graph.print_common_storage()

    def test_print_common_storage_with_performance_info(self):
        """Test print_common_storage includes performance info"""
        data = {
            'fs': [{
                'device': '/dev/sda1',
                'mount': '/data',
                '4k_randwrite_bw': '500MB/s',
                '1m_seqwrite_bw': '2GB/s'
            }]
        }
        self.graph.add_node_data('node1', data)

        # Should not raise any errors
        self.graph.print_common_storage()

    def test_print_node_details_existing(self):
        """Test print_node_details for existing node"""
        data = {
            'fs': [
                {
                    'device': '/dev/sda1',
                    'mount': '/data',
                    'dev_type': 'ssd',
                    'fs_type': 'ext4',
                    'avail': '100G',
                    'model': 'Samsung 970',
                    '4k_randwrite_bw': '500MB/s',
                    '1m_seqwrite_bw': '2GB/s'
                }
            ]
        }
        self.graph.add_node_data('node1', data)

        # Should not raise any errors
        self.graph.print_node_details('node1')

    def test_print_node_details_nonexistent(self):
        """Test print_node_details for non-existent node"""
        # Should handle gracefully
        self.graph.print_node_details('nonexistent')

    def test_print_node_details_no_performance(self):
        """Test print_node_details without performance data"""
        data = {'fs': [{'device': '/dev/sda1', 'mount': '/data'}]}
        self.graph.add_node_data('node1', data)

        # Should not raise any errors
        self.graph.print_node_details('node1')


class TestResourceGraphEdgeCases(unittest.TestCase):
    """Tests for edge cases and error handling"""

    def setUp(self):
        """Set up test resource graph"""
        self.graph = ResourceGraph()

    def test_add_node_multiple_times(self):
        """Test adding data to same node multiple times"""
        data1 = {'fs': [{'device': '/dev/sda1', 'mount': '/data1'}]}
        data2 = {'fs': [{'device': '/dev/sdb1', 'mount': '/data2'}]}

        self.graph.add_node_data('node1', data1)
        self.graph.add_node_data('node1', data2)

        # Should accumulate devices
        self.assertEqual(len(self.graph.nodes['node1']), 2)

    def test_device_with_special_characters(self):
        """Test handling devices with special characters"""
        data = {
            'fs': [{
                'device': '/dev/mapper/vg-lv_name',
                'mount': '/mnt/special-path_123'
            }]
        }
        self.graph.add_node_data('node1', data)

        self.assertEqual(self.graph.nodes['node1'][0]['device'], '/dev/mapper/vg-lv_name')
        self.assertEqual(self.graph.nodes['node1'][0]['mount'], '/mnt/special-path_123')

    def test_empty_mount_point(self):
        """Test handling empty mount point"""
        data = {'fs': [{'device': '/dev/sda1', 'mount': ''}]}
        self.graph.add_node_data('node1', data)

        common = self.graph.get_common_storage()
        self.assertIn('', common)

    def test_duplicate_mount_same_node(self):
        """Test same mount point multiple times on same node"""
        data = {
            'fs': [
                {'device': '/dev/sda1', 'mount': '/data'},
                {'device': '/dev/sdb1', 'mount': '/data'}
            ]
        }
        self.graph.add_node_data('node1', data)

        # Both should be stored
        self.assertEqual(len(self.graph.nodes['node1']), 2)

    def test_very_long_values(self):
        """Test handling very long string values"""
        long_model = 'A' * 1000
        data = {'fs': [{'device': '/dev/sda1', 'model': long_model}]}

        self.graph.add_node_data('node1', data)
        self.assertEqual(self.graph.nodes['node1'][0]['model'], long_model)

    def test_unicode_in_values(self):
        """Test handling unicode characters"""
        data = {
            'fs': [{
                'device': '/dev/sda1',
                'mount': '/data/测试',
                'model': 'Samsung™ 970 EVO'
            }]
        }
        self.graph.add_node_data('node1', data)

        self.assertEqual(self.graph.nodes['node1'][0]['mount'], '/data/测试')


if __name__ == '__main__':
    unittest.main()
```

### `test/unit/util/test_size_type.py`

```python
"""
Tests for size_type.py - SizeType class and utilities
"""
import unittest
import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from jarvis_cd.util.size_type import SizeType, size_to_bytes, human_readable_size


class TestSizeType(unittest.TestCase):
    """Tests for SizeType class"""

    def test_size_type_from_string_kb(self):
        """Test SizeType from kilobyte string"""
        size = SizeType('10k')
        self.assertEqual(size.bytes, 10 * 1024)

    def test_size_type_from_string_mb(self):
        """Test SizeType from megabyte string"""
        size = SizeType('5M')
        self.assertEqual(size.bytes, 5 * 1024 * 1024)

    def test_size_type_from_string_gb(self):
        """Test SizeType from gigabyte string"""
        size = SizeType('2G')
        self.assertEqual(size.bytes, 2 * 1024 * 1024 * 1024)

    def test_size_type_from_string_tb(self):
        """Test SizeType from terabyte string"""
        size = SizeType('1T')
        self.assertEqual(size.bytes, 1 * 1024 * 1024 * 1024 * 1024)
        size2 = SizeType('2t')  # lowercase
        self.assertEqual(size2.bytes, 2 * 1024 * 1024 * 1024 * 1024)

    def test_size_type_from_int(self):
        """Test SizeType from integer (bytes)"""
        size = SizeType(1024)
        self.assertEqual(size.bytes, 1024)

    def test_size_type_from_float(self):
        """Test SizeType from float"""
        size = SizeType(1536.5)
        self.assertEqual(size.bytes, int(1536.5))

    def test_size_type_str_representation(self):
        """Test string representation of SizeType"""
        size = SizeType('1M')
        str_rep = str(size)
        self.assertIn('1048576', str_rep)  # 1M in bytes
        self.assertIn('B', str_rep)

    def test_size_type_repr(self):
        """Test repr representation of SizeType"""
        size = SizeType(2048)
        repr_str = repr(size)
        self.assertIn('SizeType', repr_str)
        self.assertIn('2048', repr_str)
        self.assertIn('bytes', repr_str)

    def test_size_type_comparison(self):
        """Test SizeType comparison operators"""
        size1 = SizeType('1k')
        size2 = SizeType('2k')
        size3 = SizeType('1024')  # Same as 1k

        self.assertLess(size1.bytes, size2.bytes)
        self.assertEqual(size1.bytes, size3.bytes)

    def test_equality_with_sizetype(self):
        """Test equality comparison with another SizeType"""
        size1 = SizeType(1024)
        size2 = SizeType('1k')
        size3 = SizeType(2048)

        self.assertEqual(size1, size2)
        self.assertNotEqual(size1, size3)

    def test_equality_with_int(self):
        """Test equality comparison with int"""
        size = SizeType(1024)
        self.assertEqual(size, 1024)
        self.assertNotEqual(size, 2048)

    def test_equality_with_float(self):
        """Test equality comparison with float"""
        size = SizeType(1536)
        self.assertEqual(size, 1536.0)
        self.assertNotEqual(size, 1536.5)

    def test_equality_with_other_type(self):
        """Test equality comparison with unsupported type returns False"""
        size = SizeType(1024)
        self.assertFalse(size == "1024")
        self.assertFalse(size == [1024])

    def test_less_than_comparison(self):
        """Test less than comparison"""
        size1 = SizeType(1024)
        size2 = SizeType(2048)

        # SizeType vs SizeType
        self.assertTrue(size1 < size2)
        self.assertFalse(size2 < size1)

        # SizeType vs int
        self.assertTrue(size1 < 2048)
        self.assertFalse(size1 < 512)

    def test_less_than_equal_comparison(self):
        """Test less than or equal comparison"""
        size1 = SizeType(1024)
        size2 = SizeType(1024)
        size3 = SizeType(2048)

        self.assertTrue(size1 <= size2)
        self.assertTrue(size1 <= size3)
        self.assertFalse(size3 <= size1)

    def test_greater_than_comparison(self):
        """Test greater than comparison"""
        size1 = SizeType(2048)
        size2 = SizeType(1024)

        # SizeType vs SizeType
        self.assertTrue(size1 > size2)
        self.assertFalse(size2 > size1)

        # SizeType vs int
        self.assertTrue(size1 > 1024)
        self.assertFalse(size1 > 4096)

    def test_greater_than_equal_comparison(self):
        """Test greater than or equal comparison"""
        size1 = SizeType(1024)
        size2 = SizeType(1024)
        size3 = SizeType(512)

        self.assertTrue(size1 >= size2)
        self.assertTrue(size1 >= size3)
        self.assertFalse(size3 >= size1)

    def test_addition_with_sizetype(self):
        """Test addition with another SizeType"""
        size1 = SizeType(1024)
        size2 = SizeType(512)
        result = size1 + size2

        self.assertIsInstance(result, SizeType)
        self.assertEqual(result.bytes, 1536)

    def test_addition_with_int(self):
        """Test addition with int"""
        size = SizeType(1024)
        result = size + 512

        self.assertIsInstance(result, SizeType)
        self.assertEqual(result.bytes, 1536)

    def test_subtraction_with_sizetype(self):
        """Test subtraction with another SizeType"""
        size1 = SizeType(2048)
        size2 = SizeType(1024)
        result = size1 - size2

        self.assertIsInstance(result, SizeType)
        self.assertEqual(result.bytes, 1024)

    def test_subtraction_with_int(self):
        """Test subtraction with int"""
        size = SizeType(2048)
        result = size - 1024

        self.assertIsInstance(result, SizeType)
        self.assertEqual(result.bytes, 1024)

    def test_multiplication(self):
        """Test multiplication"""
        size = SizeType(1024)
        result = size * 2

        self.assertIsInstance(result, SizeType)
        self.assertEqual(result.bytes, 2048)

        result2 = size * 1.5
        self.assertEqual(result2.bytes, 1536)

    def test_division_with_number(self):
        """Test division with int or float"""
        size = SizeType(2048)
        result = size / 2

        self.assertIsInstance(result, SizeType)
        self.assertEqual(result.bytes, 1024)

        result2 = size / 4.0
        self.assertEqual(result2.bytes, 512)

    def test_division_with_sizetype(self):
        """Test division with SizeType returns ratio as float"""
        size1 = SizeType(2048)
        size2 = SizeType(1024)
        result = size1 / size2

        self.assertIsInstance(result, float)
        self.assertEqual(result, 2.0)

    def test_int_conversion(self):
        """Test conversion to int"""
        size = SizeType('1k')
        self.assertEqual(int(size), 1024)

    def test_float_conversion(self):
        """Test conversion to float"""
        size = SizeType('1k')
        self.assertEqual(float(size), 1024.0)

    def test_bytes_property(self):
        """Test bytes property"""
        size = SizeType('2k')
        self.assertEqual(size.bytes, 2048)

    def test_to_bytes_method(self):
        """Test to_bytes method"""
        size = SizeType('3k')
        self.assertEqual(size.to_bytes(), 3072)

    def test_kilobytes_property(self):
        """Test kilobytes property"""
        size = SizeType(2048)
        self.assertEqual(size.kilobytes, 2.0)

    def test_megabytes_property(self):
        """Test megabytes property"""
        size = SizeType('2M')
        self.assertEqual(size.megabytes, 2.0)

    def test_gigabytes_property(self):
        """Test gigabytes property"""
        size = SizeType('3G')
        self.assertEqual(size.gigabytes, 3.0)

    def test_terabytes_property(self):
        """Test terabytes property"""
        size = SizeType('1T')
        self.assertEqual(size.terabytes, 1.0)

    def test_parse_classmethod(self):
        """Test parse class method"""
        size = SizeType.parse('5M')
        self.assertIsInstance(size, SizeType)
        self.assertEqual(size.bytes, 5 * 1024 * 1024)

    def test_from_bytes_classmethod(self):
        """Test from_bytes class method"""
        size = SizeType.from_bytes(4096)
        self.assertIsInstance(size, SizeType)
        self.assertEqual(size.bytes, 4096)

    def test_from_kilobytes_classmethod(self):
        """Test from_kilobytes class method"""
        size = SizeType.from_kilobytes(5)
        self.assertEqual(size.bytes, 5 * 1024)

    def test_from_megabytes_classmethod(self):
        """Test from_megabytes class method"""
        size = SizeType.from_megabytes(3)
        self.assertEqual(size.bytes, 3 * 1024 * 1024)

    def test_from_gigabytes_classmethod(self):
        """Test from_gigabytes class method"""
        size = SizeType.from_gigabytes(2)
        self.assertEqual(size.bytes, 2 * 1024 * 1024 * 1024)

    def test_from_terabytes_classmethod(self):
        """Test from_terabytes class method"""
        size = SizeType.from_terabytes(1)
        self.assertEqual(size.bytes, 1024 * 1024 * 1024 * 1024)

    def test_empty_string_error(self):
        """Test empty string raises ValueError"""
        with self.assertRaises(ValueError) as ctx:
            SizeType('')
        self.assertIn('Empty', str(ctx.exception))

    def test_whitespace_string_error(self):
        """Test whitespace-only string raises ValueError"""
        with self.assertRaises(ValueError) as ctx:
            SizeType('   ')
        self.assertIn('Empty', str(ctx.exception))

    def test_invalid_format_error(self):
        """Test invalid format raises ValueError"""
        with self.assertRaises(ValueError) as ctx:
            SizeType('abc')
        self.assertIn('Invalid size format', str(ctx.exception))

    def test_negative_number_error(self):
        """Test negative number raises ValueError"""
        with self.assertRaises(ValueError) as ctx:
            SizeType('-10')
        # Negative numbers are caught by regex pattern, not specific negative check
        self.assertIn('Invalid size format', str(ctx.exception))

    def test_unknown_multiplier_error(self):
        """Test unknown multiplier raises ValueError"""
        # The regex pattern only accepts k/m/g/t, so this should fail at pattern match
        with self.assertRaises(ValueError) as ctx:
            SizeType('10X')
        self.assertIn('Invalid size format', str(ctx.exception))

    def test_to_human_readable_zero(self):
        """Test human readable format for zero"""
        size = SizeType(0)
        self.assertEqual(size.to_human_readable(), '0B')

    def test_to_human_readable_bytes(self):
        """Test human readable format for bytes"""
        size = SizeType(512)
        self.assertEqual(size.to_human_readable(), '512B')

    def test_to_human_readable_kilobytes(self):
        """Test human readable format for kilobytes"""
        size = SizeType(1024)
        self.assertEqual(size.to_human_readable(), '1K')

        size2 = SizeType(int(1.5 * 1024))
        self.assertEqual(size2.to_human_readable(), '1.5K')

    def test_to_human_readable_megabytes(self):
        """Test human readable format for megabytes"""
        size = SizeType(2 * 1024 * 1024)
        self.assertEqual(size.to_human_readable(), '2M')

    def test_to_human_readable_gigabytes(self):
        """Test human readable format for gigabytes"""
        size = SizeType(3 * 1024 * 1024 * 1024)
        self.assertEqual(size.to_human_readable(), '3G')

    def test_to_human_readable_terabytes(self):
        """Test human readable format for terabytes"""
        size = SizeType(2 * 1024 * 1024 * 1024 * 1024)
        self.assertEqual(size.to_human_readable(), '2T')

    def test_whitespace_in_string(self):
        """Test size string with whitespace"""
        size = SizeType('  10k  ')
        self.assertEqual(size.bytes, 10 * 1024)

    def test_uppercase_and_lowercase_multipliers(self):
        """Test both uppercase and lowercase multipliers"""
        size_k = SizeType('1k')
        size_K = SizeType('1K')
        self.assertEqual(size_k.bytes, size_K.bytes)

        size_m = SizeType('1m')
        size_M = SizeType('1M')
        self.assertEqual(size_m.bytes, size_M.bytes)

    def test_decimal_numbers(self):
        """Test decimal numbers with multipliers"""
        size = SizeType('1.5k')
        self.assertEqual(size.bytes, int(1.5 * 1024))

        size2 = SizeType('2.75M')
        self.assertEqual(size2.bytes, int(2.75 * 1024 * 1024))

    def test_with_b_suffix(self):
        """Test strings with 'B' or 'bytes' suffix"""
        size1 = SizeType('10kB')
        self.assertEqual(size1.bytes, 10 * 1024)

        size2 = SizeType('5MB')
        self.assertEqual(size2.bytes, 5 * 1024 * 1024)

    def test_notimplemented_comparisons(self):
        """Test comparison with unsupported types returns NotImplemented"""
        size = SizeType(1024)
        # These should return NotImplemented, which Python handles
        result = size.__lt__("1024")
        self.assertEqual(result, NotImplemented)

        result = size.__gt__("1024")
        self.assertEqual(result, NotImplemented)

    def test_notimplemented_arithmetic(self):
        """Test arithmetic with unsupported types returns NotImplemented"""
        size = SizeType(1024)

        # Addition with unsupported type
        result = size.__add__("1024")
        self.assertEqual(result, NotImplemented)

        # Subtraction with unsupported type
        result = size.__sub__("1024")
        self.assertEqual(result, NotImplemented)

        # Multiplication with unsupported type
        result = size.__mul__("2")
        self.assertEqual(result, NotImplemented)

        # Division with unsupported type
        result = size.__truediv__("2")
        self.assertEqual(result, NotImplemented)


class TestSizeToBytes(unittest.TestCase):
    """Tests for size_to_bytes function"""

    def test_bytes_suffix(self):
        """Test conversion with B suffix"""
        self.assertEqual(size_to_bytes('100B'), 100)
        self.assertEqual(size_to_bytes('100'), 100)

    def test_kilobyte_suffix(self):
        """Test conversion with k/K suffix"""
        self.assertEqual(size_to_bytes('1k'), 1024)
        self.assertEqual(size_to_bytes('1K'), 1024)
        self.assertEqual(size_to_bytes('10k'), 10 * 1024)

    def test_megabyte_suffix(self):
        """Test conversion with M suffix"""
        self.assertEqual(size_to_bytes('1M'), 1024 * 1024)
        self.assertEqual(size_to_bytes('5M'), 5 * 1024 * 1024)

    def test_gigabyte_suffix(self):
        """Test conversion with G suffix"""
        self.assertEqual(size_to_bytes('1G'), 1024 * 1024 * 1024)
        self.assertEqual(size_to_bytes('2G'), 2 * 1024 * 1024 * 1024)

    def test_terabyte_suffix(self):
        """Test conversion with T suffix"""
        self.assertEqual(size_to_bytes('1T'), 1024 * 1024 * 1024 * 1024)

    def test_integer_input(self):
        """Test integer input returns as-is"""
        self.assertEqual(size_to_bytes(1024), 1024)
        self.assertEqual(size_to_bytes(5000), 5000)

    def test_float_input(self):
        """Test float input returns int"""
        self.assertEqual(size_to_bytes(1536.7), 1536)

    def test_decimal_with_suffix(self):
        """Test decimal numbers with suffix"""
        self.assertEqual(size_to_bytes('1.5k'), int(1.5 * 1024))
        self.assertEqual(size_to_bytes('2.5M'), int(2.5 * 1024 * 1024))


class TestHumanReadableSize(unittest.TestCase):
    """Tests for human_readable_size function"""

    def test_bytes_range(self):
        """Test formatting in bytes range"""
        result = human_readable_size(512)
        self.assertIn('512', result)
        self.assertIn('B', result)

    def test_kilobytes_range(self):
        """Test formatting in kilobytes range"""
        result = human_readable_size(2048)  # 2 KB
        self.assertIn('2', result)
        self.assertIn('K', result)

    def test_megabytes_range(self):
        """Test formatting in megabytes range"""
        result = human_readable_size(5 * 1024 * 1024)  # 5 MB
        self.assertIn('5', result)
        self.assertIn('M', result)

    def test_gigabytes_range(self):
        """Test formatting in gigabytes range"""
        result = human_readable_size(3 * 1024 * 1024 * 1024)  # 3 GB
        self.assertIn('3', result)
        self.assertIn('G', result)

    def test_terabytes_range(self):
        """Test formatting in terabytes range"""
        result = human_readable_size(2 * 1024 * 1024 * 1024 * 1024)  # 2 TB
        self.assertIn('2', result)
        self.assertIn('T', result)

    def test_zero_bytes(self):
        """Test zero bytes"""
        result = human_readable_size(0)
        self.assertIn('0', result)

    def test_fractional_sizes(self):
        """Test fractional sizes"""
        result = human_readable_size(int(1.5 * 1024))  # 1.5 KB
        self.assertIn('K', result)


if __name__ == '__main__':
    unittest.main()
```
